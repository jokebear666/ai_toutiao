"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3603],{169:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"daily/cs_FL/20251222-20251228","title":"20251222-20251228 (cs.FL)","description":"2025-12-22","source":"@site/docs/daily/cs_FL/20251222-20251228.md","sourceDirName":"daily/cs_FL","slug":"/daily/csfl/20251222-20251228","permalink":"/ai_toutiao/daily/csfl/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766487380000,"frontMatter":{"slug":"/daily/csfl/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"cs.FL","permalink":"/ai_toutiao/daily/csfl"},"next":{"title":"cs.GR","permalink":"/ai_toutiao/category/csgr"}}');var s=i(4848),r=i(8453);const a={slug:"/daily/csfl/20251222-20251228"},o="20251222-20251228 (cs.FL)",l={},c=[{value:"2025-12-22",id:"2025-12-22",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"20251222-20251228-csfl",children:"20251222-20251228 (cs.FL)"})}),"\n",(0,s.jsx)(t.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"tags:"})," [ai], [reinforcement learning], [timed reward machines, tabular Q-learning, timed automata, counterfactual-imagining]"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"authors:"})," Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"institution:"})," Tata Institute of Fundamental Research, Max Planck Institute for Software Systems, University of Oxford"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"link:"})," ",(0,s.jsx)(t.a,{href:"https://arxiv.org/pdf/2512.17637",children:"https://arxiv.org/pdf/2512.17637"})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Simple LLM Summary:"})," This paper proposes timed reward machines (TRMs), an extension of reward machines that incorporates timing constraints into the reward specification for reinforcement learning. The authors develop model-free RL algorithms, specifically using tabular Q-learning integrated with abstractions of timed automata and counterfactual-imagining heuristics, to learn optimal policies. The experimental results show that their approach successfully learns policies that achieve high rewards while satisfying the specified timing constraints."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>a,x:()=>o});var n=i(6540);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);