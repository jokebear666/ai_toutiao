"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6288],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},68300:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_HC/20260105-20260111","title":"20260105-20260111 (cs.HC)","description":"2026-01-05","source":"@site/docs/daily/cs_HC/20260105-20260111.md","sourceDirName":"daily/cs_HC","slug":"/daily/cshc/20260105-20260111","permalink":"/ai_toutiao/daily/cshc/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cshc/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.HC)","permalink":"/ai_toutiao/daily/cshc/20251229-20260104"},"next":{"title":"cs.IR","permalink":"/ai_toutiao/category/csir"}}');var a=i(74848),t=i(28453);const r={slug:"/daily/cshc/20260105-20260111"},o="20260105-20260111 (cs.HC)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20260105-20260111-cshc",children:"20260105-20260111 (cs.HC)"})}),"\n",(0,a.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Augmented Reality Indoor Wayfinding in Hospital Environments An Empirical Study on Navigation Efficiency, User Experience, and Cognitive Load"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-computer interaction], [augmented reality, indoor navigation, cognitive load, NASA-TLX, spatial memory]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kai Liu, Michelle L. Aebersold, Mark Lindquist, Haoting Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00001",children:"https://arxiv.org/pdf/2601.00001"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted an empirical comparison demonstrating that an AR-based handheld navigation system significantly improves navigation efficiency (faster completion, fewer errors) and reduces user anxiety and cognitive workload compared to traditional paper maps in a hospital environment. 2. Identified a key trade-off, showing that while AR aids real-time performance, paper map users exhibited stronger long-term spatial memory and learning from sketch-based recall tasks. 3. Provided actionable design implications and strategies for developing adaptive, inclusive AR navigation tools that balance efficiency with spatial learning, specifically for complex healthcare settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a9a145b7058f86aae71f0dd0c3dff386c152551ef3abf79f2283cfd3290d2e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00a9a145b7058f86aae71f0dd0c3dff386c152551ef3abf79f2283cfd3290d2e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates the effectiveness of an augmented reality (AR) handheld navigation system versus paper maps for wayfinding in a complex hospital. Through a mixed-methods experiment with 32 participants, it was found that AR users navigated faster with fewer errors and lower anxiety and workload, but paper map users demonstrated better spatial memory retention. The results highlight a trade-off between real-time navigation efficiency and long-term spatial learning, offering design strategies for adaptive AR tools in healthcare."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Augmented Reality Indoor Wayfinding in Hospital Environments<br/>\u589e\u5f3a\u73b0\u5b9e\u533b\u9662\u5ba4\u5185\u5bfb\u8def] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[Problem: Cognitively demanding hospital navigation for unfamiliar users<br/>\u6838\u5fc3\u95ee\u9898: \u5bf9\u964c\u751f\u7528\u6237\u800c\u8a00\u533b\u9662\u5bfc\u822a\u8ba4\u77e5\u8d1f\u8377\u9ad8]\n    C[Method: Empirical study comparing AR handheld system vs. paper maps<br/>\u4e3b\u8981\u65b9\u6cd5: \u5bf9\u6bd4AR\u624b\u6301\u7cfb\u7edf\u4e0e\u7eb8\u8d28\u5730\u56fe\u7684\u5b9e\u8bc1\u7814\u7a76]\n    D[Results: AR faster, fewer errors, lower anxiety/workload; Paper maps better for spatial memory<br/>\u5173\u952e\u7ed3\u679c: AR\u66f4\u5feb\u3001\u9519\u8bef\u66f4\u5c11\u3001\u7126\u8651/\u8d1f\u8377\u66f4\u4f4e\uff1b\u7eb8\u8d28\u5730\u56fe\u7a7a\u95f4\u8bb0\u5fc6\u66f4\u5f3a]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [causal reasoning], [fuzzy cognitive maps, large-language-model agent, causal feedback, equilibrium limit cycles, agentic leash]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California, Florida International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00097",children:"https://arxiv.org/pdf/2601.00097"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel LLM agent designed to autonomously extract and construct causal feedback Fuzzy Cognitive Maps (FCMs) from raw text. 2. A three-step instruction-guided process for systematically extracting key concepts and causal edges to build the FCM dynamical system. 3. Demonstration that the LLM-generated FCMs converge to the same equilibrium dynamics as human-generated ones and that mixed FCMs from different LLMs can create new equilibria."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an LLM agent to autonomously extract causal feedback Fuzzy Cognitive Maps from text. The agent uses a three-step process to identify concepts and causal edges, forming a dynamical system. The generated FCMs matched human-generated equilibrium dynamics and mixing models from different LLMs produced new equilibria for better causal approximation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to autonomously extract causal structures from text?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Design an LLM agent with a three-step instruction process to build FCMs from text.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: LLM-generated FCMs match human equilibrium dynamics; mixed FCMs create new equilibria.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [hybrid agentic framework, hallucination tax, Human Imitator, stochastic reasoning, inventory optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yaqi Duan, Yichun Hu, Jiashuo Jiang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," New York University, Cornell University, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00121",children:"https://arxiv.org/pdf/2601.00121"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and quantifies the "hallucination tax" when using LLMs as end-to-end solvers for inventory control, highlighting their limitation in grounded stochastic reasoning. 2. Proposes a novel hybrid agentic framework that decouples semantic reasoning (handled by LLM) from mathematical calculation (handled by rigorous algorithms) to create an intelligent interface for optimization. 3. Introduces the "Human Imitator," a fine-tuned digital twin of a boundedly rational manager, to enable scalable and reproducible stress-testing of interactive systems against ambiguous real-world dialogue.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5522adbd58df6d202685913f2a038b91f8e5b3730f51d9964297ad57db17174e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5522adbd58df6d202685913f2a038b91f8e5b3730f51d9964297ad57db17174e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of using LLMs for inventory management by showing that direct application incurs a "hallucination tax" due to poor stochastic reasoning. To solve this, the authors propose a hybrid agentic framework where the LLM acts as a natural-language interface that calls rigorous optimization algorithms. The framework reduces inventory costs by 32.1% compared to an LLM-only baseline, demonstrating that LLMs are best used as interfaces to make expert methods accessible, not as replacements for them.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Ask, Clarify, Optimize<br>Human-LLM Agent Collaboration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLMs as end-to-end solvers incur "hallucination tax"<br>LLMs\u65e0\u6cd5\u8fdb\u884c\u53ef\u9760\u7684\u968f\u673a\u63a8\u7406]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Hybrid Agentic Framework<br>LLM\u4f5c\u4e3a\u63a5\u53e3\uff0c\u8c03\u7528\u4e25\u683c\u7b97\u6cd5<br>\u8bed\u4e49\u63a8\u7406\u4e0e\u6570\u5b66\u8ba1\u7b97\u89e3\u8026]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Cost reduced by 32.1% vs baseline<br>LLMs are interfaces, not replacements<br>\u74f6\u9888\u662f\u8ba1\u7b97\u800c\u975e\u4fe1\u606f]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [ai safety & alignment], [synthetic reality, epistemic security, provenance, trust erosion, generative AI harms]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Emilio Ferrara"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California (USC)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00306",children:"https://arxiv.org/pdf/2601.00306"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Formalizes the concept of "synthetic reality" as a layered socio-technical stack comprising content, identity, interaction, and institutions. 2. Expands a taxonomy of Generative AI harms and articulates the qualitative shifts it introduces, such as cost collapse and provenance gaps. 3. Proposes a complementary mitigation stack and a research agenda focused on measuring epistemic security, culminating in the articulation of the "Generative AI Paradox".']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b42a2aa0b289b69471dcaa3b08e187b14ded4070e26848bb4917fcc12b2427_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b42a2aa0b289b69471dcaa3b08e187b14ded4070e26848bb4917fcc12b2427_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper argues that the primary risk of Generative AI is not just creating fake content, but the systemic erosion of shared truth and verification practices as it enables the easy creation of synthetic content, identities, and interactions. The authors formalize this as "synthetic reality," analyze its risks, and propose a multi-layered mitigation approach. They conclude with the "Generative AI Paradox": the potential for societies to rationally discount all digital evidence as synthetic media becomes ubiquitous.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Generative AI Paradox<br>\u751f\u6210\u5f0fAI\u6096\u8bba] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GenAI erodes shared truth & verification<br>GenAI\u4fb5\u8680\u5171\u540c\u8ba4\u77e5\u4e0e\u9a8c\u8bc1]\n    C --\x3e C1[Formalize synthetic reality stack & harms taxonomy<br>\u5f62\u5f0f\u5316\u5408\u6210\u73b0\u5b9e\u6808\u4e0e\u5371\u5bb3\u5206\u7c7b]\n    C --\x3e C2[Propose mitigation stack & research agenda<br>\u63d0\u51fa\u7f13\u89e3\u6808\u4e0e\u7814\u7a76\u8bae\u7a0b]\n    D --\x3e D1[Generative AI Paradox: discount digital evidence<br>\u751f\u6210\u5f0fAI\u6096\u8bba\uff1a\u8d28\u7591\u6570\u5b57\u8bc1\u636e]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI) / Computer-Supported Cooperative Work (CSCW)], [Mixed Reality, Digital Audio Workstation, Collaborative Looping, Musical Metaverse, Speculative Design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Torin Hopkins, Shih-Yu Ma, Suibi Che-Chuan Weng, Ming-Yuan Pai, Ellen Yi-Luen Do, Luca Turchet"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, University of Trento, SolJAMM Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00326",children:"https://arxiv.org/pdf/2601.00326"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposed and developed MR-DAW, a novel Mixed Reality system enabling multiple remote users to control a single, shared DAW instance while moving freely in their physical space. 2. Introduced a hands-free, collaborative interaction paradigm using physical foot pedals for remote, real-time looping control within the shared virtual session. 3. Conducted a qualitative study with 20 musicians to analyze current DAW practices, evaluate the MR-DAW system\'s usability, and provide a speculative outlook on the future of collaborative music-making in the "Musical Metaverse".']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates using Mixed Reality (MR) to overcome the limitations of traditional Digital Audio Workstations (DAWs), which tether musicians to a desk and hinder remote collaboration. The authors propose MR-DAW, a networked MR system that allows geographically dispersed musicians to control a shared DAW and use foot pedals for collaborative looping. The study with 20 musicians highlights MR's potential for unencumbered musical interaction and provides a speculative vision for future remote collaborative DAWs in the Musical Metaverse."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[DAWs\u675f\u7f1a\u97f3\u4e50\u5bb6\u5de5\u4f5c\u6d41/DAWs encumber musician workflow]\n    Problem --\x3e P2[\u8fdc\u7a0b\u534f\u4f5c\u56f0\u96be/Remote collaboration is challenging]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f00\u53d1MR-DAW\u8bbe\u8ba1\u63a2\u9488/Developed MR-DAW design probe]\n    Method --\x3e M2[\u4f7f\u7528\u811a\u8e0f\u677f\u8fdb\u884c\u534f\u4f5c\u5faa\u73af/Used foot pedal for collaborative looping]\n    Method --\x3e M3[\u5b9a\u6027\u7814\u7a76\u4e0e\u7cfb\u7edf\u8bc4\u4f30/Qualitative study & system evaluation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[MR\u652f\u6301\u65e0\u675f\u7f1a\u4ea4\u4e92/MR affords unencumbered interaction]\n    Results --\x3e R2[\u5c55\u671b\u97f3\u4e50\u5143\u5b87\u5b99\u672a\u6765/Speculative outlook on Musical Metaverse]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Effects of Limited Field of View on Musical Collaboration Experience with Avatars in Extended Reality"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-computer interaction], [extended reality, field of view, musical collaboration, co-presence, gesture recognition]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Suibi Che-Chuan Weng, Torin Hopkins, Shih-Yu Ma, Amy Banic, Ellen Yi-Luen Do"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, University of Wyoming, SolJAMM Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00333",children:"https://arxiv.org/pdf/2601.00333"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Conducted a comparative study investigating the specific impact of a limited field of view (FOV) on key aspects of musical collaboration in XR, such as co-presence and reaction time. 2. Proposed and evaluated a novel notification system ("Mini Musicians") designed to mitigate the negative effects of a limited FOV in AR-based musical collaboration. 3. Provided empirical evidence that while limited FOV degrades the collaborative experience, interface interventions like notifications can improve performance metrics like reaction time.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b86b1bc6e18987027d6a96d6343c561f4f8304599001862d66c5108a3cb6f3ad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b86b1bc6e18987027d6a96d6343c561f4f8304599001862d66c5108a3cb6f3ad_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies how the limited field of view (FOV) in XR head-mounted displays affects musical collaboration. It compares an unrestricted holographic setup (HoloJam) with a limited-FOV AR setup and tests a notification system (Mini Musicians) to improve awareness. The results show that limited FOV reduces co-presence and enjoyment, but notification systems can help improve reaction times."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Effects of Limited FOV on Musical Collaboration in XR<br/>XR\u4e2d\u6709\u9650\u89c6\u573a\u5bf9\u97f3\u4e50\u534f\u4f5c\u7684\u5f71\u54cd] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Limited FOV in XR disrupts visual cues for musicians<br/>XR\u4e2d\u7684\u6709\u9650\u89c6\u573a\u5e72\u6270\u4e86\u97f3\u4e50\u5bb6\u7684\u89c6\u89c9\u7ebf\u7d22]\n    C --\x3e C1[Compared HoloJam (unrestricted FOV) vs. AR glasses (52\xb0 FOV)<br/>\u6bd4\u8f83HoloJam(\u65e0\u9650\u5236\u89c6\u573a)\u4e0eAR\u773c\u955c(52\xb0\u89c6\u573a)]\n    C --\x3e C2[Tested AR notification system "Mini Musicians"<br/>\u6d4b\u8bd5AR\u901a\u77e5\u7cfb\u7edf"Mini Musicians"]\n    D --\x3e D1[HoloJam: higher co-presence & enjoyment<br/>HoloJam: \u66f4\u9ad8\u7684\u5171\u5728\u611f\u4e0e\u6109\u60a6\u5ea6]\n    D --\x3e D2[Mini Musicians: reduced reaction time<br/>Mini Musicians: \u964d\u4f4e\u4e86\u53cd\u5e94\u65f6\u95f4]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-computer interaction (HCI) in healthcare], [speech-to-text (STT), socio-technical framework, clinical documentation, governance, patient autonomy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nelly Elsayed"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Cincinnati"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00382",children:"https://arxiv.org/pdf/2601.00382"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Synthesis of interdisciplinary evidence to identify key socio-technical risks of clinical STT systems. 2. Development of a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. 3. Provision of a structured implementation roadmap for responsible and equitable STT adoption in healthcare."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/627d7cc2565b3e0e4364bcbd09fce3b63a8dceb715f9b162a42b7c6ff653b67c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/627d7cc2565b3e0e4364bcbd09fce3b63a8dceb715f9b162a42b7c6ff653b67c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the socio-technical risks of AI-driven speech-to-text systems in clinical settings, such as transparency and reliability issues. It proposes a governance framework and implementation roadmap by synthesizing evidence from technical, ethical, and workflow studies. The main conclusion is that safe integration requires addressing the interdependence of model performance, clinician oversight, patient rights, and institutional governance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unseen Risks of Clinical Speech-to-Text Systems<br/>\u4e34\u5e8a\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\u7684\u672a\u77e5\u98ce\u9669] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI STT \u90e8\u7f72\u5feb\u4e8e\u98ce\u9669\u7406\u89e3<br/>AI STT Deployment Outpaces Risk Understanding]\n    C --\x3e C1[\u7efc\u5408\u591a\u5b66\u79d1\u8bc1\u636e<br/>Synthesize Interdisciplinary Evidence]\n    D --\x3e D1[\u63d0\u51fa\u6cbb\u7406\u6846\u67b6\u4e0e\u8def\u7ebf\u56fe<br/>Propose Governance Framework & Roadmap]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [agentic AI, distributed agents, human-AI co-creation, progressive ideation, meta-cognitive workflow]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sankar B, Srinidhi Ranjini Girish, Aadya Bharti, Dibakar Sen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Science (IISc)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00475",children:"https://arxiv.org/pdf/2601.00475"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MIDAS, a novel framework that replaces single-AI systems with a distributed team of specialized AI agents for ideation. 2. Emulates a human meta-cognitive workflow to progressively refine ideas and assess them for both global and local novelty. 3. Establishes a new paradigm for human-AI co-creation, elevating the human from a passive filter to an active collaborative partner."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dee9a17774f8f69cd3f3a19a0507461cd7a05a21b3f1dec4db23b9a0a1c9bfa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dee9a17774f8f69cd3f3a19a0507461cd7a05a21b3f1dec4db23b9a0a1c9bfa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of AI systems generating semantically clustered ideas that hinder novel ideation in engineering design. It proposes the MIDAS framework, which uses a distributed team of specialized AI agents to progressively refine and assess ideas for novelty. This approach enables true human-AI co-creation, making the human designer an active partner rather than a passive filter."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Progressive Ideation using an Agentic AI Framework<br>\u6e10\u8fdb\u5f0f\u6784\u601d\u7684\u667a\u80fd\u4f53AI\u6846\u67b6] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u751f\u6210\u60f3\u6cd5\u8bed\u4e49\u805a\u7c7b<br>AI-generated ideas are semantically clustered]\n    B --\x3e B2[\u65b0\u624b\u8bbe\u8ba1\u5e08\u6784\u601d\u56f0\u96be<br>Ideation is challenging for novice designers]\n    C --\x3e C1[\u63d0\u51faMIDAS\u6846\u67b6<br>Propose MIDAS framework]\n    C1 --\x3e C2[\u5206\u5e03\u5f0f\u4e13\u4e1aAI\u667a\u80fd\u4f53\u56e2\u961f<br>Distributed team of specialized AI agents]\n    C2 --\x3e C3[\u6a21\u62df\u5143\u8ba4\u77e5\u5de5\u4f5c\u6d41<br>Emulate meta-cognitive workflow]\n    D --\x3e D1[\u6e10\u8fdb\u5f0f\u63d0\u70bc\u4e0e\u8bc4\u4f30\u60f3\u6cd5<br>Progressively refines and assesses ideas]\n    D --\x3e D2[\u5b9e\u73b0\u4eba-AI\u534f\u540c\u521b\u9020<br>Enables true human-AI co-creation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [conversational ai for mental health], [cognitive reappraisal, single-session intervention (SSI), GPT-4o, RoBERTa classifiers, thematic analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ananya Bhattacharjee, Jina Suh, Mohit Chandra, Javier Hernandez"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University, University of Toronto, Microsoft Research, Georgia Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00570",children:"https://arxiv.org/pdf/2601.00570"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed and evaluated an LLM-based chatbot for delivering a structured cognitive reappraisal intervention for workplace stress, demonstrating its feasibility. 2. Employed a multi-method analysis combining quantitative self-reports, automated sentiment/stress trajectory analysis, and qualitative thematic analysis to assess outcomes and user experience. 3. Identified key design tensions in LLM-based DMH tools, such as scriptedness vs. flexibility, interaction length, and reactions to AI-driven empathy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93f7db6a0dc26c21ee37cc461c2756b2a35d6c50c897f26162c9abb735305bf9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93f7db6a0dc26c21ee37cc461c2756b2a35d6c50c897f26162c9abb735305bf9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study developed a GPT-4o-based chatbot to deliver a single-session cognitive reappraisal intervention for workplace stress. The feasibility study with 100 employees showed significant short-term reductions in perceived stress intensity and improvements in stress mindset, while also revealing user-perceived design tensions. The findings highlight the potential and constraints of using conversational LLMs in structured digital mental health tools."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: DMH tools struggle with rigid scripts for flexible stress reappraisal]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: GPT-4o chatbot delivers structured SSI; Multi-method analysis (pre-post tests, RoBERTa/LLM classifiers, thematic analysis)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Stress intensity \u2193, mindset \u2191; Sentiment/stress decline during chat; Tensions around scriptedness & AI empathy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The AI Invisibility Effect: Understanding Human-AI Interaction When Users Don't Recognize Artificial Intelligence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [human-ai interaction], [sentiment classification, topic modeling, concern-benefit categorization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Obada Kraishan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Texas Tech University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00579",children:"https://arxiv.org/pdf/2601.00579"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identified the "AI invisibility effect": a major disconnect where only 11.9% of reviews mention AI despite 47.4% of apps featuring it, suggesting AI often operates below user awareness. 2. Revealed a hidden pattern where AI\'s negative impact on app ratings reverses when users explicitly recognize it, indicating recognition, not mere presence, drives negative evaluations. 3. Quantified category-specific effects and user concerns, showing privacy as the dominant concern (34.8%) and efficiency as the primary benefit (42.3%), with effects varying from positive for Assistant apps to negative for Entertainment.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c4b615742aa54b3187cdf8aa78760299e2e2352b5db6bec86f259ad28dfcb4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c4b615742aa54b3187cdf8aa78760299e2e2352b5db6bec86f259ad28dfcb4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how users perceive AI in mobile apps through a large-scale analysis of 1.5 million app reviews. Using sentiment classification, topic modeling, and categorization, it finds that AI features are often unrecognized by users, and it is this explicit recognition, not the AI's presence, that correlates with negative evaluations, challenging assumptions about technology acceptance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["The AI Invisibility Effect"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: User perception of AI in mobile apps is poorly understood."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Large-scale analysis of app reviews using sentiment classification, topic modeling, and categorization."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: AI often invisible; recognition drives negative ratings; privacy top concern, efficiency top benefit."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Evaluating Web Accessibility and Usability in Bangladesh: A Comparative Analysis of Government and Non-Government Websites"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [web accessibility evaluation], [WCAG 2.2, automated accessibility assessment, user survey, digital inclusion, usability evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sanjida Islam Era, Ishika Tarin Ime, A. B. M. Alim Al Islam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bangladesh University of Engineering and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00592",children:"https://arxiv.org/pdf/2601.00592"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a large-scale comparative evaluation of 212 Bangladeshi government and non-government websites for accessibility and usability. 2. Combined automated WCAG 2.2 compliance testing with user-reported feedback from 103 participants for a holistic assessment. 3. Identified and analyzed specific disparities and persistent barriers (e.g., navigation complexity, accessibility feature adoption) between the two website categories."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1dcc467092506358305a3271b80be39333c5a6f1a26c2602953cbd607b99e44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1dcc467092506358305a3271b80be39333c5a6f1a26c2602953cbd607b99e44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study evaluates the accessibility and usability of Bangladeshi government and non-government websites by combining automated WCAG 2.2 assessments with a user survey. The results show significant disparities, with non-government sites generally performing better on usability but both categories having inconsistent accessibility support. The findings highlight the need for regular audits and user-centered design to improve digital inclusivity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Evaluating Web Accessibility and Usability in Bangladesh<br>\u8bc4\u4f30\u5b5f\u52a0\u62c9\u56fd\u7684\u7f51\u7edc\u53ef\u8bbf\u95ee\u6027\u4e0e\u53ef\u7528\u6027] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Critical websites have accessibility & usability barriers<br>\u5173\u952e\u7f51\u7ad9\u5b58\u5728\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u7528\u6027\u969c\u788d]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Automated WCAG 2.2 tests + User survey (103 users)<br>\u81ea\u52a8\u5316WCAG 2.2\u6d4b\u8bd5 + \u7528\u6237\u8c03\u67e5(103\u4eba)]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Disparities between gov/non-gov sites, inconsistent accessibility<br>\u653f\u5e9c\u4e0e\u975e\u653f\u5e9c\u7f51\u7ad9\u5b58\u5728\u5dee\u5f02\uff0c\u53ef\u8bbf\u95ee\u6027\u652f\u6301\u4e0d\u4e00\u81f4]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST, NTU Singapore, DeepAuto.ai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00664",children:"https://arxiv.org/pdf/2601.00664"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://taekyungki.github.io/AvatarForcing",children:"https://taekyungki.github.io/AvatarForcing"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u771f\u6b63\u4e92\u52a8/Lacks truly interactive communication]\n    Problem --\x3e P2[\u5355\u5411\u53cd\u5e94\u7f3a\u4e4f\u60c5\u611f/One-way responses lack emotional engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u9a71\u52a8\u6846\u67b6/Diffusion forcing framework]\n    Method --\x3e M2[\u65e0\u6807\u7b7e\u76f4\u63a5\u504f\u597d\u4f18\u5316/Label-free direct preference optimization]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u4ea4\u4e92/Low-latency real-time interaction (~500ms)]\n    Results --\x3e R2[6.8\u500d\u52a0\u901f/6.8x speedup]\n    Results --\x3e R3[80%\u7528\u6237\u504f\u597d/Over 80% user preference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Wave2Word: A Multimodal Transformer Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [EEG-text alignment, contrastive learning, transformer encoders, time-frequency representation, neurocritical care]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Kharagpur"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00670",children:"https://arxiv.org/pdf/2601.00670"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a multimodal framework that aligns EEG signal embeddings with structured clinical text descriptions using a contrastive objective. 2. Introduced dual transformer-based encoders for complementary temporal and frequency-centric modeling, fused via an adaptive gating mechanism. 3. Introduced an EEG-conditioned text reconstruction loss as a representation-level constraint alongside classification, demonstrating that classification accuracy alone is insufficient for evaluating clinically meaningful EEG representations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/234d66f4587d5e7ce5f9b2130c03b8b16a9d880449f62b6335ed7f836b7e49ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/234d66f4587d5e7ce5f9b2130c03b8b16a9d880449f62b6335ed7f836b7e49ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Wave2Word, a multimodal transformer framework that learns EEG representations by aligning them with clinical text descriptions using contrastive learning and a text reconstruction task. It achieves high classification accuracy (0.9797) but crucially shows that removing the alignment component drastically harms cross-modal retrieval performance, proving that standard accuracy metrics do not reflect representation quality for clinical EEG modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Wave2Word: A Multimodal Transformer Framework] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Weak correspondence between learned EEG representations and clinical interpretation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Dual transformer encoders + EEG-text contrastive alignment + EEG-conditioned text reconstruction]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High classification accuracy (0.9797); Ablation shows contrastive alignment is critical for cross-modal retrieval (Recall@10 drops from 0.3390 to 0.0045)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Calling for Backup: How Children Navigate Successive Robot Communication Failures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-robot interaction], [successive robot error, child-robot interaction, error recovery, performance error, social error]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Maria Teresa Parreira, Isabel Neto, Filipa Rocha, Wendy Ju"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Cornell University, Universidade de Lisboa"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00754",children:"https://arxiv.org/pdf/2601.00754"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Reproduced the successive robot failure paradigm with children (ages 8-10) to explore their unique responses to repeated conversational errors. 2. Identified key behavioral differences between children and adults, such as children's increased disengagement (e.g., ignoring the robot, seeking adult help) and more flexible conversational expectations. 3. Provided empirical findings to inform the design of more effective and developmentally appropriate human-robot interaction systems for young users."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6516606de03da03b1c7297f4e0a7fcb61612ec1bea932c4290e5d1ba746ac52a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6516606de03da03b1c7297f4e0a7fcb61612ec1bea932c4290e5d1ba746ac52a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how children respond to repeated robot communication failures by reproducing an adult-focused error paradigm with child participants. The method involved children interacting with a robot that failed to understand their prompts three times, with their behavioral responses recorded and analyzed. The main conclusion is that while children share some error-response strategies with adults, they exhibit more disengagement behaviors and maintain a stable perception of the robot, suggesting different interaction needs that should guide robot design for young users."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Calling for Backup: How Children Navigate Successive Robot Communication Failures] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u513f\u7ae5\u5bf9\u8fde\u7eed\u673a\u5668\u4eba\u9519\u8bef\u7684\u53cd\u5e94/Children's response to successive robot errors]\n    C --\x3e C1[\u590d\u5236\u6210\u4eba\u7814\u7a76\u8303\u5f0f\uff0c\u4e0e\u513f\u7ae5\u8fdb\u884c\u673a\u5668\u4eba\u4ea4\u4e92/Reproduce adult study paradigm, child-robot interaction]\n    C --\x3e C2[\u5206\u6790\u884c\u4e3a\u89c6\u9891\u8bb0\u5f55/Analyze behavioral video recordings]\n    D --\x3e D1[\u513f\u7ae5\u4e0e\u6210\u4eba\u53cd\u5e94\u7684\u5f02\u540c/Similarities and differences vs. adult responses]\n    D --\x3e D2[\u66f4\u591a\u8131\u79bb\u884c\u4e3a\uff0c\u5982\u5bfb\u6c42\u6210\u4eba\u5e2e\u52a9/More disengagement, e.g., seeking adult help]\n    D --\x3e D3[\u5bf9\u673a\u5668\u4eba\u7684\u611f\u77e5\u672a\u53d7\u5f71\u54cd/Robot perception unaffected]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Effect of Transparency on Students' Perceptions of AI Graders"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [automated short answer grading], [autograding, transparency, student perceptions, natural language processing, educational technology]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Joslyn Orgill, Andra Rice, Max Fowler, Seth Poulsen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Utah State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00765",children:"https://arxiv.org/pdf/2601.00765"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Investigated the effect of transparency on student attitudes towards AI-based autograders. 2. Found that transparency specifically increased perceptions of accuracy and willingness to discuss the autograder. 3. Provided evidence that baseline trust levels may moderate the impact of transparency interventions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbaf43a4643b4004e18e00c3c13ff6e5941b23fdbb39e479514e725d1f9ab0bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbaf43a4643b4004e18e00c3c13ff6e5941b23fdbb39e479514e725d1f9ab0bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether making AI autograders more transparent improves students' perceptions of them. The study found that transparency increased students' views on the autograder's accuracy and their willingness to comment on it, but did not improve other attitudes like willingness to be graded by it on a test, possibly due to high baseline trust."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Effect of Transparency on Students' Perceptions of AI Graders] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b66\u751f\u4e0d\u603b\u662f\u559c\u6b22\u6216\u4fe1\u4efb\u81ea\u52a8\u8bc4\u5206\u5668/Students do not always like or trust autograders]\n    C --\x3e C1[\u6d4b\u8bd5\u900f\u660e\u5ea6\u5bf9\u6001\u5ea6\u7684\u5f71\u54cd/Test effect of transparency on attitudes]\n    D --\x3e D1[\u900f\u660e\u5ea6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u611f\u77e5\u548c\u8ba8\u8bba\u610f\u613f/Transparency increased perceived accuracy & willingness to discuss]\n    D --\x3e D2[\u672a\u6539\u5584\u5176\u4ed6\u6001\u5ea6\u5982\u6d4b\u8bd5\u8bc4\u5206\u610f\u613f/Did not improve other attitudes e.g., willingness for test grading]\n    D --\x3e D3[\u9ad8\u521d\u59cb\u4fe1\u4efb\u53ef\u80fd\u524a\u5f31\u5f71\u54cd/High initial trust may weaken impact]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);