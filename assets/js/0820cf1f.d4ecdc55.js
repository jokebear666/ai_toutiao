"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2411],{3509:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"daily/cs_PF/20251229-20260104","title":"20251229-20260104 (cs.PF)","description":"2025-12-29","source":"@site/docs/daily/cs_PF/20251229-20260104.md","sourceDirName":"daily/cs_PF","slug":"/daily/cspf/20251229-20260104","permalink":"/ai_toutiao/daily/cspf/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767009989000,"frontMatter":{"slug":"/daily/cspf/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.PF)","permalink":"/ai_toutiao/daily/cspf/20251222-20251228"},"next":{"title":"cs.PL","permalink":"/ai_toutiao/category/cspl"}}');var r=i(4848),t=i(8453);const a={slug:"/daily/cspf/20251229-20260104"},o="20251229-20260104 (cs.PF)",d={},l=[{value:"2025-12-29",id:"2025-12-29",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251229-20260104-cspf",children:"20251229-20260104 (cs.PF)"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Luki\u0107, Franck Cappello"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21433",children:"https://arxiv.org/pdf/2512.21433"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DeepCQ: \u901a\u7528\u6df1\u5ea6\u4ee3\u7406\u6846\u67b6\u7528\u4e8e\u6709\u635f\u538b\u7f29\u8d28\u91cf\u9884\u6d4b] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bc4\u4f30\u538b\u7f29\u540e\u6570\u636e\u8d28\u91cf\u7684\u8ba1\u7b97\u6210\u672c\u9ad8/Expensive to assess post-compression data quality]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u8bbe\u8ba1: \u7279\u5f81\u63d0\u53d6 + \u8f7b\u91cf\u9884\u6d4b/Two-stage design: feature extraction + lightweight prediction]\n    C --\x3e C2[\u4e13\u5bb6\u6df7\u5408\u8bbe\u8ba1\u5904\u7406\u65f6\u53d8\u6570\u636e/Mixture-of-experts for time-evolving data]\n    D --\x3e D1[\u9884\u6d4b\u8bef\u5dee\u666e\u904d\u4f4e\u4e8e10%/Prediction errors generally under 10%]\n    D --\x3e D2[\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Significantly outperforms existing methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Uppsala University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22066",children:"https://arxiv.org/pdf/2512.22066"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u63a8\u7406\u80fd\u8017\u9ad8\uff0cPrefill\u4e0eDecode\u9636\u6bb5\u74f6\u9888\u4e0d\u540c"] --\x3e Problem_Sub1["SRAM\u5927\u5c0f\u4e0e\u9891\u7387\u5982\u4f55\u5f71\u54cd\u80fd\u6548\uff1f"]\n    Problem --\x3e Problem_Sub2["\u5185\u5b58\u5e26\u5bbd\u5982\u4f55\u9650\u5236\u6027\u80fd\uff1f"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u7ed3\u5408OpenRAM, LLMCompass, ScaleSIM\u7684\u6a21\u62df\u65b9\u6cd5"] --\x3e Method_Sub1["\u80fd\u8017\u5efa\u6a21/Energy Modeling"]\n    Method --\x3e Method_Sub2["\u5ef6\u8fdf\u6a21\u62df/Latency Simulation"]\n    Method --\x3e Method_Sub3["\u64cd\u4f5c\u5f3a\u5ea6\u5206\u6790/Operational Intensity"]\n    Results["\u5173\u952e\u7ed3\u679c/Results"] --\x3e Results_Sub1["\u603b\u80fd\u8017\u4e3b\u8981\u7531SRAM\u5927\u5c0f\u51b3\u5b9a<br>\u5927\u7f13\u5b58\u589e\u52a0\u9759\u6001\u80fd\u8017"]\n    Results --\x3e Results_Sub2["\u9ad8\u9891\u53ef\u964d\u4f4e\u603b\u80fd\u8017<br>\uff08\u51cf\u5c11\u9759\u6001\u80fd\u8017\uff09"]\n    Results --\x3e Results_Sub3["\u6700\u4f18\u914d\u7f6e\uff1a\u9ad8\u9891(1200-1400MHz) + \u5c0f\u7f13\u5b58(32-64KB)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);