"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5159],{8453:(i,e,n)=>{n.d(e,{R:()=>o,x:()=>a});var s=n(6540);const r={},t=s.createContext(r);function o(i){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function a(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(r):i.components||r:o(i.components),s.createElement(t.Provider,{value:e},i.children)}},8954:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_MM/20251215-20251221","title":"20251215-20251221 (cs.MM)","description":"2025-12-18","source":"@site/docs/daily/cs_MM/20251215-20251221.md","sourceDirName":"daily/cs_MM","slug":"/daily/cs_MM/20251215-20251221","permalink":"/ai_toutiao/daily/cs_MM/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766042497000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.MM","permalink":"/ai_toutiao/category/csmm"},"next":{"title":"cs.NE","permalink":"/ai_toutiao/category/csne"}}');var r=n(4848),t=n(8453);const o={},a="20251215-20251221 (cs.MM)",l={},c=[{value:"2025-12-18",id:"2025-12-18",level:2}];function d(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...i.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"20251215-20251221-csmm",children:"20251215-20251221 (cs.MM)"})}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Snap Inc."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14938",children:"https://arxiv.org/pdf/2512.14938"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] A Preprocessing Framework for Video Machine Vision under Compression"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [video compression, neural preprocessor, differentiable virtual codec, rate-accuracy optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Peking University, Bytedance"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15331",children:"https://arxiv.org/pdf/2512.15331"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mikel Williams-Lekuona, Georgina Cosma"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Loughborough University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15372",children:"https://arxiv.org/pdf/2512.15372"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Vision Transformers (ViT), SegFormer, attention mechanisms, anomaly scoring, hybrid framework]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Opeyemi Bamigbade, Mark Scanlon, John Sheppard"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," South East Technological University, University College Dublin"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15512",children:"https://arxiv.org/pdf/2512.15512"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Generative Preprocessing for Image Compression with Pre-trained Diffusion Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion inference], [Consistent Score Identity Distillation (CiD), Rate-Perception (R-P) optimization, Stable Diffusion 2.1, parameter-efficient fine-tuning, differentiable codec surrogate]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Bytedance Inc."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15270",children:"https://arxiv.org/pdf/2512.15270"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(i={}){const{wrapper:e}={...(0,t.R)(),...i.components};return e?(0,r.jsx)(e,{...i,children:(0,r.jsx)(d,{...i})}):d(i)}}}]);