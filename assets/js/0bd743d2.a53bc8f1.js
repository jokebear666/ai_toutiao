"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5159],{8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var s=n(6540);const r={},t=s.createContext(r);function a(e){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:i},e.children)}},8954:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_MM/20251215-20251221","title":"20251215-20251221 (cs.MM)","description":"2025-12-18","source":"@site/docs/daily/cs_MM/20251215-20251221.md","sourceDirName":"daily/cs_MM","slug":"/daily/cs_MM/20251215-20251221","permalink":"/ai_toutiao/daily/cs_MM/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766141312000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.MM","permalink":"/ai_toutiao/daily/csmm"},"next":{"title":"cs.NE","permalink":"/ai_toutiao/daily/csne"}}');var r=n(4848),t=n(8453);const a={},o="20251215-20251221 (cs.MM)",l={},d=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function c(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"20251215-20251221-csmm",children:"20251215-20251221 (cs.MM)"})}),"\n",(0,r.jsx)(i.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," The Chinese University of Hong Kong, Snap Inc."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.14938",children:"https://arxiv.org/pdf/2512.14938"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251218] A Preprocessing Framework for Video Machine Vision under Compression"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [video compression, neural preprocessor, differentiable virtual codec, rate-accuracy optimization]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Peking University, Bytedance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15331",children:"https://arxiv.org/pdf/2512.15331"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Mikel Williams-Lekuona, Georgina Cosma"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Loughborough University"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15372",children:"https://arxiv.org/pdf/2512.15372"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251218] VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Vision Transformers (ViT), SegFormer, attention mechanisms, anomaly scoring, hybrid framework]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Opeyemi Bamigbade, Mark Scanlon, John Sheppard"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," South East Technological University, University College Dublin"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15512",children:"https://arxiv.org/pdf/2512.15512"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251218] Generative Preprocessing for Image Compression with Pre-trained Diffusion Models"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [diffusion inference], [Consistent Score Identity Distillation (CiD), Rate-Perception (R-P) optimization, Stable Diffusion 2.1, parameter-efficient fine-tuning, differentiable codec surrogate]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Bytedance Inc."]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15270",children:"https://arxiv.org/pdf/2512.15270"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [Finite Exponential Continuous State Machine (FECSM), Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA), Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU), HDBSCAN, min-max normalization, User Interface Change Prediction Index (UICPI)]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Shrinivass Arunachalam Balasubramanian"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Independent Researcher"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15775",children:"https://arxiv.org/pdf/2512.15775"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a dynamic web UI optimization method that uses a Finite Exponential Continuous State Machine for cross-device responsiveness assessment and a novel Quokka Nonlinear Difference Swarm Optimization Algorithm for design optimization. The core technique involves classifying user experience changes with a Bidirectional Gated Luong and Mish Recurrent Unit model. The main conclusion is that this integrated approach achieves an average fitness of 98.5632% for optimal UI design by incorporating cross-responsiveness assessment and user behavior patterns."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [point cloud super-resolution, attribute-based encryption, downsampling, upscaling]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," University of Massachusetts Amherst, Tennessee Technological University"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15823",children:"https://arxiv.org/pdf/2512.15823"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a system to reduce latency in AR/VR streaming by downsampling and partially encrypting point cloud content at the server, then using a machine learning-based super-resolution model to reconstruct it at the client. The evaluation shows this approach effectively reduces bandwidth and encryption overhead while accurately reconstructing the original point clouds with minimal error."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal training], [self-supervised learning, vision-language alignment, I-JEPA, JARVIS, masked predictive loss, frozen vision foundation models]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," University of Modena and Reggio Emilia, AMD Silo AI"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15885",children:"https://arxiv.org/pdf/2512.15885"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] A Tri-Dynamic Preprocessing Framework for UGC Video Compression"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [Tri-Dynamic Preprocessing, adaptive factor, adaptive quantization level, adaptive lambda tradeoff, video compression, UGC, deep preprocessing]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Peking University, Bytedance"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16101",children:"https://arxiv.org/pdf/2512.16101"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a Tri-Dynamic Preprocessing (TDP) framework for UGC video compression, which adaptively adjusts preprocessing intensity, quantization level, and the rate-distortion tradeoff. This method addresses the high variability of UGC videos, where traditional deep preprocessing fails. Experiments show the framework achieves exceptional performance on large-scale test sets."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [AI forensic agents, uncertainty-aware assessments, detector orchestration, multimedia forensics, authenticity verification]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Giulia Boato, Andrea Montibeller, Edward Delp, Luisa Verdoliva, Daniele Miorandi"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Truebees, University of Trento, Purdue University, University of Naples Federico II"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16614",children:"https://arxiv.org/pdf/2512.16614"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a framework for AI forensic agents that autonomously orchestrate multiple forensic detectors to verify the authenticity of multimedia content. It argues that a holistic, uncertainty-calibrated approach is necessary to address the challenges posed by generative AI, moving beyond isolated, single-purpose detectors. The main conclusion is that such explainable, uncertainty-aware agents can improve the trustworthiness and interpretability of the forensic verification process."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [cross-layer knowledge fusion MoE, VLLM, world-knowledge representation, token extraction, layer-wise fusion]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Northeastern University, LinkedIn, University of Wisconsin\u2013Madison"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16891",children:"https://arxiv.org/pdf/2512.16891"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);