"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7065],{27733:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/cs_CC/20251229-20260104","title":"20251229-20260104 (cs.CC)","description":"2025-12-29","source":"@site/docs/daily/cs_CC/20251229-20260104.md","sourceDirName":"daily/cs_CC","slug":"/daily/cscc/20251229-20260104","permalink":"/ai_toutiao/daily/cscc/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cscc/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.CC)","permalink":"/ai_toutiao/daily/cscc/20251222-20251228"},"next":{"title":"20260105-20260111 (cs.CC)","permalink":"/ai_toutiao/daily/cscc/20260105-20260111"}}');var s=i(74848),t=i(28453);const a={slug:"/daily/cscc/20251229-20260104"},o="20251229-20260104 (cs.CC)",l={},c=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2},{value:"2026-01-01",id:"2026-01-01",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251229-20260104-cscc",children:"20251229-20260104 (cs.CC)"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] A Note on the NP-Hardness of PARTITION Via First-Order Projections"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [computational complexity theory], [NP-hardness, first-order reductions, AC0 reductions, PARTITION problem, descriptive complexity]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Pa\xfal Risco Iturralde"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Independent researcher"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21448",children:"https://arxiv.org/pdf/2512.21448"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates NP-hardness of the PARTITION problem via first-order projections, 2. Overcomes the obstacle of requiring large sums in the standard reduction by using descriptive complexity techniques, 3. Fills a gap in the literature regarding the hardness of PARTITION under restricted reductions like AC0."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b5d9f8d4b42755b482904c0cf03df329316dc9a10936a82fe27b6ce034a1e56_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b5d9f8d4b42755b482904c0cf03df329316dc9a10936a82fe27b6ce034a1e56_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This note addresses the open question of whether the PARTITION problem is NP-hard under restricted reductions like AC0. It modifies classic reductions from 3SAT to SUBSET-SUM to PARTITION, defining them using first-order logical formulas (first-order projections). The main conclusion is that PARTITION is indeed NP-hard via first-order projections, which implies hardness under polynomial-size AC0 reductions, thereby resolving the gap mentioned in prior work."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: A Note on the NP-Hardness of PARTITION Via First-Order Projections] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[PARTITION\u7684NP-hardness\u5728\u53d7\u9650\u5f52\u7ea6\u4e0b\u662f\u5426\u6210\u7acb?/Is PARTITION NP-hard under restricted reductions?]\n    C --\x3e C1[\u4f7f\u7528\u4e00\u9636\u903b\u8f91\u516c\u5f0f\u5b9a\u4e49\u5f52\u7ea6/Define reductions using first-order logic formulas]\n    C --\x3e C2[\u4fee\u6539\u7ecf\u5178\u5f52\u7ea6(3SAT\u5230SUBSET-SUM\u5230PARTITION)/Modify classic reductions (3SAT to SUBSET-SUM to PARTITION)]\n    D --\x3e D1[PARTITION\u5bf9\u4e00\u9636\u6295\u5f71\u662fNP-hard\u7684/PARTITION is NP-hard via first-order projections]\n    D --\x3e D2[\u6697\u793a\u5bf9\u591a\u9879\u5f0f\u5927\u5c0fAC0\u5f52\u7ea6\u4e5f\u662fNP-hard\u7684/Implies NP-hard under polynomial-size AC0 reductions]\n    D --\x3e D3[\u586b\u8865\u4e86\u6587\u732e\u4e2d\u7684\u7a7a\u767d/Fills a gap in the literature]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] A Note on Avoid vs MCSP"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [computational complexity theory], [Range Avoidance Problem, Minimal Circuit Size Problem, AM \u2229 coAM, Turing reductions]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Edward A. Hirsch, Ilya Volkovich"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Ariel University, Boston College"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21764",children:"https://arxiv.org/pdf/2512.21764"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Presents an alternative approach to a known result linking languages reducible to the Range Avoidance Problem (Avoid) to the complexity class AM \u2229 coAM. 2. Proposes using the Minimal Circuit Size Problem (MCSP) as a potential avenue to derive this containment result. 3. Highlights the connection between two central problems in complexity theory (Avoid and MCSP) for understanding the power of reductions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/918e4b50a4df8fcc5a358172ac9f315b25fd82440914a13e94eb45224863b78a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/918e4b50a4df8fcc5a358172ac9f315b25fd82440914a13e94eb45224863b78a_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This note explores the complexity of the Range Avoidance Problem (Avoid). It proposes a new potential method, using the Minimal Circuit Size Problem (MCSP), to show that any language reducible to Avoid via deterministic or randomized Turing reductions is contained in the complexity class AM \u2229 coAM, offering an alternative to a recent proof."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["A Note on Avoid vs MCSP"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Complexity of Range Avoidance (Avoid)"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Using Minimal Circuit Size Problem (MCSP)"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Languages reducible to Avoid are in AM \u2229 coAM"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Conserved active information"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [information theory], [conserved active information, No-Free-Lunch, KL divergence, search space, information conservation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yanchen Chen, Daniel Andr\xe9s D\xedaz-Pach\xf3n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Miami"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21834",children:"https://arxiv.org/pdf/2512.21834"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces conserved active information (I\u2295), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I\u2295 can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new information-theoretic measure called conserved active information (I\u2295) to quantify net information change in search problems while respecting conservation laws. It shows that I\u2295 uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    Root[Conserved active information] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem: Limitations of average-focused information measures like KL divergence]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce conserved active information I\u2295, a symmetric extension respecting No-Free-Lunch]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results: I\u2295 reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Poincar\xe9 Duality and Multiplicative Structures on Quantum Codes"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [quantum error correction], [sheaf codes, Poincar\xe9 duality, quantum LDPC codes, transversal gates, cup/cap product]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yiming Li, Zimu Li, Zi-Wen Liu, Quynh T. Nguyen"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Harvard University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21922",children:"https://arxiv.org/pdf/2512.21922"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Generalizing Poincar\xe9 duality from manifolds to sheaf-based classical and quantum codes, establishing a rigorous duality relationship between chain and cochain complexes. 2. Constructing multiplicative structures (cup and cap products) on sheaved chain complexes, leading to an explicit isomorphism between (co)homology groups. 3. Applying the framework to obtain transversal logical gates (CZ, CCZ, higher-order controlled-Z) on families of good qLDPC and quantum locally testable codes, pointing towards fault-tolerant non-Clifford gates."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fcb4213084226768ada35e1a65f3fff10489254aaaafade43391ca7096cc47_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fcb4213084226768ada35e1a65f3fff10489254aaaafade43391ca7096cc47_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper generalizes Poincar\xe9 duality and multiplicative structures from topology to sheaf-based quantum codes. The authors rigorously prove duality relationships and construct cup/cap products, leading to an isomorphism between homology groups. As an application, they demonstrate how to construct transversal logical non-Clifford gates on good quantum LDPC codes, advancing fault-tolerant quantum computing."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root("Poincar\xe9 Duality and Multiplicative Structures on Quantum Codes<br>\u91cf\u5b50\u4ee3\u7801\u7684\u5e9e\u52a0\u83b1\u5bf9\u5076\u4e0e\u4e58\u6cd5\u7ed3\u6784")\n    Root --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Generalize Poincar\xe9 duality to codes<br>\u5c06\u5bf9\u5076\u6027\u63a8\u5e7f\u81f3\u7f16\u7801")\n    Method --\x3e M1("Sheaf theory on cell complexes<br>\u80de\u8154\u590d\u5f62\u4e0a\u7684\u5c42\u7406\u8bba")\n    Method --\x3e M2("Build cup/cap products<br>\u6784\u5efa\u676f\u79ef/\u5361\u79ef")\n    Results --\x3e R1("Duality & isomorphism proven<br>\u8bc1\u660e\u5bf9\u5076\u4e0e\u540c\u6784")\n    Results --\x3e R2("Transversal logical gates<br>\u6a2a\u622a\u903b\u8f91\u95e8")'}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251230] A Study of NP-Completeness and Undecidable Word Problems in Semigroups"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [computational complexity theory], [NP-completeness, undecidable word problem, associative calculus, polynomial reducibility, Turing machines]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Duaa Abdullah, Jasem Hamoud"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Moscow Institute of Physics and Technology"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22123",children:"https://arxiv.org/pdf/2512.22123"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Explores the relationship between complexity classes P and NP and the concept of polynomial reducibility. 2. Demonstrates the construction of an associative calculus (semigroup) with an algorithmically undecidable word problem. 3. Establishes a direct connection between a Turing machine computing a non-recursive function and the equivalence condition in the constructed calculus, linking computational complexity and algebraic undecidability."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9ade75d90324f1eb4f6ee92ec609932e78897f10c8103547d100966beb841a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9ade75d90324f1eb4f6ee92ec609932e78897f10c8103547d100966beb841a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates fundamental limits in computation by studying NP-completeness and undecidable problems. It constructs an associative calculus whose word problem is undecidable, linking it to a Turing machine that computes a non-recursive function. The work highlights the intrinsic boundaries of algorithmic solutions by connecting computational complexity theory with algebraic undecidability."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["A Study of NP-Completeness and Undecidable Word Problems in Semigroups"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Computational complexity & decidability limits"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Polynomial reducibility & Associative calculus construction"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Undecidable word problem linked to non-recursive Turing machine"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251230] Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [parameterized complexity], [tropical circuits, pathwidth, communication complexity, Traveling Salesperson Problem, dynamic programming]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Kacper Kluk, Jesper Nederlof"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Warsaw, Utrecht University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23121",children:"https://arxiv.org/pdf/2512.23121"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proves unconditional lower bounds on the size of tropical circuits (modeling pure dynamic programming) for solving connectivity problems like TSP on graphs of bounded pathwidth. 2. Establishes a connection between tropical circuit complexity and the nondeterministic communication complexity of compatibility matrices. 3. Shows that any tropical circuit for TSP on a certain graph of pathwidth k requires at least 2^\u03a9(k log log k) gates, which is higher than known algebraic algorithms, suggesting algebra is necessary for competitive worst-case times."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86590df9819e19ad7303e5df124f5b2189c5e6ab6d542206119582e1e6c83135_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86590df9819e19ad7303e5df124f5b2189c5e6ab6d542206119582e1e6c83135_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the limitations of pure dynamic programming, modeled by tropical circuits, for solving connectivity problems like the Traveling Salesperson Problem on graphs with small pathwidth. It proves an unconditional lower bound of 2^\u03a9(k log log k) gates for any tropical circuit solving TSP on a specific graph of pathwidth k. This result, established via a link to communication complexity, suggests that algebraic techniques are unavoidable for achieving the fastest known worst-case running times for these problems."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8bc4\u4f30\u7eaf\u52a8\u6001\u89c4\u5212\u5bf9\u8fde\u901a\u6027\u95ee\u9898\u7684\u80fd\u529b/Assess capability of pure DP for connectivity problems]\n    C --\x3e C1[\u5c06\u70ed\u5e26\u7535\u8def\u590d\u6742\u5ea6\u4e0e\u901a\u4fe1\u590d\u6742\u6027\u5173\u8054/Link tropical circuit complexity to communication complexity]\n    D --\x3e D1[\u8bc1\u660e\u4e0b\u754c\u9ad8\u4e8e\u5df2\u77e5\u4ee3\u6570\u7b97\u6cd5/Prove lower bound higher than known algebraic algorithms]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251230] Pseudodeterministic Algorithms for Minimum Cut Problems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [graph algorithms], [pseudodeterministic algorithms, global minimum cut, minimum s-t cut, streaming algorithms, cut-query models]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Aryan Agarwala, Nithin Varma"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Max-Planck-Institut f\xfcr Informatik, Saarland Informatics Campus; University of Cologne"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23468",children:"https://arxiv.org/pdf/2512.23468"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Presents efficient pseudodeterministic algorithms for the global minimum cut and minimum s-t cut problems. 2. Achieves an asymptotic running time for global minimum cut that is better than the fastest known sequential deterministic algorithm. 3. Implements the algorithm in multiple computational models (sequential, streaming, PRAM, cut-query) where efficient deterministic algorithms were previously unknown."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c06a5f7252dc9f1337880cbdb49ac90f3b180fa63fe8a5b0eb2c22a487cedbbd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c06a5f7252dc9f1337880cbdb49ac90f3b180fa63fe8a5b0eb2c22a487cedbbd_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces pseudodeterministic algorithms for finding global minimum cuts and minimum s-t cuts in graphs. The proposed method offers replicability by consistently outputting the same answer with high probability, while being faster than the best known deterministic algorithm for global minimum cut. The algorithms are also successfully adapted to work in sequential, streaming, PRAM, and cut-query models."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Pseudodeterministic Algorithms for Minimum Cut Problems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u786e\u5b9a\u6027\u7b97\u6cd5\u6548\u7387\u4f4e / Deterministic algorithms are inefficient)\n    B --\x3e B2(\u968f\u673a\u7b97\u6cd5\u8f93\u51fa\u4e0d\u4e00\u81f4 / Randomized algorithms lack replicability)\n    C --\x3e C1(\u4f2a\u786e\u5b9a\u6027\u7b97\u6cd5 / Pseudodeterministic Algorithms)\n    C --\x3e C2(\u9ad8\u6982\u7387\u8f93\u51fa\u76f8\u540c\u89e3 / Outputs same solution with high probability)\n    D --\x3e D1(\u6e10\u8fd1\u66f4\u5feb / Asymptotically faster)\n    D --\x3e D2(\u591a\u6a21\u578b\u5b9e\u73b0 / Implemented in multiple models)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251230] Coloring Hardness on Low Twin-Width Graphs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [graph theory, computational complexity], [twin-width, graph coloring, NP-hardness, computational complexity, graph classes]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," \xc9douard Bonnet"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Univ Lyon, CNRS, ENS de Lyon, Universit\xe9 Claude Bernard Lyon 1, LIP UMR5668"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23680",children:"https://arxiv.org/pdf/2512.23680"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proves that the Min Coloring problem is NP-hard on the class of graphs with twin-width at most 3 (T3). 2. Proves that for every k >= 3, the k-Coloring problem is NP-hard on the class of graphs with twin-width at most 4 (T4). 3. Provides structural observations about the T3 and T4 classes, highlighting their distinct properties and raising open questions about complexity transitions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77a33f460e61bf2610b2bc5a51fb237df8e78440e7279dd902d4d1e4a4dbc60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77a33f460e61bf2610b2bc5a51fb237df8e78440e7279dd902d4d1e4a4dbc60_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the computational complexity of graph coloring problems on graphs with low twin-width. It proves that Min Coloring is NP-hard on graphs of twin-width at most 3, and that k-Coloring is NP-hard on graphs of twin-width at most 4 for all k>=3. These results establish the first hardness for a problem on T3 that is easy on simpler graph classes like trees and cographs."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\nA[Coloring Hardness on Low Twin-Width Graphs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Coloring complexity on bounded twin-width graphs]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: NP-hardness proofs via reductions]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Min Coloring hard on T3, k-Coloring hard on T4]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-01",children:"2026-01-01"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [parameterized complexity], [kidney exchange, FPT algorithm, W[1]-hardness, pathwidth, treewidth]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24037",children:"https://arxiv.org/pdf/2512.24037"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) \u2248 O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>\u80be\u810f\u4ea4\u6362\uff1a\u66f4\u5feb\u7684\u53c2\u6570\u5316\u7b97\u6cd5\u4e0e\u66f4\u7d27\u7684\u4e0b\u754c"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Kidney exchange is NP-complete<br>\u80be\u810f\u4ea4\u6362\u95ee\u9898\u662fNP\u5b8c\u5168\u95ee\u9898"] --\x3e P1["\u9650\u5236/Constraint<br>Exchange via small cycles & paths<br>\u901a\u8fc7\u5c0f\u73af\u548c\u8def\u5f84\u4ea4\u6362"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Parameterized Complexity<br>\u53c2\u6570\u5316\u590d\u6742\u5ea6"] --\x3e M1["\u53c2\u6570/Parameter<br>Number of patients (t)<br>\u60a3\u8005\u6570\u91cf(t)"]\n    Method --\x3e M2["\u53c2\u6570/Parameter<br>Graph pathwidth<br>\u56fe\u8def\u5f84\u5bbd\u5ea6"]\n    Results["\u5173\u952e\u7ed3\u679c/Results"] --\x3e R1["\u7b97\u6cd5\u6539\u8fdb/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT\u7b97\u6cd5: O*((4e)^t)"]\n    Results --\x3e R2["\u4e0b\u754c/Lower Bound<br>W[1]-hard for pathwidth<br>\u5bf9\u8def\u5f84\u5bbd\u5ea6\u662fW[1]-\u96be\u7684"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] From FPT Decision to FPT Enumeration"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [parameterized complexity], [fixed-parameter tractable (FPT), enumeration, DelayP, kernelization, branching]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nadia Creignou, Timo Camillo Merkl, Reinhard Pichler, Daniel Unterberger"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Aix Marseille Universit\xe9, TU Wien"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24137",children:"https://arxiv.org/pdf/2512.24137"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a framework for studying how to extend FPT decision algorithms to FPT enumeration algorithms. 2. Inspects fundamental FPT design approaches (e.g., kernelization, branching) for their potential to yield enumeration algorithms. 3. Presents ideas and methodologies for transforming decision/optimization FPT techniques into ones suitable for enumerating all solutions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ea5a12796e6b1045900cdb4da84a92c69ff6ce730801a1cf14697d7fae1b84_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ea5a12796e6b1045900cdb4da84a92c69ff6ce730801a1cf14697d7fae1b84_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap in applying fixed-parameter tractable (FPT) algorithms to enumeration problems. It investigates how fundamental techniques for designing FPT decision algorithms, such as kernelization and branching, can be adapted to create FPT algorithms for enumerating all solutions. The main conclusion is that a systematic methodology can be developed to bridge FPT decision and FPT enumeration."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[From FPT Decision to FPT Enumeration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Intractable enumeration problems lack FPT algorithmic focus.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Inspect and extend fundamental FPT design approaches for enumeration.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Presents ideas and a framework for turning FPT decision into FPT enumeration algorithms.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Diffusion Language Models are Provably Optimal Parallel Samplers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [diffusion language models, parallel sampling, chain-of-thought, remasking, revision]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Haozhe Jiang, Nika Haghtalab, Lijie Chen"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25014",children:"https://arxiv.org/pdf/2512.25014"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Formalized a model of parallel sampling and proved that DLMs with CoT can simulate any parallel sampling algorithm with an optimal number of sequential steps. 2. Showed that enabling remasking or revision with CoT allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. 3. Established a strict expressivity gap, proving DLMs with revision or remasking are strictly more expressive than those without."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a theoretical foundation for the efficiency of Diffusion Language Models (DLMs) as parallel samplers. It proves that DLMs augmented with chain-of-thought reasoning can simulate any parallel sampling algorithm with optimal sequential steps and, when further equipped with token remasking or revision, also achieve optimal space complexity. The results theoretically justify DLMs as highly efficient parallel samplers and advocate for enabling revision capabilities in such models."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["Diffusion Language Models are Provably Optimal Parallel Samplers<br>\u6269\u6563\u8bed\u8a00\u6a21\u578b\u662f\u53ef\u8bc1\u660e\u6700\u4f18\u7684\u5e76\u884c\u91c7\u6837\u5668"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>DLMs\u7684\u7406\u8bba\u4f18\u52bf\u4e0e\u6548\u7387\u6781\u9650\u672a\u660e<br>Theoretical advantages and efficiency limits of DLMs are unclear"] --\x3e P1["\u5e76\u884c\u91c7\u6837\u6548\u7387/Parallel Sampling Efficiency"]\n    Problem --\x3e P2["\u7a7a\u95f4\u590d\u6742\u5ea6/Space Complexity"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f62\u5f0f\u5316\u5e76\u884c\u91c7\u6837\u6a21\u578b\u4e0e\u7535\u8def\u590d\u6742\u5ea6<br>Formalize parallel sampling model & circuit complexity"] --\x3e M1["\u589e\u5f3aCoT/Augment with CoT"]\n    Method --\x3e M2["\u5f15\u5165\u91cd\u63a9\u7801\u6216\u4fee\u8ba2/Introduce Remasking or Revision"]\n    Results["\u5173\u952e\u7ed3\u679c/Results"] --\x3e R1["\u6700\u4f18\u987a\u5e8f\u6b65\u9aa4/Optimal Sequential Steps"]\n    Results --\x3e R2["\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6/Optimal Space Complexity"]\n    Results --\x3e R3["\u4e25\u683c\u8868\u8fbe\u80fd\u529b\u5dee\u8ddd/Strict Expressivity Gap"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Thin Tree Verification is coNP-Complete"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [computational complexity], [thin tree, coNP-complete, spanning tree, graph cuts, edge-connectivity]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Alice Moayyedi"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Waterloo"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25043",children:"https://arxiv.org/pdf/2512.25043"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proves that the problem of verifying if a given spanning tree is \u03b1-thin is coNP-hard, resolving an open question about its computational complexity. 2. Provides a formal proof for a problem previously speculated to be NP-hard or lacking a polynomially-checkable certificate. 3. Establishes a negative result (coNP-hardness) that impacts the algorithmic prospects related to the Thin Tree Conjecture and its applications, such as approximating the Asymmetric Travelling Salesman Problem (ATSP)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57fee7ec29f3a9f8d8859080b76ea70b44eeb72ddf00dece29f07228a749a454_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57fee7ec29f3a9f8d8859080b76ea70b44eeb72ddf00dece29f07228a749a454_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the computational complexity of verifying whether a given spanning tree in a graph is \u03b1-thin, meaning it contains at most an \u03b1 proportion of edges in any graph cut. The authors prove that this verification problem is coNP-hard. This result shows that efficiently checking a candidate solution is computationally difficult, which has implications for the Thin Tree Conjecture and related approximation algorithms for problems like ATSP."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Thin Tree Verification is coNP-Complete] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9a8c\u8bc1\u7ed9\u5b9a\u751f\u6210\u6811\u662f\u5426\u4e3a\u03b1-thin\u6811/Verify if a given spanning tree is \u03b1-thin]\n    C --\x3e C1[\u8bc1\u660e\u590d\u6742\u5ea6\u4e0b\u754c/Prove complexity lower bound]\n    D --\x3e D1[\u9a8c\u8bc1\u95ee\u9898\u662fcoNP-hard\u7684/Verification problem is coNP-hard]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Syndrome aware mitigation of logical errors"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [quantum error correction and mitigation], [logical error mitigation, syndrome-aware, fault-tolerance, runtime overhead, quantum error correction]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Dorit Aharonov, Yosi Atia, Eyal Bairey, Zvika Brakerski, Itsik Cohen, Omri Golan, Ilya Gurwich, Netanel H. Lindner, Maor Shutman"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Qedma Quantum Computing, Hebrew University, Weizmann Institute of Science, Technion"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23810",children:"https://arxiv.org/pdf/2512.23810"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Syndrome-Aware Logical Error Mitigation (SALEM), a novel method that leverages syndrome data from error correction to mitigate logical errors. 2. Demonstrates that SALEM achieves an exponentially lower runtime overhead compared to prior logical error mitigation schemes, enabling the execution of significantly larger logical circuits. 3. Shows that SALEM can outperform physical error mitigation even above the standard fault-tolerance threshold, making error correction useful in physical error rate regimes where it was previously considered ineffective."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28c546f1791a48c23de235687f3df199f96775597dcac09ee65b2ab0f41d74e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28c546f1791a48c23de235687f3df199f96775597dcac09ee65b2ab0f41d74e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Syndrome-Aware Logical Error Mitigation (SALEM), a method that uses syndrome data from quantum error correction to more efficiently mitigate residual logical errors. SALEM drastically reduces the runtime overhead compared to previous approaches, allowing for the accurate execution of much larger quantum circuits. The work demonstrates that this tight integration of error correction and mitigation can be beneficial even when physical error rates are above the conventional fault-tolerance threshold."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Syndrome aware mitigation of logical errors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6709\u9650\u7269\u7406\u6bd4\u7279\u5bfc\u81f4\u6b8b\u7559\u903b\u8f91\u9519\u8bef/Limited physical qubits cause residual logical errors]\n    C --\x3e C1[\u5229\u7528\u7ea0\u9519\u4f34\u968f\u7684\u7efc\u5408\u5f81\u6570\u636e\u8fdb\u884c\u7f13\u89e3/Use syndrome data from error correction for mitigation]\n    D --\x3e D1[\u8fd0\u884c\u65f6\u5f00\u9500\u6307\u6570\u7ea7\u964d\u4f4e/Runtime overhead reduced exponentially]\n    D --\x3e D2[\u53ef\u6267\u884c\u7535\u8def\u89c4\u6a21\u6570\u91cf\u7ea7\u63d0\u5347/Executable circuit volume increased by orders of magnitude]\n    D --\x3e D3[\u53ef\u5728\u5bb9\u9519\u9608\u503c\u4ee5\u4e0a\u8d85\u8d8a\u7269\u7406\u9519\u8bef\u7f13\u89e3/Can outperform physical error mitigation above fault-tolerance threshold]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Proper colorings of a graph in linear time using a number of colors linear in the maximum degree of the graph"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [graph algorithms], [proper coloring, graph sampling, linear-time algorithm, maximum degree, Markov chain Monte Carlo]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Kritika Bhandari, Mark Huber"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," None (Inferred from arXiv submission; no explicit affiliation provided on first page)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24522",children:"https://arxiv.org/pdf/2512.24522"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. A new algorithm for exact sampling from the set of proper colorings of a graph., 2. The algorithm achieves an expected running time linear in the graph size for graphs with maximum degree \u0394., 3. It is the first algorithm with this guarantee when the number of colors exceeds 3.637\u0394 + 1."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2df3f6d8f9edf2c385953da77f4effcefb8b3b532fac0ef601829b8505fab4b3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2df3f6d8f9edf2c385953da77f4effcefb8b3b532fac0ef601829b8505fab4b3_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a new algorithm for exactly sampling proper colorings of a graph. It is the first algorithm whose expected running time is guaranteed to be linear in the graph size when the number of colors is greater than 3.637 times the graph's maximum degree plus one."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Proper colorings of a graph in linear time<br/>\u56fe\u7684\u7ebf\u6027\u65f6\u95f4\u6b63\u5e38\u7740\u8272] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Exact sampling from proper colorings<br/>\u4ece\u6b63\u5e38\u7740\u8272\u4e2d\u7cbe\u786e\u91c7\u6837]\n    C --\x3e C1[New sampling algorithm<br/>\u65b0\u7684\u91c7\u6837\u7b97\u6cd5]\n    D --\x3e D1[Linear expected runtime<br/>\u7ebf\u6027\u671f\u671b\u8fd0\u884c\u65f6\u95f4]\n    D --\x3e D2[Colors > 3.637\u0394 + 1<br/>\u989c\u8272\u6570 > 3.637\u0394 + 1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260101] Approximate Computation via Le Cam Simulability"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [computational complexity theory], [Le Cam deficiency, computational deficiency, LeCam-P, approximate reduction, statistical experiments]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Deniz Akdemir"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. The author is Deniz Akdemir, but no affiliation or email domain is given."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24860",children:"https://arxiv.org/pdf/2512.24860"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a decision-theoretic framework for computational complexity based on Le Cam simulability and statistical experiments, shifting focus from syntactic exactness to semantic approximation. 2. Defines computational deficiency (\u03b4_poly) and uses it to construct the complexity class LeCam-P (Decision-Robust Polynomial Time) for problems that are semantically easy to approximate. 3. Establishes the No-Free-Transfer Inequality, showing that strictly invariant representations inevitably destroy decision-relevant information."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eb21a7edcb8e5cb8e1788fcdd488db8897d0be53c937e14e03661081a0b7652_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eb21a7edcb8e5cb8e1788fcdd488db8897d0be53c937e14e03661081a0b7652_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new framework for computational complexity that views computation as the efficient simulation of a target statistical experiment with bounded risk distortion (Le Cam deficiency). It introduces the concept of computational deficiency and the class LeCam-P to characterize problems that are hard to solve exactly but easy to approximate for decision-making. The main conclusion is that this framework bridges algorithmic complexity and decision theory, showing that classical exact reductions are a special case of zero-deficiency simulations."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Approximate Computation via Le Cam Simulability] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7ecf\u5178\u7cbe\u786e\u8ba1\u7b97\u7406\u8bba\u5bf9\u73b0\u4ee3\u8fd1\u4f3c\u9700\u6c42\u9650\u5236/Classical exactness paradigm is restrictive for modern approximate needs]\n    C --\x3e C1[\u57fa\u4e8eLe Cam\u7f3a\u9677\u7684\u7edf\u8ba1\u5b9e\u9a8c\u6a21\u62df\u6846\u67b6/Framework based on Le Cam deficiency for simulating statistical experiments]\n    C --\x3e C2[\u5b9a\u4e49\u8ba1\u7b97\u7f3a\u9677\u4e0eLeCam-P\u7c7b/Define computational deficiency and LeCam-P class]\n    D --\x3e D1[Karp\u5f52\u7ea6\u662f\u96f6\u7f3a\u9677\u6a21\u62df\u7684\u7279\u4f8b/Karp reductions are special cases of zero-deficiency simulations]\n    D --\x3e D2[\u63d0\u51fa\u65e0\u514d\u8d39\u8f6c\u79fb\u4e0d\u7b49\u5f0f/Propose No-Free-Transfer Inequality]\n    D --\x3e D3[\u8fde\u63a5\u7b97\u6cd5\u590d\u6742\u5ea6\u4e0e\u51b3\u7b56\u7406\u8bba/Bridge algorithmic complexity and decision theory]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(96540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);