"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8964],{28453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>d});var s=i(96540);const r={},a=s.createContext(r);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(a.Provider,{value:e},n.children)}},51250:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_DC/20251222-20251228","title":"20251222-20251228 (cs.DC)","description":"2025-12-22","source":"@site/docs/daily/cs_DC/20251222-20251228.md","sourceDirName":"daily/cs_DC","slug":"/daily/csdc/20251222-20251228","permalink":"/ai_toutiao/daily/csdc/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767069056000,"frontMatter":{"slug":"/daily/csdc/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.DC)","permalink":"/ai_toutiao/daily/cs_DC/20251215-20251221"},"next":{"title":"20251229-20260104 (cs.DC)","permalink":"/ai_toutiao/daily/csdc/20251229-20260104"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/csdc/20251222-20251228"},d="20251222-20251228 (cs.DC)",l={},c=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2},{value:"2025-12-24",id:"2025-12-24",level:2},{value:"2025-12-25",id:"2025-12-25",level:2}];function o(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"20251222-20251228-csdc",children:"20251222-20251228 (cs.DC)"})}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP2]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kwangjun Ahn, Noah Amsel, John Langford"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Microsoft Research, AI Frontiers, NYU"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16928",children:"https://arxiv.org/pdf/2512.16928"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nankai University, University of Louisville, University of North Texas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17254",children:"https://arxiv.org/pdf/2512.17254"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ivan Kralj, Lodovico Giaretta, Gordan Je\u017ei\u0107, Ivana Podnar \u017darko, \u0160ar\u016bnas Girdzijauskas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17352",children:"https://arxiv.org/pdf/2512.17352"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Wuhan University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17574",children:"https://arxiv.org/pdf/2512.17574"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Nihir Chadderwala"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17913",children:"https://arxiv.org/pdf/2512.17913"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17918",children:"https://arxiv.org/pdf/2512.17918"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17910",children:"https://arxiv.org/pdf/2512.17910"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17941",children:"https://arxiv.org/pdf/2512.17941"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Fast Online Digital Twinning on FPGA for Mission Critical Applications"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17942",children:"https://arxiv.org/pdf/2512.17942"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Fast Online Digital Twinning on FPGA for Mission Critical Applications"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yi Yang, Ziyu Lin, Liesheng Wei"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18127",children:"https://arxiv.org/pdf/2512.18127"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Constrained Cuts, Flows, and Lattice-Linearity"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Robert Streit, Vijay K. Garg"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18141",children:"https://arxiv.org/pdf/2512.18141"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Constrained Cuts, Flows, and Lattice-Linearity"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18194",children:"https://arxiv.org/pdf/2512.18194"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18318",children:"https://arxiv.org/pdf/2512.18318"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18334",children:"https://arxiv.org/pdf/2512.18334"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Grammateia Kotsialou"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18444",children:"https://arxiv.org/pdf/2512.18444"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Snowveil: A Framework for Decentralised Preference Discovery"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18674",children:"https://arxiv.org/pdf/2512.18674"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] A Real-Time Digital Twin for Adaptive Scheduling"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18894",children:"https://arxiv.org/pdf/2512.18894"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Real-Time Digital Twin for Adaptive Scheduling"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ivan \u010cili\u0107, Ivana Podnar \u017darko, Pantelis Frangoudis, Schahram Dustdar"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18915",children:"https://arxiv.org/pdf/2512.18915"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19103",children:"https://arxiv.org/pdf/2512.19103"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Timely Parameter Updating in Over-the-Air Federated Learning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19131",children:"https://arxiv.org/pdf/2512.19131"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19179",children:"https://arxiv.org/pdf/2512.19179"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Simulations between Strongly Sublinear MPC and Node-Capacitated Clique"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Philipp Schneider, Julian Werthmann"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19326",children:"https://arxiv.org/pdf/2512.19326"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Simulations between Strongly Sublinear MPC and Node-Capacitated Clique"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19342",children:"https://arxiv.org/pdf/2512.19342"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19606",children:"https://arxiv.org/pdf/2512.19606"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, Jo\xe3o Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medi\u0107, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Beno\xeet Commer\xe7on, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18883",children:"https://arxiv.org/pdf/2512.18883"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-24",children:"2025-12-24"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Antonio Tarizzo, Mohammad Kazemi, Deniz G\xfcnd\xfcz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19777",children:"https://arxiv.org/pdf/2512.19777"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Andrea Sordello, Marco Mellia, Idilio Drago, Rodolfo Valentim, Francesco Musumeci, Massimo Tornatore, Federico Cerutti, Martino Trevisan, Alessio Botta, Willen Borges Coelho"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19842",children:"https://arxiv.org/pdf/2512.19842"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] An Adaptive Distributed Stencil Abstraction for GPUs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Aditya Bhosale, Laxmikant Kale"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19851",children:"https://arxiv.org/pdf/2512.19851"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," An Adaptive Distributed Stencil Abstraction for GPUs"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19849",children:"https://arxiv.org/pdf/2512.19849"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," UCCL-EP: Portable Expert-Parallel Communication"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengchao Han, Xi Huang, Yi Fang, Guojun Han"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19972",children:"https://arxiv.org/pdf/2512.19972"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Scaling Point-based Differentiable Rendering for Large-scale Reconstruction"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20017",children:"https://arxiv.org/pdf/2512.20017"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Scaling Point-based Differentiable Rendering for Large-scale Reconstruction"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yaojian Chen, Si-Qiu Gong, Lin Gan, Yanfei Liu, An Yang, Yinuo Wang, Chao-yang Lu, Guangwen Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20064",children:"https://arxiv.org/pdf/2512.20064"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Population Protocols Revisited: Parity and Beyond"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Leszek G\u0105sieniec, Tytus Grodzicki, Tomasz Jurdzi\u0144ski, Jakub Kowalski, Grzegorz Stachowiak"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20163",children:"https://arxiv.org/pdf/2512.20163"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Population Protocols Revisited: Parity and Beyond"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20178",children:"https://arxiv.org/pdf/2512.20178"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Reaching Agreement Among Reasoning LLM Agents"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chaoyi Ruan, Yiliang Wang, Ziji Shi, Jialin Li"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20184",children:"https://arxiv.org/pdf/2512.20184"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Reaching Agreement Among Reasoning LLM Agents"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yinan Ni, Xiao Yang, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20210",children:"https://arxiv.org/pdf/2512.20210"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20363",children:"https://arxiv.org/pdf/2512.20363"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mohammad Walid Charrwi, Zaid Hussain"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20394",children:"https://arxiv.org/pdf/2512.20394"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] WOC: Dual-Path Weighted Object Consensus Made Efficient"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tanisha Fonseca, Gengrui Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20485",children:"https://arxiv.org/pdf/2512.20485"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," WOC: Dual-Path Weighted Object Consensus Made Efficient"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Rui Pan, Zhuofu Chen, Ravi Netravali"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20573",children:"https://arxiv.org/pdf/2512.20573"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-25",children:"2025-12-25"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20687",children:"https://arxiv.org/pdf/2512.20687"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[PHOTON: Hierarchical Autoregressive Modeling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u6c34\u5e73\u626b\u63cf\u5bfc\u81f4KV\u7f13\u5b58\u8bfb\u5199\u6210\u4e3a\u5185\u5b58\u74f6\u9888/Horizontal scanning causes memory-bound KV-cache bottleneck]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7528\u5782\u76f4\u591a\u5206\u8fa8\u7387\u5c42\u6b21\u6a21\u578b\u66ff\u4ee3/Replace with vertical multi-resolution hierarchical model]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u6548\u7387\u4e0e\u541e\u5410\u91cf\u5927\u5e45\u63d0\u5347/Significant improvement in memory efficiency & throughput]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] SoK: Speedy Secure Finality"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [sec], [blockchain consensus], [finality, consensus protocol, Ethereum, Gasper, reorg resilience]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yash Saraswat, Abhimanyu Nag"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology, Roorkee; University of Alberta"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20715",children:"https://arxiv.org/pdf/2512.20715"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Provides a systematic survey of the state-of-the-art in Speedy Secure Finality (SSF) protocols, tracing their evolution from foundational works like Goldfish to RLMD-GHOST. 2. Introduces and explains core theoretical primitives for understanding SSF, such as reorganization resilience and the generalized sleepy model. 3. Analyzes the practical trade-offs of Single Slot Finality and surveys the 3-Slot Finality (3SF) protocol as a pragmatic solution balancing fast finality with Ethereum's engineering constraints."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper surveys research on Speedy Secure Finality (SSF) to reduce the long confirmation latency in Ethereum's Gasper protocol. It reviews the evolution of fast finality protocols, analyzes their design trade-offs, and highlights the 3-Slot Finality protocol as a practical synthesis. The main conclusion is that 3SF offers a viable path to achieve faster, secure finality while addressing the network's practical limitations."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[SoK: Speedy Secure Finality] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Ethereum Gasper\u534f\u8bae15\u5206\u949f\u6700\u7ec8\u786e\u8ba4\u5ef6\u8fdf\u5bfc\u81f4\u91cd\u7ec4\u653b\u51fb\u548cMEV\u63d0\u53d6]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7cfb\u7edf\u7efc\u8ff0\u5feb\u901f\u6700\u7ec8\u6027\u534f\u8bae\uff0c\u5206\u6790\u5355\u65f6\u9699\u6700\u7ec8\u6027\u7684\u74f6\u9888\uff0c\u8c03\u67e53\u65f6\u9699\u6700\u7ec8\u6027\u534f\u8bae]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 3SF\u534f\u8bae\u5728\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\u4e0e\u5de5\u7a0b\u7ea6\u675f\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u662f\u5b9e\u7528\u7684\u5feb\u901f\u6700\u7ec8\u6027\u65b9\u6848]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [multi-runtime middleware, hybrid AI-HPC workflows, uniform abstractions, Dragon, vLLM]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Rutgers University, Hewlett Packard Enterprise, Brookhaven National Laboratory"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20795",children:"https://arxiv.org/pdf/2512.20795"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes RHAPSODY, a multi-runtime middleware that composes and coordinates existing runtimes to support heterogeneous AI-HPC workloads within a single job allocation. 2. Introduces uniform abstractions for tasks, services, resources, and execution policies to manage conflicting requirements of simulations, AI services, and agentic workflows. 3. Demonstrates minimal runtime overhead, scalability for inference workloads, and efficient AI-HPC coupling in evaluations on leadership-class HPC platforms."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of executing hybrid AI-HPC workflows, which combine simulations, training, and inference with conflicting runtime requirements. It proposes RHAPSODY, a middleware that coordinates existing runtimes through uniform abstractions instead of replacing them. Evaluation shows RHAPSODY enables efficient, scalable execution of these heterogeneous workloads with minimal overhead."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u89c4\u6a21\u5316\u652f\u6301\u6df7\u5408AI-HPC\u5de5\u4f5c\u6d41\u7684\u5f02\u6784\u9700\u6c42 / Existing systems cannot support heterogeneous requirements of hybrid AI-HPC workflows at scale]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u591a\u8fd0\u884c\u65f6\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7\u7edf\u4e00\u62bd\u8c61\u534f\u8c03\u73b0\u6709\u8fd0\u884c\u65f6 / Multi-runtime middleware coordinating existing runtimes via uniform abstractions]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5f00\u9500\u6700\u5c0f\uff0c\u652f\u6301\u89c4\u6a21\u5316\u5f02\u6784\u4e0e\u8fd1\u7ebf\u6027\u63a8\u7406\u6269\u5c55 / Minimal overhead, sustains heterogeneity at scale, near-linear inference scaling]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Stochastic well-structured transition systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing theory], [well-structured transition systems, population protocols, probabilistic scheduling, computational complexity, BPP]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," James Aspnes"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20939",children:"https://arxiv.org/pdf/2512.20939"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Defines a new class of stochastic well-structured transition systems (SWSTSs) that unifies models like population protocols and chemical reaction networks under a probabilistic scheduling rule. 2. Proves fundamental limitations on phase clocks in SWSTSs, showing they either stop or tick too fast in expected polynomial time. 3. Provides an exact characterization of computational power, showing augmented SWSTSs compute exactly BPP languages, while unaugmented ones compute symmetric BPL languages."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper extends the theory of well-structured transition systems by incorporating probabilistic scheduling, creating a new class called stochastic well-structured transition systems (SWSTSs). It proves that any phase clock implementation in these systems has polynomial expected duration, and that terminating computations finish in expected polynomial time. These results lead to an exact characterization of computational power, showing that augmented SWSTSs compute exactly the languages in BPP."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Stochastic well-structured transition systems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Extend WSTS theory to probabilistic scheduling models)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Define SWSTS class unifying population protocols, CRNs, gossip models)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Phase clock limitations; Polynomial expected termination; Computational power = BPP/BPL)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [communication & networking], [4D Gaussian Splatting, video streaming, integer linear programming, pruning, keyframe selection]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhe Wang, Jinghang Li, Yifei Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20943",children:"https://arxiv.org/pdf/2512.20943"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[AirGS: Real-Time 4D Gaussian Streaming] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: 4DGS\u8d28\u91cf\u4e0b\u964d\u4e0e\u9ad8\u5e26\u5bbd\u5f00\u9500/4DGS Quality Degradation & High Bandwidth Overhead]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6d41\u4f18\u5316\u6846\u67b6\u4e0e\u81ea\u9002\u5e94\u526a\u679d/Streaming-Optimized Framework & Adaptive Pruning]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u8d28\u91cf\u7a33\u5b9a\u3001\u8bad\u7ec3\u52a0\u901f\u3001\u4f20\u8f93\u51cf\u5c0f/Quality Stable, Training Faster, Transmission Smaller]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [3D parallelism, heterogeneous GPUs, spot instances, load balancing, fault-tolerance]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fudan University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20953",children:"https://arxiv.org/pdf/2512.20953"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Introduces AutoHet, a system that automatically identifies optimal parallelism plans for distributed training on heterogeneous GPUs, supporting asymmetric 3D parallelism. 2. Proposes a theoretical optimization model for device grouping and load balancing to minimize per-iteration training time across GPUs with diverse capabilities. 3. Presents an efficient recovery strategy for spot instance preemption that prioritizes retrieving training states locally to minimize checkpoint downloads from cloud storage."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of distributed training of large language models on heterogeneous GPU clusters, particularly with spot instances. It proposes AutoHet, a system that automatically optimizes 3D parallelism plans and load balancing for such environments and includes an efficient fault-tolerance mechanism. Evaluations show AutoHet achieves significant speedups in both training throughput and recovery speed compared to existing systems."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[\u8bba\u6587\u6807\u9898 / Paper Title:<br>Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem:<br>Heterogeneous GPU & Spot Instance Training];\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method:<br>AutoHet System & Optimization Model];\nA --\x3e D[\u5173\u952e\u7ed3\u679c / Results:<br>1.79x Training & 4.38x Recovery Speedup];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [distributed attention, communication efficiency, Ring-Attention, communication-computation ratio, scalability]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20968",children:"https://arxiv.org/pdf/2512.20968"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Mesh-Attention<br>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5206\u5e03\u5f0f\u6ce8\u610f\u529b\u901a\u4fe1\u5f00\u9500\u5927<br>High Communication in Distributed Attention]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e2D\u8ba1\u7b97\u5757\u5212\u5206\u7684Mesh-Attention\u7b97\u6cd5<br>Mesh-Attention with 2D Tile Assignment]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u901a\u4fe1\u91cf\u51cf\u5c1185.4%, \u901f\u5ea6\u63d0\u53473.4\u500d<br>85.4% Comm Reduction, 3.4x Speedup]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [spot instance, online scheduling, deadline-aware, LoRA, integer programming]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fudan University, University of Oregon, Inria"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20967",children:"https://arxiv.org/pdf/2512.20967"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Expensive LLM fine-tuning with volatile spot instances)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Mixed instance scheduling with prediction & online policy selection)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: O(\u221aT) regret, up to 54.8% utility gain)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [hpc], [parallel computing], [hypergraph, GPU, dynamic data structure, triad counting, parallel algorithm]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Missouri University of Science and Technology, University of North Texas"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21009",children:"https://arxiv.org/pdf/2512.21009"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposed ESCHER, a novel GPU-centric parallel data structure for efficient representation and management of large-scale dynamic hypergraphs. 2. Designed a hypergraph triad-count update framework that minimizes redundant computation by leveraging ESCHER's dynamic operation capabilities. 3. Demonstrated significant performance improvements, achieving speedups of up to 104.5x, 473.7x, and 112.5x for different triad counting types on real-world and synthetic datasets."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational challenge of analyzing large, dynamic hypergraphs, which lack efficient specialized data structures. It proposes ESCHER, a GPU-centric data structure for representing hypergraph evolution, and a corresponding triad-counting update framework. The method achieves substantial speedups over state-of-the-art approaches in counting various types of hypergraph triads."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[ESCHER: Efficient and Scalable Hypergraph Evolution Representation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7f3a\u4e4f\u5206\u6790\u5927\u89c4\u6a21\u52a8\u6001\u8d85\u56fe\u7684\u9ad8\u6548\u6570\u636e\u7ed3\u6784]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faGPU\u5e76\u884c\u7684ESCHER\u6570\u636e\u7ed3\u6784\u4e0e\u4e09\u5143\u7ec4\u8ba1\u6570\u66f4\u65b0\u6846\u67b6]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u8fbe473.7\u500d\u52a0\u901f]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [federated learning], [zero-knowledge proofs, trusted execution environments, blockchain, medical AI, verifiable aggregation]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Savvy Sharma, George Petrovic, Sarthak Kaushik"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," George Brown Polytechnic"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21048",children:"https://arxiv.org/pdf/2512.21048"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[zkFL-Health] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u9690\u79c1\u6cc4\u9732\u4e0e\u805a\u5408\u5668\u4fe1\u4efb/Privacy Leakage & Aggregator Trust]\nC --\x3e C1[FL+ZKP+TEE/FL+ZKP+TEE]\nC --\x3e C2[\u94fe\u4e0a\u9a8c\u8bc1/On-chain Verification]\nD --\x3e D1[\u53ef\u9a8c\u8bc1\u7684\u9690\u79c1\u4fdd\u62a4/Verifiable Privacy]\nD --\x3e D2[\u5ba1\u8ba1\u4e0e\u5408\u89c4/Auditability & Compliance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Declarative distributed broadcast using three-valued modal logic and semitopologies"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [sys], [distributed algorithms], [modal logic, declarative specification, semitopologies, three-valued logic, axiomatic theories]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Murdoch J. Gabbay"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Heriot-Watt University (inferred from author's affiliation)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21137",children:"https://arxiv.org/pdf/2512.21137"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a novel method to formally specify distributed algorithms as declarative axiomatic theories using modal logic, 2. Demonstrates the method's application and scalability on concrete protocols (voting, broadcast, agreement), 3. Shows the method's practical utility by finding errors in a proposed industrial protocol."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel declarative approach for specifying distributed algorithms using three-valued modal logic and semitopologies. It demonstrates the method on protocols like Bracha Broadcast, providing a compact, human-readable specification that abstracts away low-level implementation details. The approach enables precise reasoning about correctness and has been used to find errors in industrial protocols."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Declarative distributed broadcast using three-valued modal logic and semitopologies] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u5bf9\u5206\u5e03\u5f0f\u7b97\u6cd5\u8fdb\u884c\u5f62\u5f0f\u5316\u3001\u58f0\u660e\u5f0f\u89c4\u8303\uff1f/How to formally specify distributed algorithms declaratively?]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u4e09\u503c\u6a21\u6001\u903b\u8f91\u548c\u534a\u62d3\u6251\u4f5c\u4e3a\u516c\u7406\u5316\u7406\u8bba/Using three-valued modal logic and semitopologies as axiomatic theories]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u521b\u5efa\u4e86\u7cbe\u786e\u3001\u7d27\u51d1\u7684\u89c4\u8303\uff0c\u53ef\u53d1\u73b0\u534f\u8bae\u9519\u8bef\uff0c\u652f\u6301\u9a8c\u8bc1/Creates precise, compact specifications that can find protocol errors and support verification]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(o,{...n})}):o(n)}}}]);