"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5882],{6548:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"daily/cs_NE/20251229-20260104","title":"20251229-20260104 (cs.NE)","description":"2025-12-29","source":"@site/docs/daily/cs_NE/20251229-20260104.md","sourceDirName":"daily/cs_NE","slug":"/daily/csne/20251229-20260104","permalink":"/ai_toutiao/daily/csne/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767007855000,"frontMatter":{"slug":"/daily/csne/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.NE)","permalink":"/ai_toutiao/daily/csne/20251222-20251228"},"next":{"title":"cs.NI","permalink":"/ai_toutiao/category/csni"}}');var s=i(4848),a=i(8453);const t={slug:"/daily/csne/20251229-20260104"},o="20251229-20260104 (cs.NE)",c={},d=[{value:"2025-12-29",id:"2025-12-29",level:2}];function l(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251229-20260104-csne",children:"20251229-20260104 (cs.NE)"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [dream-replay reinforcement learning, evolutionary algorithms, adaptive code generation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Santhosh Kumar Ravindran"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Microsoft Corporation"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21351",children:"https://arxiv.org/pdf/2512.21351"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as "genomes" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u4ee3\u7801\u751f\u6210\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9API\u53d8\u5316/LLM code generation lacks adaptability to API changes]\n    C --\x3e C1[\u5c06RL\u8f68\u8ff9\u89c6\u4e3a\u57fa\u56e0\u7ec4\u8fdb\u884c\u8fdb\u5316\u64cd\u4f5c/Treat RL trajectories as genomes for evolutionary operations]\n    C --\x3e C2[\u5728\u591c\u95f4\u56de\u653e\u9636\u6bb5\u8fdb\u884c\u7a81\u53d8\u4e0e\u9009\u62e9/Mutation and selection during nocturnal replay]\n    D --\x3e D1[\u89e3\u51b3\u65b9\u6848\u65b0\u9896\u6027\u63d0\u534735%/35% higher novelty in solutions]\n    D --\x3e D2[\u9002\u5e94\u901f\u5ea6\u52a0\u5feb25%/25% faster adaptation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Conserved active information"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [information theory], [conserved active information, No-Free-Lunch, KL divergence, search space, information conservation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yanchen Chen, Daniel Andr\xe9s D\xedaz-Pach\xf3n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Miami"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21834",children:"https://arxiv.org/pdf/2512.21834"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces conserved active information (I\u2295), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I\u2295 can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new information-theoretic measure called conserved active information (I\u2295) to quantify net information change in search problems while respecting conservation laws. It shows that I\u2295 uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    Root[Conserved active information] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem: Limitations of average-focused information measures like KL divergence]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce conserved active information I\u2295, a symmetric extension respecting No-Free-Lunch]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results: I\u2295 reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);