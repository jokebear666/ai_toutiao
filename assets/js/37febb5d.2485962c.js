"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4941],{6231:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/cs_GR/20251215-20251221","title":"20251215-20251221 (cs.GR)","description":"2025-12-18","source":"@site/docs/daily/cs_GR/20251215-20251221.md","sourceDirName":"daily/cs_GR","slug":"/daily/cs_GR/20251215-20251221","permalink":"/ai_toutiao/daily/cs_GR/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766635085000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.GR","permalink":"/ai_toutiao/category/csgr"},"next":{"title":"20251222-20251228 (cs.GR)","permalink":"/ai_toutiao/daily/csgr/20251222-20251228"}}');var s=n(4848),t=n(8453);const a={},l="20251215-20251221 (cs.GR)",o={},c=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251215-20251221-csgr",children:"20251215-20251221 (cs.GR)"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251218] Towards Physically-Based Sky-Modeling For Image Based Lighting"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [computer graphics, image-based lighting], [sky-modeling, High Dynamic Range Imagery (HDRI), Image-Based Lighting (IBL), Full Dynamic Range (FDR), deep neural network (DNN), parametric sky-models, tonemapping]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ian J. Maquignaz"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Universit\xe9 Laval"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15632",children:"https://arxiv.org/pdf/2512.15632"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes AllSky, a physically-based sky-model learned directly from captured HDR imagery to generate accurate environment maps for image-based lighting. It demonstrates that current DNN-based sky-models fail to match the photorealism and full dynamic range of physically captured imagery. The work concludes that AllSky provides intuitive user control and achieves state-of-the-art performance, highlighting the limitations of existing models for accurate scene relighting."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251218] Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [3D Gaussian Splatting, triangle mesh, hybrid representation, differentiable rendering, volumetric rendering, neural networks]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola, Giljoo Nam"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Meta Codec Avatars Lab, Meta Reality Labs, Stellon Labs"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15711",children:"https://arxiv.org/pdf/2512.15711"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces Gaussian Pixel Codec Avatars (GPiCA), a hybrid avatar representation that combines a textured triangle mesh and 3D Gaussians within a unified differentiable rendering pipeline. This approach uses neural networks to decode a facial expression code into the mesh, texture, and Gaussians, which are rendered together. The method achieves the realism of Gaussian-based avatars while maintaining the rendering efficiency of mesh-based avatars, enabling photorealistic performance on mobile devices."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] Hierarchical Neural Surfaces for 3D Mesh Compression"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [3D geometry processing], [implicit neural representations, spherical parameterization, displacement vector field, hierarchical structure, spherical harmonics]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sai Karthikey Pentapati, Gregoire Phillips, Alan Bovik"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The University of Texas at Austin, Ericsson Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15985",children:"https://arxiv.org/pdf/2512.15985"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces a method for 3D mesh compression using a hierarchical implicit neural representation (INR) built upon a spherical parameterization of the mesh. The INR encodes a displacement vector field to reconstruct the original shape, first recovering coarse structure and then adding high-frequency details. The approach achieves a state-of-the-art trade-off between reconstruction quality and compressed representation size, enabling real-time decoding of meshes at arbitrary resolutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [computer vision, computer graphics], [Gaussian Splatting, neural radiance fields, triangulated surface, view-dependent neural texture, relightable Gaussian model, albedo texture, segmentation annotations]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Haodi He, Jihun Yu, Ronald Fedkiw"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Epic Games, Stanford University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16397",children:"https://arxiv.org/pdf/2512.16397"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a method for creating high-fidelity facial avatars by using Gaussian Splatting, constrained to an underlying triangulated surface, to reconstruct geometry and texture from a small set of uncalibrated images. The approach enables the generation of a standard mesh and a delit, high-resolution albedo texture for use in traditional graphics pipelines. The main conclusion is that this method facilitates scalable and democratized avatar creation, demonstrating utility in applications like text-driven asset generation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [computer vision], [multi-scale attention, hierarchical residual encoding, spatial-and-channel attention, adaptive multi-scale feature fusion, Pix2PixHD, masked-MSE, VGG loss, edge loss, patch-LPIPS loss]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Hossein Javidnia"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Trinity College Dublin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16511",children:"https://arxiv.org/pdf/2512.16511"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces MAGINet, a multi-scale attention-guided network that predicts a light-normalized diffuse albedo map from a single RGB face image and then uses it to generate a full set of physically based rendering passes. The method achieves state-of-the-art performance for intrinsic decomposition, enabling high-quality face relighting and material editing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [diffusion inference], [autoregressive neural rendering, ControlNet, ControlLoRA, G-buffer conditioning, temporal consistency, environment-specific training]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ole Beisswenger, Jan-Niklas Dihlmann, Hendrik P.A. Lensch"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16670",children:"https://arxiv.org/pdf/2512.16670"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," FrameDiffuser is an autoregressive diffusion framework that generates photorealistic and temporally consistent video frames for interactive applications by conditioning on incoming G-buffer data and its own previous output. It uses a dual-conditioning architecture with ControlNet for structural guidance and ControlLoRA for temporal coherence. The paper concludes that environment-specific training of this model yields superior visual quality and consistency compared to generalized approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] SDFoam: Signed-Distance Foam for explicit surface reconstruction"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [signed distance field, Voronoi diagram, ray tracing, Eikonal regularization, neural radiance fields, 3D Gaussian Splatting, mesh reconstruction]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Antonella Rech, Nicola Conci, Nicola Garau"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Trento, CNIT"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16706",children:"https://arxiv.org/pdf/2512.16706"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces SDFoam, a hybrid method that jointly learns an implicit Signed Distance Field (SDF) and an explicit Voronoi Diagram to improve explicit surface reconstruction. It optimizes the scene via ray tracing with Eikonal regularization, aligning Voronoi cell faces with the SDF's zero level set. This approach achieves more accurate mesh reconstruction with comparable rendering speed and visual quality to prior methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251219] Sceniris: A Fast Procedural Scene Generation Framework"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [procedural scene generation, batch sampling, GPU acceleration, collision checking, robot reachability, cuRobo]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jinghuan Shang, Harsh Patel, Ran Gong, Karl Schmeckpeper"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Robotics and AI Institute, University of Waterloo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16896",children:"https://arxiv.org/pdf/2512.16896"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Sceniris is a fast procedural scene generation framework that uses batch sampling and GPU-accelerated collision checking via cuRobo to create large-scale, collision-free 3D scenes. It achieves at least 234x speed-up over prior methods and includes optional robot reachability checks for manipulation tasks."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}}}]);