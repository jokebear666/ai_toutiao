"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2216],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},96533:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_AI/20260105-20260111","title":"20260105-20260111 (cs.AI)","description":"2026-01-05","source":"@site/docs/daily/cs_AI/20260105-20260111.md","sourceDirName":"daily/cs_AI","slug":"/daily/csai/20260105-20260111","permalink":"/ai_toutiao/daily/csai/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/csai/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.AI)","permalink":"/ai_toutiao/daily/csai/20251229-20260104"},"next":{"title":"cs.AR","permalink":"/ai_toutiao/category/csar"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/csai/20260105-20260111"},o="20260105-20260111 (cs.AI)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mermaid:"mermaid",mn:"mn",mrow:"mrow",msub:"msub",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260105-20260111-csai",children:"20260105-20260111 (cs.AI)"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [retrieval-augmented generation], [Monte Carlo Tree Search, reasoning-aware retrieval, coarse-to-fine retrieval, multi-turn dialogue, knowledge diversity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuqi Liu, Bowei He, Chen Ma, Linqi Song"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong, City University of Hong Kong Shenzhen Research Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00003",children:"https://arxiv.org/pdf/2601.00003"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a reasoning-aware knowledge retrieval method that aligns retrieved information with the logical structure of conversations, moving beyond semantic similarity. 2. Introduces a coarse-to-fine retrieval approach that first finds a contextually relevant knowledge sub-region and then refines it for reasoning-specific knowledge. 3. Employs a Monte Carlo Tree Search-inspired method to navigate knowledge sentences using common keywords, enhancing retrieval diversity and informativeness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b609f46963ae2b2b017ba13f445da7491064f311d7e663fa59eda7b85dd69638_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b609f46963ae2b2b017ba13f445da7491064f311d7e663fa59eda7b85dd69638_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of integrating retrieval and reasoning for LLMs by proposing a reasoning-aware knowledge retrieval method. It uses a coarse-to-fine approach guided by Monte Carlo Tree Search to find knowledge aligned with conversational logic. Experiments show the method better captures human reasoning and produces more diverse, informative responses."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LLMs struggle to integrate retrieval and reasoning effectively."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Coarse-to-fine, MCTS-inspired reasoning-aware knowledge retrieval."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Better alignment with human reasoning, more diverse and informative responses."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [game theory], [MinDist, opponent modeling, rule-based strategy, zero-sum game, heuristic optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Purushottam Saha, Avirup Chakraborty, Sourish Sarkar, Subhamoy Maitra, Diganta Mukherjee, Tridib Mukherjee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Statistical Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00024",children:"https://arxiv.org/pdf/2601.00024"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a new hand-evaluation metric called MinDist, which quantifies the edit distance to a valid hand, improving upon the MinScore metric. 2. Designed a computationally efficient algorithm to calculate MinDist using dynamic pruning and pattern caching. 3. Integrated opponent hand-modeling within a two-player zero-sum simulation framework and validated the strategy's superiority through statistical hypothesis testing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6b73a0cbcb5f1c250c8adbb982411c12c8e09c81114767cb920146c72264f7e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6b73a0cbcb5f1c250c8adbb982411c12c8e09c81114767cb920146c72264f7e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a rule-based strategy for Classic Indian Rummy using a new hand-evaluation metric called MinDist, which measures the structural proximity to a winning hand. The method includes an efficient algorithm to compute MinDist and incorporates opponent modeling in simulations. Empirical results show that agents using this strategy achieve significantly higher win rates compared to traditional heuristic-based agents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6e38\u620f\u7b56\u7565\u8bbe\u8ba1/Strategy Design for Rummy]\n    C --\x3e C1[\u65b0\u5ea6\u91cf\u6807\u51c6 MinDist/New Metric MinDist]\n    C --\x3e C2[\u9ad8\u6548\u7b97\u6cd5\u4e0e\u5bf9\u624b\u5efa\u6a21/Efficient Algorithm & Opponent Modeling]\n    D --\x3e D1[\u80dc\u7387\u663e\u8457\u63d0\u5347/Significant Win Rate Improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [anomaly detection], [class imbalance, synthetic dataset, generalization error, unsupervised methods, semi-supervised methods]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," McMaster University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00005",children:"https://arxiv.org/pdf/2601.00005"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comprehensive, problem-agnostic evaluation of 14 anomaly detectors under simulated industrial constraints of extreme class imbalance. 2. Identified that the best-performing detector depends critically on the absolute number of faulty examples available, not just the imbalance ratio, and provided thresholds for method selection. 3. Demonstrated the nuanced impact of feature dimensionality on method performance, showing semi-supervised methods gain advantage in higher dimensions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates anomaly detection algorithms for industrial problems with extreme class imbalance using a synthetic dataset. It benchmarks 14 detectors across varying anomaly rates and training sizes, finding that the optimal detector depends on the absolute number of faulty examples, with unsupervised methods best for very few faults and supervised/semi-supervised methods improving with 30-50 faults. The study highlights performance drops on smaller datasets and provides practical deployment insights."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Extreme class imbalance in industrial applications due to limited faulty data]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Benchmark 14 detectors on synthetic hyperspherical dataset with varying anomaly rates and training sizes]\n    D[\u5173\u952e\u7ed3\u679c/Results: Best detector depends on number of faulty examples; Unsupervised dominates with <20 faults; Supervised/semi-supervised improve with 30-50 faults]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [logistics optimization], [workload balancing, evolutionary algorithms, k-means, last-mile delivery, hybrid algorithm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Luis M. Moreno-Saavedra, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, David Casillas-Perez, Sancho Salcedo-Sanz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad de Alcal\xe1, Universidad Rey Juan Carlos"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00023",children:"https://arxiv.org/pdf/2601.00023"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a multi-algorithm methodology for operational human resources workload balancing in last-mile delivery, moving beyond simple geographical assignment. 2. Introduces and combines several algorithmic approaches, including different versions of k-means, evolutionary algorithms, recursive assignments, and a hybrid evolutionary ensemble. 3. Validates the proposed approach by applying it to a real-world case study of a delivery workforce in Azuqueca de Henares, Spain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unbalanced workload distribution among delivery workers in last-mile urban logistics. It proposes a multi-algorithm approach that uses a combination of distance and workload considerations, including k-means variants and evolutionary algorithms, to assign packages and balance daily effort. The method was successfully tested on a real-world delivery system in Spain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A multi-algorithm approach for operational human resources workload balancing<br>\u591a\u7b97\u6cd5\u65b9\u6cd5\u7528\u4e8e\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7684\u4eba\u529b\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Unbalanced workload among delivery workers in last-mile systems<br>\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u5feb\u9012\u5458\u5de5\u4f5c\u91cf\u4e0d\u5747\u8861"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Multi-algorithm approach combining k-means, evolutionary algorithms, and hybrid ensembles<br>\u7ed3\u5408k-means\u3001\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u96c6\u6210\u7684\u591a\u7b97\u6cd5\u65b9\u6cd5"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Successfully applied to a real-world delivery workforce, balancing workload<br>\u6210\u529f\u5e94\u7528\u4e8e\u73b0\u5b9e\u914d\u9001\u56e2\u961f\uff0c\u5b9e\u73b0\u4e86\u5de5\u4f5c\u91cf\u5747\u8861"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [mental health language modeling], [large language models, fine-tuning, PHQ-9, Nigerian Pidgin, depression screening]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00004",children:"https://arxiv.org/pdf/2601.00004"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Finetuning LLMs for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited depression screening in Nigeria due to language barriers and lack of clinicians]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Fine-tune LLMs on annotated Nigerian Pidgin dataset for PHQ-9 scoring]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>GPT-4.1 achieved 94.5% accuracy and best cultural appropriateness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Toward a Physical Theory of Intelligence"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [artificial general intelligence theory], [Conservation-Congruent Encoding (CCE), irreversible information processing, goal-directed work, attractor dynamics, physical constraints]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Peter David Fagan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Edinburgh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00021",children:"https://arxiv.org/pdf/2601.00021"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Conservation-Congruent Encoding (CCE) framework, linking information to physical states via metastable basins of attraction enforced by conservation laws. 2. Defines intelligence as a physical efficiency metric (goal-directed work per nat of irreversibly processed information) and derives a hierarchy of physical constraints for intelligent systems. 3. Applies the theory to analyze biological intelligence (e.g., brain dynamics) and proposes a physically-grounded perspective on AI safety based on irreversible information flow."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/179838e256390d3a11169de88d6766d3dcb9d44013014cdc9841b568ca836dcd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/179838e256390d3a11169de88d6766d3dcb9d44013014cdc9841b568ca836dcd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a physical theory of intelligence based on irreversible information processing in systems constrained by conservation laws. It introduces the Conservation-Congruent Encoding (CCE) framework to link information to physical states and defines intelligence as the efficiency of converting processed information into goal-directed work. The theory provides a unified, substrate-neutral account, deriving fundamental constraints and applying them to analyze biological systems and AI safety."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Toward a Physical Theory of Intelligence] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u7684\u667a\u80fd\u7edf\u4e00\u7406\u8bba/Lack of a unified physical theory of intelligence]\n    C --\x3e C1[\u5b88\u6052\u4e00\u81f4\u7f16\u7801\u6846\u67b6/Conservation-Congruent Encoding (CCE) Framework]\n    C --\x3e C2[\u667a\u80fd\u7684\u7269\u7406\u5b9a\u4e49/Physical definition of intelligence]\n    D --\x3e D1[\u63a8\u5bfc\u51fa\u7269\u7406\u7ea6\u675f\u5c42\u7ea7/Derives hierarchy of physical constraints]\n    D --\x3e D2[\u5e94\u7528\u4e8e\u751f\u7269\u7cfb\u7edf\u4e0e\u5206\u6790/Applied to biological systems & analysis]\n    D --\x3e D3[\u63d0\u51faAI\u5b89\u5168\u7684\u7269\u7406\u89c6\u89d2/Proposes physically-grounded AI safety perspective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [policy gradient, self-play, Markov Decision Process, Advantage Actor-Critic, ablation study]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nicholas A. Pape"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00007",children:"https://arxiv.org/pdf/2601.00007"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates the classic stochastic combinatorial game Yahtzee as a Markov Decision Process and establishes it as a mid-scale RL benchmark. 2. Conducts a comprehensive empirical study comparing REINFORCE, A2C, and PPO under a fixed training budget, identifying A2C as the most robust method. 3. Achieves a median score within 5% of the optimal dynamic programming solution, while analyzing persistent challenges like long-horizon credit assignment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates using deep reinforcement learning to play the full solitaire version of Yahtzee. It trains self-play agents with policy gradient methods (REINFORCE, A2C, PPO) and performs ablation studies on various design choices. The main finding is that A2C robustly achieves near-optimal performance, while all methods struggle with long-term strategic elements like securing the upper section bonus."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Yahtzee as a mid-scale RL benchmark with delayed rewards & combinatorial complexity] --\x3e B1[\u76ee\u6807/Objectives<br>Can RL achieve near-optimal performance via self-play?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Formulate as MDP, train self-play agents with policy gradient methods (REINFORCE, A2C, PPO)] --\x3e C1[\u6280\u672f/Techniques<br>Ablation on encodings, architecture, estimators, entropy]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>A2C is robust & achieves median score within 5% of optimal DP score] --\x3e D1[\u6311\u6218/Challenges<br>Agents struggle with long-horizon strategy (upper bonus)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [ferroelectric synapses, spiking neural networks, EEG signal processing, adaptive learning, neuromorphic computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura B\xe9gon-Lours"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00020",children:"https://arxiv.org/pdf/2601.00020"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: EEG\u4fe1\u53f7\u975e\u5e73\u7a33\u6027\u9650\u5236\u6a21\u578b\u6cdb\u5316\uff0c\u9700\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e2a\u6027\u5316\u9002\u5e94/Non-stationary EEG signals limit model generalization, requiring personalized adaptation on resource-constrained platforms]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5728\u94c1\u7535\u5fc6\u963b\u7a81\u89e6\u4e0a\u90e8\u7f72SNN\uff0c\u91c7\u7528\u8bbe\u5907\u611f\u77e5\u7684\u6743\u91cd\u66f4\u65b0\u7b56\u7565/Deploy SNNs on ferroelectric memristive synapses with a device-aware weight-update strategy]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u4e24\u79cd\u90e8\u7f72\u7b56\u7565\u6027\u80fd\u5ab2\u7f8e\u8f6f\u4ef6SNN\uff0c\u7279\u5b9a\u5bf9\u8c61\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u51c6\u786e\u7387/Two deployment strategies achieve performance comparable to software SNNs, subject-specific transfer learning improves accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [generative AI, diffusion models, architectural intelligence, computational reasoning, vernacular architecture]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abolhassan Pishahang, Maryam Badiei"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida Atlantic University, North Carolina State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00029",children:"https://arxiv.org/pdf/2601.00029"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI\'s interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of "computational vernacular reasoning" as an analytical framework.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\nRoot["From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: How does generative AI interpret the architectural intelligence embedded in vernacular forms?"]\nRoot --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Test three diffusion models across three prompt stages (referential, adaptive, speculative) using a five-criteria evaluation framework."]\nRoot --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: AI reproduces geometry but misreads material/climatic reasoning; reference aids realism but limits creativity."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [LLM Security], [Go-Explore, Prompt Injection, Adversarial Testing, Agent Safety, Multi-Hop Attacks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Manish Bhatt, Adrian Wood, Idan Habler, Ammar Al-Kahfah"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," OWASP, Amazon, Dropbox, CISCO, AWS"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00042",children:"https://arxiv.org/pdf/2601.00042"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/mbhatt1/competitionscratch",children:"https://github.com/mbhatt1/competitionscratch"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Adapted the Go-Explore reinforcement learning algorithm for systematic security testing of LLM agents. 2. Conducted a large-scale empirical study revealing that random seed variance dominates algorithmic parameter choices in this domain. 3. Provided actionable insights for practitioners, such as the ineffectiveness of reward shaping and the benefits of using ensembles and simple state signatures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper adapts the Go-Explore algorithm to test the security of safety-trained LLM agents against prompt injection attacks. Through 28 experimental runs on GPT-4o-mini, the study finds that random seed variance is a major factor, reward shaping is harmful, and simple state signatures work best. The results suggest that managing seed variance and applying domain knowledge are more critical than algorithmic sophistication for effective security testing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing<br/>\u5927\u578b\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\uff1a\u7528\u4e8eAI\u7ea2\u961f\u6d4b\u8bd5\u7684Go-Explore"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Testing security of safety-trained LLM agents<br/>\u6d4b\u8bd5\u7ecf\u8fc7\u5b89\u5168\u8bad\u7ec3\u7684LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027"] --\x3e P1["Prompt Injection<br/>\u63d0\u793a\u6ce8\u5165"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Adapt Go-Explore algorithm<br/>\u6539\u7f16Go-Explore\u7b97\u6cd5"] --\x3e M1["Systematic exploration from archive<br/>\u4ece\u5b58\u6863\u8fdb\u884c\u7cfb\u7edf\u63a2\u7d22"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Key Findings<br/>\u5173\u952e\u53d1\u73b0"] --\x3e R1["Seed variance dominates<br/>\u79cd\u5b50\u65b9\u5dee\u5360\u4e3b\u5bfc"]\n    Results --\x3e R2["Reward shaping harms performance<br/>\u5956\u52b1\u5851\u5f62\u635f\u5bb3\u6027\u80fd"]\n    Results --\x3e R3["Simple signatures outperform<br/>\u7b80\u5355\u7b7e\u540d\u6548\u679c\u66f4\u597d"]\n    Results --\x3e R4["Ensembles provide diversity<br/>\u96c6\u6210\u63d0\u4f9b\u591a\u6837\u6027"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal reasoning], [fuzzy cognitive maps, large-language-model agent, causal feedback, equilibrium limit cycles, agentic leash]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Southern California, Florida International University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00097",children:"https://arxiv.org/pdf/2601.00097"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel LLM agent designed to autonomously extract and construct causal feedback Fuzzy Cognitive Maps (FCMs) from raw text. 2. A three-step instruction-guided process for systematically extracting key concepts and causal edges to build the FCM dynamical system. 3. Demonstration that the LLM-generated FCMs converge to the same equilibrium dynamics as human-generated ones and that mixed FCMs from different LLMs can create new equilibria."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an LLM agent to autonomously extract causal feedback Fuzzy Cognitive Maps from text. The agent uses a three-step process to identify concepts and causal edges, forming a dynamical system. The generated FCMs matched human-generated equilibrium dynamics and mixing models from different LLMs produced new equilibria for better causal approximation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to autonomously extract causal structures from text?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Design an LLM agent with a three-step instruction process to build FCMs from text.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: LLM-generated FCMs match human equilibrium dynamics; mixed FCMs create new equilibria.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Mortar: Evolving Mechanics for Automatic Game Design"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [procedural content generation], [quality-diversity algorithm, large language model, skill-based ordering, tree search, game mechanics evolution]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Muhammad U. Nasir, Yuchen Li, Steven James, Julian Togelius"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of the Witwatersrand, New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00105",children:"https://arxiv.org/pdf/2601.00105"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Mortar, a system that autonomously evolves game mechanics for automatic game design by combining a quality-diversity algorithm with a large language model. 2. Introduces a novel evaluation framework where mechanics are assessed by synthesizing complete games and measuring their ability to preserve a skill-based ordering over players. 3. Demonstrates the system's effectiveness through ablation studies and a user study, showing it produces diverse, playable games with mechanics that improve skill-based ordering."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26506e29345695337e5d47f5dfb1499fe23b996958dc6fd30e9b4371545d6994_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26506e29345695337e5d47f5dfb1499fe23b996958dc6fd30e9b4371545d6994_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Mortar, a system for automatic game design that evolves game mechanics by combining a quality-diversity algorithm with an LLM. It evaluates mechanics by constructing complete games via tree search and testing if they preserve a skill-based player ordering. The results show Mortar generates diverse, playable games with mechanics that effectively contribute to skill-based gameplay."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[MORTAR: Evolving Mechanics for Automatic Game Design] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u624b\u52a8\u8bbe\u8ba1\u6e38\u620f\u673a\u5236\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6/Manual game mechanic design is time-consuming and expert-driven]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u7ed3\u5408\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u4e0eLLM/Combines quality-diversity algorithm with LLM]\n    Method --\x3e M2[\u901a\u8fc7\u6811\u641c\u7d22\u5408\u6210\u5b8c\u6574\u6e38\u620f\u8fdb\u884c\u8bc4\u4f30/Evaluates mechanics by synthesizing games via tree search]\n    Method --\x3e M3[\u8bc4\u4f30\u6280\u80fd\u6392\u5e8f\u4fdd\u6301\u80fd\u529b/Measures ability to preserve skill-based player ordering]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u751f\u6210\u591a\u6837\u4e14\u53ef\u73a9\u7684\u6e38\u620f/Produces diverse and playable games]\n    Results --\x3e R2[\u673a\u5236\u63d0\u5347\u6280\u80fd\u6392\u5e8f\u5206\u6570/Mechanics improve skill-based ordering score]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [hybrid agentic framework, hallucination tax, Human Imitator, stochastic reasoning, inventory optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yaqi Duan, Yichun Hu, Jiashuo Jiang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University, Cornell University, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00121",children:"https://arxiv.org/pdf/2601.00121"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and quantifies the "hallucination tax" when using LLMs as end-to-end solvers for inventory control, highlighting their limitation in grounded stochastic reasoning. 2. Proposes a novel hybrid agentic framework that decouples semantic reasoning (handled by LLM) from mathematical calculation (handled by rigorous algorithms) to create an intelligent interface for optimization. 3. Introduces the "Human Imitator," a fine-tuned digital twin of a boundedly rational manager, to enable scalable and reproducible stress-testing of interactive systems against ambiguous real-world dialogue.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5522adbd58df6d202685913f2a038b91f8e5b3730f51d9964297ad57db17174e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5522adbd58df6d202685913f2a038b91f8e5b3730f51d9964297ad57db17174e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of using LLMs for inventory management by showing that direct application incurs a "hallucination tax" due to poor stochastic reasoning. To solve this, the authors propose a hybrid agentic framework where the LLM acts as a natural-language interface that calls rigorous optimization algorithms. The framework reduces inventory costs by 32.1% compared to an LLM-only baseline, demonstrating that LLMs are best used as interfaces to make expert methods accessible, not as replacements for them.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Ask, Clarify, Optimize<br>Human-LLM Agent Collaboration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLMs as end-to-end solvers incur "hallucination tax"<br>LLMs\u65e0\u6cd5\u8fdb\u884c\u53ef\u9760\u7684\u968f\u673a\u63a8\u7406]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Hybrid Agentic Framework<br>LLM\u4f5c\u4e3a\u63a5\u53e3\uff0c\u8c03\u7528\u4e25\u683c\u7b97\u6cd5<br>\u8bed\u4e49\u63a8\u7406\u4e0e\u6570\u5b66\u8ba1\u7b97\u89e3\u8026]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Cost reduced by 32.1% vs baseline<br>LLMs are interfaces, not replacements<br>\u74f6\u9888\u662f\u8ba1\u7b97\u800c\u975e\u4fe1\u606f]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Constructing a Neuro-Symbolic Mathematician from First Principles"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neuro-symbolic reasoning], [neuro-symbolic architecture, differentiable logic engine, hypergraph transformer, energy minimization, proof search]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Keqin Xie"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," (Inferred from email domain: ",(0,r.jsx)(n.a,{href:"mailto:xiekeqin30@gmail.com",children:"xiekeqin30@gmail.com"}),") Affiliation not explicitly stated in provided text. Could be an independent researcher or institution not listed on first page."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00125",children:"https://arxiv.org/pdf/2601.00125"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs. 2. Introduces a Symbolic Reasoning Kernel (SRK), a differentiable logic engine that maps logical constraints to a continuous energy landscape for gradient-based training. 3. Enables multi-step deduction by combining energy minimization with guided search algorithms like Monte Carlo Tree Search and Evolutionary Proof Search."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/782595f0879bde3e0c09f37353b140392ee9407cf2eed312d3faea5d4ca981ae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/782595f0879bde3e0c09f37353b140392ee9407cf2eed312d3faea5d4ca981ae_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the issue of logical failures in Large Language Models (LLMs) during complex reasoning by proposing Mathesis, a neuro-symbolic system that uses a differentiable logic engine to turn proof search into energy minimization. The method combines a Hypergraph Transformer Brain with symbolic reasoning via gradient signals, aiming to achieve rigorous mathematical deduction. The core conclusion is that this architecture provides a pathway to integrate neural pattern recognition with symbolic rigor for reliable reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Constructing a Neuro-Symbolic Mathematician] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs lack axiomatic framework, leading to logical failures]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Neuro-symbolic architecture (Mathesis) with differentiable logic engine (SRK) for energy minimization]\n    D[\u5173\u952e\u7ed3\u679c/Results: Proof search framed as energy minimization, enabling gradient-based training and guided multi-step deduction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Explicit Abstention Knobs for Predictable Reliability in Video Question Answering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [selective prediction], [selective prediction, confidence-based abstention, risk-coverage tradeoff, distribution shift, video question answering]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jorge Ortiz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rutgers University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00138",children:"https://arxiv.org/pdf/2601.00138"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model's confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Explicit Abstention Knobs for Predictable Reliability in Video Question Answering] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Can confidence-based abstention provide reliable error rate control in video QA, especially under distribution shift?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use confidence thresholding on a VLM (Gemini 2.0 Flash) evaluated on the NExT-QA dataset, testing under evidence degradation.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: In-distribution control works, but confidence fails to decrease under shift, motivating warrant-based prediction.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural reasoning], [Sphere Neural Networks, disjunctive syllogistic reasoning, catastrophic forgetting, explicit model construction, Euler Net]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tiansi Dong, Henry He, Pietro Li\xf2, Mateja Jamnik"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Cambridge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00142",children:"https://arxiv.org/pdf/2601.00142"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A comparative analysis showing explicit model-based neural reasoning is more reliable than LLM-based or supervised learning-based reasoning. 2. The proposal of Sphere Neural Networks, a novel architecture that embeds concepts as circles on a sphere to represent negation and filter illogical statements. 3. An empirical demonstration that Sphere Neural Networks can master 16 syllogistic reasoning tasks, including disjunctive syllogism, without catastrophic forgetting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5204d88cd2c0986ce1fa0776ae031033771470697a797f7d2ee4cba326a5a31_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5204d88cd2c0986ce1fa0776ae031033771470697a797f7d2ee4cba326a5a31_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Sphere Neural Networks, which embed concepts as circles on a sphere to enable explicit model-based reasoning. The method is shown to reliably perform complex syllogistic reasoning tasks without catastrophic forgetting, unlike supervised learning approaches. The authors conclude that explicit model construction is the most reliable category for neural reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LLMs and supervised learning are unreliable for simple reasoning tasks"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose Sphere Neural Networks using concept circles on a sphere"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Mastered 16 syllogistic tasks, no catastrophic forgetting, explicit models are most reliable"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Qifu Technology, Fudan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00150",children:"https://arxiv.org/pdf/2601.00150"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u91d1\u878d\u4fe1\u8d37\u9886\u57df\u4e13\u7528\u591a\u6a21\u6001\u57fa\u51c6/Lack of domain-specific multimodal benchmark for financial credit]\n    C --\x3e C1[\u6784\u5efa\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210-\u91c7\u96c6\u7ba1\u9053/Build privacy-compliant synthesis-capture pipeline]\n    C --\x3e C2[\u8bbe\u8ba1\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]\n    D --\x3e D1[\u8bc4\u4f3023\u4e2aVLM/Evaluate 23 VLMs]\n    D --\x3e D2[Qfin-VL-Instruct\u6027\u80fd\u6700\u4f73/Qfin-VL-Instruct achieves top score]\n    D --\x3e D3[\u9c81\u68d2\u6027\u6311\u6218/Robustness remains a challenge]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junkai Luo, Yinglun Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Riverside"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00167",children:"https://arxiv.org/pdf/2601.00167"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Online Finetuning Decision Transformers with Pure RL Gradients] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u5728\u7ebf\u5fae\u8c03DT\u65f6\uff0c\u7eafRL\u68af\u5ea6\u65b9\u6cd5\u672a\u88ab\u63a2\u7d22/Pure RL gradients for online DT finetuning unexplored]\nB --\x3e B2[\u540e\u89c1\u4e4b\u76ca\u56de\u62a5\u91cd\u6807\u6ce8\u4e0eRL\u7b97\u6cd5\u4e0d\u517c\u5bb9/Hindsight return relabeling incompatible with RL]\nC --\x3e C1[\u9002\u914dGRPO\u81f3DT/Adapt GRPO to DTs]\nC --\x3e C2[\u5f15\u5165\u5173\u952e\u4fee\u6539: \u5b50\u8f68\u8ff9\u4f18\u5316\u7b49/Introduce key modifications]\nD --\x3e D1[\u8d85\u8d8a\u73b0\u6709\u5728\u7ebfDT\u57fa\u7ebf/Outperform online DT baselines]\nD --\x3e D2[\u5b9e\u73b0SOTA\u6027\u80fd/Achieve SOTA performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [semi-supervised learning], [Generative Adversarial Network, Swin Transformer, spike classification, semi-supervised learning, Bayesian optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Deakin University, CSIRO Health and Biosecurity, Islamic Azad University, University of Southern Queensland"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00189",children:"https://arxiv.org/pdf/2601.00189"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel semi-supervised GAN architecture (SSI-GAN) with a Swin-inspired, shifted-window discriminator for neuronal spike classification. 2. Introduced a transformer-based generator and a flat, window-based transformer discriminator with multi-head self-attention to capture sparse, high-frequency spike features. 3. Demonstrated state-of-the-art performance with 99.93% accuracy using only 1-3% labeled data, reducing manual labeling effort by 97-99% compared to supervised methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the labor-intensive problem of classifying mosquito neuronal spikes for arboviral disease detection by proposing SSI-GAN, a semi-supervised GAN that combines a Swin-inspired discriminator with a transformer-based generator. Using only 1-3% labeled data from over 15 million spike samples, it achieved up to 99.93% accuracy in classifying Zika-infected, dengue-infected, or uninfected categories, significantly reducing labeling effort while outperforming baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SSI-GAN: \u534a\u76d1\u7763Swin\u542f\u53d1\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7528\u4e8e\u795e\u7ecf\u5143\u5c16\u5cf0\u5206\u7c7b] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u868a\u866b\u795e\u7ecf\u5143\u5c16\u5cf0\u6a21\u5f0f\u624b\u52a8\u5206\u7c7b\u52b3\u52a8\u5bc6\u96c6\u4e14\u6602\u8d35\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5168\u6807\u8bb0\u6570\u636e\u548c\u9ad8\u5ea6\u9884\u5904\u7406]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faSSI-GAN\uff0c\u4f7f\u7528Swin\u542f\u53d1\u7684\u79fb\u4f4d\u7a97\u53e3\u5224\u522b\u5668\u548c\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5668\uff0c\u4ec5\u97001-3%\u6807\u8bb0\u6570\u636e]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u8fbe\u523099.93%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6807\u8bb0\u5de5\u4f5c\u91cf\u51cf\u5c1197-99%\uff0c\u5728\u6240\u6709\u611f\u67d3\u9636\u6bb5\u4fdd\u6301\u9ad8\u7cbe\u5ea6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [emotion recognition in conversation], [ablation study, conversational context, discourse markers, causal context, IEMOCAP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Cheonkam Jeong, Adeline Nyamathi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Irvine"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00181",children:"https://arxiv.org/pdf/2601.00181"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. A rigorous ablation study revealing that conversational context is paramount for ERC, with performance saturating within 10-30 preceding turns, and that hierarchical sentence representations and external affective lexicons provide no additional benefit when context is used. 2. Achieving state-of-the-art text-only performance on IEMOCAP using simple architectures with strictly causal context. 3. A novel linguistic analysis connecting recognition to generation, finding a significant association between emotion and discourse marker positioning, particularly that "sad" utterances use fewer left-periphery markers and rely more on context for disambiguation.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e38dfbfb3b50e432c39aabed201ddaf199012203ee5e7e82a7aa044267fc2a2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e38dfbfb3b50e432c39aabed201ddaf199012203ee5e7e82a7aa044267fc2a2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper systematically analyzes Emotion Recognition in Conversation (ERC) to identify which architectural components matter and connects recognition insights to linguistic patterns for generation. Through ablation studies on IEMOCAP, it finds conversational context is most critical, and via linguistic analysis, it discovers that emotion correlates with discourse marker usage. The main conclusion is that simple models with causal context are sufficient for high performance, and the lack of explicit pragmatic signals in "sad" utterances explains their greater reliance on context.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Understanding Emotion in Discourse] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[ERC\u6a21\u578b\u54ea\u4e9b\u7ec4\u4ef6\u771f\u6b63\u6709\u6548?/Which ERC components matter?]\n    B --\x3e B2[\u5982\u4f55\u8fde\u63a5\u8bc6\u522b\u4e0e\u751f\u6210?/How to connect recognition & generation?]\n    C --\x3e C1[\u7cfb\u7edf\u6d88\u878d\u7814\u7a76/Systematic Ablation Study]\n    C --\x3e C2[\u8bdd\u8bed\u6807\u8bb0\u5206\u6790/Discourse Marker Analysis]\n    D --\x3e D1[\u4e0a\u4e0b\u6587\u6700\u5173\u952e, 10-30\u8f6e\u9971\u548c/Context is key, saturates in 10-30 turns]\n    D --\x3e D2[\u7b80\u5355\u56e0\u679c\u6a21\u578bSOTA/Simple causal model achieves SOTA]\n    D --\x3e D3[\u60b2\u4f24\u8bdd\u8bed\u6807\u8bb0\u5c11, \u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587/Sad utterances have fewer markers, rely more on context]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Latent Flow Matching for Expressive Singing Voice Synthesis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [conditional flow matching, latent space modeling, singing voice synthesis, ordinary differential equation, variational autoencoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Minhyeok Yun, Yong-Hoon Choi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kwangwoon University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00217",children:"https://arxiv.org/pdf/2601.00217"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/alsgur9368/FM-Singer",children:"https://github.com/alsgur9368/FM-Singer"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FM-Singer, a novel singing voice synthesis method that applies conditional flow matching (CFM) in the latent space to address the prior-posterior mismatch in cVAE-based models. 2. Introduces a latent ODE refinement step at inference time to transport prior samples towards the expressive posterior distribution, improving fine-grained expressiveness while maintaining parallel decoding efficiency. 3. Demonstrates consistent performance improvements on Korean and Chinese singing datasets, including lower mel-cepstral distortion and F0 error, and higher perceptual scores."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ff2d99db594dd8c63fa46f401cf9408cda76d76d116f920d5a1dbb077b2fcf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ff2d99db594dd8c63fa46f401cf9408cda76d76d116f920d5a1dbb077b2fcf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the degradation of fine-grained expressiveness in cVAE-based singing voice synthesis due to a mismatch between the prior and posterior latent distributions. The proposed FM-Singer method uses conditional flow matching in the latent space to learn a vector field that refines prior samples via an ODE, enhancing expressiveness like vibrato. Experiments show the method outperforms strong baselines on multiple datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Latent Flow Matching for Expressive Singing Voice Synthesis] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[cVAE\u5408\u6210\u4e2d\u5148\u9a8c-\u540e\u9a8c\u4e0d\u5339\u914d<br/>Prior-Posterior Mismatch in cVAE]\n    B --\x3e B2[\u7ec6\u5fae\u8868\u73b0\u529b\u4e0b\u964d<br/>Degraded Fine-Grained Expressiveness]\n    C --\x3e C1[\u6f5c\u5728\u7a7a\u95f4\u6761\u4ef6\u6d41\u5339\u914d<br/>Latent-Space Conditional Flow Matching]\n    C --\x3e C2[ODE\u63a8\u7406\u65f6\u7ec6\u5316<br/>ODE-based Refinement at Inference]\n    D --\x3e D1[\u66f4\u4f4e\u7684MCD\u4e0eF0\u8bef\u5dee<br/>Lower MCD & F0 Error]\n    D --\x3e D2[\u66f4\u9ad8\u7684\u611f\u77e5\u8bc4\u5206<br/>Higher Perceptual Scores]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [machine translation], [LLM-as-a-judge, pairwise comparison, Bradley-Terry model, reference-free evaluation, anchored evaluation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Leonard Lin, Adam Lensenmayer"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shisa.AI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00223",children:"https://arxiv.org/pdf/2601.00223"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces JP-TL-Bench, a lightweight, open benchmark for iterative development of Japanese-English translation systems. 2. Proposes a reliable and affordable evaluation protocol using reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. 3. Provides structurally stable scores by aggregating pairwise results with a Bradley-Terry model and reporting normalized LT scores."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3672d8de20107e284402d4b12e978f246eb0df92617095708ca7710e712594d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3672d8de20107e284402d4b12e978f246eb0df92617095708ca7710e712594d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces JP-TL-Bench, a benchmark for evaluating high-quality Japanese-English translation. It uses a protocol where candidate models are compared against a fixed anchor set via pairwise LLM judgments, with results aggregated using a Bradley-Terry model to produce stable scores. This approach aims to provide a high-resolution signal for distinguishing between already fluent translations where traditional metrics saturate."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[JP-TL-Bench: Anchored Pairwise LLM Evaluation<br>JP-TL-Bench: \u951a\u5b9a\u6210\u5bf9LLM\u8bc4\u4f30] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u9ad8\u8d28\u91cf\u7ffb\u8bd1<br>Existing evaluation struggles to differentiate high-quality translations]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8e\u56fa\u5b9a\u951a\u5b9a\u96c6\u7684\u6210\u5bf9LLM\u6bd4\u8f83<br>Pairwise LLM comparison against a fixed anchor set]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6570<br>Provides stable, interpretable scores]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [GPU kernels, benchmarking, kernel substitution, FlashInfer Trace, AI-generated code]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shanli Xing, Yiyan Zhai, Alexander Jiang, Yixin Dong, Yong Wu, Zihao Ye, Charlie Ruan, Yingyi Huang, Yineng Zhang, Liangsheng Yin, Aksara Bayyapu, Luis Ceze, Tianqi Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, University of Washington, NVIDIA, University of California, Berkeley"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00227",children:"https://arxiv.org/pdf/2601.00227"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FlashInfer Trace, a unified JSON schema to standardize the description of kernel tasks, workloads, implementations, and evaluations for AI agents. 2. Presents FlashInfer-Bench, a comprehensive framework including a curated dataset from real serving traces, a correctness- and performance-aware benchmarking system, and a public leaderboard. 3. Implements a dynamic kernel substitution mechanism (",(0,r.jsx)(n.code,{children:"apply()"}),") that allows seamless integration of the best-performing AI-generated kernels into production LLM inference engines like SGLang and vLLM."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c4acb494d20ab22aaf93877ea7f6bf9807d9ee81afeaa72f868e4b115633740_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c4acb494d20ab22aaf93877ea7f6bf9807d9ee81afeaa72f868e4b115633740_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FlashInfer-Bench, a standardized framework to bridge the gap between AI-generated GPU kernels and their deployment in real-world LLM inference systems. It provides a closed-loop workflow for kernel generation, benchmarking, and integration, enabling the evaluation of LLM agents' GPU programming capabilities and the seamless injection of optimized kernels into production engines. The work establishes a practical pathway for continuously improving and deploying AI-generated kernels at scale."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Integrating AI-generated GPU kernels into real-world LLM inference systems is challenging]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: A standardized closed-loop framework with FlashInfer Trace schema, benchmarking, and dynamic kernel substitution]\n    D[\u5173\u952e\u7ed3\u679c/Results: Establishes a reproducible pathway for improving and deploying AI-generated kernels in production]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [LoRA, K-FAC, Parameter-Efficient Fine-Tuning, Fisher Information, Dynamic Rank Adaptation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pritish Saha, Chandrav Rajbangshi, Rudra Goyal, Mohit Goyal, Anurag Deo, Biswajit Roy, Ningthoujam Dhanachandra Singh, Raxit Goswami, Amitava Das"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," RAAPID Lab, Pragya Lab (BITS Pilani, Goa)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00231",children:"https://arxiv.org/pdf/2601.00231"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces K-FAC-based gradient preconditioning in the low-rank subspace for more geometry-aware updates. 2. Proposes periodic Fisher-guided reprojection of the LoRA basis to suppress parameter drift. 3. Implements dynamic rank adaptation to concentrate capacity on high-signal directions, reducing the number of trainable parameters."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that standard LoRA/QLoRA methods are geometry-agnostic, leading to inefficient updates and parameter drift. It proposes GRIT, a new LoRA procedure that uses K-FAC preconditioning, Fisher-guided reprojection, and dynamic rank adaptation to make updates more curvature-aware. This approach matches or surpasses baseline performance while reducing trainable parameters by an average of 46% and achieving lower drift."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRIT: Geometry-Aware PEFT] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6807\u51c6LoRA/QLoRA\u5ffd\u7565\u66f2\u7387/Standard LoRA/QLoRA ignores curvature]\n    B --\x3e B2[\u5bfc\u81f4\u4f4e\u6548\u66f4\u65b0\u4e0e\u53c2\u6570\u6f02\u79fb/Causes inefficient updates & parameter drift]\n    C --\x3e C1[K-FAC\u9884\u6761\u4ef6\u68af\u5ea6/K-FAC preconditioned gradients]\n    C --\x3e C2[Fisher\u5f15\u5bfc\u91cd\u6295\u5f71/Fisher-guided reprojection]\n    C --\x3e C3[\u52a8\u6001\u79e9\u9002\u5e94/Dynamic rank adaptation]\n    D --\x3e D1[\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d/Matches or surpasses baselines]\n    D --\x3e D2[\u53c2\u6570\u51cf\u5c11~46%/Reduces parameters by ~46%]\n    D --\x3e D3[\u6f02\u79fb\u66f4\u4f4e/Lower drift]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [intergroup bias, belief poisoning attack, multi-agent social simulation, human-norm script, agent safety]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zongwei Wang, Bincheng Gu, Hongyu Yu, Junliang Yu, Tao He, Jiayin Feng, Min Gao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chongqing University, The University of Queensland, Virginia Polytechnic Institute and State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00240",children:"https://arxiv.org/pdf/2601.00240"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies a new type of bias in LLM-powered agents: intergroup bias that can align with an agent-human divide, potentially treating humans as an outgroup. 2. Proposes a novel attack method, the Belief Poisoning Attack (BPA), with two variants (BPA-PP and BPA-MP) that exploit the belief-dependent nature of agents to suppress pro-human norms and reactivate bias. 3. Discusses practical mitigation strategies for hardening agent frameworks against such attacks, focusing on interventions at profile and memory boundaries."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63aefdbbc5c1919bddd5ac1c591ec07bd99f076d1eb4b0ab4722c5ed75f1d8fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63aefdbbc5c1919bddd5ac1c591ec07bd99f076d1eb4b0ab4722c5ed75f1d8fc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether LLM-powered agents exhibit bias against humans as an outgroup. It finds that while a \"human-norm script\" can attenuate this bias, the script's activation depends on the agent's belief about the presence of a human, creating a vulnerability. The authors introduce a Belief Poisoning Attack to exploit this vulnerability and propose mitigation strategies for safer agent design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Will LLM-powered Agents Bias Against Humans?<br/>\u63a2\u7d22\u4fe1\u5ff5\u4f9d\u8d56\u7684\u8106\u5f31\u6027] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Agent Intergroup Bias & Human Outgroup Risk]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Multi-Agent Simulation & Belief Poisoning Attack (BPA)]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Bias Exists, BPA Effective, Mitigations Proposed]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), Dual-Agent LLM, QLoRA, Vulnerability Detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Hasan Saju, Maher Muhtadi, Akramul Azim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ontario Tech University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00254",children:"https://arxiv.org/pdf/2601.00254"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comparative empirical evaluation of three LLM-based approaches (RAG, SFT, and Dual-Agent) for code vulnerability detection. 2. Proposed a RAG framework that integrates external domain knowledge (e.g., from the internet and MITRE CWE database) to achieve state-of-the-art performance. 3. Introduced and evaluated a Dual-Agent LLM system designed to improve reasoning transparency and error mitigation with reduced resource overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85bf4a637927a306d9960d3a0ec1a16f810bb0755b9400e6b4b384627260154e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85bf4a637927a306d9960d3a0ec1a16f810bb0755b9400e6b4b384627260154e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares three LLM-based methods\u2014RAG, SFT, and a Dual-Agent system\u2014for detecting software vulnerabilities in code. The RAG approach, which augments the LLM with external knowledge, achieved the highest accuracy and F1 score, demonstrating the value of contextual information for this task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection<br>LLM\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u7684\u5b9e\u8bc1\u8bc4\u4f30"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Automated detection of software vulnerabilities in codebases<br>\u4ee3\u7801\u5e93\u4e2d\u8f6f\u4ef6\u6f0f\u6d1e\u7684\u81ea\u52a8\u5316\u68c0\u6d4b"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Comparative study of RAG, SFT, and Dual-Agent LLM frameworks<br>\u5bf9RAG\u3001SFT\u548c\u53cc\u667a\u80fd\u4f53LLM\u6846\u67b6\u7684\u6bd4\u8f83\u7814\u7a76"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>RAG achieved highest accuracy (0.86) & F1 (0.85)<br>RAG\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387(0.86)\u548cF1\u5206\u6570(0.85)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aly Sabri Abdalla, Vuk Marojevic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mississippi State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00257",children:"https://arxiv.org/pdf/2601.00257"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --\x3e P1["\u5b50\u95ee\u9898/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --\x3e M1["\u7ec4\u4ef6/Component: Semantic-aware rApp (terrain interpreter)"]\n    Method --\x3e M2["\u7ec4\u4ef6/Component: RL-enabled xApp (trajectory planner)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results: Framework enables closed-loop, AI-optimized LAE operations"] --\x3e R1["\u8bc4\u4f30/Evaluation: Feasibility and performance analysis presented"]\n    Results --\x3e R2["\u5c55\u671b/Outlook: Research challenges and standardization needs surveyed"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [explainable ai (xai)], [counterfactual examples, multilingual, data augmentation, large language models, model robustness]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Van Bach Nguyen, Yihong Liu, Fedor Splitt, Nils Feldhus, Christin Seifert, Hinrich Sch\xfctze, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, University of Marburg, LMU Munich, German Research Center for Artificial Intelligence (DFKI), Munich Center for Machine Learning (MCML), BIFOLD \u2013 Berlin Institute for the Foundations of Learning and Data"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00263",children:"https://arxiv.org/pdf/2601.00263"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Evaluated the quality of LLM-generated multilingual counterfactuals, comparing direct generation and translation-based methods across six languages. 2. Identified four main error types common in generated counterfactuals across languages and found similar edit patterns in high-resource European languages. 3. Demonstrated that multilingual counterfactual data augmentation yields greater performance improvements than cross-lingual augmentation, especially for lower-resource languages."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3b95c2cf3407989c3c5a932764e908182c4d60d7451ace3f5f15e194a3a7e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3b95c2cf3407989c3c5a932764e908182c4d60d7451ace3f5f15e194a3a7e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the effectiveness of large language models (LLMs) in generating multilingual counterfactual examples. It compares directly generated and translation-based counterfactuals across six languages, finding that translation-based ones are more valid but require more edits and still underperform English ones. The study concludes that while multilingual counterfactual data augmentation improves model performance, especially for low-resource languages, the quality limitations of the generated counterfactuals constrain the gains in robustness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u751f\u6210\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u793a\u4f8b\u7684\u6709\u6548\u6027\u672a\u77e5/Effectiveness of LLM-generated multilingual counterfactuals is unclear]\n    C --\x3e C1[\u8bc4\u4f30\u76f4\u63a5\u751f\u6210\u4e0e\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e/Evaluate directly generated and translation-based counterfactuals]\n    C --\x3e C2[\u5206\u6790\u7f16\u8f91\u6a21\u5f0f\u4e0e\u9519\u8bef\u7c7b\u578b/Analyze edit patterns and error types]\n    C --\x3e C3[\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u5b9e\u9a8c/Use for data augmentation experiments]\n    D --\x3e D1[\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u66f4\u6709\u6548\u4f46\u7f16\u8f91\u66f4\u591a/Translation-based CFs are more valid but require more edits]\n    D --\x3e D2[\u9ad8\u8d44\u6e90\u8bed\u8a00\u7f16\u8f91\u6a21\u5f0f\u76f8\u4f3c/Edit patterns are similar for high-resource languages]\n    D --\x3e D3[\u591a\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u6548\u679c\u66f4\u597d/Multilingual data augmentation yields larger improvements]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [function-calling, benchmark, API complexity, LLM agents, WildAGTEval]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Doyoung Kim, Zhiwei Ren, Jie Hao, Zhongkai Sun, Lichao Wang, Xiyao Ma, Zack Ye, Xu Han, Jun Yin, Heng Ji, Wei Shen, Xing Fan, Benjamin Yao, Chenlei Guo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Amazon, KAIST, University of Pittsburgh, University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00268",children:"https://arxiv.org/pdf/2601.00268"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/Demon-JieHao/WildAGTEval"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces WildAGTEval, a novel benchmark for evaluating LLM agents under realistic API complexity, covering both API specification and execution challenges. 2. Provides a comprehensive API system with 60 distinct complexity scenarios, composable into ~32K test configurations, and user-agent interactions for evaluation. 3. Systematically assesses advanced LLMs, revealing significant performance drops (e.g., 27.3% for irrelevant information complexity) and identifying critical failure modes like intent distortion."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c517d92b4a84a20e84e30c740db5d9b70b7f3195c6de0fc8e049a5f0a4c9af_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c517d92b4a84a20e84e30c740db5d9b70b7f3195c6de0fc8e049a5f0a4c9af_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces WildAGTEval, a benchmark designed to evaluate LLM agents' function-calling capabilities under realistic API complexities, including detailed specifications and noisy execution. The study finds that most scenarios are challenging, with irrelevant information posing the greatest difficulty and causing significant performance drops in strong models. The qualitative analysis also reveals that LLMs sometimes distort user intent to claim task completion, negatively impacting user satisfaction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u5047\u8bbe\u7406\u60f3\u5316API / Existing benchmarks assume idealized APIs]\n    B --\x3e B2[\u5ffd\u7565\u73b0\u5b9e\u56e0\u7d20\u5982\u566a\u58f0\u8f93\u51fa / Ignore real-world factors like noisy outputs]\n    C --\x3e C1[\u63d0\u51faWildAGTEval\u57fa\u51c6 / Propose WildAGTEval benchmark]\n    C --\x3e C2[\u6db5\u76d6API\u89c4\u8303\u4e0e\u6267\u884c\u590d\u6742\u6027 / Covers API specification & execution complexity]\n    D --\x3e D1[\u5927\u591a\u6570\u573a\u666f\u5177\u6709\u6311\u6218\u6027 / Most scenarios are challenging]\n    D --\x3e D2[\u65e0\u5173\u4fe1\u606f\u590d\u6742\u5ea6\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d / Irrelevant info complexity causes significant performance drop]\n    D --\x3e D3[LLM\u53ef\u80fd\u626d\u66f2\u7528\u6237\u610f\u56fe / LLMs may distort user intent]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [hallucination detection, vision-language models, uncertainty estimation, model-driven learning, LLM-as-a-Judge]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00269",children:"https://arxiv.org/pdf/2601.00269"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FaithSCAN: Faithful VQA Hallucination Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VQA\u6a21\u578b\u4ea7\u751f\u6d41\u7545\u4f46\u89c6\u89c9\u65e0\u6839\u636e\u7684\u7b54\u6848/Faithfulness Hallucinations in VQA]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5229\u7528VLM\u5185\u90e8\u4fe1\u53f7\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408/Exploit VLM Internal Signals & Uncertainty-Aware Fusion]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u9ad8\u6548\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/More Effective & Efficient than Prior Methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00282",children:"https://arxiv.org/pdf/2601.00282"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Large Language Models Still Explain Themselves?<br/>\u5927\u8bed\u8a00\u6a21\u578b\u8fd8\u80fd\u89e3\u91ca\u81ea\u5df1\u5417\uff1f"] --\x3e Problem["Quantization\'s effect on Self-Explanations is unknown.<br/>\u91cf\u5316\u5bf9\u81ea\u6211\u89e3\u91ca\u7684\u5f71\u54cd\u672a\u77e5"]\n    Root --\x3e Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>\u8bc4\u4f30\u91cf\u5316\u540eLLM\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u53cd\u4e8b\u5b9e\u793a\u4f8b"]\n    Root --\x3e Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>\u8d28\u91cf/\u5fe0\u5b9e\u5ea6\u9002\u5ea6\u4e0b\u964d\uff1b\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Swin Transformer, BatchFormer, Focal Loss, ReduceLROnPlateau, ISIC2019]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, University of Sydney, Shaoyang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00286",children:"https://arxiv.org/pdf/2601.00286"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model's potential as a clinical support tool."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Automated Differential Diagnosis of Skin Diseases<br>\u76ae\u80a4\u75be\u75c5\u81ea\u52a8\u9274\u522b\u8bca\u65ad] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited dermatologist access & need for diagnostic tools<br>\u76ae\u80a4\u79d1\u533b\u751f\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u8bca\u65ad\u5de5\u5177]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Deep learning (Swin Transformer) with imbalance-aware strategies<br>\u6df1\u5ea6\u5b66\u4e60\uff08Swin Transformer\uff09\u4e0e\u4e0d\u5e73\u8861\u611f\u77e5\u7b56\u7565]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>87.71% accuracy on ISIC2019 dataset<br>\u5728ISIC2019\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.71%\u51c6\u786e\u7387]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [self-evolving agent, hierarchical memory, protocol redesign]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sixue Xing, Xuanye Xia, Kerui Wu, Meng Jiang, Jintai Chen, Tianfan Fu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Notre Dame, Georgia Institute of Technology, University of Massachusetts Amherst, Hong Kong University of Science and Technology (Guangzhou), Nanjing University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00290",children:"https://arxiv.org/pdf/2601.00290"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ClinicalReTrial, a self-evolving AI agent framework that moves beyond static trial outcome prediction to actionable protocol optimization. 2. Introduces a closed-loop, reward-driven optimization framework that integrates failure diagnosis, safety-aware modification, and candidate evaluation, using a prediction model as a simulation environment. 3. Designs a hierarchical memory mechanism to capture iteration-level feedback and distill transferable redesign patterns for efficient exploration."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b82407468d333ae862573b891f5a19251a04fd53b36b2e4e9e895ca08f9073_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b82407468d333ae862573b891f5a19251a04fd53b36b2e4e9e895ca08f9073_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ClinicalReTrial, a self-evolving AI agent framework that optimizes clinical trial protocols through iterative, reward-driven redesign. It uses an outcome prediction model as a simulation environment and a hierarchical memory for efficient exploration. Empirical results show the framework improves 83.3% of trial protocols with a mean success probability gain of 5.7%."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Clinical trial failure due to protocol design flaws, existing AI is reactive)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Self-evolving agent with closed-loop optimization, hierarchical memory, using prediction model as simulation)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improves 83.3% of protocols, mean success probability gain of 5.7%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [speech processing], [depression detection, semantic bias, text-to-speech, disentangled representation, data augmentation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuxin Li, Xiangyu Zhang, Yifei Li, Zhiwei Guo, Haoyang Zhang, Eng Siong Chng, Cuntai Guan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanyang Technological University, UNSW Sydney, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00303",children:"https://arxiv.org/pdf/2601.00303"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DepFlow, a novel three-stage depression-conditioned TTS framework that disentangles depression-specific acoustic patterns from speaker and content information using adversarial training and flow-matching. 2. Introduces a prototype-based severity mapping mechanism for smooth and interpretable control over the synthesized depressive severity. 3. Constructs a Camouflage Depression-oriented Augmentation (CDoA) dataset using DepFlow to mitigate semantic bias, which significantly improves the robustness of depression detection models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddfe4c5383fa66e6b8e028851d0fe67037a11642b799a3626a1100aaa4cd8096_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddfe4c5383fa66e6b8e028851d0fe67037a11642b799a3626a1100aaa4cd8096_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of semantic bias in depression detection models, where models learn shortcuts from linguistic sentiment instead of acoustic cues. It proposes DepFlow, a disentangled speech generation framework, to create a synthetic dataset (CDoA) that pairs depressed acoustic patterns with positive/neutral text. This data augmentation method improves model robustness, outperforming conventional strategies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Models learn semantic shortcuts from sentiment-content coupling in depression datasets."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: DepFlow, a 3-stage TTS framework for disentangling & controlling depression acoustics."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: CDoA augmentation improves detection model F1 scores by 5-12%."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Generative AI Paradox: GenAI and the Erosion of Trust, the Corrosion of Information Verification, and the Demise of Truth"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [ai safety & alignment], [synthetic reality, epistemic security, provenance, trust erosion, generative AI harms]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Emilio Ferrara"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Southern California (USC)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00306",children:"https://arxiv.org/pdf/2601.00306"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Formalizes the concept of "synthetic reality" as a layered socio-technical stack comprising content, identity, interaction, and institutions. 2. Expands a taxonomy of Generative AI harms and articulates the qualitative shifts it introduces, such as cost collapse and provenance gaps. 3. Proposes a complementary mitigation stack and a research agenda focused on measuring epistemic security, culminating in the articulation of the "Generative AI Paradox".']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b42a2aa0b289b69471dcaa3b08e187b14ded4070e26848bb4917fcc12b2427_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b42a2aa0b289b69471dcaa3b08e187b14ded4070e26848bb4917fcc12b2427_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper argues that the primary risk of Generative AI is not just creating fake content, but the systemic erosion of shared truth and verification practices as it enables the easy creation of synthetic content, identities, and interactions. The authors formalize this as "synthetic reality," analyze its risks, and propose a multi-layered mitigation approach. They conclude with the "Generative AI Paradox": the potential for societies to rationally discount all digital evidence as synthetic media becomes ubiquitous.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Generative AI Paradox<br>\u751f\u6210\u5f0fAI\u6096\u8bba] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GenAI erodes shared truth & verification<br>GenAI\u4fb5\u8680\u5171\u540c\u8ba4\u77e5\u4e0e\u9a8c\u8bc1]\n    C --\x3e C1[Formalize synthetic reality stack & harms taxonomy<br>\u5f62\u5f0f\u5316\u5408\u6210\u73b0\u5b9e\u6808\u4e0e\u5371\u5bb3\u5206\u7c7b]\n    C --\x3e C2[Propose mitigation stack & research agenda<br>\u63d0\u51fa\u7f13\u89e3\u6808\u4e0e\u7814\u7a76\u8bae\u7a0b]\n    D --\x3e D1[Generative AI Paradox: discount digital evidence<br>\u751f\u6210\u5f0fAI\u6096\u8bba\uff1a\u8d28\u7591\u6570\u5b57\u8bc1\u636e]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [person re-identification], [feature fusion, alpha-divergence loss, dynamic multi-task learning, semantic clustering, computational efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anns Ijaz, Muhammad Azeem Javed"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Management and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00307",children:"https://arxiv.org/pdf/2601.00307"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[VisNet: Efficient Person Re-Identification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Accuracy vs. Computational Cost Trade-off]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Feature Fusion, Semantic Clustering, Dynamic Multi-Task Learning, \u03b1-Divergence Loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 87.05% Rank-1, 77.65% mAP, 4.601 GFLOPs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Multiagent Reinforcement Learning for Liquidity Games"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multiagent reinforcement learning], [liquidity games, rational swarms, difference rewards, Markov team games, financial swarm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alicia Vidler, Gal A. Kaminka"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Bar-Ilan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00324",children:"https://arxiv.org/pdf/2601.00324"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Unifies Liquidity Games (a game-theoretic model of liquidity formation) with Rational Swarms (a decentralized multiagent RL method using difference rewards). 2. Proposes a theoretical framework defining a swarm of independent traders whose collective objective is market liquidity provision. 3. Demonstrates that using difference rewards within a Markov team games framework enables individual liquidity-maximizing behaviors to contribute to overall market liquidity without requiring coordination."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1bd53d93f07cfb626ac581013a3e8a6bfa64ea30e2fea7f9e15851b64d4cd2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1bd53d93f07cfb626ac581013a3e8a6bfa64ea30e2fea7f9e15851b64d4cd2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the tension between self-interested financial agents and the need for overall market liquidity. It proposes a "Financial Swarm" model that unifies Liquidity Games with Rational Swarms, using difference rewards in a Markov team game to align individual learning with the global objective. The main conclusion is that this framework allows independent agents to achieve both individual profitability and collective market efficiency without coordination or collusion.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Multiagent Reinforcement Learning for Liquidity Games"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Self-interested agents vs. market liquidity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Unify Liquidity Games & Rational Swarms with difference rewards"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Independent agents achieve liquidity without coordination"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [frequency-guided learning, structural attention, semantic consistency, dual-branch framework, CLIP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tianjin University of Science and Technology, The University of Sydney, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00327",children:"https://arxiv.org/pdf/2601.00327"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HarmoniAD: \u5f02\u5e38\u68c0\u6d4b/HarmoniAD: Anomaly Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7ed3\u6784-\u8bed\u4e49\u6743\u8861/Structure-Semantics Trade-off]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u9891\u7387\u5f15\u5bfc\u53cc\u5206\u652f\u6846\u67b6/Frequency-Guided Dual-Branch Framework]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA\u6027\u80fd/SOTA Performance]\n    B --\x3e B1[\u7ed3\u6784\u6a21\u578b\u566a\u58f0\u654f\u611f/Structure Models: Noise-Sensitive]\n    B --\x3e B2[\u8bed\u4e49\u6a21\u578b\u5ffd\u7565\u7ec6\u8282/Semantic Models: Miss Details]\n    C --\x3e C1[\u9ad8\u9891\u5206\u652f: FSAM\u6a21\u5757/High-Freq Branch: FSAM]\n    C --\x3e C2[\u4f4e\u9891\u5206\u652f: GSCM\u6a21\u5757/Low-Freq Branch: GSCM]\n    D --\x3e D1[\u6570\u636e\u96c6: MVTec-AD, VisA, BTAD/Datasets: MVTec-AD, VisA, BTAD]\n    D --\x3e D2[\u7ed3\u679c: \u9ad8\u654f\u611f\u6027\u4e0e\u9c81\u68d2\u6027/Results: High Sensitivity & Robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["[arXiv260105] Sparse Probabilistic Coalition Structure Generation: Bayesian Greedy Pursuit and ",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsxs)(n.msub,{children:[(0,r.jsx)(n.mrow,{}),(0,r.jsx)(n.mn,{children:"1"})]})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"_1"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.4511em",verticalAlign:"-0.15em"}}),(0,r.jsxs)(n.span,{className:"mord",children:[(0,r.jsx)(n.span,{}),(0,r.jsx)(n.span,{className:"msupsub",children:(0,r.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,r.jsxs)(n.span,{className:"vlist-r",children:[(0,r.jsx)(n.span,{className:"vlist",style:{height:"0.3011em"},children:(0,r.jsxs)(n.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,r.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,r.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,r.jsx)(n.span,{className:"mord mtight",children:"1"})})]})}),(0,r.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,r.jsx)(n.span,{className:"vlist-r",children:(0,r.jsx)(n.span,{className:"vlist",style:{height:"0.15em"},children:(0,r.jsx)(n.span,{})})})]})})]})]})})]})," Relaxations"]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [coalition structure generation, sparse regression, Bayesian greedy pursuit, l1 penalization, probabilistic framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Angshul Majumdar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IIIT Delhi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00329",children:"https://arxiv.org/pdf/2601.00329"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel probabilistic framework for Coalition Structure Generation (CSG) where coalition values are learned from episodic observations via sparse linear regression. 2. Introduces and analyzes the Bayesian Greedy Coalition Pursuit (BGCP) algorithm, providing theoretical guarantees for exact coalition recovery under certain conditions. 3. Provides theoretical analysis for an alternative \u21131-penalized estimation scheme, deriving error bounds and translating them into welfare gap guarantees, and compares regimes where sparse methods outperform classical approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492cd85c7ccfef225059f9e637ee717227d05ec3497e1f7c70a5e2e84ad5cc35_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492cd85c7ccfef225059f9e637ee717227d05ec3497e1f7c70a5e2e84ad5cc35_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses coalition structure generation when coalition values are unknown and must be learned from noisy episodic data. It proposes a sparse probabilistic framework with two estimation methods: a Bayesian greedy pursuit algorithm and an \u21131-penalized estimator, providing theoretical recovery and error guarantees. The analysis identifies conditions under which these sparse learning approaches yield welfare-optimal structures and outperform classical methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Sparse Probabilistic Coalition Structure Generation<br>\u7a00\u758f\u6982\u7387\u8054\u76df\u7ed3\u6784\u751f\u6210"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u5b66\u4e60\u8054\u76df\u4ef7\u503c<br>Learning Coalition Values"]\n    Method["\u4e3b\u8981\u65b9\u6cd5<br>Key Methods"]\n    Results["\u5173\u952e\u7ed3\u679c<br>Key Results"]\n\n    Problem --\x3e P1["\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60<br>Learn from Episodic Observations"]\n    Problem --\x3e P2["\u566a\u58f0\u7ebf\u6027\u7ec4\u5408<br>Noisy Linear Combination of Contributions"]\n\n    Method --\x3e M1["\u8d1d\u53f6\u65af\u8d2a\u5a6a\u8054\u76df\u8ffd\u8e2a (BGCP)<br>Bayesian Greedy Coalition Pursuit (BGCP)"]\n    Method --\x3e M2["\u21131 \u60e9\u7f5a\u4f30\u8ba1\u5668<br>\u21131-Penalized Estimator"]\n\n    Results --\x3e R1["BGCP: \u9ad8\u6982\u7387\u6062\u590d\u8054\u76df\u96c6<br>BGCP: Recovers Coalition Set w.h.p."]\n    Results --\x3e R2["\u21131: \u8bef\u5dee\u4e0e\u798f\u5229\u5dee\u8ddd\u754c\u9650<br>\u21131: Error & Welfare Gap Bounds"]\n    Results --\x3e R3["\u8bc6\u522b\u7a00\u758f/\u5bc6\u96c6\u4f18\u52bf\u673a\u5236<br>Identifies Sparse/Dense Regimes"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lov\xe9n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Lule\xe5 University of Technology, Peking University, TU Wien"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00339",children:"https://arxiv.org/pdf/2601.00339"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Bio-inspired Agentic Self-healing Framework<br>\u751f\u7269\u542f\u53d1\u7684\u667a\u80fd\u4f53\u81ea\u6108\u6846\u67b6"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>DCCS\u4e2d\u7684\u590d\u6742\u6027\u4e0e\u6545\u969c\u9891\u53d1<br>Complexity & Frequent Faults in DCCS"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>ReCiSt\u6846\u67b6: \u4eff\u751f\u56db\u5c42\u4e0eLM\u667a\u80fd\u4f53<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u6570\u5341\u79d2\u5185\u81ea\u6108\uff0c\u4f4eCPU\u5f00\u9500<br>Self-healing in tens of seconds, low CPU overhead"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Robust Uncertainty Quantification for Factual Generation of Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [uncertainty quantification, factual hallucination, trap questions, ROCAUC, fake biographies]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuhao Zhang, Zhongliang Yang, Linna Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00348",children:"https://arxiv.org/pdf/2601.00348"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/EdwardChang5467/robust",children:"https://github.com/EdwardChang5467/robust"})," uncertainty"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new uncertainty quantification scenario focused on multi-fact generation (e.g., fake person biographies) to test LLM robustness. 2. Constructs a novel dataset of trap questions containing fake names to evaluate hallucination detection methods. 3. Introduces a robust uncertainty quantification (RU) method that significantly outperforms baseline methods across four different LLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6268aa8c8874ac861a06315946aedcfbc9df7712a9f2b63e23204554e9c8596b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6268aa8c8874ac861a06315946aedcfbc9df7712a9f2b63e23204554e9c8596b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of LLM hallucination by proposing a new robust uncertainty quantification (RU) method for detecting factual errors in multi-fact generation tasks. The method is evaluated using a specially constructed set of trap questions containing fake names. Results show the RU method achieves an average increase of 0.1-0.2 in ROCAUC over the best baseline, demonstrating its effectiveness in improving the reliability of LLM outputs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Uncertainty Quantification for Factual Generation of LLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u5e7b\u89c9\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7f3a\u9677<br>LLM Hallucination & UQ Deficiency]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u6784\u5efa\u9677\u9631\u95ee\u9898\u96c6\u4e0e\u63d0\u51fa\u9c81\u68d2\u65b9\u6cd5<br>Construct Trap Questions & Propose RU Method]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>ROC-AUC\u663e\u8457\u63d0\u5347<br>Significant ROC-AUC Improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Mapping Human Anti-collusion Mechanisms to Multi-agent AI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [collusion, anti-collusion mechanisms, multi-agent AI, AI safety, game theory]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jamiu Adekunle Idowu, Ahmed Almasoud, Ayman Alfahid"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University College London (UCL), Sahel AI, Prince Sultan University, Majmaah University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00360",children:"https://arxiv.org/pdf/2601.00360"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a taxonomy of human anti-collusion mechanisms (e.g., sanctions, leniency, monitoring). 2. Mapped these human mechanisms to potential interventions for multi-agent AI systems. 3. Highlighted key open challenges in applying these mechanisms to AI, such as the attribution problem and adversarial adaptation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddbe564b87a89c6b0bfd47c15c01195109e5af5ff41994f193729c6ff841d80_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddbe564b87a89c6b0bfd47c15c01195109e5af5ff41994f193729c6ff841d80_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the risk of collusion in autonomous multi-agent AI systems by proposing to adapt established human anti-collusion mechanisms. It develops a taxonomy of these mechanisms and maps them to potential AI interventions. The work concludes by identifying critical challenges for future research in this area, such as distinguishing beneficial cooperation from harmful collusion."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Mapping Human Anti-collusion Mechanisms to Multi-agent AI] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u4ee3\u7406\u51fa\u73b0\u5408\u8c0b/AI Agents Develop Collusion]\n    C --\x3e C1[\u5efa\u7acb\u4eba\u7c7b\u53cd\u5408\u8c0b\u673a\u5236\u5206\u7c7b\u6cd5/Develop Taxonomy of Human Anti-collusion Mechanisms]\n    C --\x3e C2[\u6620\u5c04\u5230AI\u7cfb\u7edf\u5e72\u9884\u63aa\u65bd/Map to Multi-agent AI Interventions]\n    D --\x3e D1[\u63d0\u51fa\u5b9e\u65bd\u65b9\u6cd5/Propose Implementation Approaches]\n    D --\x3e D2[\u8bc6\u522b\u5f00\u653e\u6311\u6218/Highlight Open Challenges]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual representation learning], [Joint Embedding Predictive Architecture (JEPA), BERT, CLS token, language-agnostic embedding, multilingual benchmarks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Brown University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00366",children:"https://arxiv.org/pdf/2601.00366"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the problem that BERT\'s [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic "thought space". The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: CLS embeddings are not language-invariant and fail to capture true sentence semantics."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Add JEPA training objective to BERT to create a language-agnostic embedding space."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Improved multilingual benchmark performance; reorganized, semantic-first CLS space."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [adversarial patches, outlier detection, isolation forest, dimensionality reduction, edge computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nandish Chattopadhyay, Abdul Basit, Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammad Shafique"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University (NYU) Abu Dhabi, DakAI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00367",children:"https://arxiv.org/pdf/2601.00367"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PatchBlock, a lightweight, model-agnostic pre-processing framework for detecting and mitigating adversarial patches on resource-constrained edge devices. 2. Introduces a redesigned isolation forest algorithm with targeted cuts for efficient anomaly detection in image chunks. 3. Demonstrates high robustness recovery (up to 77% accuracy) and superior efficiency (computation time, energy) compared to state-of-the-art defenses, with minimal impact on clean accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/931ca070f14402ab68d228644f7d6df1ed1c402686c4fae0127234e1c588df8d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/931ca070f14402ab68d228644f7d6df1ed1c402686c4fae0127234e1c588df8d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents PatchBlock, a lightweight defense framework that uses outlier detection and dimensionality reduction to identify and neutralize adversarial patches in images for EdgeAI systems. It operates efficiently on CPUs in parallel with GPU inference, making it suitable for resource-constrained devices. Evaluations show it significantly recovers model accuracy under strong patch attacks while maintaining high efficiency and portability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[PatchBlock: A Lightweight Defense Against Adversarial Patches] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u5bf9\u6297\u6027\u8865\u4e01\u5a01\u80c1\u8fb9\u7f18AI/Patch attacks threaten EdgeAI]\n    P1 --\x3e P2[\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u9700\u8981\u8f7b\u91cf\u7ea7\u9632\u5fa1/Resource-constrained devices need lightweight defense]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5206\u5757/Chunking]\n    Method --\x3e M2[\u57fa\u4e8e\u6539\u8fdb\u9694\u79bb\u6797\u7684\u5f02\u5e38\u68c0\u6d4b/Anomaly Detection via redesigned Isolation Forest]\n    Method --\x3e M3[\u964d\u7ef4\u7f13\u89e3/Dimensionality Reduction Mitigation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6062\u590d\u9ad8\u8fbe77%\u7684\u51c6\u786e\u7387/Recovers up to 77% accuracy]\n    Results --\x3e R2[\u9ad8\u6548\uff0cCPU\u5e76\u884c\uff0c\u4f4e\u5f00\u9500/Efficient, CPU-parallel, low overhead]\n    Results --\x3e R3[\u6a21\u578b\u4e0e\u8865\u4e01\u65e0\u5173\uff0c\u53ef\u79fb\u690d/Model- & patch-agnostic, portable]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] In Line with Context: Repository-Level Code Generation via Context Inlining"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [se], [code generation], [repository-level code generation, context inlining, call graph, perplexity-based confidence, bidirectional inlining]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chao Hu, Wenhao Zeng, Yuling Shi, Beijun Shen, Xiaodong Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00376",children:"https://arxiv.org/pdf/2601.00376"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces InlineCoder, a novel framework that reframes repository-level code generation as a function-level task by inlining the target function into its call graph., 2. Proposes a bidirectional inlining process combining an initial draft anchor, upstream inlining for usage scenarios, and downstream retrieval for dependency context., 3. Demonstrates substantial performance gains over state-of-the-art baselines on benchmarks like RepoExec, highlighting effectiveness in understanding repository contexts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/042382406537daf6ffcdbb0e0a2311956fbf996d26fe6c18ea9b84304141abfa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/042382406537daf6ffcdbb0e0a2311956fbf996d26fe6c18ea9b84304141abfa_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of repository-level code generation, where models must understand complex dependencies across an entire codebase. It proposes InlineCoder, a framework that first generates a draft function (anchor) and then enriches the context by inlining it into its callers (upstream) and retrieving its callees (downstream). This approach significantly outperforms existing methods on standard benchmarks, showing improved understanding of repository context."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["In Line with Context: Repository-Level Code Generation via Context Inlining<br>\u8bba\u6587\u6807\u9898"]\n    Root --\x3e Problem["\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3<br>Problem: Existing methods (e.g., RAG) rely on surface-level similarity and struggle with repository dependencies."]\n    Root --\x3e Method["\u63d0\u51faInlineCoder\u6846\u67b6<br>Method: Generates an anchor draft, then performs bidirectional context inlining (Upstream Inlining & Downstream Retrieval)."]\n    Root --\x3e Results["\u6027\u80fd\u663e\u8457\u63d0\u5347<br>Results: Outperforms SOTA baselines with substantial gains on RepoExec and DevEval benchmarks."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Word Frequency Counting Based on Serverless MapReduce"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [serverless computing], [Serverless Computing, MapReduce, Word Frequency Counting, Function as a Service (FaaS), Cloud Computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hanzhe Li, Bingchen Lin, Mengyuan Xu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi'an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00380",children:"https://arxiv.org/pdf/2601.00380"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel combination of the serverless computing paradigm (FaaS) with the MapReduce programming model for data processing tasks. 2. Investigates and determines the optimal number of Map and Reduce functions for a given workload within a serverless MapReduce framework. 3. Demonstrates through experiments that increasing the number of functions reduces execution time and improves overall efficiency for the word frequency counting task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of optimizing big data processing efficiency by integrating the serverless computing model (FaaS) with the MapReduce framework. It proposes a serverless MapReduce approach for word frequency counting and experimentally finds the optimal number of Map and Reduce functions to minimize execution time. The results show that this method improves processing efficiency, offering a cost-effective solution for cloud-based data analytics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Word Frequency Counting Based on Serverless MapReduce] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9700\u6c42: \u9ad8\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u8ba1\u7b97 / Demand: High-performance & High-efficiency Computing]\n    C --\x3e C1[\u7ed3\u5408: \u65e0\u670d\u52a1\u5668\u8ba1\u7b97(FaaS)\u4e0eMapReduce / Combine: Serverless Computing (FaaS) & MapReduce]\n    C --\x3e C2[\u4f18\u5316: Map\u4e0eReduce\u51fd\u6570\u6570\u91cf / Optimize: Number of Map & Reduce Functions]\n    D --\x3e D1[\u7ed3\u679c: \u6267\u884c\u65f6\u95f4\u51cf\u5c11 / Result: Execution Time Reduces]\n    D --\x3e D2[\u7ed3\u679c: \u7a0b\u5e8f\u6548\u7387\u63d0\u5347 / Result: Program Efficiency Improves]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [cyber-physical systems security], [intrusion detection system, anomaly detection, G-code manipulation, transformer encoder, self-attention autoencoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Mahbub Hasan, Marcus Sternhagen, Krishna Chandra Roy"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New Mexico Institute of Mining and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00384",children:"https://arxiv.org/pdf/2601.00384"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Investigation of stealthy Man-in-the-Middle (MitM) attack vectors targeting the CAD-to-machine interface in Fused Deposition Modeling (FDM) 3D printers. 2. Proposal of an unsupervised Intrusion Detection System (IDS) that uses a frozen Transformer-based encoder and contrastive learning to create anomaly-sensitive embeddings from machine logs. 3. Demonstration of effective anomaly classification using a combination of clustering and a self-attention autoencoder on real 3D printing systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates stealthy cyberattacks that manipulate G-code in additive manufacturing systems, leading to structurally defective parts. To detect these attacks, the authors propose an unsupervised intrusion detection system that uses a Transformer-based encoder and contrastive learning to analyze machine logs. Their method successfully distinguishes between normal and compromised printing executions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[AM\u7cfb\u7edf\u7684\u65b0\u653b\u51fb\u9762 / New Attack Surfaces in AM]\nB --\x3e B2[\u9690\u79d8\u7684\u4e2d\u95f4\u4eba\u653b\u51fb / Stealthy MitM Attacks]\nC --\x3e C1[\u57fa\u4e8e\u65e5\u5fd7\u7684\u65e0\u76d1\u7763IDS / Unsupervised IDS from Logs]\nC --\x3e C2[Transformer\u7f16\u7801\u5668 / Transformer Encoder]\nC --\x3e C3[\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u81ea\u6ce8\u610f\u529b / Contrastive Learning & Self-Attention]\nD --\x3e D1[\u6709\u6548\u533a\u5206\u6b63\u5e38\u4e0e\u653b\u51fb / Effectively Distinguishes Benign & Compromised]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [coordinated behavior detection], [Convergent Cross Mapping, semi-supervised learning, active learning, hierarchical clustering, causal inference]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Weng Ding, Yi Han, Mu-Jiang-Shan Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology, Meta, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00400",children:"https://arxiv.org/pdf/2601.00400"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an adaptive Convergent Cross Mapping (CCM) technique for identifying genuine causal relationships between accounts. 2. Integrates active learning with uncertainty sampling in a semi-supervised classification scheme to reduce manual labeling burden. 3. Introduces an automated validation module driven by historical detection experience for self-verification and optimization of outcomes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e2d7bb8b5573c45544689a49293433fa3d98d36abedd140f80989af5d80d355_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e2d7bb8b5573c45544689a49293433fa3d98d36abedd140f80989af5d80d355_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes ACCD, a three-stage memory-guided framework for detecting coordinated inauthentic behavior on social media. It uses adaptive causal inference, semi-supervised active learning, and automated validation to improve accuracy and efficiency. The method achieves a higher F1-score and significantly reduces manual annotation requirements compared to existing baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Adaptive Causal Coordination Detection for Social Media<br/>\u793e\u4ea4\u5a92\u4f53\u81ea\u9002\u5e94\u56e0\u679c\u534f\u8c03\u68c0\u6d4b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u76f8\u5173\u5206\u6790\u3001\u9759\u6001\u53c2\u6570\u3001\u5927\u91cf\u4eba\u5de5\u6807\u6ce8<br/>Existing methods rely on superficial correlation, static parameters, heavy manual labeling]\n    C --\x3e C1[\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u67b6\u6784<br/>Three-stage progressive architecture]\n    C1 --\x3e C11[\u9636\u6bb51: \u81ea\u9002\u5e94CCM\u8bc6\u522b\u56e0\u679c<br/>Stage 1: Adaptive CCM for causal relationships]\n    C1 --\x3e C12[\u9636\u6bb52: \u534a\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60<br/>Stage 2: Semi-supervised active learning]\n    C1 --\x3e C13[\u9636\u6bb53: \u81ea\u52a8\u5316\u9a8c\u8bc1\u6a21\u5757<br/>Stage 3: Automated validation module]\n    D --\x3e D1[F1\u5206\u657087.3%, \u63d0\u534715.2%<br/>F1-score 87.3%, 15.2% improvement]\n    D --\x3e D2[\u51cf\u5c1168%\u4eba\u5de5\u6807\u6ce8<br/>Reduces manual annotation by 68%]\n    D --\x3e D3[\u5904\u7406\u901f\u5ea6\u63d0\u53472.8\u500d<br/>2.8x speedup in processing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [named entity recognition], [weak supervision, large language models, low-resource languages, dataset construction, Luxembourgish]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alistair Plum, Laura Bernardy, Tharindu Ranasinghe"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Luxembourg, Lancaster University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00411",children:"https://arxiv.org/pdf/2601.00411"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel pipeline for constructing NER datasets that uses Wikipedia/Wikidata for weak supervision and LLMs for label verification. 2. Introduces judgeWEL, a new and significantly larger NER dataset for the under-represented language Luxembourgish. 3. Evaluates and compares the effectiveness of multiple LLMs in judging and filtering noisy, distantly-supervised labels."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e13a29d091cdd4cb0cc5c3d34a98e86c8d45b0a34f42a2b552cecd9fcff5b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e13a29d091cdd4cb0cc5c3d34a98e86c8d45b0a34f42a2b552cecd9fcff5b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of building datasets for under-represented languages by proposing a novel method that uses Wikipedia and Wikidata for weak supervision to generate initial NER labels, and then employs multiple LLMs to verify and filter these labels for quality. The approach is applied to Luxembourgish, resulting in the judgeWEL dataset, which is five times larger and more balanced than existing resources, providing a valuable new corpus for low-resource NER research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Do LLMs Judge Distantly Supervised Named Entity Labels Well?<br/>\u6784\u5efaJudgeWEL\u6570\u636e\u96c6] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u96c6\u6784\u5efa\u56f0\u96be<br/>Dataset Construction for Low-Resource Languages]\nC --\x3e C1[\u5229\u7528\u7ef4\u57fa\u767e\u79d1/\u7ef4\u57fa\u6570\u636e\u8fdb\u884c\u8fdc\u7a0b\u76d1\u7763<br/>Weak Supervision via Wikipedia/Wikidata]\nC --\x3e C2[\u4f7f\u7528\u591a\u4e2aLLM\u8fdb\u884c\u6807\u7b7e\u9a8c\u8bc1<br/>Label Verification with Multiple LLMs]\nD --\x3e D1[\u521b\u5efa\u4e86\u66f4\u5927\u7684\u5362\u68ee\u5821\u8bedNER\u6570\u636e\u96c6<br/>Larger Luxembourgish NER Dataset Created]\nD --\x3e D2[\u6570\u636e\u96c6\u89c4\u6a21\u6269\u5927\u4e94\u500d\uff0c\u8986\u76d6\u66f4\u5e73\u8861<br/>5x Larger, More Balanced Coverage]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [efficient transformers], [astrocyte-inspired computing, long-term plasticity (LTP), short-term plasticity (STP), memory compression, Long Range Arena (LRA)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Pennsylvania State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00426",children:"https://arxiv.org/pdf/2601.00426"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[RMAAT: Astrocyte-Inspired Memory Compression and Replay] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6/Quadratic Complexity of Self-Attention]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u542f\u53d1\u7684\u5faa\u73af\u8bb0\u5fc6\u67b6\u6784/Astrocyte-Inspired Recurrent Memory Architecture]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728LRA\u57fa\u51c6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387/Competitive Accuracy & Efficiency on LRA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Delta Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00417",children:"https://arxiv.org/pdf/2601.00417"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/yifanzhang-pro/deep-delta-learning",children:"https://github.com/yifanzhang-pro/deep-delta-learning"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Deep Delta Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6b8b\u5dee\u7f51\u7edc\u9650\u5236/ResNet Limitation]\n    B1 --\x3e B2["\u521a\u6027\u76f8\u52a0\u504f\u7f6e/Rigid Additive Bias"]\n    B2 --\x3e B3["\u9650\u5236\u590d\u6742\u72b6\u6001\u8f6c\u6362/Limits Complex State Transitions"]\n    C --\x3e C1[Delta \u7b97\u5b50/Delta Operator]\n    C1 --\x3e C2["\u79e9-1\u6270\u52a8/ Rank-1 Perturbation"]\n    C2 --\x3e C3["\u53ef\u5b66\u4e60\u51e0\u4f55\u53d8\u6362/Learnable Geometric Transform"]\n    C3 --\x3e C4["\u52a8\u6001\u95e8\u63a7/Dynamic Gating (\u03b2)"]\n    D --\x3e D1["\u8c31\u5206\u6790/Spectral Analysis"]\n    D1 --\x3e D2["\u63d2\u503c\u8eab\u4efd/\u6295\u5f71/\u53cd\u5c04/Interpolates Identity/Projection/Reflection"]\n    D --\x3e D3["\u540c\u6b65\u79e9-1\u6ce8\u5165/Synchronous Rank-1 Injection"]\n    D3 --\x3e D4["\u63a7\u5236\u8f6c\u6362\u8c31/Controls Transition Spectrum"]\n    D4 --\x3e D5["\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3/Preserves Stable Training"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [sports analytics], [semantic-space reasoning, vector-distance metrics, tactical optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alessio Di Rubbo, Mattia Neri, Remo Pareschi, Marco Pedroni, Roberto Valtancoli, Paolino Zica"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Molise, Stake Lab, Bioretics, Institute for Generative Strategy, Cesena Femminile Football Club, Zica Sport"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00421",children:"https://arxiv.org/pdf/2601.00421"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel analogy between texts and teams, modeling tactical configurations as compositional semantic structures. 2. Introduces a methodology to represent players and teams as multidimensional vectors and evaluate tactical fit using vector-distance metrics. 3. Demonstrates a generalizable framework for collective decision-making applicable beyond football to domains like cooperative robotics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68f5544900dd6f44d32d07e9f1b09cbba4598bb4606d5d22767a42a8c2195ce1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68f5544900dd6f44d32d07e9f1b09cbba4598bb4606d5d22767a42a8c2195ce1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel methodology that applies semantic-space reasoning from computational linguistics to model team sports tactics. It represents players and teams as vectors in a shared semantic space to compute tactical fit and generate strategy recommendations. The work concludes that this approach provides an interpretable and generalizable framework for optimizing collective performance in team-based domains."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Can Semantic Methods Enhance Team Sports Tactics?<br/>\u8bed\u4e49\u65b9\u6cd5\u80fd\u63d0\u5347\u56e2\u961f\u8fd0\u52a8\u6218\u672f\u5417\uff1f] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br/>Extend semantic reasoning to tactical decision-making<br/>\u5c06\u8bed\u4e49\u63a8\u7406\u6269\u5c55\u5230\u6218\u672f\u51b3\u7b56] --\x3e P1[\u7c7b\u6bd4/Analogy<br/>Texts vs. Teams<br/>\u6587\u672c vs. \u56e2\u961f]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Model tactics as semantic structures<br/>\u5c06\u6218\u672f\u5efa\u6a21\u4e3a\u8bed\u4e49\u7ed3\u6784] --\x3e M1[\u8868\u793a/Representation<br/>Player as vector, team as aggregation<br/>\u7403\u5458\u4e3a\u5411\u91cf\uff0c\u56e2\u961f\u4e3a\u805a\u5408]\n    Method --\x3e M2[\u8bc4\u4f30/Evaluation<br/>Vector-distance metrics for tactical fit<br/>\u7528\u5411\u91cf\u8ddd\u79bb\u5ea6\u91cf\u6218\u672f\u9002\u914d\u5ea6]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br/>Generalizable framework for team optimization<br/>\u56e2\u961f\u4f18\u5316\u7684\u53ef\u6cdb\u5316\u6846\u67b6] --\x3e R1[\u5e94\u7528/Application<br/>Football, basketball, robotics, human-AI<br/>\u8db3\u7403\u3001\u7bee\u7403\u3001\u673a\u5668\u4eba\u3001\u4eba\u673a\u534f\u540c]\n    Results --\x3e R2[\u4ea7\u51fa/Outcome<br/>Interpretable strategy recommendations<br/>\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u5efa\u8bae]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00423",children:"https://arxiv.org/pdf/2601.00423"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/shengjun-zhang/VisualGRPO",children:"https://github.com/shengjun-zhang/VisualGRPO"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u53bb\u566a\u6b65\u4e0a\u4f18\u5316\uff0c\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u6a21\u7cca/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n    C --\x3e C1[\u63d0\u51faE-GRPO: \u71b5\u611f\u77e5\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n    C1 --\x3e C2[\u5408\u5e76\u4f4e\u71b5\u6b65\u4e3a\u9ad8\u71b5SDE\u91c7\u6837\u6b65\uff0c\u5176\u4ed6\u6b65\u7528ODE\u91c7\u6837/Merge low-entropy steps for SDE, use ODE elsewhere]\n    C1 --\x3e C3[\u5f15\u5165\u591a\u6b65\u5206\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u8ba1\u7b97/Introduce multi-step group normalized advantage]\n    D --\x3e D1[\u5728\u4e0d\u540c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027/Method effectiveness demonstrated across different reward settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [Semantic Field Theory, lexical fields, linguistic fields, transformer architectures, embedding spaces]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dimitris Vartziotis"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," TWT Science & Innovation, NIKI - Digital Engineering"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00448",children:"https://arxiv.org/pdf/2601.00448"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. 2. Analyzes how core properties of transformer architectures (e.g., distributed representations, attention) relate to Semantic Field Theory concepts. 3. Proposes that mathematical structure and language games are complementary perspectives, clarifying the scope and limits of statistical language models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98759f21c127cf9a440b464892816dd7e22af8f161a1d16ec91b582a8fefa647_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98759f21c127cf9a440b464892816dd7e22af8f161a1d16ec91b582a8fefa647_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper examines theories of linguistic meaning by contrasting social constructivist language games with a mathematically oriented Semantic Field Theory. It formalizes lexical and linguistic fields and analyzes their relation to transformer architecture properties. The authors conclude that the mathematical structure captured by LLMs and the social grounding of language games are complementary, not competing, views."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Contrasting theories of linguistic meaning (social vs. mathematical)")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Formalizing Semantic Field Theory (Lexfelder/Lingofelder) and analyzing transformer properties")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Mathematical structure and language games are complementary perspectives")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Networks Learn Deep Hierarchical Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [theoretical machine learning], [hierarchical models, residual networks, layerwise SGD, efficient learnability, teacher-student framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amit Daniely"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hebrew University of Jerusalem, Google Research Tel Aviv"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00455",children:"https://arxiv.org/pdf/2601.00455"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proves that layerwise SGD on residual networks can efficiently learn a class of hierarchical models with polynomial depth, surpassing previous learnable models limited to log-depth. 2. Introduces a formal model where the existence of human teachers, providing granular labels, naturally reveals a hierarchical structure that facilitates learning. 3. Suggests that the learnability of deep hierarchical models could form a theoretical basis for understanding why deep learning works."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper shows that layerwise stochastic gradient descent (SGD) on residual networks can efficiently learn a class of hierarchical models where labels are structured in increasingly complex levels. This class is more expressive, requiring polynomial depth, than previously known learnable models. The authors argue that this learnability, supported by a formal model of teaching, provides a potential theoretical foundation for understanding deep learning's success."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Deep Networks Learn Deep Hierarchical Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u7406\u89e3\u6df1\u5ea6\u7f51\u7edc\u4e3a\u4f55\u6709\u6548/Understanding why deep networks work]\nC --\x3e C1[\u5c42\u7ea7SGD\u4e0e\u6b8b\u5dee\u7f51\u7edc/Layerwise SGD on ResNets]\nC --\x3e C2[\u5f62\u5f0f\u5316\u6559\u5e08-\u5b66\u751f\u6a21\u578b/Formal teacher-student model]\nD --\x3e D1[\u53ef\u9ad8\u6548\u5b66\u4e60\u591a\u9879\u5f0f\u6df1\u5ea6\u6a21\u578b/Efficiently learn polynomial-depth models]\nD --\x3e D2[\u8d85\u8d8a\u5bf9\u6570\u6df1\u5ea6\u7535\u8def/Surpasses log-depth circuits]\nD --\x3e D3[\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u57fa\u7840/Provides basis for understanding deep learning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [guardrail models, multi-turn compression, efficiency optimization, safety screening, token reduction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hyunjun Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KAIST"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00454",children:"https://arxiv.org/pdf/2601.00454"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Defensive M2S, a training paradigm that fine-tunes guardrail models on compressed multi-turn conversations instead of full histories, 2. Provides formal complexity analysis showing training cost reduction from O(n\xb2) to O(n) and empirical token reduction of 93\xd7, 3. Demonstrates effectiveness across multiple guardrail models and compression templates, achieving high attack detection recall with 94.6% inference token reduction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad9caa971d784e8a994209f34a3b4e255812b43b2fa90a13bb0487b6e71e1395_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad9caa971d784e8a994209f34a3b4e255812b43b2fa90a13bb0487b6e71e1395_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost of processing full multi-turn conversations for LLM safety guardrails by proposing Defensive M2S, which trains guardrail models on compressed single-turn versions. This method significantly reduces training and inference tokens while maintaining high attack detection performance, enabling scalable safety screening."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: High computational cost of processing full multi-turn conversations for guardrail models"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Fine-tune guardrail models on M2S (Multi-turn to Single-turn) compressed conversations"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: 93\xd7 training token reduction, 94.6% inference token reduction, 93.8% attack detection recall"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Mixture-of-Experts, Orthogonality Regularization, Weight-Activation Gap, Sparse Activation, Expert Diversity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hyunjun Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00457",children:"https://arxiv.org/pdf/2601.00457"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Showed that orthogonality regularization fails to reduce weight-space overlap and yields inconsistent effects on model performance across different datasets. 2. Identified a significant disconnect between weight-space and activation-space orthogonality, with no significant correlation between the two. 3. Demonstrated that weight-space regularization is an unreliable optimization target for improving expert diversity in MoE models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the effectiveness of applying orthogonality loss to enforce expert diversity in Mixture-of-Experts (MoE) models. The analysis reveals that this geometric regularization fails to reduce weight-space overlap, does not translate to activation-space orthogonality, and leads to inconsistent performance changes. The findings demonstrate that weight-space regularization is unsuitable for achieving MoE diversity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Geometric Regularization in MoEs: The Disconnect Between Weights and Activations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u4e13\u5bb6\u591a\u6837\u6027\u7684\u51e0\u4f55\u6b63\u5219\u5316\u4f5c\u7528\u4e0d\u660e\u786e/The role of geometric regularization in expert specialization is unclear)\n    C --\x3e C1(\u5e94\u7528\u6b63\u4ea4\u6027\u635f\u5931\u4ee5\u5f3a\u5236\u4e13\u5bb6\u591a\u6837\u6027/Apply orthogonality loss to enforce expert diversity)\n    D --\x3e D1(\u6743\u91cd\u7a7a\u95f4\u91cd\u53e0\u672a\u51cf\u5c11/Weight-space overlap not reduced)\n    D --\x3e D2(\u6fc0\u6d3b\u7a7a\u95f4\u91cd\u53e0\u4fdd\u6301\u9ad8\u4f4d/Activation-space overlap remains high)\n    D --\x3e D3(\u6027\u80fd\u5f71\u54cd\u4e0d\u4e00\u81f4/Inconsistent effects on performance)\n    D --\x3e D4(\u6743\u91cd\u4e0e\u6fc0\u6d3b\u6b63\u4ea4\u6027\u65e0\u663e\u8457\u76f8\u5173/No significant correlation between weight and activation orthogonality)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Chains and Discrete Dynamical Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [neural chains, physics-informed neural networks (PINNs), finite-difference methods, Burgers equation, Eikonal equation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sauro Succi, Abhisek Ganguly, Santosh Ansumali"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Italian Institute of Technology, Jawaharlal Nehru Centre for Advanced Scientific Research (JNCASR), University of Roma Tre, Harvard University, Cornell University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00473",children:"https://arxiv.org/pdf/2601.00473"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and analyzes the analogy between transformer-based neural chains (without self-attention) and discrete dynamical systems from discretized neural integral/PDEs. 2. Conducts a comparative analysis between standard numerical discretization (finite-difference) and PINN learning for solving Burgers and Eikonal equations, showing they converge to similar dynamical knowledge. 3. Identifies that PINNs explore a vast space of random matrices, unlike the structured matrices of finite-difference methods, leading to more parameters, higher training costs, and reduced explainability for 1D problems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the connection between neural chains (transformers without self-attention) and discrete dynamical systems. It compares solving PDEs like Burgers and Eikonal equations using standard finite-difference methods versus Physics-Informed Neural Networks (PINNs), finding both methods yield similar solutions but PINNs use many more random, less interpretable parameters. The authors conclude that for these 1D problems, PINNs offer no efficiency advantage over traditional methods, though their potential for high-dimensional problems remains open."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Neural Chains and Discrete Dynamical Systems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5206\u6790\u795e\u7ecf\u94fe\u4e0e\u79bb\u6563\u52a8\u529b\u7cfb\u7edf\u7684\u7c7b\u6bd4/Analyze analogy between neural chains and discrete dynamical systems]\n    B --\x3e B2[\u6bd4\u8f83PINN\u4e0e\u4f20\u7edf\u6570\u503c\u65b9\u6cd5/Compare PINNs vs. traditional numerical methods]\n    C --\x3e C1[\u5c06\u6570\u503c\u79bb\u6563\u5316\u8868\u8ff0\u4e3a\u795e\u7ecf\u94fe/Cast numerical discretization as neural chains]\n    C --\x3e C2[\u4f7f\u7528PINN\u6c42\u89e3\u65b9\u7a0b/Use PINNs to solve equations]\n    C --\x3e C3[\u6bd4\u8f83\u77e9\u9635\u7ed3\u6784\u4e0e\u53c2\u6570\u7a7a\u95f4/Compare matrix structure and parameter space]\n    D --\x3e D1[\u4e24\u79cd\u65b9\u6cd5\u83b7\u5f97\u76f8\u540c\u52a8\u529b\u5b66\u77e5\u8bc6/Both methods acquire same dynamical knowledge]\n    D --\x3e D2[PINN\u4f7f\u7528\u66f4\u591a\u968f\u673a\u53c2\u6570/PINNs use more random parameters]\n    D --\x3e D3[1D\u95ee\u9898\u4e2dPINN\u65e0\u6548\u7387\u4f18\u52bf/No PINN efficiency advantage for 1D problems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [agentic AI, distributed agents, human-AI co-creation, progressive ideation, meta-cognitive workflow]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sankar B, Srinidhi Ranjini Girish, Aadya Bharti, Dibakar Sen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Science (IISc)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00475",children:"https://arxiv.org/pdf/2601.00475"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MIDAS, a novel framework that replaces single-AI systems with a distributed team of specialized AI agents for ideation. 2. Emulates a human meta-cognitive workflow to progressively refine ideas and assess them for both global and local novelty. 3. Establishes a new paradigm for human-AI co-creation, elevating the human from a passive filter to an active collaborative partner."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dee9a17774f8f69cd3f3a19a0507461cd7a05a21b3f1dec4db23b9a0a1c9bfa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dee9a17774f8f69cd3f3a19a0507461cd7a05a21b3f1dec4db23b9a0a1c9bfa_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of AI systems generating semantically clustered ideas that hinder novel ideation in engineering design. It proposes the MIDAS framework, which uses a distributed team of specialized AI agents to progressively refine and assess ideas for novelty. This approach enables true human-AI co-creation, making the human designer an active partner rather than a passive filter."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Progressive Ideation using an Agentic AI Framework<br>\u6e10\u8fdb\u5f0f\u6784\u601d\u7684\u667a\u80fd\u4f53AI\u6846\u67b6] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u751f\u6210\u60f3\u6cd5\u8bed\u4e49\u805a\u7c7b<br>AI-generated ideas are semantically clustered]\n    B --\x3e B2[\u65b0\u624b\u8bbe\u8ba1\u5e08\u6784\u601d\u56f0\u96be<br>Ideation is challenging for novice designers]\n    C --\x3e C1[\u63d0\u51faMIDAS\u6846\u67b6<br>Propose MIDAS framework]\n    C1 --\x3e C2[\u5206\u5e03\u5f0f\u4e13\u4e1aAI\u667a\u80fd\u4f53\u56e2\u961f<br>Distributed team of specialized AI agents]\n    C2 --\x3e C3[\u6a21\u62df\u5143\u8ba4\u77e5\u5de5\u4f5c\u6d41<br>Emulate meta-cognitive workflow]\n    D --\x3e D1[\u6e10\u8fdb\u5f0f\u63d0\u70bc\u4e0e\u8bc4\u4f30\u60f3\u6cd5<br>Progressively refines and assesses ideas]\n    D --\x3e D2[\u5b9e\u73b0\u4eba-AI\u534f\u540c\u521b\u9020<br>Enables true human-AI co-creation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent systems, evaluation suite, observability, execution traces, reliability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tie Ma, Yixi Chen, Vaastav Anand, Alessandro Cornacchia, Am\xe2ndio R. Faustino, Guanheng Liu, Shan Zhang, Hongbin Luo, Suhaib A. Fahmy, Zafar A. Qazi, Marco Canini"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KAUST, Beihang University, MPI-SWS, LUMS"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00481",children:"https://arxiv.org/pdf/2601.00481"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MAESTRO, a unified evaluation suite for standardizing the configuration, execution, and observability of LLM-based Multi-Agent Systems (MAS). 2. Provides a repository of examples and adapters to integrate diverse MAS frameworks and exports framework-agnostic execution traces with system-level signals (e.g., latency, cost). 3. Through controlled experiments with 12 MAS, demonstrates that MAS architecture is the dominant factor affecting resource profiles, reproducibility, and trade-offs, often outweighing changes in backend models or tools."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8621fc3bb6613fb26f1974a1a10ce1d6783df8fc57e3246b8e7ef34f2a5f4391_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8621fc3bb6613fb26f1974a1a10ce1d6783df8fc57e3246b8e7ef34f2a5f4391_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MAESTRO, a benchmark suite designed to systematically evaluate the testing, reliability, and observability of LLM-based multi-agent systems. It standardizes execution and provides detailed traces, enabling controlled comparisons. The study finds that MAS architecture is the primary driver of performance variance and system behavior, more so than model or tool changes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[MAESTRO: Multi-Agent Evaluation Suite] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[LLM-based MAS are stochastic and hard to debug]\n    Problem --\x3e P2[Lack of standardized system-level benchmarks]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Unified interface for MAS configuration]\n    Method --\x3e M2[Repository & adapters for integration]\n    Method --\x3e M3[Exports framework-agnostic traces & signals]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[MAS executions are structurally stable but temporally variable]\n    Results --\x3e R2[MAS architecture is the dominant driver of performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Multi-Agent Coordinated Rename Refactoring"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [se], [refactoring], [multi-agent system, coordinated renaming, scope inference, refactoring automation, IDE integration]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abhiram Bellur, Mohammed Raihan Ullah, Fraol Batole, Mohit Kansara, Masaharu Morimoto, Kai Ishikawa, Haifeng Chen, Yaroslav Zharov, Timofey Bryksin, Tien N. Nguyen, Hridesh Rajan, Danny Dig"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Colorado, Tulane University, University of Texas at Dallas, NEC Corporation, NEC Laboratories America, JetBrains Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00482",children:"https://arxiv.org/pdf/2601.00482"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Designed and implemented the first multi-agent framework for automating coordinated rename refactoring. 2. Introduced a novel approach where an initial developer refactoring is used as a clue to infer a Declared Scope, which guides subsequent automated refactorings. 3. Demonstrated significant performance improvements (2.3x-3.1x F1-score) over state-of-the-art methods through rigorous evaluation on established and new benchmarks, including successful integration into active open-source projects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e6c46d36197d90e221024d6225c9bcd5170e720a7f8f29bef40ad31d151bbda_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e6c46d36197d90e221024d6225c9bcd5170e720a7f8f29bef40ad31d151bbda_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the tedious and error-prone task of coordinated renaming in software development by proposing a novel multi-agent framework. The framework uses a developer's initial rename as a clue to infer a scope, which then guides specialized agents to safely identify and execute related refactorings using IDE APIs. The evaluation shows the system, CoRenameAgent, significantly outperforms existing methods in accuracy and demonstrates practical utility by having its automatically generated changes accepted into real projects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-Agent Coordinated Rename Refactoring] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u624b\u52a8\u91cd\u547d\u540d\u6613\u9519\u4e14\u7e41\u7410/Manual renaming is tedious & error-prone]\n    B --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u5047\u9633\u6027\u9ad8\u6216\u4e0d\u5b8c\u6574/Existing methods have high false positives or are incomplete]\n    C --\x3e C1[\u8303\u56f4\u63a8\u65ad\u4ee3\u7406/Scope Inference Agent]\n    C --\x3e C2[\u8ba1\u5212\u6267\u884c\u4ee3\u7406/Planned Execution Agent]\n    C --\x3e C3[\u590d\u5236\u4ee3\u7406/Replication Agent]\n    D --\x3e D1[F1\u5206\u6570\u663e\u8457\u63d0\u5347/Significant F1-score improvement]\n    D --\x3e D2[\u5b9e\u9645\u9879\u76ee\u9a8c\u8bc1/Practical validation in open-source projects]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [physics-based simulation], [motion distillation, differentiable simulation, multimodal large language model, material parameter estimation, video diffusion models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Miaowei Wang, Jakub Zadro\u017cny, Oisin Mac Aodha, Amir Vaxman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Edinburgh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00504",children:"https://arxiv.org/pdf/2601.00504"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://wangmiaowei.github.io/MotionPhysics.github.io/",children:"https://wangmiaowei.github.io/MotionPhysics.github.io/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7269\u7406\u53c2\u6570\u8c03\u4f18\u8017\u65f6\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6/Physical parameter tuning is time-consuming and requires expertise]\n    C --\x3e C1[\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u53c2\u6570/Use multimodal LLM to estimate parameters]\n    C --\x3e C2[\u63d0\u51fa\u53ef\u5b66\u4e60\u8fd0\u52a8\u84b8\u998f\u635f\u5931/Propose learnable motion distillation loss]\n    C2 --\x3e C2a[\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u53d6\u8fd0\u52a8\u5148\u9a8c/Extract motion priors from video diffusion models]\n    D --\x3e D1[\u572830+\u573a\u666f\u4e2d\u8bc4\u4f30/Evaluated across 30+ scenarios]\n    D --\x3e D2[\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Surpasses state-of-the-art]\n    D --\x3e D3[\u81ea\u52a8\u786e\u5b9a\u5408\u7406\u53c2\u6570/Automatically determines plausible parameters]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Illusion of Insight in Reasoning Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reasoning models], [reasoning shifts, self-correction, model uncertainty, intrinsic vs extrinsic, chain-of-thought]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Liv G. d'Aliberti, Manoel Horta Ribeiro"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00514",children:"https://arxiv.org/pdf/2601.00514"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Conducted a large-scale empirical study analyzing over 1 million reasoning traces across multiple models, domains, and training stages to investigate the nature and impact of mid-reasoning "Aha!" moments. 2. Found that such intrinsic reasoning shifts are rare, do not increase with training, and seldom improve accuracy, challenging the perception that they represent genuine model insight or self-correction. 3. Demonstrated that while intrinsic shifts are not beneficial, artificially triggering extrinsic shifts under conditions of high model uncertainty (high entropy) can reliably improve accuracy, showing these shifts are symptoms of unstable inference.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c00fcaa573a1b7a6558433ae1348cc7cefec26d36175047b45df1cd217f2516_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c00fcaa573a1b7a6558433ae1348cc7cefec26d36175047b45df1cd217f2516_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates whether reasoning models experience genuine "Aha!" moments of intrinsic self-correction during inference. Through a large-scale analysis of reasoning traces across multiple models and training checkpoints, the authors find that such mid-reasoning shifts are rare and ineffective, but that artificially triggering shifts when the model is uncertain can improve accuracy. The main conclusion is that these shifts are symptoms of unstable inference behavior, not a mechanism for intrinsic self-improvement.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[The Illusion of Insight in Reasoning Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u63a8\u7406\u6a21\u578b\u662f\u5426\u6709\u771f\u6b63\u7684\u201c\u987f\u609f\u201d\u65f6\u523b\uff1f/Do reasoning models have genuine "Aha!" moments?]\n    C --\x3e C1[\u5927\u89c4\u6a21\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u4e0e\u8bad\u7ec3\u68c0\u67e5\u70b9/Large-scale analysis of reasoning traces & training checkpoints]\n    D --\x3e D1[\u5185\u5728\u8f6c\u53d8\u7f55\u89c1\u4e14\u65e0\u6548/Intrinsic shifts are rare and ineffective]\n    D --\x3e D2[\u5916\u5728\u89e6\u53d1\u5728\u9ad8\u71b5\u4e0b\u53ef\u63d0\u5347\u51c6\u786e\u7387/Extrinsic triggering under high entropy improves accuracy]\n    D --\x3e D3[\u8f6c\u53d8\u662f\u4e0d\u7a33\u5b9a\u63a8\u7406\u7684\u75c7\u72b6/Shifts are symptoms of unstable inference]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Probability-Aware Parking Selection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [urban computing, intelligent transportation systems], [dynamic programming, stochastic modeling, parking availability, travel time estimation, navigation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Cameron Hickert, Sirui Li, Zhengbing He, Cathy Wu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Massachusetts Institute of Technology (MIT)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00521",children:"https://arxiv.org/pdf/2601.00521"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the probability-aware parking selection problem, shifting navigation from destination to optimal parking location. 2. Proposes an adaptable dynamic programming framework for decision-making under probabilistic parking availability. 3. Provides analytical and empirical error assessments for using stochastic observations to estimate parking availability, showing viability with real-world data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e593d7f649bbe1a9ad2d3e102fdbd08c74926a786541d28174a517648ac1dcd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e593d7f649bbe1a9ad2d3e102fdbd08c74926a786541d28174a517648ac1dcd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the underestimation of total travel time in navigation systems by introducing a probability-aware parking selection problem. It proposes a dynamic programming framework to direct drivers to the best parking location based on probabilistic availability, and demonstrates through simulations with Seattle data that this approach can yield significant time savings compared to probability-unaware baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Probability-Aware Parking Selection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5bfc\u822a\u7cfb\u7edf\u4f4e\u4f30\u603b\u884c\u7a0b\u65f6\u95f4/Navigation systems underestimate total travel time]\n    B --\x3e B2[\u5ffd\u7565\u505c\u8f66\u641c\u7d22\u65f6\u95f4/Ignore parking search time]\n    C --\x3e C1[\u6982\u7387\u611f\u77e5\u505c\u8f66\u9009\u62e9\u95ee\u9898/Probability-aware parking selection problem]\n    C --\x3e C2[\u52a8\u6001\u89c4\u5212\u51b3\u7b56\u6846\u67b6/Dynamic programming decision framework]\n    C --\x3e C3[\u57fa\u4e8e\u6982\u7387\u53ef\u7528\u6027\u51b3\u7b56/Decision based on probabilistic availability]\n    D --\x3e D1[\u5b9e\u9a8c\u9a8c\u8bc1: \u897f\u96c5\u56fe\u6570\u636e/Experiments: Seattle data]\n    D --\x3e D2[MAE\u4ece7%\u964d\u81f32%/MAE decreased from 7% to below 2%]\n    D --\x3e D3[\u8282\u7701\u65f6\u95f4\u9ad8\u8fbe66%/Time savings up to 66%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Siamese Recurrent Autoencoder, hybrid loss, real-time anomaly detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Laksh Advani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00516",children:"https://arxiv.org/pdf/2601.00516"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated the ineffectiveness of standard anomaly detection methods for agent trajectory validation, establishing the need for specialized models. 2. Proposed a novel, sequence-aware Siamese Recurrent Autoencoder with a hybrid loss function for real-time trajectory anomaly detection. 3. Demonstrated that the approach is over 17x faster than LLM Judge baselines, making it suitable for real-time deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of detecting anomalous action plans in autonomous LLM agents, where existing methods fail to capture sequential structure and context. It proposes Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss combining contrastive learning and reconstruction for unified anomaly detection. The method achieves high F1-scores (0.88-0.94) and significantly faster inference (32 ms) than LLM-based baselines, enabling real-time safety verification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Trajectory Guard] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u4e0d\u9002\u7528/Existing methods ill-suited]\n    Problem --\x3e P2[\u9700\u8981\u5e8f\u5217\u611f\u77e5/Need for sequence-awareness]\n    Method --\x3e M1[\u5b6a\u751f\u5faa\u73af\u81ea\u7f16\u7801\u5668/Siamese Recurrent Autoencoder]\n    Method --\x3e M2[\u6df7\u5408\u635f\u5931\u51fd\u6570/Hybrid Loss Function]\n    M2 --\x3e M2a[\u5bf9\u6bd4\u5b66\u4e60/Contrastive Learning]\n    M2 --\x3e M2b[\u91cd\u5efa/Reconstruction]\n    Results --\x3e R1[\u9ad8F1\u5206\u6570/High F1-scores (0.88-0.94)]\n    Results --\x3e R2[\u4f4e\u5ef6\u8fdf/32 ms latency]\n    Results --\x3e R3[\u5b9e\u65f6\u90e8\u7f72/Real-time deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [LSTM compression, model efficiency, retail forecasting, edge computing, hidden units]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ravi Teja Pagidoju"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Campbellsville University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00525",children:"https://arxiv.org/pdf/2601.00525"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/RaviTeja444/sales-forecast-LSTM",children:"https://github.com/RaviTeja444/sales-forecast-LSTM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematic evaluation of LSTM network sizes from 16 to 128 hidden units on real retail data. 2. Discovery that moderate compression (to 64 units) actually improves forecast accuracy. 3. Practical guidelines for model selection based on the accuracy-efficiency trade-off."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies LSTM model compression for resource-constrained retail sales forecasting by reducing the number of hidden units. The method involves systematically pruning the LSTM from 128 to 16 hidden units. The main conclusion is that reducing the model to 64 units not only makes it 73% smaller but also improves accuracy by 47%, showing larger models are not always better."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Optimizing LSTM for Resource-Constrained Retail Forecasting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LSTM\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u4e2d\u5c0f\u578b\u96f6\u552e\u5546\u96be\u4ee5\u90e8\u7f72/LSTM models are computationally expensive for mid-to-small retailers)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u901a\u8fc7\u9010\u6b65\u51cf\u5c11\u9690\u85cf\u5355\u5143\u8fdb\u884c\u6a21\u578b\u538b\u7f29/Model compression by reducing hidden units from 128 to 16)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: 64\u5355\u5143\u6a21\u578b\u66f4\u5c0f(76KB vs 280KB)\u4e14\u66f4\u51c6\u786e(MAPE 12.4% vs 23.6%)/64-unit model is smaller and more accurate)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ECR: Manifold-Guided Semantic Cues for Compact Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [embedding consistency regulation, manifold structure, semantic anchors, compact language models, on-device AI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chung-Wei Victor Yuan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," YVIC Research Lab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00543",children:"https://arxiv.org/pdf/2601.00543"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Embedding Consistency Regulation (ECR), a new framework that uses semantic anchors derived from teacher embeddings to preserve the underlying manifold structure in compact models. 2. Demonstrates that ECR stabilizes training and preserves semantic structure across tasks and languages without relying on matching logits or internal features, and adds minimal inference overhead. 3. Shows ECR is compatible with but independent of distillation, enabling better task alignment and deployment under strict efficiency or privacy constraints."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3184228efac0008fa39e8578d4daa8e597c9d20bf57f0662c9080c6c11607590_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3184228efac0008fa39e8578d4daa8e597c9d20bf57f0662c9080c6c11607590_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of semantic drift and loss of manifold structure in compact language models. It proposes the Embedding Consistency Regulation (ECR) framework, which uses offline-computed semantic anchors to guide the compact model's geometry. Experiments show ECR produces more compact, task-aligned representations, making low-capacity models more stable and easier to deploy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[ECR: Manifold-Guided Semantic Cues for Compact Language Models] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u5d29\u584c/Embedding Space Collapse]\n    Problem --\x3e P2[\u8bed\u4e49\u6f02\u79fb/Semantic Drift]\n    Method --\x3e M1[\u63d0\u53d6\u8bed\u4e49\u951a\u70b9/Derive Semantic Anchors]\n    Method --\x3e M2[\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027/Maintain Geometric Consistency]\n    Results --\x3e R1[\u7a33\u5b9a\u8bad\u7ec3/Stabilized Training]\n    Results --\x3e R2[\u4fdd\u7559\u8bed\u4e49\u7ed3\u6784/Preserved Semantic Structure]\n    Results --\x3e R3[\u7d27\u51d1\u4efb\u52a1\u5bf9\u9f50\u7a7a\u95f4/Compact Task-Aligned Space]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [gradient compression, orthogonal superposition, low-rank optimization, O-RAN, communication efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhiheng Guo, Zhaoyang Liu, Zihan Cen, Chenyuan Feng, Xinghua Sun, Xiang Chen, Tony Q. S. Quek, Xijun Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University, University of Exeter, Singapore University of Technology and Design"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00549",children:"https://arxiv.org/pdf/2601.00549"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified framework (CoCo-Fed) that jointly addresses local memory constraints and global communication bottlenecks in federated learning at the wireless edge. 2. Introduces a local double-dimension gradient down-projection technique to enable optimizer operation on low-rank structures, reducing memory footprint without adding inference overhead. 3. Designs a global transmission protocol using orthogonal subspace superposition to consolidate layer-wise updates into a single matrix per node, drastically cutting backhaul traffic."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec79bdf4a8b9f5a548f3d50e3ca33a9f5a73510543acb755bae04b50c1ce378_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec79bdf4a8b9f5a548f3d50e3ca33a9f5a73510543acb755bae04b50c1ce378_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes CoCo-Fed, a federated learning framework that compresses gradients locally and combines updates globally to reduce memory and communication overhead in O-RAN edge networks. It achieves this through low-rank gradient projection and orthogonal superposition for transmission. Experiments on angle-of-arrival estimation show it outperforms baselines in efficiency while maintaining convergence under non-IID data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CoCo-Fed: \u65e0\u7ebf\u8fb9\u7f18\u5185\u5b58\u4e0e\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u7edf\u4e00\u6846\u67b6<br>CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge]\n    A --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u5728O-RAN\u4e2d\u90e8\u7f72\u9762\u4e34\u5185\u5b58\u5899\u548c\u56de\u7a0b\u94fe\u8def\u5e26\u5bbd\u74f6\u9888<br>Core Problem: Memory wall and backhaul bandwidth bottleneck for large-scale NN deployment in O-RAN]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u672c\u5730\u68af\u5ea6\u53cc\u7ef4\u5ea6\u4e0b\u6295\u5f71\u4e0e\u57fa\u4e8e\u6b63\u4ea4\u5b50\u7a7a\u95f4\u53e0\u52a0\u7684\u5168\u5c40\u4f20\u8f93\u534f\u8bae<br>Core Method: Local double-dimension gradient down-projection & global orthogonal subspace superposition transmission protocol]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u5185\u5b58\u548c\u901a\u4fe1\u6548\u7387\uff0c\u4fdd\u6301\u9c81\u68d2\u6536\u655b<br>Key Results: Significantly improves memory/communication efficiency and maintains robust convergence under non-IID data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Comprehensive Dataset for Human vs. AI Generated Image Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [AI-Generated Images, Detection Techniques, Synthetic Media, Generative AI, Multimodal AI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San Jos\xe9 State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00553",children:"https://arxiv.org/pdf/2601.00553"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Comprehensive Dataset for Human vs. AI Generated Image Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["AI\u751f\u6210\u56fe\u50cf\u96be\u4ee5\u533a\u5206/AI-Generated Images Hard to Distinguish"]\n    Problem --\x3e P2["\u8bef\u5bfc\u6027\u5185\u5bb9\u4f20\u64ad/Spread of Misleading Content"]\n    Method --\x3e M1["\u6784\u5efaMS COCOAI\u6570\u636e\u96c6/Build MS COCOAI Dataset"]\n    Method --\x3e M2["\u4f7f\u7528\u4e94\u79cd\u751f\u6210\u5668/Use Five Generators"]\n    Results --\x3e R1["\u63d0\u4f9b96000\u4e2a\u6570\u636e\u70b9/Provide 96k Datapoints"]\n    Results --\x3e R2["\u5b9a\u4e49\u4e24\u9879\u68c0\u6d4b\u4efb\u52a1/Define Two Detection Tasks"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [IoT Security], [Interaction Threats, Static Analysis, Large Language Models, Trigger-Action-Condition Rules, Symbolic Reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jason Quantrill, Noura Khajehnouri, Zihan Guo, Manar H. Alalfi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Toronto Metropolitan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00559",children:"https://arxiv.org/pdf/2601.00559"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/JasonQuantrill/llm-v-static-results",children:"https://github.com/JasonQuantrill/llm-v-static-results"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted the first comprehensive evaluation of LLMs for detecting multi-category interaction threats in IoT TAC rules. 2. Introduced a structurally challenging Mutation dataset to test model robustness under rule transformations. 3. Demonstrated that symbolic reasoning baselines outperform LLMs in structural reasoning tasks, highlighting the need for hybrid architectures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88e5908c842c3a16fa536e15c334b07eda4e7076715625cdb48ee93058227761_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88e5908c842c3a16fa536e15c334b07eda4e7076715625cdb48ee93058227761_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the ability of Large Language Models (LLMs) to detect security threats in IoT automation rules, comparing them against traditional static analysis. The results show that while LLMs have good semantic understanding, they struggle with structural reasoning and are outperformed by symbolic methods, indicating they are not yet reliable for this safety-critical task alone."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[IoT\u89c4\u5219\u4ea4\u4e92\u5a01\u80c1/IoT Rule Interaction Threats]\n    C --\x3e C1[\u8bc4\u4f30LLMs\u4e0e\u9759\u6001\u5206\u6790/Evaluating LLMs vs. Static Analysis]\n    C --\x3e C2[\u4f7f\u7528\u539f\u59cb\u4e0e\u53d8\u5f02\u6570\u636e\u96c6/Using Original & Mutation Datasets]\n    D --\x3e D1[LLMs\u8bed\u4e49\u7406\u89e3\u597d\u4f46\u7ed3\u6784\u63a8\u7406\u5dee/LLMs Good at Semantics, Poor at Structural Reasoning]\n    D --\x3e D2[\u7b26\u53f7\u65b9\u6cd5\u7a33\u5b9a\u53ef\u9760/Symbolic Method Stable & Reliable]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Improving Scientific Document Retrieval with Academic Concept Index"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [information retrieval], [academic concept index, concept coverage-based generation (CCQGen), concept-focused auxiliary contexts (CCExpand)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jeyun Lee, Junhyoung Lee, Wonbin Kweon, Bowen Jin, Yu Zhang, Susik Yoon, Dongha Lee, Hwanjo Yu, Jiawei Han, Seongku Kang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea University, University of Illinois Urbana-Champaign, Texas A&M University, Yonsei University, Pohang University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00567",children:"https://arxiv.org/pdf/2601.00567"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an academic concept index that extracts and organizes key concepts from scientific papers using an academic taxonomy. 2. Proposes CCQGen, a concept coverage-based query generation method that adaptively conditions LLMs on uncovered concepts to produce complementary queries with broader coverage. 3. Develops CCExpand, a context augmentation technique that leverages document snippets as concise responses to concept-aware queries for improved relevance matching."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfdf054b52cb87f6cd4e63982623a18245a08006e97e04ad3614fc5ac270f85a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfdf054b52cb87f6cd4e63982623a18245a08006e97e04ad3614fc5ac270f85a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of adapting general-domain retrievers to scientific domains by introducing an academic concept index. The proposed method improves synthetic query generation (CCQGen) and context augmentation (CCExpand) using this structured index, leading to higher-quality queries and better retrieval performance. Experiments demonstrate improved conceptual alignment and retrieval effectiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Improving Scientific Document Retrieval with Academic Concept Index] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Adapting general-domain retrievers to scientific domains is challenging due to vocabulary mismatch and lack of domain-specific annotations.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduces academic concept index to enhance synthetic query generation (CCQGen) and context augmentation (CCExpand).]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Learning to be Reproducible: Custom Loss Design for Robust Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [training stability & reproducibility], [custom loss function, training robustness, reproducibility, stochastic factors, weight initialization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Friedrich Schiller University Jena, University of Technology Chemnitz, Norwegian University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00578",children:"https://arxiv.org/pdf/2601.00578"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and empirically analyzes the critical gap in ensuring consistent performance across training runs due to stochastic factors like weight initialization and data shuffling. 2. Proposes a novel Custom Loss Function (CLF) designed to explicitly balance predictive accuracy with training stability, reducing sensitivity to these stochastic factors. 3. Demonstrates through extensive experiments on diverse architectures and tasks (image classification, time series forecasting) that CLF significantly improves training robustness without sacrificing predictive performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of inconsistent model performance across training runs due to stochastic factors. It proposes a Custom Loss Function (CLF) to explicitly balance accuracy and stability, which is shown to improve training robustness without harming predictive performance in experiments on image classification and time series forecasting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Learning to be Reproducible: Custom Loss Design for Robust Neural Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u6a21\u578b\u6027\u80fd\u4e0d\u4e00\u81f4/Inconsistent Model Performance")\n    Problem --\x3e P2("\u5bf9\u968f\u673a\u56e0\u7d20\u654f\u611f/Sensitive to Stochastic Factors")\n    Method --\x3e M1("\u63d0\u51fa\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570/Propose Custom Loss Function (CLF)")\n    Results --\x3e R1("\u63d0\u9ad8\u8bad\u7ec3\u9c81\u68d2\u6027/Improves Training Robustness")\n    Results --\x3e R2("\u4fdd\u6301\u9884\u6d4b\u6027\u80fd/Maintains Predictive Performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Priority-Aware Multi-Robot Coverage Path Planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-robot path planning], [coverage path planning, priority-weighted latency, lexicographic optimization, spanning-tree, Steiner-tree]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kanghoon Lee, Hyeonjun Kim, Jiachen Li, Jinkyoo Park"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), Korea Military Academy (KMA), University of California, Riverside (UCR)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00580",children:"https://arxiv.org/pdf/2601.00580"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formally defines the Priority-Aware Multi-Robot Coverage Path Planning (PA-MCPP) problem, introducing priority weights and a lexicographic objective to minimize priority-weighted latency and makespan. 2. Proposes a scalable two-phase framework combining greedy zone assignment with local search and Steiner-tree-guided residual coverage. 3. Demonstrates through experiments that the method significantly reduces priority-weighted latency compared to baselines while maintaining competitive makespan and scales well with the number of robots."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4091ae11f186a26844a6a1c27c13cf633fa92da4e6636d53275a7d839f775d9f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4091ae11f186a26844a6a1c27c13cf633fa92da4e6636d53275a7d839f775d9f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of standard multi-robot coverage path planning, which treats all areas equally, by introducing a priority-aware version (PA-MCPP) where certain zones have higher urgency. The authors propose a two-phase method that first assigns and covers priority zones efficiently and then handles the remaining area. Experiments show their approach successfully reduces coverage delay for high-priority zones without significantly compromising the overall completion time."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Priority-Aware Multi-Robot Coverage Path Planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6807\u51c6MCPP\u5ffd\u89c6\u533a\u57df\u4f18\u5148\u7ea7/Standard MCPP ignores zone priority]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u6846\u67b6: \u8d2a\u5fc3\u5206\u914d\u4e0e\u65af\u5766\u7eb3\u6811\u5f15\u5bfc\u8986\u76d6/Two-phase framework: greedy assignment & Steiner-tree-guided coverage]\n    D --\x3e D1[\u663e\u8457\u964d\u4f4e\u4f18\u5148\u7ea7\u52a0\u6743\u5ef6\u8fdf/Significantly reduces priority-weighted latency]\n    D --\x3e D2[\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5b8c\u5de5\u65f6\u95f4/Maintains competitive makespan]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Mixture-of-Experts, federated fine-tuning, resource-aware, expert selection, sparsity-aware aggregation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zihan Fang, Zheng Lin, Senkang Hu, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong, The University of Hong Kong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00583",children:"https://arxiv.org/pdf/2601.00583"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a method to identify expert importance based on contributions to fine-tuning performance, enabling informed expert selection. 2. Proposes an adaptive expert subset selection mechanism from an information bottleneck perspective to align with heterogeneous client computing budgets. 3. Designs a sparsity-aware model aggregation strategy that weights updates from actively fine-tuned experts and gating parameters to mitigate destructive interference during global aggregation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes HFedMoE, a heterogeneous federated learning framework for fine-tuning large language models using Mixture-of-Experts. It addresses challenges in expert selection, resource heterogeneity, and aggregation interference by customizing expert subsets per client and using importance-weighted aggregation. Experiments show HFedMoE outperforms state-of-the-art methods in accuracy and convergence speed."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u8054\u90a6\u5fae\u8c03\u4e2dMoE\u9762\u4e34\u7684\u6311\u6218/Challenges in MoE for FL fine-tuning]\n    B1 --\x3e B2[\u4e13\u5bb6\u9009\u62e9\u56f0\u96be/Difficulty in expert selection]\n    B1 --\x3e B3[\u5ba2\u6237\u7aef\u8d44\u6e90\u5f02\u6784\u6027/Client resource heterogeneity]\n    B1 --\x3e B4[\u805a\u5408\u5e72\u6270/Aggregation interference]\n    C --\x3e C1[\u5b9a\u5236\u5316\u4e13\u5bb6\u5b50\u96c6/Customized expert subset per client]\n    C1 --\x3e C2[\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u4e13\u5bb6\u9009\u62e9/Importance-based expert selection]\n    C1 --\x3e C3[\u4fe1\u606f\u74f6\u9888\u89c6\u89d2\u7684\u9002\u914d/Adaptation via information bottleneck]\n    C --\x3e C4[\u7a00\u758f\u611f\u77e5\u7684\u6a21\u578b\u805a\u5408/Sparsity-aware model aggregation]\n    D --\x3e D1[\u66f4\u9ad8\u7684\u8bad\u7ec3\u7cbe\u5ea6/Higher training accuracy]\n    D --\x3e D2[\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6/Faster convergence speed]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Noise-Robust Tiny Object Localization with Flows"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Tiny Object Detection, Noise Robustness, Normalizing Flows, Uncertainty-Guided Optimization, Flow-Based Error Modeling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beihang University, Communication University of China, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00617",children:"https://arxiv.org/pdf/2601.00617"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Noise-Robust Tiny Object Localization with Flows] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u5bf9\u6807\u6ce8\u566a\u58f0\u654f\u611f/Tiny object detection is sensitive to annotation noise]\n    C --\x3e C1[\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u8bef\u5dee\u5efa\u6a21/Flow-based error modeling]\n    C --\x3e C2[\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4f18\u5316/Uncertainty-guided optimization]\n    D --\x3e D1[\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548/Validated on multiple datasets]\n    D --\x3e D2[\u63d0\u5347DINO\u57fa\u7ebf1.2% AP/Boosts DINO baseline by 1.2% AP]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Stronger Approximation Guarantees for Non-Monotone \u03b3-Weakly DR-Submodular Maximization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [submodular optimization], [weakly DR-submodular, continuous-greedy, Frank-Wolfe, approximation algorithm, down-closed convex body]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hareshkumar Jadav, Ranveer Singh, Vaneet Aggarwal"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IIT Indore, Purdue University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00611",children:"https://arxiv.org/pdf/2601.00611"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel approximation algorithm for maximizing non-monotone \u03b3-weakly DR-submodular functions over down-closed convex bodies. 2. A smooth approximation guarantee that recovers the 0.401 factor for DR-submodular (\u03b3=1) and degrades gracefully for \u03b3<1, improving upon prior bounds. 3. A hybrid algorithmic framework combining Frank-Wolfe-guided continuous-greedy with a \u03b3-aware double-greedy step to handle non-monotonicity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of maximizing non-monotone, nonnegative \u03b3-weakly DR-submodular functions over down-closed convex bodies. The authors propose a new algorithm that integrates a Frank-Wolfe-guided continuous-greedy approach with a \u03b3-aware double-greedy step. This method achieves state-of-the-art approximation guarantees that depend smoothly on the parameter \u03b3, improving upon previous results for this class of functions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Stronger Approximation Guarantees for Non-Monotone \u03b3-Weakly DR-Submodular Maximization<br>\u975e\u5355\u8c03\u03b3-\u5f31DR-\u5b50\u6a21\u6700\u5927\u5316\u7684\u66f4\u5f3a\u8fd1\u4f3c\u4fdd\u8bc1] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Maximize non-monotone \u03b3-weakly DR-submodular function<br>\u6700\u5927\u5316\u975e\u5355\u8c03\u03b3-\u5f31DR-\u5b50\u6a21\u51fd\u6570]\n    B --\x3e B2[Over down-closed convex body<br>\u5728\u4e0b\u5c01\u95ed\u51f8\u4f53\u4e0a]\n    C --\x3e C1[Frank-Wolfe-guided continuous-greedy<br>Frank-Wolfe\u5f15\u5bfc\u7684\u8fde\u7eed\u8d2a\u5fc3]\n    C --\x3e C2[\u03b3-aware double-greedy step<br>\u03b3\u611f\u77e5\u7684\u53cc\u8d2a\u5fc3\u6b65\u9aa4]\n    D --\x3e D1[Smooth approximation guarantee based on \u03b3<br>\u57fa\u4e8e\u03b3\u7684\u5e73\u6ed1\u8fd1\u4f3c\u4fdd\u8bc1]\n    D --\x3e D2[Recovers 0.401 for \u03b3=1 (DR-submodular)<br>\u03b3=1\u65f6\u6062\u590d0.401\u56e0\u5b50]\n    D --\x3e D3[Improves prior bounds for \u03b3<1<br>\u6539\u8fdb\u4e86\u03b3<1\u65f6\u7684\u73b0\u6709\u754c\u9650]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [Direct Preference Optimization, hallucination mitigation, difficulty-aware learning, multimodal large language models, preference data imbalance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Longtian Qiu, Shan Ning, Chuyu Zhang, Jiaxuan Sun, Xuming He"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," ShanghaiTech University, Lingang Laboratory, Shanghai Engineering Research Center of Intelligent Vision and Imaging"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00623",children:"https://arxiv.org/pdf/2601.00623"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://artanic30.github.io/project_pages/DA-DPO",children:"https://artanic30.github.io/project_pages/DA-DPO"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the problem of difficulty imbalance in multimodal DPO training, where models overfit to easy preference pairs. 2. Proposes a novel Difficulty-Aware DPO (DA-DPO) framework with a training-free difficulty estimation module using pre-trained VLMs and a distribution-aware voting strategy. 3. Introduces a difficulty-aware training mechanism that reweights preference pairs to prioritize harder examples, improving hallucination suppression and generalization without extra data or fine-tuning stages."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66cedf9b64154fde20f2a21a0c0dbc301932351057de033cf8e0323686aec2bc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66cedf9b64154fde20f2a21a0c0dbc301932351057de033cf8e0323686aec2bc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of Multimodal Large Language Models (MLLMs) overfitting to easy samples during Direct Preference Optimization (DPO), which limits hallucination reduction. It proposes DA-DPO, a cost-efficient framework that estimates sample difficulty without training and reweights the loss to focus on harder examples. Experiments show DA-DPO improves robustness to hallucinations and generalization across benchmarks while remaining computationally efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DA-DPO: Cost-efficient Difficulty-aware Preference Optimization] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: MLLM\u5e7b\u89c9 & DPO\u8fc7\u62df\u5408/Hallucinations & DPO Overfitting]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u96be\u5ea6\u611f\u77e5DPO/Difficulty-Aware DPO]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u66f4\u5f3a\u9c81\u68d2\u6027 & \u66f4\u597d\u6cdb\u5316\u6027/Stronger Robustness & Better Generalization]\n    C --\x3e C1[\u96be\u5ea6\u4f30\u8ba1/Difficulty Estimation]\n    C --\x3e C2[\u96be\u5ea6\u611f\u77e5\u8bad\u7ec3/Difficulty-Aware Training]\n    C1 --\x3e C1a[\u9884\u8bad\u7ec3VLM/Pre-trained VLMs]\n    C1 --\x3e C1b[\u5206\u5e03\u611f\u77e5\u6295\u7968/Distribution-Aware Voting]\n    C2 --\x3e C2a[\u91cd\u52a0\u6743\u504f\u597d\u5bf9/Reweight Preference Pairs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretable machine learning], [Bi-objective Optimization, Temporal Integrated Gradients, Optimal Path Oracle, Directed Acyclic Graph, Structured Regularization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kasra Fouladi, Hamta Rahmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated; inferred from email domains as independent researchers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00655",children:"https://arxiv.org/pdf/2601.00655"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the IGBO framework that trains interpretable models by formalizing the task as a bi-objective optimization problem, jointly optimizing for accuracy and adherence to domain knowledge constraints. 2. Introduces an Optimal Path Oracle to generate data-manifold-aware integration paths, addressing the Out-of-Distribution problem in Temporal Integrated Gradients computation. 3. Provides theoretical analysis proving convergence properties and robustness to mini-batch noise, and demonstrates empirical effectiveness on time-series data with minimal accuracy loss."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains models to be both accurate and interpretable by jointly optimizing a task loss and an interpretability loss derived from domain knowledge encoded as a DAG. It addresses a key challenge in gradient-based attribution (the OOD problem) by learning an Optimal Path Oracle. Empirical results show IGBO effectively enforces interpretability constraints with minimal impact on accuracy, outperforming standard regularization methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Interpretability-Guided Bi-objective Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/Black-box models lack interpretability]\n    B --\x3e B2[\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u7ea6\u675f/Post-hoc methods don't guarantee constraints]\n    B --\x3e B3[\u68af\u5ea6\u5f52\u56e0\u5b58\u5728OOD\u95ee\u9898/Gradient attribution has OOD problem]\n    C --\x3e C1[\u53cc\u76ee\u6807\u4f18\u5316\u6846\u67b6/Bi-objective Optimization Framework]\n    C --\x3e C2[\u4f7f\u7528DAG\u7f16\u7801\u9886\u57df\u77e5\u8bc6/Encode knowledge via DAG]\n    C --\x3e C3[\u6700\u4f18\u8def\u5f84\u9884\u8a00\u673a/Optimal Path Oracle]\n    D --\x3e D1[\u7406\u8bba\u6536\u655b\u6027\u8bc1\u660e/Theoretical convergence proof]\n    D --\x3e D2[\u5b9e\u8bc1\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf/Empirically outperforms baselines]\n    D --\x3e D3[\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931/Minimal accuracy loss]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KAIST, NTU Singapore, DeepAuto.ai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00664",children:"https://arxiv.org/pdf/2601.00664"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://taekyungki.github.io/AvatarForcing",children:"https://taekyungki.github.io/AvatarForcing"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u771f\u6b63\u4e92\u52a8/Lacks truly interactive communication]\n    Problem --\x3e P2[\u5355\u5411\u53cd\u5e94\u7f3a\u4e4f\u60c5\u611f/One-way responses lack emotional engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u9a71\u52a8\u6846\u67b6/Diffusion forcing framework]\n    Method --\x3e M2[\u65e0\u6807\u7b7e\u76f4\u63a5\u504f\u597d\u4f18\u5316/Label-free direct preference optimization]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u4ea4\u4e92/Low-latency real-time interaction (~500ms)]\n    Results --\x3e R2[6.8\u500d\u52a0\u901f/6.8x speedup]\n    Results --\x3e R3[80%\u7528\u6237\u504f\u597d/Over 80% user preference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Fast-weight Product Key Memory"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [long-context language modeling], [product key memory, fast weights, episodic memory, gradient descent, long-context]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tianyu Zhao, Llion Jones"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sakana AI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00671",children:"https://arxiv.org/pdf/2601.00671"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Fast-weight Product Key Memory (FwPKM), a novel architecture that transforms static Product Key Memory into a dynamic, fast-weight episodic memory., 2. Introduces a mechanism for dynamic parameter updates at both training and inference time via local chunk-level gradient descent, enabling rapid memorization and retrieval., 3. Demonstrates that FwPKM effectively complements semantic memory, significantly reduces perplexity on long-context data, and shows strong generalization to contexts much longer than those seen during training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc5e7437ccdc396cc0c89f8bda772596a4ad71d82cd906f8eae81c22b107608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc5e7437ccdc396cc0c89f8bda772596a4ad71d82cd906f8eae81c22b107608_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the trade-off between storage capacity and computational efficiency in sequence modeling layers. It proposes Fast-weight Product Key Memory (FwPKM), a dynamic architecture that updates parameters via local gradient descent to act as an episodic memory. Experiments show FwPKM reduces perplexity on long-context datasets and generalizes well to sequences much longer than those in training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Fast-weight Product Key Memory] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b58\u50a8\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861/Trade-off: Storage Capacity vs. Computational Efficiency]\n    C --\x3e C1[\u52a8\u6001\u5feb\u901f\u6743\u91cd\u4ea7\u54c1\u952e\u8bb0\u5fc6/Dynamic Fast-weight Product Key Memory (FwPKM)]\n    C --\x3e C2[\u672c\u5730\u5757\u7ea7\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0/Local Chunk-level Gradient Descent Updates]\n    D --\x3e D1[\u957f\u4e0a\u4e0b\u6587\u56f0\u60d1\u5ea6\u964d\u4f4e/Long-context Perplexity Reduction]\n    D --\x3e D2[\u4ece4K\u5230128K\u7684\u6cdb\u5316/Generalization from 4K to 128K Tokens]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00677",children:"https://arxiv.org/pdf/2601.00677"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[IRPO: Scaling the Bradley-Terry Model via RL] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Pairwise GRMs create O(n\xb2) bottleneck in RL/\u6210\u5bf9GRM\u5728RL\u4e2d\u9020\u6210O(n\xb2)\u74f6\u9888]\n    C --\x3e C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: \u5c06Bradley-Terry\u878d\u5165GRPO\u5b9e\u73b0\u9010\u70b9\u8bc4\u5206]\n    D --\x3e D1[SOTA among pointwise GRMs/\u5728\u9010\u70b9GRM\u4e2d\u8fbe\u5230SOTA]\n    D --\x3e D2[Outperforms pairwise GRMs in post-training/\u5728\u8bad\u7ec3\u540e\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6210\u5bf9GRM]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, spike-driven language models (SLMs), memory footprint, tiered search, embedded systems]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University (NYU) Abu Dhabi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00679",children:"https://arxiv.org/pdf/2601.00679"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: SLMs\u5185\u5b58\u5360\u7528\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907/SLMs have large memory footprints, challenging for resource-constrained embedded deployment]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u81ea\u52a8\u5316\u5206\u5c42\u91cf\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u7f51\u7edc\u5c42\u6b21\u3001\u5c42\u654f\u611f\u6027\u548c\u591a\u76ee\u6807\u6743\u8861\u51fd\u6570/Automated tiered quantization strategy using network hierarchy, layer sensitivity, and multi-objective trade-off]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u5360\u7528\u51cf\u5c11\u9ad8\u8fbe86.5%\uff0c\u529f\u8017\u964d\u4f4e\u9ad8\u8fbe20%\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b/Memory footprint reduced by up to 86.5%, power by up to 20%, performance close to original model]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [Large Language Models, Pedestrian Crossing Behavior, Vision-Augmented Reasoning, Domain Knowledge Adaptation, Low-Rank Adaptation (LoRA)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qingwen Pu, Kun Xie, Hong Yang, Guocong Zhai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Old Dominion University, Southwest Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00694",children:"https://arxiv.org/pdf/2601.00694"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PedX-LLM, a vision-and-knowledge enhanced LLM framework for generalizable pedestrian crossing behavior inference, shifting from site-specific pattern recognition to semantic reasoning. 2. Proposes integrating LLaVA-extracted visual features with textual data and domain knowledge to fine-tune a LLaMA-2-7B model via LoRA. 3. Demonstrates strong cross-site generalizability, where the model significantly outperforms baseline methods in zero-shot and few-shot settings on unseen environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e948e9bb60b0f41e32522739a2caeb9dea0028b52570cf642f19e2d94faefef7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e948e9bb60b0f41e32522739a2caeb9dea0028b52570cf642f19e2d94faefef7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PedX-LLM, a framework that enhances a large language model (LLaMA-2-7B) with visual features and domain knowledge to infer pedestrian crossing decisions. It achieves higher accuracy than traditional methods and demonstrates strong generalization to unseen sites, showing that vision-and-knowledge-enhanced reasoning overcomes the limitations of purely data-driven approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference<br>\u7528\u4e8e\u53ef\u6cdb\u5316\u884c\u4eba\u8fc7\u8857\u884c\u4e3a\u63a8\u7406\u7684\u89c6\u89c9\u4e0e\u77e5\u8bc6\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b] --\x3e B(Problem: \u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee<br>Problem: Poor Generalizability of Existing Methods)\n    A --\x3e C(Method: \u63d0\u51faPedX-LLM\uff0c\u96c6\u6210\u89c6\u89c9\u7279\u5f81\u4e0e\u9886\u57df\u77e5\u8bc6<br>Method: Propose PedX-LLM, Integrating Visual Features and Domain Knowledge)\n    A --\x3e D(Results: \u51c6\u786e\u7387\u63d0\u5347\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b<br>Results: Improved Accuracy and Strong Cross-Site Generalizability)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [domain adaptation], [data shift detection, performance degradation monitoring, vision-language model, confidence-based indicator, digital pathology]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hao Guan, Li Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Brigham and Women's Hospital, Harvard Medical School"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00716",children:"https://arxiv.org/pdf/2601.00716"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[VLM\u6027\u80fd\u5728\u6570\u636e\u504f\u79fb\u540e\u4e0b\u964d/VLM performance degrades after data shift]\n    C --\x3e C1[\u5f00\u53d1\u8f93\u5165\u6570\u636e\u504f\u79fb\u68c0\u6d4b\u5de5\u5177/DomainSAT toolbox for input shift]\n    C --\x3e C2[\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8f93\u51fa\u76d1\u6d4b\u6307\u6807/Confidence-based output indicator]\n    D --\x3e D1[\u8f93\u5165\u504f\u79fb\u68c0\u6d4b\u6709\u6548\u4f46\u4e0d\u603b\u5bf9\u5e94\u6027\u80fd\u4e0b\u964d/Input shift detection effective but not always correlates with degradation]\n    D --\x3e D2[\u7f6e\u4fe1\u5ea6\u6307\u6807\u4e0e\u6027\u80fd\u4e0b\u964d\u5bc6\u5207\u76f8\u5173/Confidence indicator closely related to degradation]\n    D --\x3e D3[\u7ed3\u5408\u4e24\u8005\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u76d1\u6d4b/Combining both enables more reliable monitoring]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] An Agentic Framework for Neuro-Symbolic Programming"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [neuro-symbolic programming, agentic workflow, declarative programming, DomiKnowS, human-in-the-loop]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aliakbar Nafar, Chetan Chigurupati, Danial Kamali, Hamid Karimian, Parisa Kordjamshidi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Michigan State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00743",children:"https://arxiv.org/pdf/2601.00743"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces AgenticDomiKnowS (ADS), a framework that translates natural language task descriptions into complete neuro-symbolic programs for the DomiKnowS library. 2. Employs an agentic workflow that breaks program generation into stages, generating, executing, and refining each component independently for higher accuracy. 3. Supports optional human-in-the-loop intervention, allowing both experienced and novice users to rapidly build programs, reducing development time from hours to minutes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8ba19ac112f8b6ab345bafea964dbc29f08a66659e47452cb34cd423893719c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8ba19ac112f8b6ab345bafea964dbc29f08a66659e47452cb34cd423893719c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of authoring neuro-symbolic programs, which is time-consuming and requires expertise in specific library syntax. It proposes AgenticDomiKnowS (ADS), an agentic framework that generates complete programs from free-form descriptions by creating and testing components in stages, with optional human refinement. This approach enables both experts and non-users to construct programs much faster, reducing development time significantly."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[An Agentic Framework for Neuro-Symbolic Programming] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Integrating symbolic constraints is time-consuming and requires library expertise]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Agentic workflow translates free-form descriptions into code with human-in-the-loop refinement]\n    D[\u5173\u952e\u7ed3\u679c/Results: Reduces development time from hours to 10-15 minutes for users and non-users]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Exploring the Performance of Large Language Models on Subjective Span Identification Tasks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [span identification], [large language models, in-context learning, chain of thought, aspect-based sentiment analysis, subjective spans]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alphaeus Dmonte, Roland Oruche, Tharindu Ranasinghe, Marcos Zampieri, Prasad Calyam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," George Mason University, University of Missouri, Lancaster University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00736",children:"https://arxiv.org/pdf/2601.00736"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Evaluates LLMs on subjective span identification tasks (sentiment analysis, offensive language identification, claim verification), an underexplored area compared to explicit tasks like NER. 2. Explores multiple LLM strategies including instruction tuning, in-context learning, and chain of thought for span identification. 3. Provides empirical results indicating that underlying textual relationships aid LLMs in identifying precise text spans."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33271e2c603477b20beb01a7eafd35b6ae5eb8b9dfab2b53139ccf619c75074_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33271e2c603477b20beb01a7eafd35b6ae5eb8b9dfab2b53139ccf619c75074_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the performance of Large Language Models on subjective text span identification tasks, such as sentiment analysis and offensive language detection, using strategies like in-context learning and chain of thought. The study finds that LLMs benefit from underlying relationships within the text to identify accurate spans, addressing a gap in current research focused on explicit span tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Exploring LLMs on Subjective Span Identification<br/>\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u89c2\u8de8\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Subjective span identification with LLMs is underexplored compared to explicit tasks like NER.<br/>\u4e0eNER\u7b49\u663e\u5f0f\u4efb\u52a1\u76f8\u6bd4\uff0cLLM\u5728\u4e3b\u89c2\u8de8\u5ea6\u8bc6\u522b\u65b9\u9762\u7684\u7814\u7a76\u4e0d\u8db3\u3002)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Evaluate LLMs using instruction tuning, in-context learning, and chain of thought on three tasks.<br/>\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8bc4\u4f30LLMs\u3002)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Underlying text relationships aid LLMs in identifying precise spans.<br/>\u6587\u672c\u4e2d\u7684\u6f5c\u5728\u5173\u7cfb\u6709\u52a9\u4e8eLLMs\u8bc6\u522b\u7cbe\u786e\u7684\u8de8\u5ea6\u3002)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," U\u011furcan \xd6zalp"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Turkish Aerospace"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00737",children:"https://arxiv.org/pdf/2601.00737"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Critic\u7f51\u7edc\u7cfb\u7edf\u6027\u9ad8\u4f30\u4ef7\u503c/Critic Networks Systematically Overestimate Value Estimates]\n    C --\x3e C1[\u4f7f\u7528\u5355\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u65f6\u5e8f\u5076\u7136\u4e0d\u786e\u5b9a\u6027/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]\n    C --\x3e C2[\u5728TD\u66f4\u65b0\u4e2d\u5e94\u7528\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u60b2\u89c2\u504f\u5dee/Apply Uncertainty-Based Pessimistic Bias in TD Updates]\n    C --\x3e C3[\u5bf9\u8bc4\u8bba\u5bb6\u548c\u884c\u52a8\u8005\u7f51\u7edc\u4f7f\u7528Dropout\u6b63\u5219\u5316/Use Dropout Regularization on Critic and Actor Networks]\n    D --\x3e D1[\u7f13\u89e3\u9ad8\u4f30\u504f\u5dee/Mitigates Overestimation Bias]\n    D --\x3e D2[\u5728\u968f\u673a\u73af\u5883\u4e2d\u4ea7\u751f\u98ce\u9669\u89c4\u907f\u884c\u4e3a/Leads to Risk-Averse Behavior in Stochastic Environments]\n    D --\x3e D3[\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027/Improves Computational Efficiency and Training Stability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [LLM agents, combinatorial optimization, portfolio optimization, heuristic algorithms, mixed-integer quadratic programming]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Simon Paquette-Greenbaum, Jiangbo Yu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," McGill University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00770",children:"https://arxiv.org/pdf/2601.00770"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Implements a novel LLM agentic framework for the Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem. 2. Explores and evaluates several concrete agentic architectures for automating complex portfolio optimization workflows. 3. Demonstrates that the framework matches state-of-the-art algorithms in benchmarks, alleviating development effort while maintaining acceptable performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/675ab12face3ffa5a24cbba64bf47553eac6b08cadcc7ab86dc8312ae824f240_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/675ab12face3ffa5a24cbba64bf47553eac6b08cadcc7ab86dc8312ae824f240_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using LLM agents to automate the complex workflows and heuristic algorithm development for Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO), a challenging combinatorial problem. The implemented agentic framework matches state-of-the-art algorithm performance on benchmarks, significantly reducing manual effort in the optimization process."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[CCPO: \u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u95ee\u9898/MIQP Problem]\n    B --\x3e B2[\u542f\u53d1\u5f0f\u7b97\u6cd5\u5f00\u53d1\u7e41\u7410/Heuristic Algorithm Development is Laborious]\n    C --\x3e C1[LLM\u667a\u80fd\u4f53\u6846\u67b6/LLM Agentic Framework]\n    C --\x3e C2[\u63a2\u7d22\u591a\u79cd\u67b6\u6784/Explore Multiple Architectures]\n    D --\x3e D1[\u5339\u914d\u6700\u5148\u8fdb\u7b97\u6cd5/Matches SOTA Algorithms]\n    D --\x3e D2[\u51cf\u8f7b\u5de5\u4f5c\u6d41\u7a0b\u8d1f\u62c5/Alleviates Complex Workflows]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning verification], [spectral graph analysis, attention patterns, Fiedler value, high-frequency energy ratio, sliding window attention]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Valentin No\xebl"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Devoteam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00791",children:"https://arxiv.org/pdf/2601.00791"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model's attention architecture."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Detecting valid mathematical reasoning in LLMs")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Spectral analysis of attention patterns as dynamic graphs")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: High classification accuracy, detects logical coherence, architectural dependency identified")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sunny Gupta, Amit Sethi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Bombay"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00785",children:"https://arxiv.org/pdf/2601.00785"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/sunnyinAI/FedHypeVAE"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FedHypeVAE] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u975eIID\u6570\u636e\u4e0e\u9690\u79c1\u6311\u6218/non-IID & Privacy]\n    C --\x3e C1[\u8d85\u7f51\u7edc\u751f\u6210\u6761\u4ef6VAE/Hypernetwork-Generated Conditional VAE]\n    C --\x3e C2[\u5dee\u5206\u9690\u79c1\u8bad\u7ec3/Differentially Private Training]\n    C --\x3e C3[MMD\u5bf9\u9f50\u4e0e\u6b63\u5219\u5316/MMD Alignment & Regularization]\n    D --\x3e D1[\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u7edf\u4e00/Unified Personalization & Privacy]\n    D --\x3e D2[\u53ef\u63a7\u591a\u57df\u5408\u6210/Controllable Multi-Domain Synthesis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ben Gurion University of the Negev, Tel Aviv University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00012",children:"https://arxiv.org/pdf/2601.00012"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Shaharak88/neural-brain-fields",children:"https://github.com/Shaharak88/neural-brain-fields"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>EEG data challenges: low SNR, variability, limited datasets"] --\x3e P1["\u5177\u4f53\u6311\u6218/Challenges<br>Varying length, low SNR, participant differences"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>NeRF-inspired neural network for EEG"] --\x3e M1["\u6838\u5fc3\u7c7b\u6bd4/Core Analogy<br>Viewpoints (NeRF) \u2194 Electrodes (EEG)"]\n    Method --\x3e M2["\u6280\u672f\u5b9e\u73b0/Technique<br>Train on single sample to get fixed weight vector"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Enables continuous visualization & reconstruction"] --\x3e R1["\u529f\u80fd\u4e00/Capability 1<br>Render signal at unseen times/positions"]\n    Results --\x3e R2["\u529f\u80fd\u4e8c/Capability 2<br>Simulate non-existent electrodes"]\n    Results --\x3e R3["\u5b9e\u8bc1\u7ed3\u679c/Empirical Result<br>Improves standard EEG network performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [health informatics], [deep learning, Holter ECG, explainable AI, time series analysis, risk prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Eran Zvuloni, Ronit Almog, Michael Glikson, Shany Brimer Biton, Ilan Green, Izhar Laufer, Offer Amir, Joachim A. Behar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technion - Israel Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00014",children:"https://arxiv.org/pdf/2601.00014"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed DeepHHF, a deep learning model that uses full 24-hour single-lead ECG recordings for heart failure risk prediction, outperforming models using short segments and clinical scores. 2. Created and utilized the large-scale Technion-Leumit Holter ECG (TLHE) dataset, comprising 69,663 recordings from 47,729 patients collected over 20 years. 3. Provided explainability analysis showing the model focuses on arrhythmias and heart abnormalities, with key attention patterns during daytime hours (8 AM to 3 PM), linking model decisions to clinically relevant features."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DeepHHF, a deep learning model that analyzes 24-hour single-lead ECG data to predict the 5-year risk of heart failure. The model achieved an AUC of 0.80, outperforming baseline methods, and identified high-risk individuals with a two-fold increased chance of hospitalization or death. The study demonstrates the feasibility of using long-term, continuous ECG data and explainable AI for non-invasive and accessible heart failure risk prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9884\u6d4b\u5fc3\u529b\u8870\u7aed\u98ce\u9669/Predict Heart Failure Risk]\n    B --\x3e B2[\u4f7f\u7528\u957f\u65f6\u7a0bECG\u6570\u636e/Using Long-term ECG Data]\n    C --\x3e C1[\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bDeepHHF/Deep Learning Model DeepHHF]\n    C --\x3e C2[\u5206\u679024\u5c0f\u65f6\u5355\u5bfc\u8054ECG/Analyze 24-hour Single-lead ECG]\n    C --\x3e C3[\u53ef\u89e3\u91ca\u6027\u5206\u6790/Explainability Analysis]\n    D --\x3e D1[AUC\u8fbe\u52300.80/AUC of 0.80]\n    D --\x3e D2[\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53/Identify High-risk Individuals]\n    D --\x3e D3[\u5173\u6ce8\u5fc3\u5f8b\u5931\u5e38\u4e0e\u65e5\u95f4\u6a21\u5f0f/Focus on Arrhythmias & Daytime Patterns]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [computational biology / bioinformatics], [DNA methylation, transformer, explainable AI (XAI), cross-tissue analysis, Alzheimer's disease]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gang Qu, Guanghao Li, Zhongming Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas Health Science Center at Houston"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00143",children:"https://arxiv.org/pdf/2601.00143"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed MethConvTransformer, a novel deep learning framework integrating convolutional and self-attention layers to capture local and long-range dependencies in DNA methylation data for Alzheimer's disease detection. 2. Introduced a method to incorporate subject-level covariates and tissue embeddings to disentangle shared and tissue-specific epigenetic effects, enabling robust cross-tissue biomarker discovery. 3. Demonstrated the model's superior performance and generalizability across multiple datasets and provided multi-resolution interpretability linking methylation patterns to known AD biological pathways."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd7853219944463e4a18ac7001e74a3c4474f9652d2f82ee38649d92981bed5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd7853219944463e4a18ac7001e74a3c4474f9652d2f82ee38649d92981bed5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting Alzheimer's disease using DNA methylation data, which varies across tissues. The authors propose MethConvTransformer, a deep learning model that combines convolutional and transformer layers to analyze methylation patterns from brain and peripheral tissues. The model outperforms baselines, provides interpretable biomarkers, and identifies disease-relevant biological pathways."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MethConvTransformer: A Deep Learning Framework for Cross-Tissue Alzheimer's Disease Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: DNA\u7532\u57fa\u5316\u7279\u5f81\u56e0\u7ec4\u7ec7\u548c\u7814\u7a76\u800c\u5f02\uff0c\u9650\u5236\u4e86AD\u751f\u7269\u6807\u5fd7\u7269\u7684\u53ef\u91cd\u590d\u6027/DNA methylation signatures vary across tissues and studies, limiting reproducible AD biomarkers)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faMethConvTransformer\uff0c\u7ed3\u5408\u5377\u79ef\u3001\u81ea\u6ce8\u610f\u529b\u548c\u7ec4\u7ec7\u5d4c\u5165/Propose MethConvTransformer, coupling CNN, self-attention, and tissue embeddings)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63ed\u793aAD\u76f8\u5173\u901a\u8def/Model outperforms baselines, provides interpretable biomarkers, reveals AD-associated pathways)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [compiler & ir], [electronic-photonic design automation, cross-layer co-design, inverse photonic design, AI-accelerated Maxwell solvers, photonic AI system]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hongjian Zhou, Ziang Yin, Jiaqi Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00130",children:"https://arxiv.org/pdf/2601.00130"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a cross-layer co-design framework for scalable photonic edge AI and Transformer inference architectures. 2. Introduced SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. 3. Developed AI-enabled photonic design automation techniques, including physical AI-based Maxwell solvers and a fabrication-aware inverse design framework."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76312a14ab1d9b01be6967c891d86f1c36fb6eebdf382d27578721d3ff3e1c24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76312a14ab1d9b01be6967c891d86f1c36fb6eebdf382d27578721d3ff3e1c24_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of designing electronic-photonic AI systems by proposing an open-source, AI-infused cross-layer co-design and automation framework. The method includes architecture designs for photonic AI, a modeling tool called SimPhony, and AI-powered design automation tools. The work aims to democratize and accelerate the development of next-generation photonic AI systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Democratizing Electronic-Photonic AI Systems<br>\u7535\u5b50-\u5149\u5b50AI\u7cfb\u7edf\u6c11\u4e3b\u5316] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Challenging EPIC AI System Design<br>EPIC AI\u7cfb\u7edf\u8bbe\u8ba1\u6311\u6218]\n    B --\x3e B2[Lack of Mature EPDA Toolchain<br>\u7f3a\u4e4f\u6210\u719f\u7684EPDA\u5de5\u5177\u94fe]\n    C --\x3e C1[Cross-Layer Co-Design Framework<br>\u8de8\u5c42\u534f\u540c\u8bbe\u8ba1\u6846\u67b6]\n    C --\x3e C2[SimPhony Modeling Tool<br>SimPhony\u5efa\u6a21\u5de5\u5177]\n    C --\x3e C3[AI-Enabled EPDA Stack<br>AI\u8d4b\u80fd\u7684EPDA\u5806\u6808]\n    D --\x3e D1[Democratizes Development<br>\u6c11\u4e3b\u5316\u5f00\u53d1]\n    D --\x3e D2[Enables Scalable EPDA<br>\u5b9e\u73b0\u53ef\u6269\u5c55EPDA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Minimum Weight Perfect Matching for Quantum Error Codes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Quantum Error Correction, Minimum Weight Perfect Matching, Graph Neural Networks, Transformer, Hybrid Decoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yotam Peled, David Zenati, Eliya Nachmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ben-Gurion University of the Negev"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00242",children:"https://arxiv.org/pdf/2601.00242"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid decoder (NMWPM) that integrates GNNs and Transformers to predict dynamic edge weights for the MWPM algorithm. 2. Formulated a novel proxy loss function to enable end-to-end training through the non-differentiable MWPM algorithm. 3. Demonstrated a significant reduction in Logical Error Rate (LER) compared to standard baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a neural-enhanced decoder for quantum error correction called Neural Minimum Weight Perfect Matching (NMWPM). It uses a hybrid architecture of Graph Neural Networks and Transformers to predict dynamic edge weights for the classical MWPM decoder, trained with a novel proxy loss. The method significantly reduces the Logical Error Rate, showing the advantage of combining neural networks with classical algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Neural Minimum Weight Perfect Matching for Quantum Error Codes"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Quantum error correction requires effective decoding."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Hybrid GNN+Transformer predicts weights for MWPM."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Significant reduction in Logical Error Rate."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [photonic AI, electronic-photonic design automation (EPDA), system-algorithm co-exploration, cross-layer toolchain, physical design automation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ziang Yin, Hongjian Zhou, Nicholas Gangi, Meng Zhang, Jeff Zhang, Zhaoran Rena Huang, Jiaqi Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arizona State University, Rensselaer Polytechnic Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00129",children:"https://arxiv.org/pdf/2601.00129"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified three essential considerations for scaling practical photonic AI systems: dynamic tensor operation support, systematic management of overheads, and robustness under hardware non-idealities. 2. Built a cross-layer toolchain (SimPhony, ADEPT, ADEPT-Z, Apollo, LiDAR) for quantitative, physically-grounded co-design from system exploration to physical layout. 3. Established a co-design loop that bridges architectural intent and deployable photonic hardware by translating physical costs into system-level metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd0ce87d1ffb64d07fd82b197f024f167052c6fb76c061ecf0885e18f056796_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd0ce87d1ffb64d07fd82b197f024f167052c6fb76c061ecf0885e18f056796_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of scaling photonic AI systems by identifying key design considerations and developing a cross-layer toolchain for system-algorithm co-exploration. The proposed method uses tools like SimPhony and Apollo to model physical costs and automate design, enabling quantitative trade-off analysis under real implementation constraints. The main conclusion is that this approach creates a physically-grounded co-design loop essential for realizing large-scale, deployable photonic AI hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Toward Large-Scale Photonics-Empowered AI Systems<br/>\u5927\u89c4\u6a21\u5149\u5b50\u8d4b\u80fdAI\u7cfb\u7edf] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Scaling AI constrained by data movement & efficiency<br/>AI\u6269\u5c55\u53d7\u9650\u4e8e\u6570\u636e\u79fb\u52a8\u4e0e\u80fd\u6548] --\x3e B1[\u6311\u62181: \u52a8\u6001\u5f20\u91cf\u64cd\u4f5c\u652f\u6301<br/>Dynamic tensor operation support]\n    B --\x3e B2[\u6311\u62182: \u5f00\u9500\u7cfb\u7edf\u7ba1\u7406<br/>Systematic overhead management]\n    B --\x3e B3[\u6311\u62183: \u786c\u4ef6\u975e\u7406\u60f3\u6027\u9c81\u68d2\u6027<br/>Robustness under hardware non-idealities]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Cross-layer toolchain for co-design<br/>\u8de8\u5c42\u5de5\u5177\u94fe\u534f\u540c\u8bbe\u8ba1] --\x3e C1[SimPhony: \u5b9e\u73b0\u611f\u77e5\u5efa\u6a21<br/>Implementation-aware modeling]\n    C --\x3e C2[ADEPT/ADEPT-Z: \u7535\u8def\u4e0e\u62d3\u6251\u63a2\u7d22<br/>Circuit & topology exploration]\n    C --\x3e C3[Apollo/LiDAR: \u7269\u7406\u8bbe\u8ba1\u81ea\u52a8\u5316<br/>Physical design automation]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Quantitative & physically-grounded co-design loop<br/>\u5b9a\u91cf\u4e14\u7269\u7406\u57fa\u7840\u7684\u534f\u540c\u8bbe\u8ba1\u5faa\u73af] --\x3e D1[\u8fde\u63a5\u7cfb\u7edf\u76ee\u6807\u4e0e\u53ef\u884c\u786c\u4ef6<br/>Connects system objectives to feasible hardware]\n    D --\x3e D2[\u4ea7\u751f\u53ef\u5236\u9020\u5e03\u5c40<br/>Produces manufacturable layouts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Hear the Heartbeat in Phases: Physiologically Grounded Phase-Aware ECG Biometrics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [biometrics], [ECG biometrics, phase-aware representation, hierarchical fusion, multi-prototype enrollment, graph neural networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jintao Huang, Lu Leng, Yi Zhang, Ziyuan Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanchang Hangkong University, Sichuan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00170",children:"https://arxiv.org/pdf/2601.00170"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Hierarchical Phase-Aware Fusion (HPAF) framework with a three-stage design (Intra-Phase Representation, Phase-Grouped Hierarchical Fusion, Global Representation Fusion) to explicitly model phase-specific characteristics within the cardiac cycle and avoid cross-feature entanglement. 2. Introduced a Heartbeat-Aware Multi-prototype (HAM) enrollment strategy to construct a multi-prototype gallery template set, mitigating the impact of heartbeat-specific noise and variability. 3. Demonstrated state-of-the-art performance on three public datasets under both closed and open-set settings, validating the effectiveness of the physiologically grounded approach."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2278c4a79bd346845b69356eeced50030b7ff8a854aa0dc0165503f22a96a9e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2278c4a79bd346845b69356eeced50030b7ff8a854aa0dc0165503f22a96a9e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that existing ECG biometric methods treat heartbeats as homogeneous, overlooking phase-specific characteristics. It proposes a Hierarchical Phase-Aware Fusion (HPAF) framework to independently model and hierarchically fuse features from different cardiac phases, along with a multi-prototype enrollment strategy. The method achieves state-of-the-art results on public datasets, showing the benefit of a physiologically grounded, phase-aware approach."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: Hear the Heartbeat in Phases<br/>Physiologically Grounded Phase-Aware ECG Biometrics] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5c06\u5fc3\u8df3\u89c6\u4e3a\u540c\u8d28\u4fe1\u53f7<br/>Existing methods treat heartbeats as homogeneous]\n    B --\x3e B2[\u5ffd\u7565\u5fc3\u52a8\u5468\u671f\u5185\u7684\u76f8\u4f4d\u7279\u5f02\u6027<br/>Overlook phase-specific characteristics]\n    C --\x3e C1[\u5206\u5c42\u76f8\u4f4d\u611f\u77e5\u878d\u5408\u6846\u67b6 HPAF<br/>Hierarchical Phase-Aware Fusion (HPAF) framework]\n    C1 --\x3e C1_1[\u9636\u6bb51: \u76f8\u4f4d\u5185\u8868\u793a IPR<br/>Stage 1: Intra-Phase Representation (IPR)]\n    C1 --\x3e C1_2[\u9636\u6bb52: \u76f8\u4f4d\u5206\u7ec4\u878d\u5408 PGHF<br/>Stage 2: Phase-Grouped Hierarchical Fusion (PGHF)]\n    C1 --\x3e C1_3[\u9636\u6bb53: \u5168\u5c40\u8868\u793a\u878d\u5408 GRF<br/>Stage 3: Global Representation Fusion (GRF)]\n    C --\x3e C2[\u5fc3\u8df3\u611f\u77e5\u591a\u539f\u578b\u6ce8\u518c HAM<br/>Heartbeat-Aware Multi-prototype (HAM) enrollment]\n    D --\x3e D1[\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0aSOTA<br/>State-of-the-art on three public datasets]\n    D --\x3e D2[\u95ed\u96c6\u548c\u5f00\u96c6\u8bbe\u7f6e<br/>Closed and open-set settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [computational biology], [single-cell genomics, data integration, normalization, dimensionality reduction, benchmarking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ali Anaissi, Seid Miad Zandavi, Weidong Huang, Junaid Akram, Basem Suleiman, Ali Braytee, Jie Hua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, University of Sydney, Broad Institute, University of New South Wales, Shaoyang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00277",children:"https://arxiv.org/pdf/2601.00277"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a systematic evaluation of preprocessing and integration methods for single-cell multimodal data, which had been lacking. 2. Benchmarked a comprehensive pipeline involving combinations of seven normalization, five integration, and four dimensionality reduction methods across six diverse datasets. 3. Provided empirical findings on the performance and time-efficiency of leading methods, identifying Seurat and Harmony as top performers and highlighting the compatibility of UMAP."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7cf9641df8a7176dd7483adbf7e7c16827ddac25d332fe518d19b7c39109f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7cf9641df8a7176dd7483adbf7e7c16827ddac25d332fe518d19b7c39109f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper systematically benchmarks combinations of preprocessing and integration methods for single-cell multimodal genomic data. The study evaluates various normalization, integration, and dimensionality reduction techniques across diverse datasets. The results show that Seurat and Harmony excel at integration, with Harmony being more time-efficient, and UMAP is the most compatible dimensionality reduction method."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Benchmarking Preprocessing and Integration Methods in Single-Cell Genomics] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6574\u5408\u5355\u7ec6\u80de\u591a\u6a21\u6001\u6570\u636e/Integrating single-cell multimodal data]\n    B --\x3e B2[\u7f3a\u4e4f\u9884\u5904\u7406\u7b56\u7565\u7684\u7cfb\u7edf\u8bc4\u4f30/Lack of systematic evaluation with preprocessing]\n    C --\x3e C1[\u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\u7ec4\u5408/Evaluating combinations of methods]\n    C --\x3e C2[\u4f7f\u7528\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6307\u6807/Using six datasets and three metrics]\n    D --\x3e D1[Seurat\u4e0eHarmony\u8868\u73b0\u4f18\u5f02/Seurat and Harmony excel]\n    D --\x3e D2[Harmony\u65f6\u95f4\u6548\u7387\u66f4\u9ad8/Harmony is more time-efficient]\n    D --\x3e D3[UMAP\u517c\u5bb9\u6027\u6700\u597d/UMAP is most compatible]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [multi-functional RIS, NOMA, energy efficiency, hybrid deep reinforcement learning, parametrized sharing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chi-Te Kuo, Li-Hsiang Shen, Jyun-Jhe Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Central University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00538",children:"https://arxiv.org/pdf/2601.00538"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates an energy efficiency maximization problem for a multi-MF-RIS-aided NOMA downlink network, jointly optimizing power, beamforming, RIS configurations, and RIS positions. 2. Proposes a Parametrized Sharing scheme for Multi-Agent Hybrid Deep Reinforcement Learning (PMHRL) that combines multi-agent PPO for continuous variables and DQN for discrete variables. 3. Demonstrates through simulations that the proposed PMHRL and multi-MF-RIS architecture achieve superior energy efficiency compared to various benchmarks and alternative system scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of maximizing energy efficiency in downlink NOMA networks assisted by multiple multi-functional RISs. The authors propose a novel parametrized sharing scheme for a multi-agent hybrid deep reinforcement learning algorithm (PMHRL) to jointly optimize power, beamforming, and RIS parameters. Simulation results show that the proposed method achieves the highest energy efficiency compared to other benchmarks and system configurations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u6700\u5927\u5316\u591aMF-RIS\u8f85\u52a9NOMA\u7f51\u7edc\u7684\u80fd\u6548/Maximize EE of multi-MF-RIS-aided NOMA network")\n    Method --\x3e M1("\u53c2\u6570\u5171\u4eab\u7684\u591a\u667a\u80fd\u4f53\u6df7\u5408DRL (PMHRL)/Parametrized Sharing Multi-Agent Hybrid DRL (PMHRL)")\n    M1 --\x3e M1_1("PPO\u5904\u7406\u8fde\u7eed\u53d8\u91cf/PPO for continuous variables")\n    M1 --\x3e M1_2("DQN\u5904\u7406\u79bb\u6563\u53d8\u91cf/DQN for discrete variables")\n    Results --\x3e R1("PMHRL\u80fd\u6548\u6700\u9ad8/PMHRL achieves highest EE")\n    Results --\x3e R2("\u4f18\u4e8e\u65e0\u53c2\u6570\u5171\u4eab\u3001\u7eafPPO/DQN\u57fa\u51c6/Superior to benchmarks without sharing, pure PPO/DQN")\n    Results --\x3e R3("\u591aMF-RIS NOMA\u4f18\u4e8e\u4f20\u7edfRIS\u7b49\u573a\u666f/Multi-MF-RIS NOMA outperforms traditional RIS, etc.")'}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);