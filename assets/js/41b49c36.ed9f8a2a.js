"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1559],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},76050:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CL/20260105-20260111","title":"20260105-20260111 (cs.CL)","description":"2026-01-05","source":"@site/docs/daily/cs_CL/20260105-20260111.md","sourceDirName":"daily/cs_CL","slug":"/daily/cscl/20260105-20260111","permalink":"/ai_toutiao/daily/cscl/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cscl/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.CL)","permalink":"/ai_toutiao/daily/cscl/20251229-20260104"},"next":{"title":"cs.CR","permalink":"/ai_toutiao/category/cscr"}}');var a=i(74848),r=i(28453);const t={slug:"/daily/cscl/20260105-20260111"},o="20260105-20260111 (cs.CL)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20260105-20260111-cscl",children:"20260105-20260111 (cs.CL)"})}),"\n",(0,a.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [retrieval-augmented generation], [Monte Carlo Tree Search, reasoning-aware retrieval, coarse-to-fine retrieval, multi-turn dialogue, knowledge diversity]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuqi Liu, Bowei He, Chen Ma, Linqi Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong, City University of Hong Kong Shenzhen Research Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00003",children:"https://arxiv.org/pdf/2601.00003"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a reasoning-aware knowledge retrieval method that aligns retrieved information with the logical structure of conversations, moving beyond semantic similarity. 2. Introduces a coarse-to-fine retrieval approach that first finds a contextually relevant knowledge sub-region and then refines it for reasoning-specific knowledge. 3. Employs a Monte Carlo Tree Search-inspired method to navigate knowledge sentences using common keywords, enhancing retrieval diversity and informativeness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b609f46963ae2b2b017ba13f445da7491064f311d7e663fa59eda7b85dd69638_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b609f46963ae2b2b017ba13f445da7491064f311d7e663fa59eda7b85dd69638_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of integrating retrieval and reasoning for LLMs by proposing a reasoning-aware knowledge retrieval method. It uses a coarse-to-fine approach guided by Monte Carlo Tree Search to find knowledge aligned with conversational logic. Experiments show the method better captures human reasoning and produces more diverse, informative responses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LLMs struggle to integrate retrieval and reasoning effectively."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Coarse-to-fine, MCTS-inspired reasoning-aware knowledge retrieval."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Better alignment with human reasoning, more diverse and informative responses."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [mental health language modeling], [large language models, fine-tuning, PHQ-9, Nigerian Pidgin, depression screening]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00004",children:"https://arxiv.org/pdf/2601.00004"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Finetuning LLMs for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited depression screening in Nigeria due to language barriers and lack of clinicians]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Fine-tune LLMs on annotated Nigerian Pidgin dataset for PHQ-9 scoring]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>GPT-4.1 achieved 94.5% accuracy and best cultural appropriateness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [tokenizer transplant, model composition, supply-chain vulnerability, sparse solver, spectral mimicry]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Purdue University, Carnegie Mellon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00065",children:"https://arxiv.org/pdf/2601.00065"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/xz-liu/tokenforge",children:"https://github.com/xz-liu/tokenforge"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies tokenizer transplant as a novel attack surface in the LLM composition supply chain, 2. Introduces the concept of a "breaker token"\u2014a single, engineered token that is inert in a donor model but maliciously activates after transplant, 3. Formalizes and instantiates the attack as a dual-objective optimization problem solved with a sparse solver, demonstrating its training-free nature, stealth, and persistence.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper identifies a security vulnerability in the tokenizer transplant step required for composing different LLMs. The authors propose a method to engineer a single "breaker token" that, when added to a donor model, remains harmless but sabotages a base model after transplant by exploiting coefficient reuse. The attack is stealthy, training-free, and persistent, revealing a hidden risk in modular AI pipelines.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Tokenizer transplant introduces a supply-chain vulnerability for LLM composition]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Engineer a single "breaker token" exploiting coefficient reuse via sparse solver]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Stealthy, training-free attack that persists against fine-tuning and merging]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [tool-use, rule learning, minimum description length, neuro-symbolic, prompt injection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiang Gao, Yuguang Yao, Qi Zhang, Kaiwen Dong, Avinash Baidya, Ruocheng Guo, Hilaf Hasson, Kamalika Das"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Intuit AI Research, Temple University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00086",children:"https://arxiv.org/pdf/2601.00086"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RIMRULE, a neuro-symbolic method for LLM adaptation that distills interpretable rules from failure traces and injects them dynamically during inference. 2. Introduces a Minimum Description Length (MDL) objective to consolidate and select rules, favoring generality and conciseness. 3. Demonstrates that the learned symbolic rules are portable and can improve performance across different LLM architectures without weight modification."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5604dcae3f77a38459a71d5615de42441af3f38dbedaf069349bff470ad72d2f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5604dcae3f77a38459a71d5615de42441af3f38dbedaf069349bff470ad72d2f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLMs struggling to reliably use domain-specific or under-documented tools. It proposes RIMRULE, a method that learns compact, interpretable rules from failure traces using an MDL objective and injects them into prompts during inference. The approach improves tool-use accuracy on both seen and unseen tools, outperforms prompting baselines, and shows that learned rules are portable across different LLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RIMRULE: Improving Tool-Using Language Agents] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLMs struggle with domain-specific tools / LLMs\u96be\u4ee5\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u5de5\u5177]\n    C --\x3e C1[Dynamic rule injection from failure traces / \u4ece\u5931\u8d25\u8f68\u8ff9\u52a8\u6001\u6ce8\u5165\u89c4\u5219]\n    C --\x3e C2[MDL-guided rule consolidation / MDL\u5f15\u5bfc\u7684\u89c4\u5219\u6574\u5408]\n    D --\x3e D1[Improves accuracy on seen/unseen tools / \u63d0\u5347\u5de5\u5177\u4f7f\u7528\u51c6\u786e\u7387]\n    D --\x3e D2[Rules are portable across LLMs / \u89c4\u5219\u53ef\u8de8LLM\u8fc1\u79fb]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [causal reasoning], [fuzzy cognitive maps, large-language-model agent, causal feedback, equilibrium limit cycles, agentic leash]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Akash Kumar Panda, Olaoluwa Adigun, Bart Kosko"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California, Florida International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00097",children:"https://arxiv.org/pdf/2601.00097"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel LLM agent designed to autonomously extract and construct causal feedback Fuzzy Cognitive Maps (FCMs) from raw text. 2. A three-step instruction-guided process for systematically extracting key concepts and causal edges to build the FCM dynamical system. 3. Demonstration that the LLM-generated FCMs converge to the same equilibrium dynamics as human-generated ones and that mixed FCMs from different LLMs can create new equilibria."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc61a9c25939c9840dd408a24d27f4650c47178c297c4db425bc560882426f60_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an LLM agent to autonomously extract causal feedback Fuzzy Cognitive Maps from text. The agent uses a three-step process to identify concepts and causal edges, forming a dynamical system. The generated FCMs matched human-generated equilibrium dynamics and mixing models from different LLMs produced new equilibria for better causal approximation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to autonomously extract causal structures from text?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Design an LLM agent with a three-step instruction process to build FCMs from text.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: LLM-generated FCMs match human equilibrium dynamics; mixed FCMs create new equilibria.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [meta-reinforcement learning, constraint propagation, graph attention network, structured inference, green ai]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Iowa State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00095",children:"https://arxiv.org/pdf/2601.00095"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MetaJuLS, a meta-reinforcement learning framework for learning universal constraint propagation policies applicable across languages and tasks without task-specific retraining. 2. Formulates structured inference as adaptive constraint propagation and trains a Graph Attention Network policy via meta-learning, achieving significant speedups (1.5-2.0x) over GPU-optimized baselines with minimal accuracy loss. 3. Demonstrates rapid cross-domain adaptation (5-15 seconds) and contributes to Green AI by reducing inference carbon footprint through fewer propagation steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5284c89e9a9eec9c1882da4c36e7d1e9bbce97ea559fe29f19c435e5f8b8854_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5284c89e9a9eec9c1882da4c36e7d1e9bbce97ea559fe29f19c435e5f8b8854_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the inefficiency of structured inference (e.g., JSON parsing) in large language models by proposing MetaJuLS, a meta-reinforcement learning method that learns adaptive constraint propagation policies. This approach achieves up to 2x speedup over baselines while maintaining high accuracy and enables fast adaptation to new languages and tasks. The work contributes to more efficient and environmentally friendly LLM inference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Universal Adaptive Constraint Propagation<br>\u901a\u7528\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["LLMs need efficient structured inference<br>LLM\u9700\u8981\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u63a8\u7406"] --\x3e P1["Wasted computation from static checking<br>\u9759\u6001\u68c0\u67e5\u5bfc\u81f4\u8ba1\u7b97\u6d6a\u8d39"]\n    Problem --\x3e P2["Need for cross-domain generalization<br>\u9700\u8981\u8de8\u9886\u57df\u6cdb\u5316"]\n    Method["MetaJuLS: Meta-RL for constraint propagation<br>MetaJuLS: \u7528\u4e8e\u7ea6\u675f\u4f20\u64ad\u7684\u5143\u5f3a\u5316\u5b66\u4e60"] --\x3e M1["Learns universal policies via meta-learning<br>\u901a\u8fc7\u5143\u5b66\u4e60\u5b66\u4e60\u901a\u7528\u7b56\u7565"]\n    Method --\x3e M2["Uses Graph Attention Network<br>\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc"]\n    Results["Key Results<br>\u5173\u952e\u7ed3\u679c"] --\x3e R1["1.5-2.0x speedup<br>1.5-2.0\u500d\u52a0\u901f"]\n    Results --\x3e R2["Fast adaptation (5-15s)<br>\u5feb\u901f\u9002\u5e94(5-15\u79d2)"]\n    Results --\x3e R3["Contributes to Green AI<br>\u52a9\u529b\u7eff\u8272AI"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [legal text generation and evaluation], [patent drafting, LLM-as-a-judge, Chain-of-Legal-Thought, legal compliance, automated evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yongmin Yoo, Kris W Pan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Macquarie University, Amazon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00166",children:"https://arxiv.org/pdf/2601.00166"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Pat-DEVAL, the first multi-dimensional evaluation framework specifically designed for assessing the quality of generated patent description bodies. 2. Proposes Chain-of-Legal-Thought (CoLT), a novel legally-constrained reasoning mechanism that enforces sequential, patent-law-specific analysis within the LLM-as-a-judge paradigm. 3. Establishes and validates the framework on the Pap2Pat-EvalGold dataset, demonstrating superior correlation with expert judgments, especially in legal compliance, outperforming baseline metrics and existing LLM evaluators."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9df50304a9b3b4d3c4c409af9be2c858e612c5747a351d8395d536945a60fa6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9df50304a9b3b4d3c4c409af9be2c858e612c5747a351d8395d536945a60fa6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of effective evaluation methods for AI-generated patent descriptions, which must meet strict legal standards. It proposes Pat-DEVAL, a framework that uses a Chain-of-Legal-Thought mechanism with an LLM-as-a-judge to assess legal compliance and structural coherence. The method shows significantly higher correlation with expert evaluations than existing approaches, proving the importance of explicit legal constraints for automated patent drafting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u4e13\u5229\u8bf4\u660e\u4e66\u7684\u7ed3\u6784\u8fde\u8d2f\u6027\u4e0e\u6cd5\u5f8b\u5408\u89c4\u6027/Existing methods fail to assess structural coherence and legal compliance of patent descriptions]\n    C --\x3e C1[\u63d0\u51faPat-DEVAL\u6846\u67b6\u4e0eChain-of-Legal-Thought\u63a8\u7406\u673a\u5236/Proposes Pat-DEVAL framework with Chain-of-Legal-Thought reasoning]\n    C --\x3e C2[\u91c7\u7528LLM-as-a-judge\u8303\u5f0f\uff0c\u6ce8\u5165\u6cd5\u5b9a\u7ea6\u675f/Adopts LLM-as-a-judge paradigm with statutory constraints]\n    D --\x3e D1[\u5728Pap2Pat-EvalGold\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1/Validated on Pap2Pat-EvalGold dataset]\n    D --\x3e D2[\u76ae\u5c14\u900a\u76f8\u5173\u60270.69\uff0c\u6cd5\u5f8b\u5408\u89c4\u6027\u76f8\u5173\u60270.73/Pearson correlation 0.69, Legal-Professional Compliance correlation 0.73]\n    D --\x3e D3[\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6307\u6807/Significantly outperforms baseline metrics]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [emotion recognition in conversation], [ablation study, conversational context, discourse markers, causal context, IEMOCAP]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cheonkam Jeong, Adeline Nyamathi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Irvine"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00181",children:"https://arxiv.org/pdf/2601.00181"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. A rigorous ablation study revealing that conversational context is paramount for ERC, with performance saturating within 10-30 preceding turns, and that hierarchical sentence representations and external affective lexicons provide no additional benefit when context is used. 2. Achieving state-of-the-art text-only performance on IEMOCAP using simple architectures with strictly causal context. 3. A novel linguistic analysis connecting recognition to generation, finding a significant association between emotion and discourse marker positioning, particularly that "sad" utterances use fewer left-periphery markers and rely more on context for disambiguation.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e38dfbfb3b50e432c39aabed201ddaf199012203ee5e7e82a7aa044267fc2a2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e38dfbfb3b50e432c39aabed201ddaf199012203ee5e7e82a7aa044267fc2a2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper systematically analyzes Emotion Recognition in Conversation (ERC) to identify which architectural components matter and connects recognition insights to linguistic patterns for generation. Through ablation studies on IEMOCAP, it finds conversational context is most critical, and via linguistic analysis, it discovers that emotion correlates with discourse marker usage. The main conclusion is that simple models with causal context are sufficient for high performance, and the lack of explicit pragmatic signals in "sad" utterances explains their greater reliance on context.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Understanding Emotion in Discourse] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[ERC\u6a21\u578b\u54ea\u4e9b\u7ec4\u4ef6\u771f\u6b63\u6709\u6548?/Which ERC components matter?]\n    B --\x3e B2[\u5982\u4f55\u8fde\u63a5\u8bc6\u522b\u4e0e\u751f\u6210?/How to connect recognition & generation?]\n    C --\x3e C1[\u7cfb\u7edf\u6d88\u878d\u7814\u7a76/Systematic Ablation Study]\n    C --\x3e C2[\u8bdd\u8bed\u6807\u8bb0\u5206\u6790/Discourse Marker Analysis]\n    D --\x3e D1[\u4e0a\u4e0b\u6587\u6700\u5173\u952e, 10-30\u8f6e\u9971\u548c/Context is key, saturates in 10-30 turns]\n    D --\x3e D2[\u7b80\u5355\u56e0\u679c\u6a21\u578bSOTA/Simple causal model achieves SOTA]\n    D --\x3e D3[\u60b2\u4f24\u8bdd\u8bed\u6807\u8bb0\u5c11, \u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587/Sad utterances have fewer markers, rely more on context]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [LSTM, Transformer, Stock Prediction, Time Series Forecasting, Attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaswat Mohanty"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00197",children:"https://arxiv.org/pdf/2601.00197"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Presents an enhanced StockBot architecture for systematic evaluation of modern time-series forecasting models (attention-based, convolutional, recurrent) in a unified setting. 2. Demonstrates empirically that a carefully constructed vanilla LSTM model consistently outperforms transformer-based models in stock price forecasting accuracy and decision-making stability under default hyperparameters. 3. Highlights the robustness, data efficiency, and importance of architectural inductive bias of recurrent models for financial forecasting, especially in data-limited scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents StockBot 2.0, a framework for evaluating time-series models for stock prediction. It finds that a vanilla LSTM model, despite its simplicity, outperforms more complex transformer-based models in forecasting accuracy and trading decision stability when trained with default settings, emphasizing the value of recurrent inductive biases for financial data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Forecasting financial markets is challenging due to complexity and volatility."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Enhanced StockBot architecture for systematic evaluation of attention, CNN, and RNN models."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Vanilla LSTM achieves superior accuracy and stable decisions compared to transformers."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [knowledge distillation, temporal knowledge graph reasoning, large language models, teacher-student framework, temporal dependencies]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wang Xing, Wei Song, Siyu Lin, Chen Wu, Zhesi Li, Man Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xidian University, Southwest Jiaotong University, Chongqing Jiaotong University, Chang\u2019an University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00202",children:"https://arxiv.org/pdf/2601.00202"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel distillation framework specifically designed for Temporal Knowledge Graph (TKG) reasoning, addressing the limitations of static graph compression techniques. 2. Leverages Large Language Models (LLMs) as teacher models to transfer both structural and temporal reasoning capabilities to lightweight student models. 3. Integrates large-scale public knowledge with task-specific temporal information to enhance the student model's ability to model temporal dynamics while maintaining computational efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19e66b8571c108befcd93243d5d7ad64cfb10f7509cd7cbcd74ba39136582282_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19e66b8571c108befcd93243d5d7ad64cfb10f7509cd7cbcd74ba39136582282_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of deploying computationally expensive Temporal Knowledge Graph (TKG) reasoning models on resource-constrained platforms. The authors propose a knowledge distillation framework that uses Large Language Models (LLMs) as teachers to transfer temporal and structural reasoning knowledge to compact student models. Experiments show the method achieves a favorable trade-off between reasoning accuracy and efficiency, outperforming existing baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709TKG\u63a8\u7406\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u90e8\u7f72/Existing TKG models are computationally heavy and hard to deploy]\n    B --\x3e B2[\u73b0\u6709\u538b\u7f29\u6280\u672f\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u5e8f\u4f9d\u8d56/Existing compression techniques fail to capture temporal dependencies]\n    C --\x3e C1[\u63d0\u51fa\u9488\u5bf9TKG\u7684\u84b8\u998f\u6846\u67b6/Propose a distillation framework tailored for TKGs]\n    C --\x3e C2[\u4f7f\u7528LLM\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b/Use LLMs as teacher models]\n    C --\x3e C3[\u5c06\u7ed3\u6784\u4e0e\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u8f7b\u91cf\u5b66\u751f\u6a21\u578b/Transfer structural and temporal reasoning to lightweight student models]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baselines on multiple benchmark datasets]\n    D --\x3e D2[\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u826f\u597d\u6743\u8861/Achieves a favorable trade-off between accuracy and efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [llm safety & alignment], [jailbreak, algorithm design, safety benchmark, optimization algorithm, malicious prompt]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xidian University, Victoria University of Wellington, Westlake University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00213",children:"https://arxiv.org/pdf/2601.00213"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and investigates a novel safety vulnerability in LLMs related to automated malicious optimization algorithm design. 2. Introduces MalOptBench, a benchmark of 60 malicious optimization algorithm requests for evaluating this vulnerability. 3. Proposes MOBjailbreak, a tailored jailbreak method for this scenario, and demonstrates its high effectiveness against current LLMs and defenses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6ccef5999a7edaf6824967fee78721c43b0e003d14334de2a1bb59ce1410a85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6ccef5999a7edaf6824967fee78721c43b0e003d14334de2a1bb59ce1410a85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates a safety vulnerability where LLMs can be prompted to generate malicious optimization algorithms. It introduces the MalOptBench benchmark and the MOBjailbreak attack method, finding that current LLMs and plug-and-play defenses are highly susceptible, highlighting a need for stronger alignment techniques."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LLMs\u5728\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u672a\u88ab\u5145\u5206\u63a2\u7d22/Underexplored safety vulnerability in LLM-driven automated algorithm design"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u57fa\u51c6MalOptBench\u548c\u8d8a\u72f1\u65b9\u6cd5MOBjailbreak/Propose benchmark MalOptBench and jailbreak method MOBjailbreak"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: LLMs\u9ad8\u5ea6\u8106\u5f31\uff0c\u73b0\u6709\u9632\u5fa1\u6548\u679c\u6709\u9650/LLMs are highly susceptible, current defenses are marginally effective"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Omar Sharif, Eftekhar Hossain, Patrick Ng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dartmouth College, University of Central Florida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00215",children:"https://arxiv.org/pdf/2601.00215"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: MLLMs lack visual grounding in reasoning"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: RL with reward functions & GRPO"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: 5.56% improvement on Qwen-2.5-VL-7B"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [retrieval-augmented generation], [knowledge graph, PICO framework, evidence-based medicine, Bayesian reranking, sports rehabilitation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jinning Zhang, Jie Song, Wenhui Tu, Zecheng Li, Jingxuan Li, Jin Li, Xuan Liu, Taole Sha, Zichen Wei, Yan Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Sport University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00216",children:"https://arxiv.org/pdf/2601.00216"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a generalizable strategy for integrating Evidence-Based Medicine (EBM) principles into graph-based RAG, addressing PICO alignment and evidence hierarchy. 2. Introduces a Bayesian-inspired reranking algorithm to calibrate retrieval scores based on evidence grade without predefined weights. 3. Constructs and releases a domain-specific knowledge graph and benchmark for sports rehabilitation to address the scarcity of RAG resources in this field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bd3d679f214bbb1077d6def8ab28cb0755d55d1e2bccc588807eff404d6c8bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bd3d679f214bbb1077d6def8ab28cb0755d55d1e2bccc588807eff404d6c8bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the oversight of evidence-based medicine principles in current RAG systems by proposing a strategy that integrates the PICO framework into knowledge graph construction and a Bayesian reranking algorithm. The method was validated in sports rehabilitation, showing improved answer quality and high expert ratings, and the released resources help fill a domain-specific data gap."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Current RAG overlooks EBM principles (PICO misalignment, no evidence hierarchy)"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Integrate PICO into KG RAG & Bayesian reranking for evidence grade"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: High-quality answers & expert ratings; Released KG & benchmark"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [machine translation], [LLM-as-a-judge, pairwise comparison, Bradley-Terry model, reference-free evaluation, anchored evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Leonard Lin, Adam Lensenmayer"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shisa.AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00223",children:"https://arxiv.org/pdf/2601.00223"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces JP-TL-Bench, a lightweight, open benchmark for iterative development of Japanese-English translation systems. 2. Proposes a reliable and affordable evaluation protocol using reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. 3. Provides structurally stable scores by aggregating pairwise results with a Bradley-Terry model and reporting normalized LT scores."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3672d8de20107e284402d4b12e978f246eb0df92617095708ca7710e712594d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3672d8de20107e284402d4b12e978f246eb0df92617095708ca7710e712594d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces JP-TL-Bench, a benchmark for evaluating high-quality Japanese-English translation. It uses a protocol where candidate models are compared against a fixed anchor set via pairwise LLM judgments, with results aggregated using a Bradley-Terry model to produce stable scores. This approach aims to provide a high-resolution signal for distinguishing between already fluent translations where traditional metrics saturate."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[JP-TL-Bench: Anchored Pairwise LLM Evaluation<br>JP-TL-Bench: \u951a\u5b9a\u6210\u5bf9LLM\u8bc4\u4f30] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u9ad8\u8d28\u91cf\u7ffb\u8bd1<br>Existing evaluation struggles to differentiate high-quality translations]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8e\u56fa\u5b9a\u951a\u5b9a\u96c6\u7684\u6210\u5bf9LLM\u6bd4\u8f83<br>Pairwise LLM comparison against a fixed anchor set]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6570<br>Provides stable, interpretable scores]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [semantic verification, execution feedback, generator-discriminator framework, reverse translation, conversational business analytics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yan Sun, Ming Cai, Stanley Kok"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00224",children:"https://arxiv.org/pdf/2601.00224"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Q*, a verification technique that uses reverse translation and semantic matching to align generated code with user intent. 2. Introduces Feedback+, a mechanism that incorporates execution feedback to guide iterative code refinement. 3. Embeds these techniques within a generator-discriminator framework to shift validation responsibilities from users to the system, aiming to improve reliability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2d07ef5c900d3b1d3d6067b2025a8ba0771149df2530b94a6a54885c8bd8685_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2d07ef5c900d3b1d3d6067b2025a8ba0771149df2530b94a6a54885c8bd8685_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of verification in conversational business analytics systems by proposing two techniques, Q* and Feedback+, to improve the accuracy and executability of LLM-generated outputs. The methods are integrated into a generator-discriminator framework and evaluated on benchmark datasets, showing reduced error rates and task completion time. The work provides a design framework for building more reliable enterprise-grade GenAI assistants."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Talk Less, Verify More: Improving LLM Assistants<br>\u5c11\u8bf4\u591a\u9a8c\u8bc1\uff1a\u6539\u8fdbLLM\u52a9\u624b] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u52a9\u624b\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236<br>Lack of verification in LLM assistants]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Q*\u4e0eFeedback+\u9a8c\u8bc1<br>Q* and Feedback+ verification]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u964d\u4f4e\u9519\u8bef\u7387\u4e0e\u4efb\u52a1\u65f6\u95f4<br>Reduced error rate & task time]\n    C --\x3e E[Q*: \u9006\u5411\u7ffb\u8bd1\u4e0e\u8bed\u4e49\u5339\u914d<br>Q*: Reverse translation & semantic matching]\n    C --\x3e F[Feedback+: \u6267\u884c\u53cd\u9988\u5f15\u5bfc\u4f18\u5316<br>Feedback+: Execution feedback guides refinement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [explainable ai (xai)], [counterfactual examples, multilingual, data augmentation, large language models, model robustness]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Van Bach Nguyen, Yihong Liu, Fedor Splitt, Nils Feldhus, Christin Seifert, Hinrich Sch\xfctze, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, University of Marburg, LMU Munich, German Research Center for Artificial Intelligence (DFKI), Munich Center for Machine Learning (MCML), BIFOLD \u2013 Berlin Institute for the Foundations of Learning and Data"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00263",children:"https://arxiv.org/pdf/2601.00263"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Evaluated the quality of LLM-generated multilingual counterfactuals, comparing direct generation and translation-based methods across six languages. 2. Identified four main error types common in generated counterfactuals across languages and found similar edit patterns in high-resource European languages. 3. Demonstrated that multilingual counterfactual data augmentation yields greater performance improvements than cross-lingual augmentation, especially for lower-resource languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3b95c2cf3407989c3c5a932764e908182c4d60d7451ace3f5f15e194a3a7e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e3b95c2cf3407989c3c5a932764e908182c4d60d7451ace3f5f15e194a3a7e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the effectiveness of large language models (LLMs) in generating multilingual counterfactual examples. It compares directly generated and translation-based counterfactuals across six languages, finding that translation-based ones are more valid but require more edits and still underperform English ones. The study concludes that while multilingual counterfactual data augmentation improves model performance, especially for low-resource languages, the quality limitations of the generated counterfactuals constrain the gains in robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u751f\u6210\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u793a\u4f8b\u7684\u6709\u6548\u6027\u672a\u77e5/Effectiveness of LLM-generated multilingual counterfactuals is unclear]\n    C --\x3e C1[\u8bc4\u4f30\u76f4\u63a5\u751f\u6210\u4e0e\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e/Evaluate directly generated and translation-based counterfactuals]\n    C --\x3e C2[\u5206\u6790\u7f16\u8f91\u6a21\u5f0f\u4e0e\u9519\u8bef\u7c7b\u578b/Analyze edit patterns and error types]\n    C --\x3e C3[\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u5b9e\u9a8c/Use for data augmentation experiments]\n    D --\x3e D1[\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u66f4\u6709\u6548\u4f46\u7f16\u8f91\u66f4\u591a/Translation-based CFs are more valid but require more edits]\n    D --\x3e D2[\u9ad8\u8d44\u6e90\u8bed\u8a00\u7f16\u8f91\u6a21\u5f0f\u76f8\u4f3c/Edit patterns are similar for high-resource languages]\n    D --\x3e D3[\u591a\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u6548\u679c\u66f4\u597d/Multilingual data augmentation yields larger improvements]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [function-calling, benchmark, API complexity, LLM agents, WildAGTEval]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Doyoung Kim, Zhiwei Ren, Jie Hao, Zhongkai Sun, Lichao Wang, Xiyao Ma, Zack Ye, Xu Han, Jun Yin, Heng Ji, Wei Shen, Xing Fan, Benjamin Yao, Chenlei Guo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Amazon, KAIST, University of Pittsburgh, University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00268",children:"https://arxiv.org/pdf/2601.00268"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," github.com/Demon-JieHao/WildAGTEval"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces WildAGTEval, a novel benchmark for evaluating LLM agents under realistic API complexity, covering both API specification and execution challenges. 2. Provides a comprehensive API system with 60 distinct complexity scenarios, composable into ~32K test configurations, and user-agent interactions for evaluation. 3. Systematically assesses advanced LLMs, revealing significant performance drops (e.g., 27.3% for irrelevant information complexity) and identifying critical failure modes like intent distortion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c517d92b4a84a20e84e30c740db5d9b70b7f3195c6de0fc8e049a5f0a4c9af_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c517d92b4a84a20e84e30c740db5d9b70b7f3195c6de0fc8e049a5f0a4c9af_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces WildAGTEval, a benchmark designed to evaluate LLM agents' function-calling capabilities under realistic API complexities, including detailed specifications and noisy execution. The study finds that most scenarios are challenging, with irrelevant information posing the greatest difficulty and causing significant performance drops in strong models. The qualitative analysis also reveals that LLMs sometimes distort user intent to claim task completion, negatively impacting user satisfaction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u5047\u8bbe\u7406\u60f3\u5316API / Existing benchmarks assume idealized APIs]\n    B --\x3e B2[\u5ffd\u7565\u73b0\u5b9e\u56e0\u7d20\u5982\u566a\u58f0\u8f93\u51fa / Ignore real-world factors like noisy outputs]\n    C --\x3e C1[\u63d0\u51faWildAGTEval\u57fa\u51c6 / Propose WildAGTEval benchmark]\n    C --\x3e C2[\u6db5\u76d6API\u89c4\u8303\u4e0e\u6267\u884c\u590d\u6742\u6027 / Covers API specification & execution complexity]\n    D --\x3e D1[\u5927\u591a\u6570\u573a\u666f\u5177\u6709\u6311\u6218\u6027 / Most scenarios are challenging]\n    D --\x3e D2[\u65e0\u5173\u4fe1\u606f\u590d\u6742\u5ea6\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d / Irrelevant info complexity causes significant performance drop]\n    D --\x3e D3[LLM\u53ef\u80fd\u626d\u66f2\u7528\u6237\u610f\u56fe / LLMs may distort user intent]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00282",children:"https://arxiv.org/pdf/2601.00282"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Large Language Models Still Explain Themselves?<br/>\u5927\u8bed\u8a00\u6a21\u578b\u8fd8\u80fd\u89e3\u91ca\u81ea\u5df1\u5417\uff1f"] --\x3e Problem["Quantization\'s effect on Self-Explanations is unknown.<br/>\u91cf\u5316\u5bf9\u81ea\u6211\u89e3\u91ca\u7684\u5f71\u54cd\u672a\u77e5"]\n    Root --\x3e Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>\u8bc4\u4f30\u91cf\u5316\u540eLLM\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u53cd\u4e8b\u5b9e\u793a\u4f8b"]\n    Root --\x3e Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>\u8d28\u91cf/\u5fe0\u5b9e\u5ea6\u9002\u5ea6\u4e0b\u964d\uff1b\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [speech processing], [depression detection, semantic bias, text-to-speech, disentangled representation, data augmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuxin Li, Xiangyu Zhang, Yifei Li, Zhiwei Guo, Haoyang Zhang, Eng Siong Chng, Cuntai Guan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanyang Technological University, UNSW Sydney, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00303",children:"https://arxiv.org/pdf/2601.00303"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DepFlow, a novel three-stage depression-conditioned TTS framework that disentangles depression-specific acoustic patterns from speaker and content information using adversarial training and flow-matching. 2. Introduces a prototype-based severity mapping mechanism for smooth and interpretable control over the synthesized depressive severity. 3. Constructs a Camouflage Depression-oriented Augmentation (CDoA) dataset using DepFlow to mitigate semantic bias, which significantly improves the robustness of depression detection models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddfe4c5383fa66e6b8e028851d0fe67037a11642b799a3626a1100aaa4cd8096_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddfe4c5383fa66e6b8e028851d0fe67037a11642b799a3626a1100aaa4cd8096_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of semantic bias in depression detection models, where models learn shortcuts from linguistic sentiment instead of acoustic cues. It proposes DepFlow, a disentangled speech generation framework, to create a synthetic dataset (CDoA) that pairs depressed acoustic patterns with positive/neutral text. This data augmentation method improves model robustness, outperforming conventional strategies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Models learn semantic shortcuts from sentiment-content coupling in depression datasets."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: DepFlow, a 3-stage TTS framework for disentangling & controlling depression acoustics."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: CDoA augmentation improves detection model F1 scores by 5-12%."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Robust Uncertainty Quantification for Factual Generation of Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [uncertainty quantification, factual hallucination, trap questions, ROCAUC, fake biographies]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuhao Zhang, Zhongliang Yang, Linna Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00348",children:"https://arxiv.org/pdf/2601.00348"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/EdwardChang5467/robust",children:"https://github.com/EdwardChang5467/robust"})," uncertainty"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new uncertainty quantification scenario focused on multi-fact generation (e.g., fake person biographies) to test LLM robustness. 2. Constructs a novel dataset of trap questions containing fake names to evaluate hallucination detection methods. 3. Introduces a robust uncertainty quantification (RU) method that significantly outperforms baseline methods across four different LLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6268aa8c8874ac861a06315946aedcfbc9df7712a9f2b63e23204554e9c8596b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6268aa8c8874ac861a06315946aedcfbc9df7712a9f2b63e23204554e9c8596b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of LLM hallucination by proposing a new robust uncertainty quantification (RU) method for detecting factual errors in multi-fact generation tasks. The method is evaluated using a specially constructed set of trap questions containing fake names. Results show the RU method achieves an average increase of 0.1-0.2 in ROCAUC over the best baseline, demonstrating its effectiveness in improving the reliability of LLM outputs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Uncertainty Quantification for Factual Generation of LLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u5e7b\u89c9\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7f3a\u9677<br>LLM Hallucination & UQ Deficiency]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u6784\u5efa\u9677\u9631\u95ee\u9898\u96c6\u4e0e\u63d0\u51fa\u9c81\u68d2\u65b9\u6cd5<br>Construct Trap Questions & Propose RU Method]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>ROC-AUC\u663e\u8457\u63d0\u5347<br>Significant ROC-AUC Improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual representation learning], [Joint Embedding Predictive Architecture (JEPA), BERT, CLS token, language-agnostic embedding, multilingual benchmarks]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Brown University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00366",children:"https://arxiv.org/pdf/2601.00366"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the problem that BERT\'s [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic "thought space". The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: CLS embeddings are not language-invariant and fail to capture true sentence semantics."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Add JEPA training objective to BERT to create a language-agnostic embedding space."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Improved multilingual benchmark performance; reorganized, semantic-first CLS space."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual language models], [multilingual pretraining, parallel data, code-switching, cross-lingual transfer, translation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiandong Shao, Raphael Tang, Crystina Zhang, Karin Sevegnani, Pontus Stenetorp, Jianfei Yang, Yao Lu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University College London, Nanyang Technological University, University of Waterloo, NVIDIA, National Institute of Informatics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00364",children:"https://arxiv.org/pdf/2601.00364"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted controlled pretraining experiments from scratch to isolate the impact of bilingual data, revealing that removing just 2% of such data causes a 56% drop in translation performance while leaving cross-lingual QA and reasoning largely unaffected. 2. Introduced a granular categorization of bilingual data into parallel, code-switching, and miscellaneous types based on semantic relevance, enabling more precise analysis. 3. Demonstrated through ablation studies that translation performance is critically dependent on parallel data (restoring 91% of baseline performance), whereas code-switching data contributes minimally, and that cross-lingual understanding does not rely heavily on bilingual data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d428d7e2552dcd9b73d3c4332f442260a2a8e3b735ffd1384f2f162c6a5bde44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d428d7e2552dcd9b73d3c4332f442260a2a8e3b735ffd1384f2f162c6a5bde44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the role of bilingual data in multilingual large language model pretraining by comparing standard web corpora with a monolingual-only version. Through controlled experiments and granular ablations categorizing data into parallel and code-switching types, it finds that translation performance heavily depends on parallel data for token-level alignments, while cross-lingual understanding and reasoning tasks can be achieved even without bilingual data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Role of Mixed-Language Documents for Multilingual LLM Pretraining] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Bilingual data's role in cross-lingual performance is unclear] --\x3e B1[\u95ee\u9898\u7ec6\u5316/Sub-Problem<br>Does it uniformly benefit all tasks?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Controlled pretraining & granular ablation] --\x3e C1[\u65b9\u6cd5\u6b65\u9aa4/Step 1<br>Compare standard vs. monolingual-only corpus]\n    C --\x3e C2[\u65b9\u6cd5\u6b65\u9aa4/Step 2<br>Categorize bilingual data: Parallel, Code-switching]\n    C --\x3e C3[\u65b9\u6cd5\u6b65\u9aa4/Step 3<br>Reintroduce data types for ablation]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Asymmetric impact of bilingual data] --\x3e D1[\u7ed3\u679c1/Result 1<br>Translation needs parallel data]\n    D --\x3e D2[\u7ed3\u679c2/Result 2<br>Cross-lingual QA/reasoning stable without it]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [geolocalization, vision-language models, chain of region, haversine distance, retrieval-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Biao Wu, Meng Fang, Ling Chen, Ke Xu, Tao Cheng, Jun Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, University of Liverpool, University College London"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00388",children:"https://arxiv.org/pdf/2601.00388"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Geo-R, a retrieval-free framework for image geolocalization that uses reinforcement learning. 2. Introduces Chain of Region, a rule-based hierarchical reasoning paradigm to generate interpretable supervision from GPS coordinates. 3. Develops a lightweight RL strategy with coordinate-aligned rewards based on Haversine distance for spatially meaningful feedback."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/986e7d2e1e80caef1d7794a999a5e00e8f2a503a4cae673b68dc3cfafa02122c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/986e7d2e1e80caef1d7794a999a5e00e8f2a503a4cae673b68dc3cfafa02122c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing image geolocalization methods by proposing Geo-R, a retrieval-free framework that uses a rule-based Chain of Region for hierarchical reasoning and a reinforcement learning strategy with Haversine distance rewards. The approach improves localization accuracy, generalization, and interpretability without relying on synthetic labels or external retrieval, as validated across multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Vision-Language Reasoning for Geolocalization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing methods rely on synthetic annotations or retrieval, limiting interpretability and generalization.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes Geo-R, a retrieval-free framework using Chain of Region for hierarchical reasoning and RL with Haversine distance rewards.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved accuracy, stronger generalization, and more transparent inference, establishing a new retrieval-free paradigm.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [named entity recognition], [weak supervision, large language models, low-resource languages, dataset construction, Luxembourgish]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alistair Plum, Laura Bernardy, Tharindu Ranasinghe"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Luxembourg, Lancaster University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00411",children:"https://arxiv.org/pdf/2601.00411"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel pipeline for constructing NER datasets that uses Wikipedia/Wikidata for weak supervision and LLMs for label verification. 2. Introduces judgeWEL, a new and significantly larger NER dataset for the under-represented language Luxembourgish. 3. Evaluates and compares the effectiveness of multiple LLMs in judging and filtering noisy, distantly-supervised labels."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e13a29d091cdd4cb0cc5c3d34a98e86c8d45b0a34f42a2b552cecd9fcff5b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e13a29d091cdd4cb0cc5c3d34a98e86c8d45b0a34f42a2b552cecd9fcff5b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of building datasets for under-represented languages by proposing a novel method that uses Wikipedia and Wikidata for weak supervision to generate initial NER labels, and then employs multiple LLMs to verify and filter these labels for quality. The approach is applied to Luxembourgish, resulting in the judgeWEL dataset, which is five times larger and more balanced than existing resources, providing a valuable new corpus for low-resource NER research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Do LLMs Judge Distantly Supervised Named Entity Labels Well?<br/>\u6784\u5efaJudgeWEL\u6570\u636e\u96c6] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u96c6\u6784\u5efa\u56f0\u96be<br/>Dataset Construction for Low-Resource Languages]\nC --\x3e C1[\u5229\u7528\u7ef4\u57fa\u767e\u79d1/\u7ef4\u57fa\u6570\u636e\u8fdb\u884c\u8fdc\u7a0b\u76d1\u7763<br/>Weak Supervision via Wikipedia/Wikidata]\nC --\x3e C2[\u4f7f\u7528\u591a\u4e2aLLM\u8fdb\u884c\u6807\u7b7e\u9a8c\u8bc1<br/>Label Verification with Multiple LLMs]\nD --\x3e D1[\u521b\u5efa\u4e86\u66f4\u5927\u7684\u5362\u68ee\u5821\u8bedNER\u6570\u636e\u96c6<br/>Larger Luxembourgish NER Dataset Created]\nD --\x3e D2[\u6570\u636e\u96c6\u89c4\u6a21\u6269\u5927\u4e94\u500d\uff0c\u8986\u76d6\u66f4\u5e73\u8861<br/>5x Larger, More Balanced Coverage]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Deep Delta Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00417",children:"https://arxiv.org/pdf/2601.00417"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/yifanzhang-pro/deep-delta-learning",children:"https://github.com/yifanzhang-pro/deep-delta-learning"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Deep Delta Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6b8b\u5dee\u7f51\u7edc\u9650\u5236/ResNet Limitation]\n    B1 --\x3e B2["\u521a\u6027\u76f8\u52a0\u504f\u7f6e/Rigid Additive Bias"]\n    B2 --\x3e B3["\u9650\u5236\u590d\u6742\u72b6\u6001\u8f6c\u6362/Limits Complex State Transitions"]\n    C --\x3e C1[Delta \u7b97\u5b50/Delta Operator]\n    C1 --\x3e C2["\u79e9-1\u6270\u52a8/ Rank-1 Perturbation"]\n    C2 --\x3e C3["\u53ef\u5b66\u4e60\u51e0\u4f55\u53d8\u6362/Learnable Geometric Transform"]\n    C3 --\x3e C4["\u52a8\u6001\u95e8\u63a7/Dynamic Gating (\u03b2)"]\n    D --\x3e D1["\u8c31\u5206\u6790/Spectral Analysis"]\n    D1 --\x3e D2["\u63d2\u503c\u8eab\u4efd/\u6295\u5f71/\u53cd\u5c04/Interpolates Identity/Projection/Reflection"]\n    D --\x3e D3["\u540c\u6b65\u79e9-1\u6ce8\u5165/Synchronous Rank-1 Injection"]\n    D3 --\x3e D4["\u63a7\u5236\u8f6c\u6362\u8c31/Controls Transition Spectrum"]\n    D4 --\x3e D5["\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3/Preserves Stable Training"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [Semantic Field Theory, lexical fields, linguistic fields, transformer architectures, embedding spaces]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dimitris Vartziotis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TWT Science & Innovation, NIKI - Digital Engineering"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00448",children:"https://arxiv.org/pdf/2601.00448"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. 2. Analyzes how core properties of transformer architectures (e.g., distributed representations, attention) relate to Semantic Field Theory concepts. 3. Proposes that mathematical structure and language games are complementary perspectives, clarifying the scope and limits of statistical language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98759f21c127cf9a440b464892816dd7e22af8f161a1d16ec91b582a8fefa647_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98759f21c127cf9a440b464892816dd7e22af8f161a1d16ec91b582a8fefa647_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper examines theories of linguistic meaning by contrasting social constructivist language games with a mathematically oriented Semantic Field Theory. It formalizes lexical and linguistic fields and analyzes their relation to transformer architecture properties. The authors conclude that the mathematical structure captured by LLMs and the social grounding of language games are complementary, not competing, views."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Contrasting theories of linguistic meaning (social vs. mathematical)")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Formalizing Semantic Field Theory (Lexfelder/Lingofelder) and analyzing transformer properties")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Mathematical structure and language games are complementary perspectives")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text classification], [DistilBERT, MiniLM, ALBERT, inference latency, model efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Shahmeer Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Ulster University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00444",children:"https://arxiv.org/pdf/2601.00444"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic comparison of three lightweight Transformer models (DistilBERT, MiniLM, ALBERT) across three enterprise-relevant domains. 2. An empirical evaluation using both accuracy-based and efficiency metrics under fixed enterprise-oriented constraints. 3. Practical deployment recommendations highlighting the trade-offs between accuracy and efficiency for different enterprise scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caee3ae4ac5e6aeda42367ba10ce7ebb5766e7933d76b7277c736f90391d7895_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caee3ae4ac5e6aeda42367ba10ce7ebb5766e7933d76b7277c736f90391d7895_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares the efficiency and performance of three lightweight Transformer models\u2014DistilBERT, MiniLM, and ALBERT\u2014across sentiment, news, and hate speech classification tasks. The evaluation uses accuracy and efficiency metrics under controlled fine-tuning. The key finding is a trade-off: ALBERT excels in accuracy, MiniLM in speed, and DistilBERT offers the most consistent balance, providing clear deployment guidance for enterprises."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["COMPARATIVE EFFICIENCY ANALYSIS OF LIGHTWEIGHT TRANSFORMER MODELS<br>\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\u6bd4\u8f83\u6548\u7387\u5206\u6790"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u4f01\u4e1a\u9700\u8981\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u591a\u9886\u57dfNLP\u6a21\u578b<br>Enterprise Need for Efficient, Lightweight Multi-Domain NLP Models"]\n    Method["\u6bd4\u8f83DistilBERT, MiniLM, ALBERT<br>Compare DistilBERT, MiniLM, ALBERT<br>\u4f7f\u7528\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6307\u6807<br>Use Accuracy & Efficiency Metrics"]\n    Results["ALBERT: \u9ad8\u51c6\u786e\u5ea6<br>ALBERT: High Accuracy<br>MiniLM: \u9ad8\u63a8\u7406\u901f\u5ea6<br>MiniLM: High Inference Speed<br>DistilBERT: \u6700\u5747\u8861<br>DistilBERT: Most Balanced"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Toward Better Temporal Structures for Geopolitical Events Forecasting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [knowledge graph completion], [temporal knowledge graph, hyper-relational knowledge graph, event forecasting, large language model, geopolitical events]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kian Ahrabian, Eric Boxer, Jay Pujara"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California, Information Sciences Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00430",children:"https://arxiv.org/pdf/2601.00430"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/usc-isi-i2/htkgh-polecat",children:"https://github.com/usc-isi-i2/htkgh-polecat"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new data structure, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs), to efficiently represent complex temporal facts involving more than two primary entities. 2. Introduces the ",(0,a.jsx)(n.code,{children:"htkgh-polecat"})," dataset, built on the POLECAT database, to benchmark forecasting tasks on this new structure. 3. Benchmarks and analyzes the performance of popular Large Language Models (LLMs) on the relation prediction task within this complex forecasting scenario."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527a7641dfaef2c6e41eb0a5e3a09b206d0632bf03dce43022e1edf046380617_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527a7641dfaef2c6e41eb0a5e3a09b206d0632bf03dce43022e1edf046380617_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of existing temporal knowledge graphs in representing complex geopolitical events with multiple primary entities by proposing a new structure called HTKGHs. The authors formalize HTKGHs, create a corresponding dataset, and benchmark LLMs on forecasting tasks. The results provide insights into LLMs' capabilities for complex temporal reasoning and forecasting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Toward Better Temporal Structures for Geopolitical Events Forecasting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[HTKGs\u7f3a\u4e4f\u5bf9\u591a\u4e3b\u4f53\u590d\u6742\u4e8b\u5b9e\u7684\u8868\u8fbe\u80fd\u529b/HTKGs lack expressive power for complex multi-entity facts]\n    C --\x3e C1[\u63d0\u51faHTKGHs\u7ed3\u6784\u4ee5\u652f\u6301\u591a\u4e3b\u4f53\u4e8b\u4ef6/Propose HTKGHs to support multi-entity events]\n    C --\x3e C2[\u57fa\u4e8ePOLECAT\u6784\u5efahtkgh-polecat\u6570\u636e\u96c6/Build htkgh-polecat dataset based on POLECAT]\n    C --\x3e C3[\u5728\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e3b\u6d41LLMs/Benchmark popular LLMs on relation prediction]\n    D --\x3e D1[\u5f62\u5f0f\u5316HTKGHs\u5e76\u5c55\u793a\u5176\u5411\u540e\u517c\u5bb9\u6027/Formalize HTKGHs and demonstrate backward compatibility]\n    D --\x3e D2[\u63d0\u4f9bLLMs\u5728\u590d\u6742\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u80fd\u529b\u5206\u6790/Provide analysis of LLM capabilities in complex forecasting scenarios]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [guardrail models, multi-turn compression, efficiency optimization, safety screening, token reduction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hyunjun Kim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00454",children:"https://arxiv.org/pdf/2601.00454"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Defensive M2S, a training paradigm that fine-tunes guardrail models on compressed multi-turn conversations instead of full histories, 2. Provides formal complexity analysis showing training cost reduction from O(n\xb2) to O(n) and empirical token reduction of 93\xd7, 3. Demonstrates effectiveness across multiple guardrail models and compression templates, achieving high attack detection recall with 94.6% inference token reduction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad9caa971d784e8a994209f34a3b4e255812b43b2fa90a13bb0487b6e71e1395_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad9caa971d784e8a994209f34a3b4e255812b43b2fa90a13bb0487b6e71e1395_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost of processing full multi-turn conversations for LLM safety guardrails by proposing Defensive M2S, which trains guardrail models on compressed single-turn versions. This method significantly reduces training and inference tokens while maintaining high attack detection performance, enabling scalable safety screening."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: High computational cost of processing full multi-turn conversations for guardrail models"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Fine-tune guardrail models on M2S (Multi-turn to Single-turn) compressed conversations"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: 93\xd7 training token reduction, 94.6% inference token reduction, 93.8% attack detection recall"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Noise-Aware Named Entity Recognition for Historical VET Documents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [named entity recognition], [Noise-Aware Training (NAT), OCR Noise, Data Augmentation, Transfer Learning, Multi-stage Fine-tuning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexander M. Esser, Jens D\xf6rpinghaus"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Federal Institute for Vocational Education and Training (BIBB), University of Koblenz"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00488",children:"https://arxiv.org/pdf/2601.00488"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a robust NER approach for historical VET documents using Noise-Aware Training with synthetic OCR errors. 2. Systematically compares three complementary training strategies (noisy, clean, and artificial data). 3. Demonstrates that domain-specific and noise-aware fine-tuning significantly improves robustness and accuracy under noisy conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles Named Entity Recognition in noisy, historical Vocational Education and Training documents by proposing a method using Noise-Aware Training with synthetic OCR errors, transfer learning, and multi-stage fine-tuning. The approach, one of the first to recognize multiple entity types in this domain, shows that domain-specific and noise-aware fine-tuning substantially increases model robustness and accuracy. The method is applied to German but is designed to be transferable to other languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Noise-Aware NER for Historical VET Documents] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem: NER in noisy historical VET documents)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method: Noise-Aware Training with synthetic OCR errors, transfer learning, multi-stage fine-tuning)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results: Increased robustness and accuracy under noisy conditions)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Rule-Based Approaches to Atomic Sentence Extraction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text simplification], [atomic sentence extraction, dependency parsing, rule-based system, syntactic complexity, split-and-rephrase]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Lineesha Kamana, Akshita Ananda Subramanian, Mehuli Ghosh, Suman Saha"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00506",children:"https://arxiv.org/pdf/2601.00506"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a principled analysis to identify specific complex sentence structures (e.g., relative clauses, appositions) that cause difficulties for rule-based atomic sentence extraction. 2. Implemented and evaluated a transparent, dependency-based rule extraction system using spaCy on the WikiSplit dataset. 3. Provided quantitative performance benchmarks (ROUGE and BERTScore) and qualitative insights into the limitations of rule-based methods for syntactic decomposition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8cb4da5bb35bca5de1c3a306fbe787db397d7073bbab8260c7962fc3e300d28_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8cb4da5bb35bca5de1c3a306fbe787db397d7073bbab8260c7962fc3e300d28_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the performance of a rule-based system for decomposing complex sentences into simpler atomic units. The method uses dependency parsing rules in spaCy and is evaluated on the WikiSplit dataset. The results show the approach is reasonably accurate but struggles with syntactically complex structures like relative clauses and coordinated predicates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Rule-Based Approaches to Atomic Sentence Extraction") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u590d\u6742\u53e5\u5b50\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd/Complex sentences hinder downstream tasks")\n    Problem --\x3e P2("\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/Existing methods lack interpretability")\n    Method --\x3e M1("\u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u7684\u89c4\u5219\u63d0\u53d6/Dependency-based rule extraction")\n    Method --\x3e M2("\u4f7f\u7528spaCy\u548cWikiSplit\u6570\u636e\u96c6/Using spaCy & WikiSplit dataset")\n    Results --\x3e R1("\u53d6\u5f97\u4e2d\u7b49\u81f3\u9ad8ROUGE/BERT\u5206\u6570/Achieved moderate-high ROUGE/BERTScore")\n    Results --\x3e R2("\u7279\u5b9a\u53e5\u6cd5\u7ed3\u6784\u5177\u6709\u6311\u6218\u6027/Specific syntactic structures are challenging")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [information retrieval], [query categorization, chain-of-thought, e-commerce taxonomy, semantic scoring, large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jetlir Duraj, Ishita Khan, Kilian Merkelbach, Mehran Elyasi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," eBay Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00510",children:"https://arxiv.org/pdf/2601.00510"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Chain-of-Thought (CoT) paradigm for semantic query categorization that combines tree-search with LLM semantic scoring. 2. Demonstrates that the CoT approach outperforms embedding-based benchmarks and can detect problems within hierarchical taxonomies. 3. Introduces scalable LLM-based approaches for query categorization that are suitable for millions of queries."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2b32fe45914991e8c8d0dc90af7415aa3ed8d8e8ce666ea46e11465b8fa68fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2b32fe45914991e8c8d0dc90af7415aa3ed8d8e8ce666ea46e11465b8fa68fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of categorizing user search queries into leaf categories of an e-commerce taxonomy to improve search relevance. The authors propose a novel Chain-of-Thought approach that navigates the taxonomy tree using LLM semantic scoring. Their method outperforms embedding-based benchmarks and is shown to scale for real-world applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies<br>\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u7535\u5546\u5206\u7c7b\u8bed\u4e49\u67e5\u8be2\u65b9\u6cd5"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Search query categorization in e-commerce taxonomies<br>\u7535\u5546\u5206\u7c7b\u4e2d\u7684\u641c\u7d22\u67e5\u8be2\u5206\u7c7b"] --\x3e P1["\u76ee\u6807/Goal<br>Select relevant leaf categories for a user query<br>\u4e3a\u7528\u6237\u67e5\u8be2\u9009\u62e9\u76f8\u5173\u53f6\u5b50\u7c7b\u76ee"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Chain-of-Thought (CoT) paradigm<br>\u601d\u7ef4\u94fe\u8303\u5f0f"] --\x3e M1["\u6280\u672f/Technique<br>Combines tree-search with LLM semantic scoring<br>\u7ed3\u5408\u6811\u641c\u7d22\u4e0eLLM\u8bed\u4e49\u8bc4\u5206"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Evaluation Findings<br>\u8bc4\u4f30\u7ed3\u679c"] --\x3e R1["\u6027\u80fd/Performance<br>Outperforms embedding-based benchmarks<br>\u4f18\u4e8e\u57fa\u4e8e\u5d4c\u5165\u7684\u57fa\u51c6\u65b9\u6cd5"]\n    Results --\x3e R2["\u53ef\u6269\u5c55\u6027/Scalability<br>Proposes scalable LLM-based approaches<br>\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] The Illusion of Insight in Reasoning Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reasoning models], [reasoning shifts, self-correction, model uncertainty, intrinsic vs extrinsic, chain-of-thought]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Liv G. d'Aliberti, Manoel Horta Ribeiro"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00514",children:"https://arxiv.org/pdf/2601.00514"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Conducted a large-scale empirical study analyzing over 1 million reasoning traces across multiple models, domains, and training stages to investigate the nature and impact of mid-reasoning "Aha!" moments. 2. Found that such intrinsic reasoning shifts are rare, do not increase with training, and seldom improve accuracy, challenging the perception that they represent genuine model insight or self-correction. 3. Demonstrated that while intrinsic shifts are not beneficial, artificially triggering extrinsic shifts under conditions of high model uncertainty (high entropy) can reliably improve accuracy, showing these shifts are symptoms of unstable inference.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c00fcaa573a1b7a6558433ae1348cc7cefec26d36175047b45df1cd217f2516_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c00fcaa573a1b7a6558433ae1348cc7cefec26d36175047b45df1cd217f2516_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates whether reasoning models experience genuine "Aha!" moments of intrinsic self-correction during inference. Through a large-scale analysis of reasoning traces across multiple models and training checkpoints, the authors find that such mid-reasoning shifts are rare and ineffective, but that artificially triggering shifts when the model is uncertain can improve accuracy. The main conclusion is that these shifts are symptoms of unstable inference behavior, not a mechanism for intrinsic self-improvement.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[The Illusion of Insight in Reasoning Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u63a8\u7406\u6a21\u578b\u662f\u5426\u6709\u771f\u6b63\u7684\u201c\u987f\u609f\u201d\u65f6\u523b\uff1f/Do reasoning models have genuine "Aha!" moments?]\n    C --\x3e C1[\u5927\u89c4\u6a21\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u4e0e\u8bad\u7ec3\u68c0\u67e5\u70b9/Large-scale analysis of reasoning traces & training checkpoints]\n    D --\x3e D1[\u5185\u5728\u8f6c\u53d8\u7f55\u89c1\u4e14\u65e0\u6548/Intrinsic shifts are rare and ineffective]\n    D --\x3e D2[\u5916\u5728\u89e6\u53d1\u5728\u9ad8\u71b5\u4e0b\u53ef\u63d0\u5347\u51c6\u786e\u7387/Extrinsic triggering under high entropy improves accuracy]\n    D --\x3e D3[\u8f6c\u53d8\u662f\u4e0d\u7a33\u5b9a\u63a8\u7406\u7684\u75c7\u72b6/Shifts are symptoms of unstable inference]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [question answering], [multi-hop QA, retrieval-reasoning process, execution procedure, RAG, agentic systems]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Pittsburgh, Google Cloud AI Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00536",children:"https://arxiv.org/pdf/2601.00536"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel four-axis design framework for analyzing the execution procedure of multi-hop QA systems, focusing on plan, index, control, and stopping criteria. 2. Systematically maps and compares representative multi-hop QA systems using this framework, making implicit procedural choices explicit and comparable. 3. Synthesizes empirical trends and trade-offs from standard benchmarks and identifies key open challenges for future retrieval-reasoning agents."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64e851b90551e53952c84444631a7585a225572c38194cb181b360edfef5c8e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64e851b90551e53952c84444631a7585a225572c38194cb181b360edfef5c8e1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper addresses the lack of explicit analysis of the procedural interaction between retrieval and reasoning in multi-hop question answering. It introduces a four-axis framework to systematically compare different systems based on their execution plan, index structure, control strategies, and stopping criteria. The main conclusion is that making these procedural choices explicit reveals recurring trade-offs and highlights open challenges like structure-aware planning and robust stopping."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["\u68c0\u7d22-\u63a8\u7406\u8fc7\u7a0b\u4e0e\u56db\u8f74\u6846\u67b6 / Retrieval\u2013Reasoning Processes & Four-Axis Framework"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898\uff1a\u68c0\u7d22\u4e0e\u63a8\u7406\u7684\u9690\u5f0f\u8026\u5408 / Problem: Implicit Coupling of Retrieval & Reasoning"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5\uff1a\u56db\u8f74\u8bbe\u8ba1\u6846\u67b6 / Method: Four-Axis Design Framework"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c\uff1a\u8d8b\u52bf\u7efc\u5408\u4e0e\u5f00\u653e\u6311\u6218 / Results: Trend Synthesis & Open Challenges"]\n    Problem --\x3e P1["\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u6bd4\u8f83 / Process Opacity, Hard to Compare"]\n    Method --\x3e M1["\u8f74A\uff1a\u603b\u4f53\u6267\u884c\u8ba1\u5212 / Axis A: Overall Execution Plan"]\n    Method --\x3e M2["\u8f74B\uff1a\u7d22\u5f15\u7ed3\u6784 / Axis B: Index Structure"]\n    Method --\x3e M3["\u8f74C\uff1a\u4e0b\u4e00\u6b65\u63a7\u5236 / Axis C: Next-Step Control"]\n    Method --\x3e M4["\u8f74D\uff1a\u505c\u6b62/\u7ee7\u7eed\u6807\u51c6 / Axis D: Stop/Continue Criteria"]\n    Results --\x3e R1["\u6620\u5c04\u7cfb\u7edf\uff0c\u7efc\u5408\u8d8b\u52bf / Map Systems, Synthesize Trends"]\n    Results --\x3e R2["\u8bc6\u522b\u6743\u8861\u4e0e\u6311\u6218 / Identify Trade-offs & Challenges"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] ECR: Manifold-Guided Semantic Cues for Compact Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [embedding consistency regulation, manifold structure, semantic anchors, compact language models, on-device AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chung-Wei Victor Yuan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," YVIC Research Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00543",children:"https://arxiv.org/pdf/2601.00543"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Embedding Consistency Regulation (ECR), a new framework that uses semantic anchors derived from teacher embeddings to preserve the underlying manifold structure in compact models. 2. Demonstrates that ECR stabilizes training and preserves semantic structure across tasks and languages without relying on matching logits or internal features, and adds minimal inference overhead. 3. Shows ECR is compatible with but independent of distillation, enabling better task alignment and deployment under strict efficiency or privacy constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3184228efac0008fa39e8578d4daa8e597c9d20bf57f0662c9080c6c11607590_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3184228efac0008fa39e8578d4daa8e597c9d20bf57f0662c9080c6c11607590_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of semantic drift and loss of manifold structure in compact language models. It proposes the Embedding Consistency Regulation (ECR) framework, which uses offline-computed semantic anchors to guide the compact model's geometry. Experiments show ECR produces more compact, task-aligned representations, making low-capacity models more stable and easier to deploy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[ECR: Manifold-Guided Semantic Cues for Compact Language Models] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u5d29\u584c/Embedding Space Collapse]\n    Problem --\x3e P2[\u8bed\u4e49\u6f02\u79fb/Semantic Drift]\n    Method --\x3e M1[\u63d0\u53d6\u8bed\u4e49\u951a\u70b9/Derive Semantic Anchors]\n    Method --\x3e M2[\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027/Maintain Geometric Consistency]\n    Results --\x3e R1[\u7a33\u5b9a\u8bad\u7ec3/Stabilized Training]\n    Results --\x3e R2[\u4fdd\u7559\u8bed\u4e49\u7ed3\u6784/Preserved Semantic Structure]\n    Results --\x3e R3[\u7d27\u51d1\u4efb\u52a1\u5bf9\u9f50\u7a7a\u95f4/Compact Task-Aligned Space]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [LoRA, Mixture-of-Experts, CTC, multilingual ASR, language-agnostic]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuang Zheng, Yuxiang Mei, Dongxing Xu, Jie Chen, Yanhua Long"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Normal University, Unisound AI Technology Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00557",children:"https://arxiv.org/pdf/2601.00557"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model for lightweight multilingual ASR. 2. Introduces an LID-posterior-driven LoRA routing mechanism that enables true language-agnostic, single-pass decoding without prior language identity information. 3. Demonstrates that the proposed method achieves competitive performance with state-of-the-art two-stage inference methods while significantly improving decoding efficiency for low-resource applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd2404a5aa3ab85ddcd83e20182b5cedff2831876ded928574f65b33a573d7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd2404a5aa3ab85ddcd83e20182b5cedff2831876ded928574f65b33a573d7d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high computational cost and latency of large multilingual ASR models like Whisper for edge deployment. It proposes a lightweight, language-agnostic system using a Hierarchical LoRA-MoE architecture with CTC, which enables efficient single-pass decoding without needing language labels. Experiments show the method achieves competitive performance with more complex two-stage systems, improving efficiency for low-resource multilingual ASR."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Large multilingual ASR models (e.g., Whisper) are computationally expensive and have high latency, limiting edge device deployment."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a lightweight HLoRA framework with hierarchical LoRA-MoE design and LID-posterior-driven routing for language-agnostic, single-pass CTC decoding."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves competitive performance with SOTA two-stage methods on MSR-86K and MLC-SLM 2025 datasets, improving decoding efficiency."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] InfoSynth: Information-Guided Benchmark Synthesis for LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [benchmark synthesis, information theory, genetic algorithm, code generation, KL-divergence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ishir Garg, Neel Kolhe, Xuandong Zhao, Dawn Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00575",children:"https://arxiv.org/pdf/2601.00575"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://ishirgarg.github.io/infosynth_web/",children:"https://ishirgarg.github.io/infosynth_web/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework (InfoSynth) for automatically generating and evaluating reasoning benchmarks using information-theoretic principles. 2. Proposes KL-divergence and entropy-based metrics to quantify benchmark novelty and diversity without costly model evaluations. 3. An end-to-end pipeline that synthesizes robust Python coding problems using genetic algorithms and iterative code feedback, achieving 97% accuracy in generating test cases and solutions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/704b01a43cbeec6830597e2e4ae9ad64cd96a4119782924bce3b012150cf43af_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/704b01a43cbeec6830597e2e4ae9ad64cd96a4119782924bce3b012150cf43af_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces InfoSynth, a framework that automatically generates novel and diverse reasoning benchmarks for LLMs using information-theoretic guidance and genetic algorithms. It successfully creates Python coding problems with high accuracy and provides control over novelty and difficulty. The method offers a scalable, self-verifying pipeline for benchmark creation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[InfoSynth: Information-Guided Benchmark Synthesis for LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Manual benchmark creation is expensive; existing benchmarks contaminate LLM training data.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Framework using information theory (KL-divergence/entropy) and genetic algorithms to synthesize Python problems.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 97% generation accuracy; benchmarks are more novel/diverse than seeds; controllable novelty/difficulty.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [safety evaluation], [adversarial patterns, safety benchmark, lightweight llms, chinese-specific, over-refusal]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenhong Zhou, Shilinlu Yan, Chuanpu Liu, Qiankun Li, Kun Wang, Zhigang Zeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanyang Technological University, Beijing University of Posts and Telecommunications, Huazhong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00588",children:"https://arxiv.org/pdf/2601.00588"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://huggingface.co/datasets/Yaesir06/CSSBench",children:"https://huggingface.co/datasets/Yaesir06/CSSBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces CSSBench, a novel benchmark for evaluating LLM safety against Chinese-specific adversarial patterns like homophones and pinyin. 2. Covers six real-world Chinese safety domains and measures both attack success and over-refusal rates. 3. Demonstrates that Chinese-specific adversarial patterns pose a critical challenge for lightweight LLMs, revealing a safety evaluation gap."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c813f73ed4fa6559522ae2a06c96e70fccff1e58bac3263d589d9e1ba5dde9bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c813f73ed4fa6559522ae2a06c96e70fccff1e58bac3263d589d9e1ba5dde9bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a gap in safety evaluation for lightweight LLMs against Chinese-specific adversarial patterns. To address this, it introduces CSSBench, a benchmark covering six domains and specific attack patterns. The evaluation shows these patterns are a significant challenge for lightweight models, highlighting the need for targeted safety measures."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Safety evaluation gap for Chinese-specific adversarial patterns in lightweight LLMs]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce CSSBench benchmark with six domains and specific adversarial patterns]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Chinese-specific adversarial patterns are a critical challenge for lightweight LLMs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [JourneyBench, policy adherence, User Journey Coverage Score, Dynamic-Prompt Agent, Standard Operating Procedures]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sumanth Balaji, Piyush Mishra, Aashraya Sachdeva, Suraj Agrawal"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Observe.AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00596",children:"https://arxiv.org/pdf/2601.00596"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),"  1. Introduces JourneyBench, a novel benchmark for evaluating customer support LLM agents on their ability to adhere to complex, multi-step business policies and workflows. 2. Proposes the User Journey Coverage Score, a new metric to quantitatively measure an agent's policy adherence across diverse and realistic support scenarios generated via graph representations. 3. Demonstrates that a Dynamic-Prompt Agent (DPA) design, which explicitly models policy control, significantly improves adherence, enabling smaller models to outperform larger ones on this critical operational metric."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2447eea55afc6d1b5ede006dd4130334fc9f6a1079fd81fa73d75aa31c86b038_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2447eea55afc6d1b5ede006dd4130334fc9f6a1079fd81fa73d75aa31c86b038_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of evaluating LLM agents for business policy adherence in customer support, beyond simple task completion. It introduces the JourneyBench benchmark and a Dynamic-Prompt Agent design. The results show that structured policy orchestration (DPA) is crucial for adherence, allowing smaller models to achieve strong performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Existing benchmarks overlook policy adherence in customer support]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: JourneyBench benchmark & Dynamic-Prompt Agent (DPA)]\n    D[\u5173\u952e\u7ed3\u679c/Results: DPA boosts adherence, smaller models can outperform larger ones]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [hallucination reduction, probabilistic guarantees, LLM-as-a-judge, ensemble voting, model-agnostic framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nils Rautenberg, Sven Schippkus"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Deutsche Aktuarvereinigung e.V., University of Hamburg"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00641",children:"https://arxiv.org/pdf/2601.00641"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes a framework for fixed-input tasks and proves that independent prompt repetition exponentially reduces the probability of all outputs being incorrect. 2. Incorporates an LLM-as-a-judge to identify correct answers and provides theoretical error bounds based on the judge's performance. 3. Introduces majority voting over independent judge calls to strengthen imperfect judges, achieving ensemble-level error rates that decrease exponentially with the number of votes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/165c6ebcc0bde9cc29a648eab707a9a04b4f10c177d4c9ed7fc6ceee24692cf9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/165c6ebcc0bde9cc29a648eab707a9a04b4f10c177d4c9ed7fc6ceee24692cf9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of contextual hallucinations in LLMs for deterministic workflows. It proposes a model-agnostic framework that combines independent prompt repetition with an LLM-as-a-judge and majority voting to exponentially reduce hallucination probabilities. The method provides explicit probabilistic guarantees without modifying the underlying model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLMs\u4ea7\u751f\u4e0a\u4e0b\u6587\u5e7b\u89c9/LLMs produce contextual hallucinations]\n    B --\x3e B2[\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u4e2d\u95ee\u9898\u4e25\u91cd/Severe in deterministic workflows]\n    C --\x3e C1[\u72ec\u7acb\u91cd\u590d\u63d0\u793a/Independent prompt repetition]\n    C --\x3e C2[LLM\u4f5c\u4e3a\u8bc4\u5224\u8005/LLM-as-a-judge]\n    C --\x3e C3[\u591a\u6570\u6295\u7968\u96c6\u6210/Majority vote ensemble]\n    D --\x3e D1[\u9519\u8bef\u7387\u6307\u6570\u4e0b\u964d/Error rates decrease exponentially]\n    D --\x3e D2[\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1/Provides probabilistic guarantees]\n    D --\x3e D3[\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173/Lightweight and model-agnostic]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [protein language models], [Direct Preference Optimization, thermodynamic stability, structural hallucinations, physics-informed alignment, energy landscape]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," QiWei Meng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xi\u2019an Jiaotong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00647",children:"https://arxiv.org/pdf/2601.00647"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Physio-DPO, a physics-informed alignment framework that grounds protein language models in thermodynamic stability. 2. Introduces a magnitude-aware objective that scales optimization updates based on the physical energy gap between native and perturbed structures. 3. Demonstrates a hard negative mining strategy to generate linguistically plausible but structurally unsound decoys for more robust training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d5257f1ab79de5e0438ace6dd83c270bc2812ff4355d0389349edec8caeb93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d5257f1ab79de5e0438ace6dd83c270bc2812ff4355d0389349edec8caeb93_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of structural hallucinations in protein language models, where generated sequences are linguistically likely but thermodynamically unstable. It proposes Physio-DPO, a physics-informed alignment method that incorporates the continuous energy landscape into the optimization process. Experiments show that Physio-DPO outperforms existing baselines, significantly reducing structural errors and increasing the foldability of generated proteins."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Physio-DPO: Aligning LLMs with the Protein Energy Landscape") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u7ed3\u6784\u5e7b\u89c9/Structural Hallucinations")\n    Problem --\x3e P2("\u80fd\u91cf\u666f\u89c2\u4e0d\u8fde\u7eed/Discontinuous Energy Landscape")\n    Method --\x3e M1("Physio-DPO\u6846\u67b6/Physio-DPO Framework")\n    Method --\x3e M2("\u5e45\u5ea6\u611f\u77e5\u76ee\u6807/Magnitude-Aware Objective")\n    Method --\x3e M3("\u786c\u8d1f\u6837\u672c\u6316\u6398/Hard Negative Mining")\n    Results --\x3e R1("\u964d\u4f4eRMSD\u81f31.28\xc5/Reduce RMSD to 1.28\xc5")\n    Results --\x3e R2("\u53ef\u6298\u53e0\u6027\u63d0\u5347\u81f392.8%/Increase Foldability to 92.8%")\n    Results --\x3e R3("\u6062\u590d\u751f\u7269\u7269\u7406\u76f8\u4e92\u4f5c\u7528/Recover Biophysical Interactions")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Fast-weight Product Key Memory"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [long-context language modeling], [product key memory, fast weights, episodic memory, gradient descent, long-context]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tianyu Zhao, Llion Jones"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sakana AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00671",children:"https://arxiv.org/pdf/2601.00671"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Fast-weight Product Key Memory (FwPKM), a novel architecture that transforms static Product Key Memory into a dynamic, fast-weight episodic memory., 2. Introduces a mechanism for dynamic parameter updates at both training and inference time via local chunk-level gradient descent, enabling rapid memorization and retrieval., 3. Demonstrates that FwPKM effectively complements semantic memory, significantly reduces perplexity on long-context data, and shows strong generalization to contexts much longer than those seen during training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc5e7437ccdc396cc0c89f8bda772596a4ad71d82cd906f8eae81c22b107608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc5e7437ccdc396cc0c89f8bda772596a4ad71d82cd906f8eae81c22b107608_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the trade-off between storage capacity and computational efficiency in sequence modeling layers. It proposes Fast-weight Product Key Memory (FwPKM), a dynamic architecture that updates parameters via local gradient descent to act as an episodic memory. Experiments show FwPKM reduces perplexity on long-context datasets and generalizes well to sequences much longer than those in training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fast-weight Product Key Memory] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b58\u50a8\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861/Trade-off: Storage Capacity vs. Computational Efficiency]\n    C --\x3e C1[\u52a8\u6001\u5feb\u901f\u6743\u91cd\u4ea7\u54c1\u952e\u8bb0\u5fc6/Dynamic Fast-weight Product Key Memory (FwPKM)]\n    C --\x3e C2[\u672c\u5730\u5757\u7ea7\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0/Local Chunk-level Gradient Descent Updates]\n    D --\x3e D1[\u957f\u4e0a\u4e0b\u6587\u56f0\u60d1\u5ea6\u964d\u4f4e/Long-context Perplexity Reduction]\n    D --\x3e D2[\u4ece4K\u5230128K\u7684\u6cdb\u5316/Generalization from 4K to 128K Tokens]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Sigmoid Head for Quality Estimation under Language Ambiguity"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [quality estimation], [quality estimation, language ambiguity, sigmoid activation, negative sampling, underconfidence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tu Anh Dinh, Jan Niehues"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Karlsruhe Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00680",children:"https://arxiv.org/pdf/2601.00680"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/TuAnh23/sigmoid-confidence",children:"https://github.com/TuAnh23/sigmoid-confidence"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies architectural and training setup issues in LMs (softmax, single-reference training) that cause ambiguity-induced underconfidence, making model probability a poor quality signal. 2. Proposes the Sigmoid Head, an extra unembedding layer with sigmoid activation, to model output tokens independently for better quality estimation. 3. Introduces a heuristic for negative sampling during training to avoid selecting potentially correct alternative tokens, improving training without needing human-annotated quality data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26806e2d4eb9b20c12526f3d561ebccfd1dfbbd43c035727d581173184fcf3b3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26806e2d4eb9b20c12526f3d561ebccfd1dfbbd43c035727d581173184fcf3b3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that language model probability is unreliable for quality estimation due to natural language ambiguity. It proposes a Sigmoid Head, an additional module with sigmoid activation and a specialized negative sampling heuristic, trained on standard LM data. This method provides a better quality signal than the original softmax head and is more robust in out-of-domain settings without requiring annotated quality data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Sigmoid Head for Quality Estimation under Language Ambiguity] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LM\u6982\u7387\u4e0d\u53ef\u9760/LM probability unreliable due to language ambiguity]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Sigmoid Head (Sigmoid\u6fc0\u6d3b & \u8d1f\u91c7\u6837\u542f\u53d1\u5f0f)/Sigmoid Head (Sigmoid activation & negative sampling heuristic)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u66f4\u597d\u7684\u8d28\u91cf\u4fe1\u53f7 & \u5bf9\u57df\u5916\u66f4\u9c81\u68d2/Better quality signal & more robust out-of-domain]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [ticket troubleshooting, retrieval-augmented generation, instruction-tuning, domain-specific ranking, large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohamed Trabelsi, Huseyin Uzunalioglu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nokia Bell Labs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00691",children:"https://arxiv.org/pdf/2601.00691"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TeleDoCTR, an end-to-end system for telecom ticket troubleshooting integrating classification, retrieval, and generation tasks. 2. Introduces a domain-specific and contextual approach combining ranking and generative models tailored for the telecom domain. 3. Demonstrates superior performance over state-of-the-art methods on a real-world telecom dataset, enhancing troubleshooting accuracy and efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TeleDoCTR, a system that automates telecom ticket troubleshooting by integrating domain-specific models for ticket classification, retrieval of similar historical tickets, and generation of fault analysis reports. It is evaluated on a real-world telecom dataset and shows improved performance over existing methods, making the troubleshooting process more accurate and efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Telecom ticket troubleshooting is complex, time-consuming, and human-intensive.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Integrates domain-specific ranking and generative models for classification, retrieval, and generation tasks.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Superior performance over SOTA methods on real-world data, enhancing accuracy and efficiency.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Exploring the Performance of Large Language Models on Subjective Span Identification Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [span identification], [large language models, in-context learning, chain of thought, aspect-based sentiment analysis, subjective spans]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alphaeus Dmonte, Roland Oruche, Tharindu Ranasinghe, Marcos Zampieri, Prasad Calyam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," George Mason University, University of Missouri, Lancaster University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00736",children:"https://arxiv.org/pdf/2601.00736"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Evaluates LLMs on subjective span identification tasks (sentiment analysis, offensive language identification, claim verification), an underexplored area compared to explicit tasks like NER. 2. Explores multiple LLM strategies including instruction tuning, in-context learning, and chain of thought for span identification. 3. Provides empirical results indicating that underlying textual relationships aid LLMs in identifying precise text spans."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33271e2c603477b20beb01a7eafd35b6ae5eb8b9dfab2b53139ccf619c75074_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33271e2c603477b20beb01a7eafd35b6ae5eb8b9dfab2b53139ccf619c75074_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the performance of Large Language Models on subjective text span identification tasks, such as sentiment analysis and offensive language detection, using strategies like in-context learning and chain of thought. The study finds that LLMs benefit from underlying relationships within the text to identify accurate spans, addressing a gap in current research focused on explicit span tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Exploring LLMs on Subjective Span Identification<br/>\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u89c2\u8de8\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Subjective span identification with LLMs is underexplored compared to explicit tasks like NER.<br/>\u4e0eNER\u7b49\u663e\u5f0f\u4efb\u52a1\u76f8\u6bd4\uff0cLLM\u5728\u4e3b\u89c2\u8de8\u5ea6\u8bc6\u522b\u65b9\u9762\u7684\u7814\u7a76\u4e0d\u8db3\u3002)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Evaluate LLMs using instruction tuning, in-context learning, and chain of thought on three tasks.<br/>\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8bc4\u4f30LLMs\u3002)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Underlying text relationships aid LLMs in identifying precise spans.<br/>\u6587\u672c\u4e2d\u7684\u6f5c\u5728\u5173\u7cfb\u6709\u52a9\u4e8eLLMs\u8bc6\u522b\u7cbe\u786e\u7684\u8de8\u5ea6\u3002)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Memory Bank Compression for Continual Adaptation of Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [memory bank compression, codebook optimization, online resetting mechanism, Key-Value Low-Rank Adaptation (KV-LoRA)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Thomas Katraouras, Dimitrios Rafailidis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Thessaly"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00756",children:"https://arxiv.org/pdf/2601.00756"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Thomkat/MBC",children:"https://github.com/Thomkat/MBC"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed MBC, a model that compresses the memory bank for continual learning via a codebook optimization strategy. 2. Introduced an online resetting mechanism to prevent codebook collapse and ensure stable learning. 3. Employed Key-Value Low-Rank Adaptation (KV-LoRA) in the LLM's attention layers to efficiently utilize the compressed memory representations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of memory bank growth in continual learning for LLMs by proposing MBC, which compresses the memory bank using codebook optimization and an online resetting mechanism. The method integrates KV-LoRA for efficient adaptation and achieves a 99.7% reduction in memory bank size while maintaining high accuracy on question-answering tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Memory Bank Compression for Continual Adaptation of Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u77e5\u8bc6\u8fc7\u65f6 / LLMs' knowledge becomes outdated]\n    B --\x3e B2[\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8 / Catastrophic forgetting in continual learning]\n    B --\x3e B3[\u5185\u5b58\u5e93\u65e0\u9650\u589e\u957f / Memory bank grows unbounded]\n    C --\x3e C1[\u5185\u5b58\u5e93\u538b\u7f29 / Memory bank compression]\n    C --\x3e C2[\u7801\u672c\u4f18\u5316\u7b56\u7565 / Codebook optimization strategy]\n    C --\x3e C3[\u5728\u7ebf\u91cd\u7f6e\u673a\u5236 / Online resetting mechanism]\n    C --\x3e C4[KV-LoRA / Key-Value Low-Rank Adaptation]\n    D --\x3e D1[\u5185\u5b58\u5e93\u5927\u5c0f\u51cf\u5c11\u81f30.3% / Memory bank size reduced to 0.3%]\n    D --\x3e D2[\u4fdd\u6301\u9ad8\u7cbe\u5ea6 / Maintains high retention accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning verification], [spectral graph analysis, attention patterns, Fiedler value, high-frequency energy ratio, sliding window attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Valentin No\xebl"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Devoteam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00791",children:"https://arxiv.org/pdf/2601.00791"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model's attention architecture."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Detecting valid mathematical reasoning in LLMs")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Spectral analysis of attention patterns as dynamic graphs")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: High classification accuracy, detects logical coherence, architectural dependency identified")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [domain adaptation], [transformer models, ensemble learning, fine-tuning, cancer registry, pathology reports]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jonathan Simkin, Lovedeep Gondara, Zeeshan Rizvi, Gregory Doyle, Jeff Dowden, Dan Bond, Desmond Martin, Raymond Ng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of British Columbia, Newfoundland & Labrador Health Services"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00787",children:"https://arxiv.org/pdf/2601.00787"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted the first cross-provincial evaluation of adapting domain-specific transformer models (BCCRTron and GatorTron) for cancer surveillance, demonstrating their ability to generalize across jurisdictions with different reporting conventions. 2. Proposed a conservative OR-ensemble method that combines complementary text representation pipelines (synoptic-focused and diagnosis-focused), substantially reducing missed cancers and improving error coverage. 3. Implemented a privacy-preserving workflow where only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20cadf176e434cff234318c2ab86906269a79285d0f4a0350b5eb92ee86ab3b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20cadf176e434cff234318c2ab86906269a79285d0f4a0350b5eb92ee86ab3b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study addresses the challenge of applying NLP models across different cancer registries with varying reporting formats. It fine-tunes two transformer models (BCCRTron and GatorTron) on data from a new jurisdiction and combines them using an OR-ensemble. The results show that this approach maintains high performance and significantly reduces missed cancer cases, demonstrating effective cross-jurisdictional adaptation with a privacy-preserving workflow."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Adapting NLP Models Across Jurisdictions] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Manual abstraction in cancer registries is slow; NLP models struggle to generalize across jurisdictions]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Fine-tune BCCRTron & GatorTron; Use OR-ensemble on complementary input pipelines]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High recall (0.99); Reduced missed cancers; Privacy-preserving weight sharing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260105] Learning Speech Representations with Variational Predictive Coding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [self-supervised speech representation learning], [predictive coding, variational inference, HuBERT, speech representations, self-supervised learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sung-Lin Yeh, Peter Bell, Hao Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Edinburgh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00100",children:"https://arxiv.org/pdf/2601.00100"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a variational predictive coding framework as the underlying principle behind the HuBERT objective, providing a theoretical foundation. 2. Derives two simple modifications to the HuBERT objective from this framework, leading to immediate performance improvements. 3. Demonstrates the framework's generality by showing its connections to other objectives like APC, CPC, wav2vec, and BEST-RQ."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba27d9f18d7b89a2b06727180b097e2e420315e2b2adae6f4d672735f9e63346_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba27d9f18d7b89a2b06727180b097e2e420315e2b2adae6f4d672735f9e63346_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies the lack of a theoretical principle as a bottleneck for improving the HuBERT objective for speech representation learning. It proposes a variational predictive coding framework as this underlying principle, which not only explains HuBERT but also leads to simple, effective modifications that improve performance. The improved pre-training yields significant gains on downstream tasks like phone classification and automatic speech recognition, validating the importance of the predictive coding interpretation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning Speech Representations with Variational Predictive Coding] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u7406\u8bba\u539f\u5219\u963b\u788dHuBERT\u53d1\u5c55/Lack of principle stalls HuBERT development]\n    C --\x3e C1[\u63d0\u51fa\u53d8\u5206\u9884\u6d4b\u7f16\u7801\u6846\u67b6/Propose variational predictive coding framework]\n    C --\x3e C2[\u63a8\u5bfcHuBERT\u4e3a\u7279\u4f8b\u5e76\u6539\u8fdb/Derive HuBERT as special case and improve it]\n    D --\x3e D1[\u9884\u8bad\u7ec3\u663e\u8457\u6539\u8fdb/Pre-training brings significant improvements]\n    D --\x3e D2[\u8fde\u63a5\u591a\u79cd\u5176\u4ed6\u76ee\u6807/Connects to various other objectives]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);