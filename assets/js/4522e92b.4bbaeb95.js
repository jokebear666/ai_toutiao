"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[220],{8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}},9424:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251215-20251221","title":"20251215-20251221","description":"2025-12-15","source":"@site/docs/daily/20251215-20251221.md","sourceDirName":"daily","slug":"/daily/20251215-20251221","permalink":"/ai_toutiao/daily/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765970259000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251208-20251214","permalink":"/ai_toutiao/daily/20251208-20251214"},"next":{"title":"Paper","permalink":"/ai_toutiao/category/paper"}}');var s=n(4848),t=n(8453);const a={},l="20251215-20251221",o={},c=[{value:"2025-12-15",id:"2025-12-15",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251215-20251221",children:"20251215-20251221"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-15",children:"2025-12-15"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 15"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed network algorithms], [multi-packet messaging, pruning, closeness centrality, decentralized computation, message efficiency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Patrick D. Manya, Eugene M. Mbuyi, Gothy T. Ngoie, Jordan F. Masakuna"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Kinshasa"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11512",children:"https://arxiv.org/pdf/2512.11512"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper enhances a distributed pruning method for closeness centrality by using multi-packet messaging to batch data into larger blocks. This reduces the number of exchanged messages and improves communication efficiency, particularly for large networks, with a manageable trade-off in local memory overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Seamless Transitions: A Comprehensive Review of Live Migration Technologies"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [virtualization], [live migration, container migration, virtual machine migration, migration techniques, migration units, infrastructure characteristics]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sima Attar-Khorasani, Lincoln Sherpa, Matthias Lieber, Siavash Ghiasvand"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," TUD Dresden University of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10979",children:"https://arxiv.org/pdf/2512.10979"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper provides a comprehensive review of live migration technologies, focusing on container-based and virtual machine-based approaches. It analyzes migration techniques, units, and infrastructure, concluding that practical challenges and resource demands can sometimes outweigh the benefits, making implementation difficult to justify."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] An Efficient Approach for Energy Conservation in Cloud Computing Environment"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [cloud computing], [task scheduling, resource utilization, fitness value, multi-criteria energy-efficient task scheduling (MCEETS), MaxUtil]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sohan Kumar Pande, Sanjaya Kumar Panda, Preeti Ranjan Sahu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Not specified"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10974",children:"https://arxiv.org/pdf/2512.10974"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a multi-criteria energy-efficient task scheduling (MCEETS) algorithm for cloud computing, which uses a fitness value based on CPU, disk, I/O utilization, and task processing time to improve resource utilization. Simulation results show that the proposed algorithm consumes less energy than the existing MaxUtil algorithm."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [dynamic scheduling, multi-objective scheduling, hybrid priority scheduler, predictive backfill, smart batch, fragmentation reduction, job starvation, GPU utilization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Akhmadillo Mamirov"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The College of Wooster"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10980",children:"https://arxiv.org/pdf/2512.10980"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces three dynamic multi-objective schedulers (Hybrid Priority, Predictive Backfill, and Smart Batch) designed to reduce fragmentation and starvation in GPU clusters. Through simulation, these schedulers significantly outperform static baselines in utilization, throughput, and fairness, demonstrating that adaptive scheduling can meaningfully improve GPU efficiency in heterogeneous AI clusters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] An LLVM-Based Optimization Pipeline for SPDZ"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [secure multiparty computation], [LLVM, SPDZ, secret sharing, batching, GPU kernels, non-blocking scheduler]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tianye Dai, Hammurabi Mendes, Heuichan Lim"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Davidson College"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11112",children:"https://arxiv.org/pdf/2512.11112"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents an LLVM-based compiler pipeline for the SPDZ MPC protocol, which uses C with privacy annotations and LLVM IR to automatically batch operations and a runtime scheduler to overlap communication and computation, including GPU kernel mapping. The evaluation shows significant speedups over MP-SPDZ, indicating that leveraging LLVM with protocol-aware scheduling is effective for extracting parallelism without sacrificing usability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [hybrid parallelism, data parallelism, pipeline parallelism, model partitioning, network scheduling, runtime adaptation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jianli Jin, Ziyang Lin, Qianli Dong, Yi Chen, Jayanth Srinivasa, Myungjin Lee, Zhaowei Tan, Fan Lai"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," UIUC, Northwestern University, University of California, Riverside, Cisco Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10990",children:"https://arxiv.org/pdf/2512.10990"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Dora is a framework that uses heterogeneity-aware model partitioning, contention-aware network scheduling, and a runtime adapter to achieve QoE-aware hybrid parallelism for distributed edge AI. It demonstrates significant improvements in execution speed and energy efficiency while maintaining quality of experience under dynamic runtime conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [GPU kernels], [GPU-native compilation, parallel traditional compilation, neural compilation, sequence-to-sequence translation, probabilistic verification, hybrid architecture, in-VRAM iteration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Institute of Information Technology, Kyrgyz State Technical University named after I. Razzakov"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11200",children:"https://arxiv.org/pdf/2512.11200"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper establishes theoretical foundations for three GPU-native compilation approaches\u2014parallel traditional, neural, and hybrid\u2014to eliminate CPU-GPU data transfers during code iteration cycles. It demonstrates that these methods can achieve 10-100x speedups by keeping compilation and execution entirely in GPU memory. The main conclusion is that GPU-native compilation offers a path to drastically reduce latency and energy consumption in AI code generation systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [post-training], [cluster scheduling, disaggregated architecture, co-execution group, two-tier scheduling, round-robin, warm-start context switching]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Hong Kong University of Science and Technology, UIUC, Alibaba Group"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11306",children:"https://arxiv.org/pdf/2512.11306"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces RollMux, a cluster scheduling framework that improves efficiency in disaggregated RL post-training by multiplexing jobs to utilize idle phases. Its core method involves a two-tier scheduler and a co-execution group abstraction to orchestrate cross-cluster execution. The evaluation shows RollMux significantly improves cost efficiency over standard disaggregated and co-located baselines while maintaining service-level objectives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [hierarchical federated learning, aggregated federated learning, continual federated learning, decentralized aggregation, gossip-based protocols]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sumit Chongder"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Maharashtra Institute of Technology - Art, Design and Technology University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10987",children:"https://arxiv.org/pdf/2512.10987"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes an evaluation framework to compare centralized and decentralized aggregation algorithms in federated learning systems. It finds that decentralized methods (Aggregated and Continual Federated Learning) outperform centralized Hierarchical Federated Learning in metrics like precision and recall on standard datasets, highlighting the advantages of distributed computation and aggregation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Agentic Operator Generation for ML ASICs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Triton, ATen kernels, PyTorch OpInfo, JIT compilation, large language models, agentic AI, kernel generation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Alec M. Hammond, Aram Markosyan, Aman Dontula, Simon Mahns, Zacharias Fisches, Dmitrii Pedchenko, Keyur Muzumdar, Natacha Supper, Mark Saroufim, Joe Isaacson, Laura Wang, Warren Hunt, Kaustubh Gondkar, Roman Levenstein, Gabriel Synnaeve, Richard Li, Jacob Kahn, Ajit Mathews"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Meta"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10977",children:"https://arxiv.org/pdf/2512.10977"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents TritorX, an agentic AI system that uses large language models combined with a custom linter, JIT compilation, and a PyTorch OpInfo test harness to automatically generate functionally correct Triton ATen kernels for ML accelerators like MTIA. The system prioritizes broad operator coverage over performance for a limited set, successfully generating kernels for 481 unique operators that pass over 20,000 tests. This approach enables the rapid overnight generation of complete PyTorch ATen backends for new accelerator platforms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [runtime parallelization, branch-aware memory management, adaptive scheduling, DAG partitioning, buffer reuse, heterogeneous inference]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Chong Tang, Hao Dai, Jagmohan Chauhan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Southampton, University College London"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11532",children:"https://arxiv.org/pdf/2512.11532"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Parallax is a framework that accelerates mobile DNN inference by partitioning the computation graph to expose parallelism and using branch-aware memory management with adaptive scheduling to handle operator fallbacks. It reduces latency by up to 46% and energy consumption by up to 30% compared to state-of-the-art frameworks, while controlling memory overhead, without requiring model refactoring."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [continuous learning, camera grouping, GPU allocation, transmission control, cross-camera correlation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yuze He, Ferdi Kossmann, Srinivasan Seshan, Peter Steenkiste"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Carnegie Mellon University, Massachusetts Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11727",children:"https://arxiv.org/pdf/2512.11727"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," ECCO is a video analytics framework that improves resource efficiency by grouping cameras with similar data drift patterns to share retrained models. It uses a dynamic grouping algorithm, GPU allocator, and transmission controller to reduce compute and communication costs. The system increases retraining accuracy by 6.7%-18.1% or supports 3.3x more cameras at the same accuracy compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed systems], [Snowflake algorithm, network-derived identity, private IPv4 address, bit-allocation scheme (1-41-16-6), stateless microservices, container orchestration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Manideep Reddy Chinthareddy"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11643",children:"https://arxiv.org/pdf/2512.11643"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a stateless, cloud-agnostic distributed ID generator that eliminates the need for explicit worker IDs by deriving node uniqueness from a container's private IPv4 address. It introduces a modified bit-allocation scheme to incorporate this network-derived entropy while preserving monotonicity. The method demonstrates performance comparable to traditional stateful generators while offering improved scalability and operational simplicity in containerized environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [HPC resource access], [RESTful API, performance testing, proxy-based APIs, I/O bottlenecks, architectural redesign, security, authorization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Elia Palme, Juan Pablo Dorsch, Ali Khosravi, Giovanni Pizzi, Francesco Pagnamenta, Andrea Ceriani, Eirini Koutsaniti, Rafael Sarmiento, Ivano Bonesana, Alejandro Dabin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," CSCS \u2013 Swiss National Supercomputing Centre, PSI Center for Scientific Computing, Theory, and Data"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11634",children:"https://arxiv.org/pdf/2512.11634"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents FirecREST v2, a redesigned RESTful API for programmatic access to HPC resources, focusing on integrating enhanced security and high throughput. Through systematic performance testing, the authors identified and addressed bottlenecks in proxy-based APIs, achieving a ~100x performance improvement. The key conclusion is that a ground-up architectural redesign was necessary to meet growing user demands for scalable and secure HPC resource access."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251215] Hypergraph based Multi-Party Payment Channel"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [blockchain scalability], [hypergraph, payment channel networks, multi-party channels, off-chain scaling, DAG updates]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ayush Nainwal, Atharva Kamble, Nitin Awathare"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Indian Institute of Technology, Jodhpur; Indian Institute of Technology, Bombay"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11775",children:"https://arxiv.org/pdf/2512.11775"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces traditional bilateral channels with collectively funded hyperedges to enable leaderless, concurrent payments. This design addresses liquidity fragmentation and channel depletion in existing payment networks. An implementation demonstrates a high transaction success rate of approximately 94%, highlighting the robustness of the approach."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 16'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11179",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11463",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Rethinking Expert Trajectory Utilization in LLM Post-training ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11470",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] In-Context Multi-Objective Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11114",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11277",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11345",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11421",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11247",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Three methods, one problem: Classical and AI approaches to no-three-in-line ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11469",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11169",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11270",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11342",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11391",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11558",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Agile Flight Emerges from Multi-Agent Competitive Racing ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11781",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:['[arXiv251215] Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness ',(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.10985",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] A Scalable Multi-GPU Framework for Encrypted Large-Model Inference ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11269",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11187",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11127",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11282",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11342",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251215] Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.11587",children:"link"})]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);