"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1480],{3497:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>t,contentTitle:()=>l,default:()=>o,frontMatter:()=>d,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_MM/20251222-20251228","title":"20251222-20251228 (cs.MM)","description":"2025-12-22","source":"@site/docs/daily/cs_MM/20251222-20251228.md","sourceDirName":"daily/cs_MM","slug":"/daily/csmm/20251222-20251228","permalink":"/ai_toutiao/daily/csmm/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766557777000,"frontMatter":{"slug":"/daily/csmm/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.MM)","permalink":"/ai_toutiao/daily/cs_MM/20251215-20251221"},"next":{"title":"cs.MS","permalink":"/ai_toutiao/daily/csms"}}');var a=i(4848),r=i(8453);const d={slug:"/daily/csmm/20251222-20251228"},l="20251222-20251228 (cs.MM)",t={},c=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2},{value:"2025-12-24",id:"2025-12-24",level:2}];function h(n){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"20251222-20251228-csmm",children:"20251222-20251228 (cs.MM)"})}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [modular reuse, on-device execution, model decomposition, parallel execution, quantization]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Amherst, Adobe Research"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17108",children:"https://arxiv.org/pdf/2512.17108"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27\u201333% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Nanjing University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17319",children:"https://arxiv.org/pdf/2512.17319"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Region-Constraint In-Context Generation for Instructional Video Editing"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion training], [in-context generation, video diffusion, latent regularization, attention regularization, region-constraint, joint denoising]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Science and Technology of China, HiDream.ai Inc."]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17650",children:"https://arxiv.org/pdf/2512.17650"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17946",children:"https://arxiv.org/pdf/2512.17946"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Changxu Duan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18115",children:"https://arxiv.org/pdf/2512.18115"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Changxu Duan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18122",children:"https://arxiv.org/pdf/2512.18122"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18318",children:"https://arxiv.org/pdf/2512.18318"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18660",children:"https://arxiv.org/pdf/2512.18660"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18804",children:"https://arxiv.org/pdf/2512.18804"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18809",children:"https://arxiv.org/pdf/2512.18809"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Alina Elena Baia, Andrea Cavallaro"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18864",children:"https://arxiv.org/pdf/2512.18864"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsxs)(e.strong,{children:["[arXiv251223] D",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mo,{stretchy:"false",children:"{"})]}),(0,a.jsx)(e.mn,{children:"2"}),(0,a.jsx)(e.mo,{stretchy:"false",children:"}"})]}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^\\{2\\}"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"1.138em",verticalAlign:"-0.25em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.888em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mopen mtight",children:"{"})})]})})})})})]}),(0,a.jsx)(e.span,{className:"mord",children:"2"}),(0,a.jsx)(e.span,{className:"mclose",children:"}"})]})})]}),"Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection"]})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Junhao Xiao, Shun Feng, Zhiyu Wu, Jianjun Li, Zhiyuan Ma, Yi Chen"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19130",children:"https://arxiv.org/pdf/2512.19130"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/711fd748a4cf4a217aaf71023f458a85baa17b26b01b97cc24877d0ea5e48f6e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/711fd748a4cf4a217aaf71023f458a85baa17b26b01b97cc24877d0ea5e48f6e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," D",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsxs)(e.mrow,{children:[(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mo,{stretchy:"false",children:"{"})]}),(0,a.jsx)(e.mn,{children:"2"}),(0,a.jsx)(e.mo,{stretchy:"false",children:"}"})]}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^\\{2\\}"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"1.138em",verticalAlign:"-0.25em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.888em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mopen mtight",children:"{"})})]})})})})})]}),(0,a.jsx)(e.span,{className:"mord",children:"2"}),(0,a.jsx)(e.span,{className:"mclose",children:"}"})]})})]}),"Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19379",children:"https://arxiv.org/pdf/2512.19379"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-24",children:"2025-12-24"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20059",children:"https://arxiv.org/pdf/2512.20059"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20292",children:"https://arxiv.org/pdf/2512.20292"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19703",children:"https://arxiv.org/pdf/2512.19703"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Daichi Arai, Yuichi Kondo, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20093",children:"https://arxiv.org/pdf/2512.20093"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/812f9579f0639bc36d83f19f057f2959cd40c919ca5879492cd5209f569425ef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/812f9579f0639bc36d83f19f057f2959cd40c919ca5879492cd5209f569425ef_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation"]}),"\n"]}),"\n"]}),"\n"]})]})}function o(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>d,x:()=>l});var s=i(6540);const a={},r=s.createContext(a);function d(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:d(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);