"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1809],{19335:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_SD/20260105-20260111","title":"20260105-20260111 (cs.SD)","description":"2026-01-05","source":"@site/docs/daily/cs_SD/20260105-20260111.md","sourceDirName":"daily/cs_SD","slug":"/daily/cssd/20260105-20260111","permalink":"/ai_toutiao/daily/cssd/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cssd/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.SD)","permalink":"/ai_toutiao/daily/cssd/20251229-20260104"},"next":{"title":"cs.SE","permalink":"/ai_toutiao/daily/csse"}}');var t=i(74848),r=i(28453);const a={slug:"/daily/cssd/20260105-20260111"},o="20260105-20260111 (cs.SD)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"20260105-20260111-cssd",children:"20260105-20260111 (cs.SD)"})}),"\n",(0,t.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] IKFST: IOO and KOO Algorithms for Accelerated and Precise WFST-based End-to-End Automatic Speech Recognition"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Weighted Finite-State Transducer (WFST), Connectionist Temporal Classification (CTC), decoding algorithm, speech recognition, inference acceleration]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Zhuoran Zhuang, Ye Chen, Chao Luo, Tian-Hao Zhang, Xuewei Zhang, Jian Ma, Jiatong Shi, Wei Zhang"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Alibaba (Fliggy Alibaba), University of Science and Technology Beijing, Carnegie Mellon University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00160",children:"https://arxiv.org/pdf/2601.00160"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. A systematic analysis of CTC outputs, identifying that blank frames encode positional information and non-blank frames carry semantic content. 2. The proposal of two novel decoding algorithms, Keep-Only-One (KOO) and Insert-Only-One (IOO), which exploit this structural insight. 3. Demonstration of state-of-the-art recognition accuracy with significantly reduced decoding latency on multiple datasets, enabling efficient WFST-based ASR."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515f6922494c8cf7b8b1e07b8fe5db67575518e2d23346ce2f97d6c182358e17_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515f6922494c8cf7b8b1e07b8fe5db67575518e2d23346ce2f97d6c182358e17_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of frame-by-frame WFST decoding in CTC-based speech recognition. It introduces two new algorithms, IOO and KOO, which leverage the distinct roles of blank and non-blank frames to accelerate inference. Experiments show these methods achieve high accuracy while substantially reducing decoding latency."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root("IKFST: IOO and KOO Algorithms") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("WFST\u89e3\u7801\u6548\u7387\u4f4e/WFST decoding is inefficient")\n    P1 --\x3e P2("\u9010\u5e27\u81ea\u56de\u5f52\u641c\u7d22\u6162/Frame-by-frame autoregressive search is slow")\n    Method --\x3e M1("\u5206\u6790CTC\u8f93\u51fa/Analyze CTC outputs")\n    M1 --\x3e M2("\u7a7a\u767d\u5e27:\u4f4d\u7f6e/Blank: Position")\n    M1 --\x3e M3("\u975e\u7a7a\u767d\u5e27:\u8bed\u4e49/Non-blank: Semantic")\n    Method --\x3e M4("\u63d0\u51fa\u65b0\u7b97\u6cd5/Propose new algorithms")\n    M4 --\x3e M5("IOO (Insert-Only-One)")\n    M4 --\x3e M6("KOO (Keep-Only-One)")\n    Results --\x3e R1("\u9ad8\u8bc6\u522b\u7cbe\u5ea6/High recognition accuracy")\n    Results --\x3e R2("\u4f4e\u89e3\u7801\u5ef6\u8fdf/Low decoding latency")\n    Results --\x3e R3("SOTA\u7ed3\u679c/SOTA results")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Latent Flow Matching for Expressive Singing Voice Synthesis"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [conditional flow matching, latent space modeling, singing voice synthesis, ordinary differential equation, variational autoencoder]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Minhyeok Yun, Yong-Hoon Choi"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Kwangwoon University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00217",children:"https://arxiv.org/pdf/2601.00217"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/alsgur9368/FM-Singer",children:"https://github.com/alsgur9368/FM-Singer"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FM-Singer, a novel singing voice synthesis method that applies conditional flow matching (CFM) in the latent space to address the prior-posterior mismatch in cVAE-based models. 2. Introduces a latent ODE refinement step at inference time to transport prior samples towards the expressive posterior distribution, improving fine-grained expressiveness while maintaining parallel decoding efficiency. 3. Demonstrates consistent performance improvements on Korean and Chinese singing datasets, including lower mel-cepstral distortion and F0 error, and higher perceptual scores."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ff2d99db594dd8c63fa46f401cf9408cda76d76d116f920d5a1dbb077b2fcf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88ff2d99db594dd8c63fa46f401cf9408cda76d76d116f920d5a1dbb077b2fcf_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the degradation of fine-grained expressiveness in cVAE-based singing voice synthesis due to a mismatch between the prior and posterior latent distributions. The proposed FM-Singer method uses conditional flow matching in the latent space to learn a vector field that refines prior samples via an ODE, enhancing expressiveness like vibrato. Experiments show the method outperforms strong baselines on multiple datasets."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[Latent Flow Matching for Expressive Singing Voice Synthesis] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[cVAE\u5408\u6210\u4e2d\u5148\u9a8c-\u540e\u9a8c\u4e0d\u5339\u914d<br/>Prior-Posterior Mismatch in cVAE]\n    B --\x3e B2[\u7ec6\u5fae\u8868\u73b0\u529b\u4e0b\u964d<br/>Degraded Fine-Grained Expressiveness]\n    C --\x3e C1[\u6f5c\u5728\u7a7a\u95f4\u6761\u4ef6\u6d41\u5339\u914d<br/>Latent-Space Conditional Flow Matching]\n    C --\x3e C2[ODE\u63a8\u7406\u65f6\u7ec6\u5316<br/>ODE-based Refinement at Inference]\n    D --\x3e D1[\u66f4\u4f4e\u7684MCD\u4e0eF0\u8bef\u5dee<br/>Lower MCD & F0 Error]\n    D --\x3e D2[\u66f4\u9ad8\u7684\u611f\u77e5\u8bc4\u5206<br/>Higher Perceptual Scores]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Timed text extraction from Taiwanese Kua-\xe1-h\xec TV series"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [other], [music information retrieval], [OCR, Speech and Music Activity Detection, subtitle extraction, traditional Chinese, archival video processing]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Tzu-Hung Huang, Yun-En Tsai, Yun-Ning Hung, Chih-Wei Wu, I-Chieh Wei, Li Su"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Academia Sinica, National Taiwan University, Music AI, University of Auckland"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00299",children:"https://arxiv.org/pdf/2601.00299"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/z-huang/ocr-subtitle-editor",children:"https://github.com/z-huang/ocr-subtitle-editor"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Developed an interactive system for real-time OCR correction to handle low-quality archival video subtitles. 2. Proposed a two-step workflow integrating OCR-driven segmentation with Speech and Music Activity Detection (SMAD) to identify vocal segments. 3. Created a dataset of vocal segments and corresponding lyrics from Taiwanese opera TV series to support MIR tasks."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9645950400de9f4b82b5447edc80e161ca3823766faec9633bb49af2f67eccfb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9645950400de9f4b82b5447edc80e161ca3823766faec9633bb49af2f67eccfb_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of extracting timed text and lyrics from low-quality archival Taiwanese opera TV series. The authors propose a method combining an interactive OCR correction tool with a two-step segmentation approach using OCR and SMAD to efficiently create a dataset of vocal segments and lyrics. The resulting dataset is intended to support Music Information Retrieval tasks like lyrics identification and tune retrieval."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[Timed text extraction from Taiwanese Kua-\xe1-h\xec TV series] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f4e\u8d28\u91cf\u6863\u6848\u89c6\u9891\u5b57\u5e55\u63d0\u53d6\u56f0\u96be/Low-quality archival video subtitle extraction is difficult]\n    C --\x3e C1[\u4ea4\u4e92\u5f0fOCR\u5b9e\u65f6\u6821\u6b63/Interactive real-time OCR correction]\n    C --\x3e C2[OCR\u5206\u5272\u4e0e\u8bed\u97f3\u97f3\u4e50\u6d3b\u52a8\u68c0\u6d4b\u4e24\u6b65\u6cd5/Two-step OCR segmentation & SMAD]\n    D --\x3e D1[\u521b\u5efa\u5e26\u6b4c\u8bcd\u7684\u4eba\u58f0\u7247\u6bb5\u6570\u636e\u96c6/Created dataset of vocal segments with lyrics]\n    D --\x3e D2[\u652f\u6301\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1/Supports MIR tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI) / Computer-Supported Cooperative Work (CSCW)], [Mixed Reality, Digital Audio Workstation, Collaborative Looping, Musical Metaverse, Speculative Design]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Torin Hopkins, Shih-Yu Ma, Suibi Che-Chuan Weng, Ming-Yuan Pai, Ellen Yi-Luen Do, Luca Turchet"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, University of Trento, SolJAMM Research"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00326",children:"https://arxiv.org/pdf/2601.00326"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Proposed and developed MR-DAW, a novel Mixed Reality system enabling multiple remote users to control a single, shared DAW instance while moving freely in their physical space. 2. Introduced a hands-free, collaborative interaction paradigm using physical foot pedals for remote, real-time looping control within the shared virtual session. 3. Conducted a qualitative study with 20 musicians to analyze current DAW practices, evaluate the MR-DAW system\'s usability, and provide a speculative outlook on the future of collaborative music-making in the "Musical Metaverse".']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates using Mixed Reality (MR) to overcome the limitations of traditional Digital Audio Workstations (DAWs), which tether musicians to a desk and hinder remote collaboration. The authors propose MR-DAW, a networked MR system that allows geographically dispersed musicians to control a shared DAW and use foot pedals for collaborative looping. The study with 20 musicians highlights MR's potential for unencumbered musical interaction and provides a speculative vision for future remote collaborative DAWs in the Musical Metaverse."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    Root[MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[DAWs\u675f\u7f1a\u97f3\u4e50\u5bb6\u5de5\u4f5c\u6d41/DAWs encumber musician workflow]\n    Problem --\x3e P2[\u8fdc\u7a0b\u534f\u4f5c\u56f0\u96be/Remote collaboration is challenging]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f00\u53d1MR-DAW\u8bbe\u8ba1\u63a2\u9488/Developed MR-DAW design probe]\n    Method --\x3e M2[\u4f7f\u7528\u811a\u8e0f\u677f\u8fdb\u884c\u534f\u4f5c\u5faa\u73af/Used foot pedal for collaborative looping]\n    Method --\x3e M3[\u5b9a\u6027\u7814\u7a76\u4e0e\u7cfb\u7edf\u8bc4\u4f30/Qualitative study & system evaluation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[MR\u652f\u6301\u65e0\u675f\u7f1a\u4ea4\u4e92/MR affords unencumbered interaction]\n    Results --\x3e R2[\u5c55\u671b\u97f3\u4e50\u5143\u5b87\u5b99\u672a\u6765/Speculative outlook on Musical Metaverse]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [LoRA, Mixture-of-Experts, CTC, multilingual ASR, language-agnostic]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Yuang Zheng, Yuxiang Mei, Dongxing Xu, Jie Chen, Yanhua Long"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Shanghai Normal University, Unisound AI Technology Co., Ltd."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00557",children:"https://arxiv.org/pdf/2601.00557"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model for lightweight multilingual ASR. 2. Introduces an LID-posterior-driven LoRA routing mechanism that enables true language-agnostic, single-pass decoding without prior language identity information. 3. Demonstrates that the proposed method achieves competitive performance with state-of-the-art two-stage inference methods while significantly improving decoding efficiency for low-resource applications."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd2404a5aa3ab85ddcd83e20182b5cedff2831876ded928574f65b33a573d7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd2404a5aa3ab85ddcd83e20182b5cedff2831876ded928574f65b33a573d7d_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high computational cost and latency of large multilingual ASR models like Whisper for edge deployment. It proposes a lightweight, language-agnostic system using a Hierarchical LoRA-MoE architecture with CTC, which enables efficient single-pass decoding without needing language labels. Experiments show the method achieves competitive performance with more complex two-stage systems, improving efficiency for low-resource multilingual ASR."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root["A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Large multilingual ASR models (e.g., Whisper) are computationally expensive and have high latency, limiting edge device deployment."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a lightweight HLoRA framework with hierarchical LoRA-MoE design and LID-posterior-driven routing for language-agnostic, single-pass CTC decoding."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves competitive performance with SOTA two-stage methods on MSR-86K and MLC-SLM 2025 datasets, improving decoding efficiency."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [audio deepfake detection], [Multimodal Large Language Models, audio deepfake detection, zero-shot, fine-tuning, multi-prompt]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Ropar, Birla Institute of Technology and Science Pilani Hyderabad Campus, Monash University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00777",children:"https://arxiv.org/pdf/2601.00777"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Pioneering exploration of Multimodal Large Language Models for audio deepfake detection, a largely unexplored area. 2. Introduction of a text-aware, context-rich, question-answer based multi-prompt approach to facilitate multimodal understanding for the task. 3. Comprehensive evaluation of models (Qwen2-Audio-7B-Instruct, SALMONN) in zero-shot and fine-tuned modes, demonstrating their potential on in-domain data with minimal supervision."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the use of Multimodal Large Language Models for detecting audio deepfakes by combining audio inputs with text prompts. The method employs a multi-prompt, question-answer approach and evaluates models in zero-shot and fine-tuned settings. The results show that while models struggle without training and on out-of-domain data, they achieve promising performance on in-domain data with minimal supervision, indicating a viable path forward."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root["Investigating MLLMs for Audio Deepfake Detection<br/>\u63a2\u7a76MLLMs\u7528\u4e8e\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["MLLMs for audio deepfakes unexplored<br/>MLLMs\u7528\u4e8e\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u672a\u88ab\u63a2\u7d22"]\n    Method["Audio + Multi-prompt QA approach<br/>\u97f3\u9891+\u591a\u63d0\u793a\u95ee\u7b54\u65b9\u6cd5"]\n    Results["Poor zero-shot, good fine-tuned on in-domain data<br/>\u96f6\u6837\u672c\u6548\u679c\u5dee\uff0c\u57df\u5185\u5fae\u8c03\u6548\u679c\u597d"]'}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);