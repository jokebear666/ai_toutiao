"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3410],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},93086:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_RO/20251229-20260104","title":"20251229-20260104 (cs.RO)","description":"2025-12-29","source":"@site/docs/daily/cs_RO/20251229-20260104.md","sourceDirName":"daily/cs_RO","slug":"/daily/csro/20251229-20260104","permalink":"/ai_toutiao/daily/csro/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/csro/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20251222-20251228"},"next":{"title":"20260105-20260111 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20260105-20260111"}}');var a=i(74848),r=i(28453);const t={slug:"/daily/csro/20251229-20260104"},o="20251229-20260104 (cs.RO)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2},{value:"2026-01-01",id:"2026-01-01",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251229-20260104-csro",children:"20251229-20260104 (cs.RO)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, North China University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21375",children:"https://arxiv.org/pdf/2512.21375"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --\x3e B\nA --\x3e C\nA --\x3e D\nB[\u6838\u5fc3\u95ee\u9898/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]\nC[\u4e3b\u8981\u65b9\u6cd5/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]\nD[\u5173\u952e\u7ed3\u679c/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot navigation], [language-conditioned planning, map forecasting, Log-MPPI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Moorthy Mahesh, Oguzhan Goktug Poyrazoglu, Yukang Cao, Volkan Isler"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' University of Minnesota (inferred from author "Volkan Isler")']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21398",children:"https://arxiv.org/pdf/2512.21398"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PaceForecaster, a novel architecture that integrates co-pilot language instructions into local motion planning. 2. Predicts a forecasted map (Level-2) of occluded areas and an instruction-conditioned subgoal within it. 3. Demonstrates a 36% improvement in navigation performance by integrating PaceForecaster with a Log-MPPI controller."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the speed-safety trade-off in robot navigation in occluded environments by introducing PaceForecaster, a method that uses language instructions to forecast occluded map regions and generate conditioned subgoals. Integrating this with a Log-MPPI controller allows for more decisive and goal-directed planning. The approach improves navigation performance by 36% over a baseline that uses only the local sensor map."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b89\u5168\u4e0e\u901f\u5ea6\u7684\u6743\u8861<br/>Safety-Speed Trade-off]\n    B1 --\x3e B2[\u906e\u6321\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027<br/>Uncertainty from Occlusions]\n    C --\x3e C1[PaceForecaster \u67b6\u6784<br/>PaceForecaster Architecture]\n    C1 --\x3e C2[\u8f93\u5165: \u4f20\u611f\u5668\u8db3\u8ff9\u4e0e\u6307\u4ee4<br/>Input: Sensor Footprint & Instruction]\n    C2 --\x3e C3[\u8f93\u51fa: \u9884\u6d4b\u5730\u56fe\u4e0e\u5b50\u76ee\u6807<br/>Output: Forecasted Map & Subgoal]\n    C3 --\x3e C4[\u96c6\u6210 Log-MPPI \u63a7\u5236\u5668<br/>Integrated with Log-MPPI Controller]\n    D --\x3e D1[\u6027\u80fd\u63d0\u5347 36%<br/>36% Performance Improvement]\n    D1 --\x3e D2[\u8d85\u8d8a\u4ec5\u4f7f\u7528\u5c40\u90e8\u5730\u56fe\u7684\u57fa\u7ebf<br/>Over Local-Map-Only Baseline]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [Transportation Systems, Robotics], [Fundamental Diagram, Urban Air Mobility, Traffic Flow Theory, Drone Control, Physical Experiments]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hang Zhou, Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang, Xiaowei Shi, Xiaopeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison, University of Illinois Urbana-Champaign, University of Wisconsin-Milwaukee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21425",children:"https://arxiv.org/pdf/2512.21425"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/CATS-Lab/UAM-FD",children:"https://github.com/CATS-Lab/UAM-FD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework integrating theory and physical experiments to construct a Fundamental Diagram for Urban Air Mobility traffic. 2. Develops and validates the first UAM Fundamental Diagram using real-world physical test data from a reduced-scale drone testbed. 3. Creates and releases the UAMTra2Flow dataset containing simulation and physical test trajectory data for UAM traffic analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study addresses the lack of understanding of Urban Air Mobility (UAM) traffic flow by proposing a framework to construct its Fundamental Diagram (FD) through theoretical modeling and physical experiments using drones. The results show that classical ground traffic FD structures are applicable to UAM, but physical experiments reveal deviations from simulation, underscoring the need for experimental validation. The findings and a public dataset provide practical insights for future UAM system design."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\nRoot("Developing a Fundamental Diagram for Urban Air Mobility Traffic Flow / \u6784\u5efa\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u672c\u56fe")\nRoot --\x3e Problem("UAM\u4ea4\u901a\u6d41\u7279\u6027\u672a\u77e5 / UAM Traffic Flow Poorly Understood")\nRoot --\x3e Method("\u7406\u8bba\u5206\u6790+\u7269\u7406\u5b9e\u9a8c\u6846\u67b6 / Theory + Physical Experiment Framework")\nRoot --\x3e Results("\u7ecf\u5178FD\u7ed3\u6784\u9002\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5173\u952e / Classical FD Applicable, Validation Crucial")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EVE: A Generator-Verifier System for Generative Policies"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [generative policies, test-time compute, vision-language models, verifier agents, embodied control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology, Toyota Research Institute, Symbotic Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21430",children:"https://arxiv.org/pdf/2512.21430"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes EVE, a modular generator-verifier framework that improves pretrained generative visuomotor policies at test time without additional training. 2. Introduces a system of multiple zero-shot VLM-based verifier agents that propose action refinements, coupled with an action incorporator to fuse these suggestions. 3. Provides a systematic analysis and practical guidelines for designing scalable generator-verifier systems for embodied control through extensive ablations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that generative visuomotor policies degrade under distribution shifts and lack recovery capabilities. It proposes EVE, a framework that uses additional inference-time compute to refine a frozen base policy's actions via multiple zero-shot VLM verifiers. The method consistently improves task success rates across diverse manipulation tasks without any retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[EVE: A Generator-Verifier System for Generative Policies] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Generative policies degrade under distribution shifts and lack recovery.] --\x3e ProblemDetail[\u95ee\u9898\u7ec6\u8282/Problem Detail: Costly to finetune; limited test-time robustness.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: EVE framework] --\x3e MethodDetail1[\u65b9\u6cd5\u7ec6\u8282/Method Detail 1: Wraps frozen base policy with zero-shot VLM verifiers.]\n    Method --\x3e MethodDetail2[\u65b9\u6cd5\u7ec6\u8282/Method Detail 2: Verifiers propose action refinements.]\n    Method --\x3e MethodDetail3[\u65b9\u6cd5\u7ec6\u8282/Method Detail 3: Action incorporator fuses verifier outputs.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Boosts performance without training.] --\x3e ResultsDetail[\u7ed3\u679c\u7ec6\u8282/Results Detail: Improves task success rates across diverse manipulation tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Planetary Terrain Datasets and Benchmarks for Rover Path Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [path planning], [global path planning, digital terrain models, autonomous navigation, rover exploration, benchmark datasets]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Marvin Chanc\xe1n, Avijit Banerjee, George Nikolakopoulos"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lule\xe5 University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21438",children:"https://arxiv.org/pdf/2512.21438"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/mchancan/PlanetaryPathBench",children:"https://github.com/mchancan/PlanetaryPathBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the first two large, space mission-derived planetary benchmark datasets for rover path planning (MarsPlanBench and MoonPlanBench). 2. Establishes a unified framework for evaluating both classical and learning-based path planning algorithms on these planetary terrains. 3. Provides new empirical insights into algorithm performance, showing classical methods achieve high success rates on challenging terrains while learning-based methods struggle to generalize."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of standardized planetary datasets and benchmarks for rover path planning by introducing MarsPlanBench and MoonPlanBench, two large datasets derived from high-resolution digital terrain models of Mars and the Moon. The authors evaluate classical and learning-based path planning algorithms in a unified framework on these new benchmarks. The key finding is that classical algorithms achieve very high success rates (up to 100%) on challenging planetary terrains, explaining their practical use by agencies like NASA, while learning-based models still face generalization difficulties in these domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Planetary Terrain Datasets and Benchmarks for Rover Path Planning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u7f3a\u4e4f\u7528\u4e8e\u8def\u5f84\u89c4\u5212\u7684\u884c\u661f\u6570\u636e\u96c6\u4e0e\u57fa\u51c6/Lack of planetary datasets & benchmarks for path planning]\nC --\x3e C1[\u63d0\u51fa\u706b\u661f\u4e0e\u6708\u7403\u57fa\u51c6\u6570\u636e\u96c6/Propose MarsPlanBench & MoonPlanBench datasets]\nC --\x3e C2[\u5efa\u7acb\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6/Establish unified evaluation framework]\nD --\x3e D1[\u7ecf\u5178\u7b97\u6cd5\u6210\u529f\u7387\u9ad8\u8fbe100%/Classical algorithms achieve up to 100% success rate]\nD --\x3e D2[\u5b66\u4e60\u6a21\u578b\u6cdb\u5316\u56f0\u96be/Learning-based models struggle to generalize]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [control theory], [spatiotemporal tube, probabilistic safety, real-time control, uncertain dynamic environment, reach-avoid-stay]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siddhartha Upadhyay, Ratnangshu Das, Pushpak Jagtap"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science (IISc), Bengaluru"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21497",children:"https://arxiv.org/pdf/2512.21497"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extension of the Spatiotemporal Tube (STT) framework to handle Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in environments with time-varying uncertain obstacles. 2. Development of a real-time tube synthesis procedure that provides formal probabilistic safety guarantees. 3. Derivation of a closed-form, model-free, approximation-free, and optimization-free control law that confines the system within the tube for efficient real-time execution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an extension of the Spatiotemporal Tube framework to solve Probabilistic Temporal Reach-Avoid-Stay tasks in uncertain dynamic environments. The method synthesizes a time-varying safe tube online and provides a closed-form control law to keep the system inside it, offering formal probabilistic safety and task completion guarantees. The approach is validated through simulations and hardware experiments on various robotic platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Paper Title: Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Probabilistic Temporal Reach-Avoid-Stay in uncertain dynamic environments)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Real-time Spatiotemporal Tube synthesis with closed-form control)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Formal probabilistic safety guarantees & efficient real-time execution)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [soft robotics, variable stiffness actuators], [electrostatic layer jamming, helical winding, variable stiffness robotic finger, voltage-driven control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Congrui Bai, Zhenting Du, Weibang Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ShanghaiTech University, King's College London"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21534",children:"https://arxiv.org/pdf/2512.21534"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) mechanism for variable stiffness., 2. Demonstrates that the helical configuration provides exponentially greater stiffness adjustment and a reduced footprint compared to conventional planar designs., 3. Develops and validates a functional robotic finger prototype that confirms the feasibility of voltage-driven stiffness modulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel variable stiffness mechanism for robotics called Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ). It uses a helical configuration and electrostatic attraction to achieve tunable stiffness, offering superior performance and a smaller footprint than traditional planar designs. The work is validated through experiments and a functional robotic finger prototype, confirming its feasibility."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Robotic joints need variable stiffness for adaptability and safety.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Proposes HWS-ELJ, using helical winding and electrostatic attraction for tunable stiffness.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: HWS-ELJ shows superior stiffness enhancement and smaller footprint; a functional robotic finger prototype validates feasibility.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] World-Coordinate Human Motion Retargeting via SAM 3D Body"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion capture and retargeting], [SAM 3D Body, Momentum HumanRig, contact-aware optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhangzheng Tum, Kailun Su, Shaolong Zhu, Yukun Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian University of Technology, Shenzhen University, Harbin Institute of Technology, Shenzhen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21573",children:"https://arxiv.org/pdf/2512.21573"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight framework using a frozen SAM 3D Body backbone and Momentum HumanRig representation for world-coordinate human motion recovery from monocular video. 2. Introduces temporal consistency enforcement via identity/scale locking and efficient sliding-window smoothing in a low-dimensional latent space. 3. Recovers physically plausible global root trajectories using a differentiable soft foot-ground contact model and contact-aware optimization for reliable robot retargeting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a lightweight framework for recovering world-coordinate human motion from monocular video and retargeting it to a humanoid robot. The method leverages SAM 3D Body as a frozen backbone, enforces temporal consistency, smooths predictions, and uses a contact model for plausible global trajectories. Results show the method produces stable, robot-ready motion from monocular input."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[World-Coordinate Human Motion Retargeting via SAM 3D Body] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u4ece\u5355\u76ee\u89c6\u9891\u6062\u590d\u4e16\u754c\u5750\u6807\u7cfb\u4eba\u4f53\u8fd0\u52a8\u5e76\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684SAM 3D Body\uff0cMHR\u8868\u793a\uff0c\u65f6\u5e8f\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6ed1\u52a8\u7a97\u53e3\u5e73\u6ed1\uff0c\u63a5\u89e6\u611f\u77e5\u5168\u5c40\u4f18\u5316]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u7a33\u5b9a\u7684\u4e16\u754c\u8f68\u8ff9\uff0c\u53ef\u9760\u7684\u673a\u5668\u4eba\u91cd\u5b9a\u5411\uff0c\u4ece\u5355\u76ee\u8f93\u5165\u751f\u6210\u673a\u5668\u4eba\u5c31\u7eea\u7684\u8fd0\u52a8]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21618",children:"https://arxiv.org/pdf/2512.21618"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f\u4ea4\u901a\u7f16\u8f91 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u5bf9\u79f0\u81ea\u56de\u5f52\u5728\u7ebf\u4fee\u590d\u8303\u5f0f\u4e0e\u514d\u8bad\u7ec3\u534f\u8c03\u673a\u5236 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u65b0\u89c6\u89d2\u589e\u5f3a\u548c3D\u8f66\u8f86\u63d2\u5165\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AstraNav-Memory: Contexts Compression for Long Memory"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [visual context compression, image-centric memory, lifelong embodied navigation, DINOv3, Qwen2.5-VL]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Botao Ren, Junjun Hu, Xinda Xue, Minghua Luo, Jintao Chen, Haochen Bai, Liangliang You, Mu Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Alibaba Group (Amap), Tsinghua University, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21627",children:"https://arxiv.org/pdf/2512.21627"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://astra-amap.github.io/AstraNav-Memory.github.io/",children:"https://astra-amap.github.io/AstraNav-Memory.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed an image-centric memory framework for lifelong embodied navigation that uses an efficient visual context compression module to achieve long-term implicit memory. 2. Introduced a configurable visual tokenizer built on a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, enabling high compression rates (e.g., 16x) to expand context capacity. 3. Demonstrated state-of-the-art navigation performance on benchmarks (GOAT-Bench, HM3D-OVON), showing improved exploration in unfamiliar environments and shorter paths in familiar ones, with ablation studies validating the efficiency-accuracy trade-off."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of building long-term memory for lifelong embodied navigation. It proposes AstraNav-Memory, an image-centric framework that compresses visual contexts using a configurable tokenizer to efficiently store hundreds of images, coupled with a Qwen2.5-VL-based navigation policy. The method achieves state-of-the-art performance, balancing efficiency and accuracy for scalable lifelong agents."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AstraNav-Memory: Contexts Compression for Long Memory] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Lifelong embodied navigation requires efficient long-term spatial-semantic memory. Object-centric methods are limited by robustness and scalability.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose an image-centric memory framework with a visual context compression module (ViT+DINOv3+PixelUnshuffle) coupled with a Qwen2.5-VL navigation policy.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA on GOAT-Bench and HM3D-OVON. Improves exploration in unfamiliar environments and shortens paths in familiar ones. Moderate compression offers best balance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kyungpook National University, Kumoh National Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21654",children:"https://arxiv.org/pdf/2512.21654"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. \u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55/Traditional methods struggle to scale.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: A structure-induced ACO framework. \u5229\u7528\u7ed3\u6784\u5148\u9a8c\u3001\u8d1f\u8f7d\u611f\u77e5\u76ee\u6807\u548c\u91cd\u53e0\u6291\u5236/Uses structural prior, load-aware objective, and overlap suppression.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Improves route compactness, stability, and workload distribution. \u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6846\u67b6/Provides a scalable framework.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [socially compliant navigation], [meta-cognitive prompt, vision language model, multi-action generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zishuo Wang, Xinyu Zhang, Zhuonan Liu, Tomohito Kawabata, Daeun Song, Xuesu Xiao, Ling Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hokkaido University, George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21722",children:"https://arxiv.org/pdf/2512.21722"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MAction-SocialNav, a vision language model for socially compliant navigation that generates multiple plausible actions to handle real-world ambiguity. 2. Introduces a novel meta-cognitive prompt (MCP) method to enhance the model's reasoning capability. 3. Curates a new multi-action socially compliant navigation dataset with diverse conditions and dual human annotations, and designs five evaluation metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of action ambiguity in socially compliant robot navigation by proposing MAction-SocialNav, an efficient vision language model that generates multiple plausible actions per scenario using a novel meta-cognitive prompt tuning method. The method is evaluated on a newly curated dataset and shows superior decision quality, safety alignment, and real-time efficiency compared to large language models like GPT-4o and Claude."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Social norms are ambiguous; multiple actions may be acceptable, but existing methods assume a single correct action."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes an efficient VLM with a novel meta-cognitive prompt (MCP) to generate multiple plausible actions."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves higher decision quality and safety than GPT-4o/Claude while maintaining real-time efficiency."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," MIRAI, Cognitive AI Systems Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21723",children:"https://arxiv.org/pdf/2512.21723"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21853",children:"https://arxiv.org/pdf/2512.21853"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MoonBot: \u9762\u5411\u6708\u7403\u57fa\u5730\u5efa\u8bbe\u7684\u6a21\u5757\u5316\u6309\u9700\u53ef\u91cd\u6784\u673a\u5668\u4eba] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6708\u7403\u63a2\u7d22\u4e0e\u57fa\u5730\u5efa\u8bbe\u9700\u6c42 / Lunar Exploration & Base Construction Needs]\n    C --\x3e C1[\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf / Modular & Reconfigurable Robotic System]\n    C --\x3e C2[\u6982\u5ff5\u9a8c\u8bc1\u4e0e\u73b0\u573a\u6f14\u793a / Proof-of-Concept & Field Demonstration]\n    D --\x3e D1[\u6210\u529f\u6267\u884c\u6a21\u62df\u4efb\u52a1 / Successfully Executed Simulated Tasks]\n    D --\x3e D2[\u603b\u7ed3\u4e86\u8fde\u63a5\u5668\u8bbe\u8ba1\u7b49\u7ecf\u9a8c\u6559\u8bad / Summarized Lessons (e.g., Connector Design)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [space robotics], [trajectory planning, nonlinear optimization, dynamic keep-out sphere, ON/OFF thrusters, rendezvous and docking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kenta Iizuka, Akiyoshi Uchida, Kentaro Uno, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21882",children:"https://arxiv.org/pdf/2512.21882"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A trajectory planning method based on nonlinear optimization for close-range rendezvous with a tumbling target. 2. The introduction of a dynamic keep-out sphere that adapts to approach conditions for safer access. 3. A control strategy to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of safely approaching a tumbling space debris object for capture. It proposes a trajectory planning method using nonlinear optimization with a dynamic safety boundary and a control strategy for ON/OFF thrusters. The method enables closer and safer access as a preliminary step for robotic capture."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u5b89\u5168\u63a5\u8fd1\u7ffb\u6eda\u76ee\u6807/Safely approaching a tumbling target")\n    Method --\x3e M1("\u975e\u7ebf\u6027\u4f18\u5316\u8f68\u8ff9\u89c4\u5212/Nonlinear optimization-based trajectory planning")\n    Method --\x3e M2("\u52a8\u6001\u7981\u5165\u7403\u4f53/Dynamic keep-out sphere")\n    Method --\x3e M3("ON/OFF\u63a8\u8fdb\u5668\u63a7\u5236/ON/OFF thruster control strategy")\n    Results --\x3e R1("\u66f4\u8fd1\u66f4\u5b89\u5168\u7684\u8bbf\u95ee/Closer and safer access")\n    Results --\x3e R2("\u8003\u8651\u5b9e\u9645\u7ea6\u675f/Practical implementation considered")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics, dynamics and control], [inertia parameter estimation, online identification, momentum conservation, floating-base robots, space robotics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Akiyoshi Uchida, Antonine Richard, Kentaro Uno, Miguel Olivares-Mendez, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University, University of Luxembourg"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21886",children:"https://arxiv.org/pdf/2512.21886"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extended an existing online inertia identification method to be applicable to floating-base robots by incorporating momentum conservation. 2. Validated the proposed method through numerical simulations for space applications like on-orbit servicing. 3. Demonstrated accurate estimation of unknown object inertia parameters during manipulation in a simulated microgravity environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of estimating the inertia parameters of an unknown object grasped by a manipulator on a free-floating space robot. The authors extend an existing online identification method by incorporating momentum conservation to handle the floating base. Numerical simulations validate the method, showing accurate identification and highlighting its potential for on-orbit servicing missions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nRoot(Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\nRoot --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\nRoot --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\nProblem --\x3e P1(\u4f30\u8ba1\u672a\u77e5\u88ab\u6293\u53d6\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570/Estimate inertia parameters of unknown grasped object)\nProblem --\x3e P2(\u9762\u5411\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u7684\u7a7a\u95f4\u673a\u5668\u4eba/For floating-base space robots)\nMethod --\x3e M1(\u5e94\u7528\u5e76\u6269\u5c55\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5/Apply and extend online identification method)\nMethod --\x3e M2(\u7ed3\u5408\u52a8\u91cf\u5b88\u6052\u5b9a\u5f8b/Incorporate momentum conservation)\nResults --\x3e R1(\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1/Validated via numerical simulation)\nResults --\x3e R2(\u53c2\u6570\u8bc6\u522b\u51c6\u786e/Accurate parameter identification)\nResults --\x3e R3(\u9002\u7528\u4e8e\u5728\u8f68\u670d\u52a1/Applicable to on-orbit servicing)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21887",children:"https://arxiv.org/pdf/2512.21887"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[UAV\u5bfc\u822a\u7f3a\u4e4f\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u80fd\u529b/UAV navigation lacks high-level semantic planning]\n    B --\x3e B2[\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u7406\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u4e0e\u957f\u8ddd\u79bb\u89c6\u89c9\u751f\u6210/Existing models struggle with complex action space & long-horizon visual generation]\n    C --\x3e C1[\u63d0\u51faANWM\u4e16\u754c\u6a21\u578b/Propose ANWM world model]\n    C --\x3e C2[\u5f15\u5165\u672a\u6765\u5e27\u6295\u5f71\u6a21\u5757/Introduce Future Frame Projection module]\n    D --\x3e D1[\u957f\u8ddd\u79bb\u89c6\u89c9\u9884\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347/Significantly outperforms in long-distance visual forecasting]\n    D --\x3e D2[\u63d0\u9ad8\u5927\u89c4\u6a21\u73af\u5883\u5bfc\u822a\u6210\u529f\u7387/Improves UAV navigation success rates in large-scale environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chaoqi Liu, Haonan Chen, Sigmund H. H\xf8eg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21898",children:"https://arxiv.org/pdf/2512.21898"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Flexible Multitask Learning with Factorized Diffusion Policy] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u673a\u5668\u4eba\u591a\u4efb\u52a1\u5b66\u4e60/Robot Multitask Learning]\n    B1 --\x3e B2[\u52a8\u4f5c\u5206\u5e03\u590d\u6742\u591a\u6a21\u6001/Action Distribution Highly Multimodal]\n    B2 --\x3e B3[\u5355\u4f53\u6a21\u578b\u6b20\u62df\u5408\u4e0e\u4e0d\u7075\u6d3b/Monolithic Models Underfit & Inflexible]\n    C --\x3e C1[\u56e0\u5b50\u5316\u6269\u6563\u7b56\u7565/Factorized Diffusion Policy (FDP)]\n    C1 --\x3e C2[\u6a21\u5757\u5316\u6269\u6563\u4e13\u5bb6/Modular Diffusion Experts]\n    C2 --\x3e C3[\u57fa\u4e8e\u89c2\u5bdf\u7684\u8def\u7531\u5668/Observation-Conditioned Router]\n    C3 --\x3e C4[\u8fde\u7eed\u5206\u6570\u805a\u5408/Continuous Score Aggregation]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf/Outperforms Baselines]\n    D1 --\x3e D2[\u4eff\u771f\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1/Simulation & Real-World Validation]\n    D2 --\x3e D3[\u652f\u6301\u7075\u6d3b\u7b56\u7565\u9002\u5e94/Enables Flexible Policy Adaptation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic vision], [stereo vision, vision-language-action models, geometric-semantic fusion, depth estimation, robotic manipulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, The University of Hong Kong, Institute of Automation, Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21970",children:"https://arxiv.org/pdf/2512.21970"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://shengliangd.github.io/StereoVLA-Webpage",children:"https://shengliangd.github.io/StereoVLA-Webpage"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed StereoVLA, a novel Vision-Language-Action model that leverages stereo vision for enhanced spatial perception. 2. Introduced a Geometric-Semantic Feature Extraction module to fuse geometric cues from stereo differences with semantic features from a monocular view. 3. Designed an auxiliary Interaction-Region Depth Estimation task to improve spatial understanding and accelerate model convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of single-view input in Vision-Language-Action (VLA) models for robotic manipulation by introducing StereoVLA, which utilizes stereo vision. The core method involves a novel module to extract and fuse geometric and semantic features, along with an auxiliary depth estimation task. Experiments show the model significantly outperforms baselines in stereo-based tasks and is robust to camera pose variations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u5355\u76eeVLA\u6a21\u578b\u7f3a\u4e4f\u7cbe\u786e\u7684\u51e0\u4f55\u611f\u77e5/Single-view VLAs lack accurate geometry perception)\n    C --\x3e C1(\u63d0\u51faStereoVLA\u6a21\u578b/Propose StereoVLA model)\n    C1 --\x3e C2(\u51e0\u4f55-\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6a21\u5757/Geometric-Semantic Feature Extraction)\n    C2 --\x3e C3(\u4ece\u7acb\u4f53\u89c6\u56fe\u63d0\u53d6\u51e0\u4f55\u7279\u5f81/Extract geometric features from stereo views)\n    C2 --\x3e C4(\u4ece\u5355\u76ee\u89c6\u56fe\u63d0\u53d6\u8bed\u4e49\u7279\u5f81/Extract semantic features from monocular view)\n    C1 --\x3e C5(\u8f85\u52a9\u4ea4\u4e92\u533a\u57df\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1/Auxiliary Interaction-Region Depth Estimation task)\n    D --\x3e D1(\u5728\u7acb\u4f53\u8bbe\u7f6e\u4e0b\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf/Large margin outperforms baselines under stereo setting)\n    D --\x3e D2(\u5bf9\u76f8\u673a\u4f4d\u59ff\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027/Strong robustness to camera pose variations)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical robotics], [robotic intubation, closed-loop control, shape sensing, depth estimation, human-in-the-loop]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Saksham Gupta, Sarthak Mishra, Arshad Ayub, Kamran Farooque, Spandan Roy, Babita Gupta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," International Institute of Information Technology Hyderabad (IIIT-H), All India Institute of Medical Sciences, New Delhi (AIIMS)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21983",children:"https://arxiv.org/pdf/2512.21983"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A compact, integrated robotic platform (BRIS) for fiberoptic-guided intubation, featuring a steerable bronchoscope, an independent tube advancement mechanism, and a camera-augmented mouthpiece. 2. A learning-enabled closed-loop control framework that uses real-time shape sensing to map joystick inputs to precise bronchoscope tip motion, ensuring stable teleoperation despite tendon nonlinearities. 3. The use of monocular endoscopic depth estimation to classify airway regions and provide anatomy-aware guidance for safe endotracheal tube positioning relative to the carina."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the Bab Sak Robotic Intubation System (BRIS), a human-in-the-loop platform designed to assist with safe fiberoptic endotracheal intubation. It integrates a learning-enabled control framework for stable scope navigation and uses monocular depth estimation for anatomy-aware tube placement guidance. The system was validated on airway mannequins, demonstrating reliable performance as a step toward safer and more consistent robotic airway management."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[BRIS: \u5b89\u5168\u5149\u7ea4\u63d2\u7ba1\u7cfb\u7edf / BRIS: Safe Fiberoptic Intubation System] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u63d2\u7ba1\u6280\u672f\u96be\u5ea6\u9ad8\uff0c\u73b0\u6709\u7cfb\u7edf\u4e0d\u5b8c\u5584 / Problem: Intubation is difficult, existing systems are limited]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u96c6\u6210\u673a\u5668\u4eba\u5e73\u53f0\u4e0e\u5b66\u4e60\u63a7\u5236\u6846\u67b6 / Method: Integrated robotic platform with learning-enabled control]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4eba\u4f53\u6a21\u578b\u4e0a\u9a8c\u8bc1\u53ef\u9760 / Results: Validated as reliable on mannequins]\n    B --\x3e B1[\u5e76\u53d1\u75c7\u98ce\u9669\u9ad8 / High risk of complications]\n    B --\x3e B2[\u7f3a\u4e4f\u96c6\u6210\u7ba1\u63a8\u8fdb\u4e0e\u6df1\u5ea6\u9a8c\u8bc1 / Lacks integrated tube control & depth verification]\n    C --\x3e C1[\u95ed\u73af\u63a7\u5236\u4e0e\u5f62\u72b6\u611f\u77e5 / Closed-loop control & shape sensing]\n    C --\x3e C2[\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5f15\u5bfc / Monocular depth estimation guidance]\n    D --\x3e D1[\u53ef\u9760\u5bfc\u822a / Reliable navigation]\n    D --\x3e D2[\u53ef\u63a7\u7f6e\u7ba1 / Controlled tube placement]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [FPGA, HLS, Point Cloud, Model Compression, Fixed-Point]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22139",children:"https://arxiv.org/pdf/2512.22139"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/dll-ncai/HLS4PC",children:"https://github.com/dll-ncai/HLS4PC"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[GPU under-utilization due to sparse, unstructured point cloud data]\n    P1 --\x3e P2[High memory/computation demand hinders real-time performance]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Parameterizable HLS framework for FPGA]\n    M1 --\x3e M2[Hardware-aware compression: URS, quantization, pruning, fusion]\n    M2 --\x3e M3[Creates PointMLP-Lite model]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]\n    R1 --\x3e R2[3.56x higher throughput vs. prior work]\n    R2 --\x3e R3[2.3x (GPU) and 22x (CPU) higher throughput]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Meta-A3C, UAV-UGV deployment, trajectory planning, road graph, Markov Decision Process]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ndagijimana Cyprien, Mehdi Sookhak, Hosein Zarini, Chandra N Sekharan, Mohammed Atiquzzaman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Texas A&M University-Corpus Christi, University of Oklahoma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22187",children:"https://arxiv.org/pdf/2512.22187"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a joint UAV-UGV positioning and trajectory planning framework to guarantee optimal QoS for ground users in disaster scenarios. 2. Introduces a road graph model to constrain and direct UGV mobility according to real-world road network constraints. 3. Formulates the problem as an MDP and develops a novel Meta-A3C algorithm for rapid adaptation to new environments and dynamic conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c161f6205934be4fb52432c71ffd2a960c50e866e6e382c489af38b90ca39b44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c161f6205934be4fb52432c71ffd2a960c50e866e6e382c489af38b90ca39b44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of ensuring good QoS with minimal UAVs in disaster recovery by proposing a joint UAV-UGV deployment framework. The core method involves modeling UGV mobility with a road graph and solving the optimization problem using a novel Meta-A3C reinforcement learning algorithm. The results show that Meta-A3C outperforms baseline methods, achieving higher throughput and faster execution while meeting QoS requirements."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Ensuring QoS with minimal UAVs in disaster areas)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Road graph for UGV mobility & Meta-A3C for joint optimization)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: 13.1% higher throughput, 49% faster execution vs. baselines)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tasha Pais, Nikhilesh Belulkar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22220",children:"https://arxiv.org/pdf/2512.22220"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("On Extending Semantic Abstraction for Efficient Search of Hidden Objects") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Localizing hidden/occluded objects")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Use VLM relevancy maps & historical data for efficient 3D search")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Faster and accurate 3D localization vs. random search")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22263",children:"https://arxiv.org/pdf/2512.22263"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Evaluating an Adaptive Multispectral Turret System] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[RGB\u5728\u4f4e\u5149\u4e0b\u8868\u73b0\u5dee/RGB struggles in low-light]\n    Problem --\x3e P2[\u70ed\u6210\u50cf\u7f3a\u4e4f\u989c\u8272\u7eb9\u7406/Thermal lacks color & texture]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u81ea\u9002\u5e94RGB-LWIR\u878d\u5408\u6846\u67b6/Adaptive RGB-LWIR fusion framework]\n    Method --\x3e M2[\u8bad\u7ec333\u4e2aYOLO\u6a21\u578b/Trained 33 YOLO models]\n    Method --\x3e M3[11\u79cd\u878d\u5408\u6bd4\u4f8b/11 fusion ratios]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5168\u5149\u6a21\u578b: 92.8%\u7f6e\u4fe1\u5ea6/Full-light model: 92.8% confidence]\n    Results --\x3e R2[\u5fae\u5149\u6a21\u578b: 92.0%\u7f6e\u4fe1\u5ea6/Dim-light model: 92.0% confidence]\n    Results --\x3e R3[\u65e0\u5149\u6a21\u578b: 71.0%\u7f6e\u4fe1\u5ea6/No-light model: 71.0% confidence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied navigation], [interactive navigation, active dialog, benchmark dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wensi Huang, Shaohao Zhu, Meng Wei, Jinming Xu, Xihui Liu, Hanqing Wang, Tai Wang, Feng Zhao, Jiangmiao Pang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, University of Science and Technology of China, Zhejiang University, The University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22342",children:"https://arxiv.org/pdf/2512.22342"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://0309hws.github.io/VL-LN.github.io/",children:"https://0309hws.github.io/VL-LN.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Interactive Instance Object Navigation (IION) task, which requires agents to navigate and resolve ambiguity through active dialog. 2. Introduces the VL-LN benchmark, a large-scale, automatically generated dataset with over 41k dialog-augmented trajectories for training and evaluation. 3. Demonstrates that a navigation model trained on VL-LN achieves significant improvements over baselines, validating the benchmark's effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/963768d906e1031332e12b4d315a28062a7236f00bbf7e3b88c636a5d3422ff7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/963768d906e1031332e12b4d315a28062a7236f00bbf7e3b88c636a5d3422ff7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap in embodied navigation where real-world instructions are often vague. It proposes a new task (IION) and a large-scale benchmark (VL-LN) for training and evaluating agents that can navigate and ask clarifying questions. The results show that models trained with this dialog-enabled approach significantly outperform baseline methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u5b9e\u5bfc\u822a\u6307\u4ee4\u6a21\u7cca/Real-world navigation instructions are vague]\n    C --\x3e C1[\u63d0\u51faIION\u4efb\u52a1\u4e0eVL-LN\u57fa\u51c6/Proposes IION task & VL-LN benchmark]\n    C1 --\x3e C2[\u5305\u542b\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u6570\u636e\u96c6/Includes large-scale auto-generated dataset]\n    D --\x3e D1[\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347/Model achieves significant improvements]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [Heterogeneous Computing, ROS 2, FreeRTOS, PID Control, AWS IoT]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Mansoura University, Egypt"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22408",children:"https://arxiv.org/pdf/2512.22408"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Limitations of human-operated last-mile delivery (cost, safety, reliability)"] --\x3e P1["\u5b50\u95ee\u9898/Sub-Problem<br>Need for autonomous, cost-efficient delivery robot"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Unified multi-disciplinary approach"] --\x3e M1["\u5f02\u6784\u8ba1\u7b97/Heterogeneous Computing<br>RPi 5 (ROS 2) for AI & ESP32 (FreeRTOS) for control"]\n    Method --\x3e M2["\u5173\u952e\u6280\u672f/Key Tech<br>Low-latency comms, PID control, AWS IoT, failsafe"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Robust, operational autonomous delivery system"] --\x3e R1["\u6210\u679c/Outcome<br>Deterministic motor control & enhanced reliability"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Emergence of Human to Robot Transfer in Vision-Language-Action Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [vision-language-action models, human-to-robot transfer, co-training, emergent capability, embodiment-agnostic representations]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Physical Intelligence, Georgia Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22414",children:"https://arxiv.org/pdf/2512.22414"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Emergence of Human to Robot Transfer in Vision-Language-Action Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Can VLA models learn from human videos for robot control?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Simple co-training recipe on human & robot data]\n    D[\u5173\u952e\u7ed3\u679c/Results: Transfer emerges with scale; performance nearly doubles]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [swarm robotics], [collective motion, visual perception, fault-tolerance, intermittent locomotion, distance estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Peleg Shefi, Amir Ayali, Gal A. Kaminka"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bar Ilan University, Tel Aviv University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22448",children:"https://arxiv.org/pdf/2512.22448"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A robust distance estimation method for vision-based swarms that combines perceived horizontal and vertical sizes of neighbors. 2. The introduction of intermittent locomotion as a mechanism for reliably detecting faulty peers that disrupt swarm motion. 3. A fault-avoidance strategy that is robust to errors in classifying robots as faulty, improving swarm resilience in both Avoid-Attract and Alignment-based models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456a9beab88e762dd3d3dfcafbb4d0007f77b2154cd9a7b657dcdf5df21407db_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456a9beab88e762dd3d3dfcafbb4d0007f77b2154cd9a7b657dcdf5df21407db_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the brittleness of artificial swarms using vision by proposing two bio-inspired mechanisms. It introduces a robust visual distance estimation method and an intermittent locomotion strategy for fault detection and avoidance. Extensive simulations show these techniques dramatically improve swarm resilience across different collective motion models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Artificial swarms are brittle with vision sensing due to ambiguities and information loss.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: 1. Robust visual distance estimation. 2. Intermittent locomotion for fault detection.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Dramatic improvement in swarm resilience across Avoid-Attract and Alignment models.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [motion generation and editing], [residual vector quantization (RVQ), pose code, transformer, text-to-motion, motion editing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sukhyun Jeong, Yong-Hoon Choi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kwangwoon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22464",children:"https://arxiv.org/pdf/2512.22464"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/jayze3736/PGR2M",children:"https://github.com/jayze3736/PGR2M"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid motion representation (PGR\xb2M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR\xb2M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Pose-code frameworks struggle to capture subtle temporal dynamics and high-frequency details.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid representation (PGR\xb2M) with pose codes and residual codes (RVQ), using a two-stage Transformer.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Improves FID and reconstruction metrics; enables intuitive, structure-preserving edits.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Asymmetric Friction in Geometric Locomotion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [geometric mechanics, robotics], [Finsler metrics, sub-Finslerian, motility map, asymmetric friction, geometric locomotion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ross L. Hatton, Yousef Salaman, Shai Revzen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Oregon State University (inferred from author Ross L. Hatton's affiliation)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22484",children:"https://arxiv.org/pdf/2512.22484"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extends the geometric locomotion framework from systems with symmetric (Riemannian) friction to systems with asymmetric (Finsler) friction. 2. Demonstrates that the sub-Riemannian construction of the motility map naturally generalizes to a sub-Finslerian approach. 3. Identifies system properties analogous to constraint curvature that characterize motion capabilities under asymmetric friction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7325c56bd6ecd61b2e5288756589f2b41cc9256cdcdb37b465ecd8b058f30c3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7325c56bd6ecd61b2e5288756589f2b41cc9256cdcdb37b465ecd8b058f30c3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper extends geometric models of locomotion to systems with asymmetric friction, where drag coefficients differ for forward and backward motion. The authors propose a sub-Finslerian framework to construct the system's motility map, generalizing the traditional sub-Riemannian approach. The main conclusion is that this new framework allows for the characterization of locomotion capabilities in a broader class of systems with non-reciprocal environmental interactions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Asymmetric Friction in Geometric Locomotion") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Standard geometric locomotion models assume symmetric friction (Riemannian metrics)")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Generalize framework to asymmetric friction using Finsler metrics and a sub-Finslerian approach")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Identifies properties analogous to constraint curvature to characterize system motion capabilities")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [computer-aided manufacturing (CAM)], [spiral toolpath planning, scalar field optimization, topology-preserving deformation, conformal slit mapping, boundary-conforming]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shen Changqing, Xu Bingzhou, Qi Bosong, Zhang Xiaojian, Yan Sijie, Ding Han"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22502",children:"https://arxiv.org/pdf/2512.22502"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a strategy to enforce boundary conformity and eliminate zero-gradient singularities in scalar-field-based toolpath optimization for multiply connected surfaces. 2. Reformulates the optimization as a topology-preserving mesh deformation with boundary-synchronous updates to achieve globally optimized spacing and smooth transitions. 3. Demonstrates significant improvements in machining efficiency, scallop-height uniformity, and vibration reduction compared to a state-of-the-art method."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8db268a37d6b76e48aa2571b6519b562d7e55bfc3d4e8f7c07120bc4dbfc1315_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8db268a37d6b76e48aa2571b6519b562d7e55bfc3d4e8f7c07120bc4dbfc1315_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating continuous, boundary-conforming spiral toolpaths for ball-end milling on complex freeform surfaces. The proposed method uses conformal slit mapping to create an initial singularity-free scalar field and then optimizes it via a topology-preserving mesh deformation process. Experimental results show the approach increases machining efficiency by over 14%, improves surface finish uniformity, and reduces vibrations compared to existing methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6311\u6218: \u4fdd\u6301\u8fb9\u754c\u4e00\u81f4\u6027\u5e76\u6d88\u9664\u5947\u70b9/Challenge: Maintain boundary conformity & eliminate singularities]\n    C --\x3e C1[\u4f7f\u7528\u5171\u5f62\u72ed\u7f1d\u6620\u5c04\u521d\u59cb\u5316/Use conformal slit mapping for initialization]\n    C --\x3e C2[\u62d3\u6251\u4fdd\u6301\u7f51\u683c\u53d8\u5f62\u4f18\u5316/Topology-preserving mesh deformation optimization]\n    D --\x3e D1[\u6548\u7387\u63d0\u5347 14.24%/Efficiency improved by 14.24%]\n    D --\x3e D2[\u5747\u5300\u6027\u63d0\u5347 5.70%/Uniformity improved by 5.70%]\n    D --\x3e D3[\u632f\u52a8\u51cf\u5c11 >10%/Vibration reduced by >10%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic manipulation], [Vision-Language-Action (VLA), object-centric grounding, geometric grounding, perception module, clutter robustness]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Khoa Vo, Taisei Hanyu, Yuki Ikebe, Trong Thang Pham, Nhat Chung, Minh Nhat Vu, Duy Nguyen Ho Minh, Anh Nguyen, Anthony Gunderman, Chase Rainwater, Ngan Le"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Arkansas, National University of Singapore, TU Wien, Max Planck Research School for Intelligent Systems / University of Stuttgart, University of Liverpool"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22519",children:"https://arxiv.org/pdf/2512.22519"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://uark-aicv.github.io/OBEYED",children:"https://uark-aicv.github.io/OBEYED"})," VLA"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes OBEYED-VLA, a framework that explicitly disentangles perceptual grounding from action reasoning in VLA models. 2. Introduces a perception module with a VLM-based object-centric grounding stage and a complementary geometric grounding stage to create task-conditioned, object-centric, and geometry-aware observations. 3. Demonstrates substantial real-world robustness improvements in cluttered scenarios, including handling distractors, absent targets, background changes, and unseen objects."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7643cf1feb83697be4818d833bf42eecb6c488ed1ac9917cb58299ddd69c4cfe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7643cf1feb83697be4818d833bf42eecb6c488ed1ac9917cb58299ddd69c4cfe_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that monolithic Vision-Language-Action (VLA) models suffer from poor grounding in cluttered real-world scenes. To solve this, it proposes OBEYED-VLA, which adds an explicit perception module for object-centric and geometric grounding before action prediction. The method significantly improves robustness on a real-world tabletop robot across various challenging clutter and distraction scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Clutter-Resistant VLA Models<br>\u6297\u5e72\u6270VLA\u6a21\u578b] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>VLA\u6a21\u578b\u5728\u6742\u4e71\u573a\u666f\u4e2d<br>\u611f\u77e5\u4e0e\u63a7\u5236\u8026\u5408\uff0c<br>\u5bfc\u81f4\u9519\u8bef\u6267\u884c] --\x3e P1[\u8fc7\u6293\u53d6/Over-grasp]\n    Problem --\x3e P2[\u6613\u53d7\u5e72\u6270/Distracted by clutter]\n    Problem --\x3e P3[\u8fc7\u62df\u5408\u80cc\u666f/Overfit to background]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>OBEYED-VLA\u6846\u67b6] --\x3e M1[\u611f\u77e5\u6a21\u5757/Perception Module]\n    M1 --\x3e M1_1[\u57fa\u4e8eVLM\u7684\u7269\u4f53\u611f\u77e5<br>VLM-based Object Grounding]\n    M1 --\x3e M1_2[\u51e0\u4f55\u611f\u77e5<br>Geometric Grounding]\n    Method --\x3e M2[\u52a8\u4f5c\u7b56\u7565\u5fae\u8c03<br>Fine-tune VLA Policy]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u5728\u771f\u5b9eUR10e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1] --\x3e R1[\u6297\u5e72\u6270\u7269\u4f53\u9c81\u68d2<br>Robust to distractors]\n    Results --\x3e R2[\u62d2\u7edd\u65e0\u76ee\u6807\u4efb\u52a1<br>Absent-target rejection]\n    Results --\x3e R3[\u80cc\u666f\u53d8\u5316\u9c81\u68d2<br>Robust to background changes]\n    Results --\x3e R4[\u64cd\u4f5c\u672a\u89c1\u7269\u4f53<br>Manipulate unseen objects]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai / robot learning], [vision-language-action models, benchmark, generalization, robustness, structured task design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22539",children:"https://arxiv.org/pdf/2512.22539"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL (from abstract)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Difficult to quantitatively understand the limits and failure modes of VLAs]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Structured benchmark with orthogonal difficulty axes (Task Structure, Language, Visual) and systematic perturbations]\n    D[\u5173\u952e\u7ed3\u679c/Results: Revealed critical VLA limitations (memorization, asymmetric robustness); Provided open-source framework for research]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [motion planning], [Euclidean Distance Transform, sampling-based model predictive control, GPU parallelization, collision-free planning, SE(3) pose tracking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xuewei Zhang, Bailing Tian, Kai Zheng, Yulin Hui, Junjie Lu, Zhiyu Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tianjin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22575",children:"https://arxiv.org/pdf/2512.22575"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://zxw610.github.io/ParaMaP",children:"https://zxw610.github.io/ParaMaP"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A parallel framework tightly integrating GPU-based Euclidean Distance Transform mapping with a sampling-based MPC planner for real-time replanning. 2. A robot-masked update mechanism for the distance field to prevent false self-collision detections during online perception. 3. Formulating motion generation as a stochastic optimization with a unified objective and a geometrically consistent SE(3) pose tracking metric for fast, accurate convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc2c02919a284bed96ccdbd82c4e1bdf169e2e161c4adb9bfc19e68ccb0dc21_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc2c02919a284bed96ccdbd82c4e1bdf169e2e161c4adb9bfc19e68ccb0dc21_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ParaMaP, a framework for real-time, collision-free motion planning in unknown environments. It combines a GPU-accelerated distance field mapping system with a parallel sampling-based model predictive control planner. The method is validated through simulations and real-world experiments on a 7-DoF manipulator, demonstrating effective high-frequency replanning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ParaMaP: Parallel Mapping and Collision-free Motion Planning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Real-time collision-free planning in unknown environments with frequent replanning]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Parallel GPU framework integrating EDT-based mapping with SMPC planner]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Validated on 7-DoF manipulator, enables high-frequency replanning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Modeling of UAV Tether Aerodynamics for Real-Time Simulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics simulation], [tether aerodynamics, real-time simulation, catenary theory, lumped mass model, CasADi]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Max Beffert, Andreas Zell"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Cognitive Systems Group, University of T\xfcbingen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22588",children:"https://arxiv.org/pdf/2512.22588"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed an analytical tether model based on catenary theory with uniform drag for fast (<1ms) real-time simulation. 2. Developed a flexible numerical tether model using segment discretization and optimization (CasADi/IPOPT) achieving real-time performance (5ms). 3. Validated both models with real-world experiments, providing a framework for offline optimization and online tasks like control and planning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a5999cc3e2fc5ef1aa99519ee5adb890754d8b494519879e512c601ea3c9b6a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a5999cc3e2fc5ef1aa99519ee5adb890754d8b494519879e512c601ea3c9b6a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of modeling aerodynamic forces on a tether for tethered UAVs to enable continuous operation from a moving base. It proposes two complementary real-time methods: a fast analytical model and a more flexible numerical model. The work concludes that the analytical model is sufficiently accurate for most applications, while the numerical model offers higher physical fidelity when needed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Modeling of UAV Tether Aerodynamics for Real-Time Simulation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u65e0\u4eba\u673a\u7eed\u822a\u77ed\uff0c\u7cfb\u7559\u662f\u89e3\u51b3\u65b9\u6848 / UAV short endurance, tethering is a solution]\n    B --\x3e B2[\u79fb\u52a8\u57fa\u7ad9\u6216\u5f3a\u98ce\u9700\u8981\u6c14\u52a8\u6a21\u578b / Moving base or strong wind requires aerodynamic model]\n    C --\x3e C1[\u89e3\u6790\u65b9\u6cd5\uff1a\u60ac\u94fe\u7ebf\u7406\u8bba\uff0c\u5747\u5300\u963b\u529b / Analytical: Catenary theory, uniform drag]\n    C --\x3e C2[\u6570\u503c\u65b9\u6cd5\uff1a\u5206\u6bb5\u79bb\u6563\uff0c\u96c6\u4e2d\u8d28\u91cf / Numerical: Segmented discretization, lumped masses]\n    D --\x3e D1[\u89e3\u6790\u6cd5\uff1a<1ms\uff0c\u8db3\u591f\u7cbe\u786e / Analytical: <1ms, sufficiently accurate]\n    D --\x3e D2[\u6570\u503c\u6cd5\uff1a5ms\uff0c\u66f4\u9ad8\u7075\u6d3b\u6027 / Numerical: 5ms, higher flexibility]\n    D --\x3e D3[\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8f7b\u91cf\u7ea7\u6846\u67b6 / Experimental validation, lightweight framework]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Sistema de navegaci\xf3n de cobertura para veh\xedculos no holon\xf3micos en ambientes de exterior"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [mobile robotics, coverage path planning], [coverage navigation, non-holonomic vehicle, obstacle recovery, outdoor environments, mining automation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Michelle Valenzuela, Francisco Leiva, Javier Ruiz-del-Solar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Advanced Mining Technology Center, Universidad de Chile"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22734",children:"https://arxiv.org/pdf/2512.22734"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Development of a coverage navigation system for non-holonomic robots in outdoor environments. 2. Incorporation of recovery behaviors to handle dynamic or unmapped obstacles, ensuring complete coverage. 3. Validation of the system in both simulated and real outdoor settings, achieving near 90% coverage."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d51f00a4abaf62f479ca3c4a4186de6c2ffb93b0fee4c3e43193c92bebff4d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d51f00a4abaf62f479ca3c4a4186de6c2ffb93b0fee4c3e43193c92bebff4d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a coverage navigation system for non-holonomic vehicles, designed for outdoor tasks like mining. The system plans routes to cover an area and includes recovery behaviors to handle unexpected obstacles. It was tested in simulation and real-world environments, achieving near 90% coverage, with plans to scale up to industrial mining vehicles."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Sistema de navegaci\xf3n de cobertura para veh\xedculos no holon\xf3micos / Coverage Navigation System for Non-Holonomic Vehicles"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Automating coverage tasks (e.g., cleaning, material handling) in mining for improved safety."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: Route calculation with recovery behaviors for dynamic/unmapped obstacles."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: ~90% coverage achieved in simulated/real outdoor tests; scaling to mining vehicles planned."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zheng Qiu, Chih-Yuan Chiu, Glen Chou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22757",children:"https://arxiv.org/pdf/2512.22757"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Active Constraint Learning in High Dimensions from Demonstrations") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Data-inefficient constraint inference from demonstrations")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Iterative active learning with Gaussian Processes")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Outperforms baseline with sparse, informative demonstrations")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sys], [distributed computing], [autonomous mobile robots, Look-Compute-Move (LCM), computational power hierarchy, finite-state robots, robots with lights]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Naoki Kitamura, Yuichi Sudo, Koichi Wada"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Osaka, Hosei University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22770",children:"https://arxiv.org/pdf/2512.22770"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proves that under full synchrony, the FSTA (finite-state) and LUMI (robots with lights) models coincide for two robots, showing perfect synchrony can substitute for memory and communication at this minimal scale. 2. Shows that the FSTA and FCOM (finite-communication) models are orthogonal (bidirectionally incomparable), completing the landscape of incomparability. 3. Provides the first complete and exact characterization of the computational power hierarchy for two robots across all major models and schedulers using a novel simulation-free method."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides the first complete characterization of the computational power of two autonomous mobile robots across major models (OBLOT, FSTA, FCOM, LUMI) and schedulers. Using a novel simulation-free method, it reveals a landscape distinct from the general n-robot case, showing that perfect synchrony can substitute for memory and communication for two robots, and that FSTA and FCOM are orthogonal. This yields the first exact computational hierarchy for minimal robot systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Two-Robot Computational Landscape] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Two-robot computational hierarchy unresolved]\n    C --\x3e C1[Simulation-free analysis method]\n    D --\x3e D1[FSTA^F = LUMI^F under full sync]\n    D --\x3e D2[FSTA and FCOM are orthogonal]\n    D --\x3e D3[Complete landscape for two robots]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:'[arXiv251230] The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al'})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [embodied cognition, robotics], [morphological computation, embodiment, information theory, passive dynamic walker]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Matej Hoffmann"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Czech Technical University in Prague"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22868",children:"https://arxiv.org/pdf/2512.22868"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of "morphological computation". 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of "morphological computation" is merely a metaphor and that the body\'s main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root[The body is not there to compute<br>\u8eab\u4f53\u4e0d\u662f\u4e3a\u4e86\u8ba1\u7b97] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Is the body\'s primary role computational?<br>\u8eab\u4f53\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u8ba1\u7b97\u5417\uff1f]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Conceptual critique of "morphological computation"<br>\u5bf9"\u5f62\u6001\u8ba1\u7b97"\u7684\u6982\u5ff5\u6027\u6279\u5224]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results<br>Body\'s role is physical, not computational<br>\u8eab\u4f53\u7684\u4f5c\u7528\u662f\u7269\u7406\u7684\uff0c\u800c\u975e\u8ba1\u7b97\u7684]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied navigation], [socially compliant navigation, multimodal dataset, chain-of-thought, vision language models, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hokkaido University, George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22867",children:"https://arxiv.org/pdf/2512.22867"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[MUSON: \u9762\u5411\u63a8\u7406\u7684\u591a\u6a21\u6001\u57ce\u5e02\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u6570\u636e\u96c6] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u76d1\u7763/Lack explicit reasoning supervision]\n    Problem --\x3e P2[\u52a8\u4f5c\u5206\u5e03\u9ad8\u5ea6\u957f\u5c3e/Highly long-tailed action distribution]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f15\u5165MUSON\u6570\u636e\u96c6/Introduce MUSON dataset]\n    M1 --\x3e M1_Sub1[\u4e94\u6b65\u601d\u7ef4\u94fe\u6807\u6ce8/Five-step Chain-of-Thought annotation]\n    M1 --\x3e M1_Sub2[\u5e73\u8861\u7684\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4/Balanced discrete action space]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Qwen2.5-VL-3B\u53d6\u5f97\u6700\u9ad8\u7cbe\u5ea6/Qwen2.5-VL-3B achieves highest accuracy]\n    Results --\x3e R2[\u6570\u636e\u96c6\u4f5c\u4e3a\u6709\u6548\u57fa\u51c6/Dataset serves as effective benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics], [inverse kinematics, parallel mechanisms, FABRIK, workspace robustness, topological decomposition]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Daqian Cao, Quan Yuan, Weibang Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ShanghaiTech University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22927",children:"https://arxiv.org/pdf/2512.22927"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes P-FABRIK, a novel inverse kinematics method for parallel mechanisms based on the FABRIK algorithm. 2. Introduces a new topological decomposition strategy to break down parallel mechanisms into serial sub-chains for iterative solution. 3. Demonstrates the method's generality, computational efficiency, and robustness in handling targets outside the workspace."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b554beb83a556bb3fea7bd9931b48b310df4fe75825b63e9466dd5c70a26a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b554beb83a556bb3fea7bd9931b48b310df4fe75825b63e9466dd5c70a26a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes P-FABRIK, a general and robust inverse kinematics method for parallel mechanisms. It adapts the FABRIK algorithm by decomposing the mechanism into serial sub-chains and iteratively revising end targets. The method is shown to be effective, efficient, and capable of handling out-of-workspace targets for various parallel mechanisms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u51e0\u4f55\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7ea6\u675f/Traditional geometric methods rely on specific constraints]\n    B --\x3e B2[\u5197\u4f59\u5e76\u8054\u673a\u6784\u7ea6\u675f\u590d\u6742/Complex constraints for redundant parallel mechanisms]\n    B --\x3e B3[\u76ee\u6807\u4f4d\u59ff\u8d85\u51fa\u5de5\u4f5c\u7a7a\u95f4\u65e0\u89e3/No solution for out-of-workspace targets]\n    C --\x3e C1[\u57fa\u4e8eFABRIK\u7b97\u6cd5/Based on FABRIK algorithm]\n    C --\x3e C2[\u62d3\u6251\u5206\u89e3\u4e3a\u4e32\u884c\u5b50\u94fe/Topological decomposition into serial sub-chains]\n    C --\x3e C3[\u8fed\u4ee3\u4fee\u6b63\u672b\u7aef\u76ee\u6807/Iteratively revise end targets]\n    D --\x3e D1[\u9002\u7528\u4e8e\u591a\u79cd\u5e76\u8054\u673a\u6784/Applicable to diverse parallel mechanisms]\n    D --\x3e D2[\u8ba1\u7b97\u9ad8\u6548/Computationally efficient]\n    D --\x3e D3[\u9c81\u68d2\u5904\u7406\u8d85\u5de5\u4f5c\u7a7a\u95f4\u76ee\u6807/Robust to out-of-workspace targets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics control], [aerial manipulator, prescribed performance control, variable-gain extended state observer, dynamic coupling, motion control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengyu Ji, Shiliang Guo, Zhengzhen Li, Jiahao Shen, Huazi Cao, Shiyu Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Westlake University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22957",children:"https://arxiv.org/pdf/2512.22957"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel prescribed performance motion control framework (PreGME) for aerial manipulators. 2. The use of variable-gain extended state observers (ESOs) for accurate real-time estimation of rapidly varying dynamic coupling. 3. A control strategy that generates a preset error trajectory to ensure tracking errors remain within a prescribed performance envelope for high-precision control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2803479d85cfb232ae59d1133082b1c37608a63bed7f68577a5675d15b2598_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2803479d85cfb232ae59d1133082b1c37608a63bed7f68577a5675d15b2598_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PreGME, a new control framework for aerial manipulators that combines variable-gain extended state observers to estimate dynamic coupling with prescribed performance control to constrain error trajectories. The method enables high-precision control even during aggressive arm motions. Experiments, including aerial mixology and cart-pulling, validate its effectiveness under significant dynamic disturbances."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["PreGME: Prescribed Performance Control of Aerial Manipulators"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Aerial manipulator dynamic coupling affects control precision"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Variable-gain ESO + Prescribed performance control"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: High tracking performance validated by real-world experiments"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic manipulation], [robotic foundation models, high-level planning, low-level control, imitation learning, reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xi'an Jiaotong University, Hong Kong University of Science and Technology (Guangzhou), Chinese Academy of Sciences, Westlake University, Zhejiang University, University of Sydney, BAAI, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22983",children:"https://arxiv.org/pdf/2512.22983"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," Awesome-Robotics-Manipulation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified algorithmic abstraction for robot manipulation, organizing approaches into high-level planning and low-level control. 2. Extends classical task planning to include reasoning over language, code, motion, affordances, and 3D representations. 3. Introduces a training-paradigm-oriented taxonomy for learning-based control, categorizing methods by input modeling, latent representation learning, and policy learning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22135ed5ae9ef66789b2dbe7f9c4c6e6a9de91ca6dd4b2025e75cfd5d4a89d02_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22135ed5ae9ef66789b2dbe7f9c4c6e6a9de91ca6dd4b2025e75cfd5d4a89d02_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper organizes recent learning-based approaches to robot manipulation within a unified framework of high-level planning and low-level control. It extends task planning to include multimodal reasoning and proposes a new taxonomy for learning-based control. The analysis aims to clarify the design space and identifies key challenges like scalability and safety for future robotic foundation models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Embodied Robot Manipulation in the Era of Foundation Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Robot manipulation remains a central and challenging problem / \u673a\u5668\u4eba\u64cd\u4f5c\u4ecd\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218]\n    C --\x3e C1[Unified abstraction: High-level planning & Low-level control / \u7edf\u4e00\u62bd\u8c61\uff1a\u9ad8\u5c42\u89c4\u5212\u4e0e\u5e95\u5c42\u63a7\u5236]\n    C1 --\x3e C2[High-level: Reasoning over language, code, motion, etc. / \u9ad8\u5c42\uff1a\u57fa\u4e8e\u8bed\u8a00\u3001\u4ee3\u7801\u3001\u8fd0\u52a8\u7b49\u7684\u63a8\u7406]\n    C1 --\x3e C3[Low-level: Taxonomy for learning-based control / \u5e95\u5c42\uff1a\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5206\u7c7b\u6cd5]\n    D --\x3e D1[Clarifies design space of foundation models / \u9610\u660e\u57fa\u7840\u6a21\u578b\u7684\u8bbe\u8ba1\u7a7a\u95f4]\n    D --\x3e D2[Identifies open challenges: scalability, safety, etc. / \u6307\u51fa\u5f00\u653e\u6311\u6218\uff1a\u53ef\u6269\u5c55\u6027\u3001\u5b89\u5168\u6027\u7b49]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [musculoskeletal control, vision-language models, embodied learning, reward function discovery, motion representation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Saraswati Soedarmadji, Yunyue Wei, Chen Zhang, Yisong Yue, Yanan Sui"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, California Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23077",children:"https://arxiv.org/pdf/2512.23077"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MoVLR, a framework that uses Vision-Language Models (VLMs) to bridge high-level goal specification and low-level movement control for musculoskeletal systems. 2. Proposes an iterative method to explore the reward space by combining control optimization with VLM feedback, avoiding reliance on handcrafted rewards. 3. Demonstrates the framework's ability to discover and refine reward functions for complex locomotion and manipulation tasks, grounding abstract language descriptions in physical control principles."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e43a6e8e2c1f39257ec8f88c3695335f5287e369c9a159892f3f3cd6efc7ce61_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e43a6e8e2c1f39257ec8f88c3695335f5287e369c9a159892f3f3cd6efc7ce61_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of designing reward functions for high-dimensional musculoskeletal control by proposing MoVLR, a framework that leverages Vision-Language Models to iteratively align control policies with high-level goals described in language. The method transforms language and vision-based assessments into structured guidance for embodied learning. The results show that VLMs can effectively ground abstract motion descriptions in the implicit principles of physiological motor control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Reward function design for high-dimensional musculoskeletal control is challenging] --\x3e B1[\u5177\u4f53\u6311\u6218/Challenge<br>High-level goals (e.g., "walk upright") are hard to translate into low-level control rewards]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>MoVLR Framework] --\x3e C1[\u5173\u952e\u673a\u5236/Mechanism<br>Iterative exploration of reward space via VLM feedback and control optimization]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>VLMs can ground abstract motion in physical control principles] --\x3e D1[\u5e94\u7528/Application<br>Enables reward discovery for locomotion and manipulation]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] APOLLO Blender: A Robotics Library for Visualization and Animation in Blender"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics visualization], [URDF, Blender, keyframing, Python scripting, 3D animation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Peter Messina, Daniel Rakita"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23103",children:"https://arxiv.org/pdf/2512.23103"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A library for importing robots and environments directly from standardized descriptions like URDF into Blender, 2. Python-based scripting tools for keyframing robot states and visual attributes, 3. Convenient generation of primitive 3D shapes for creating schematic figures and animations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cba129e043d858f09043e81dde4891fc5ef6711639795d93574734cb37f17c5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cba129e043d858f09043e81dde4891fc5ef6711639795d93574734cb37f17c5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces APOLLO Blender, a lightweight Python library that simplifies creating high-quality robotics visualizations and animations within Blender by providing robotics-focused scripting tools. It bridges the gap between powerful 3D graphics software and robotics research needs, enabling researchers to generate publication-ready content without deep Blender expertise. The work demonstrates the library's utility through examples and discusses future extensions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[APOLLO Blender: A Robotics Library for Visualization and Animation in Blender] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Blender\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced/Steep Blender learning curve]\n    B --\x3e B2[\u7f3a\u4e4f\u673a\u5668\u4eba\u4e13\u7528\u96c6\u6210/Lack of robotics-focused integrations]\n    C --\x3e C1[\u5bfc\u5165URDF\u673a\u5668\u4eba/Import URDF robots]\n    C --\x3e C2[Python\u811a\u672c\u5173\u952e\u5e27/Python scripting for keyframing]\n    C --\x3e C3[\u751f\u62103D\u56fe\u5143/Generate 3D primitives]\n    D --\x3e D1[\u5feb\u901f\u521b\u5efa\u51fa\u7248\u7269\u56fe\u50cf/Rapid creation of publication-ready images]\n    D --\x3e D2[\u65e0\u9700\u6df1\u539aBlender\u4e13\u4e1a\u77e5\u8bc6/No extensive Blender expertise needed]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sys], [robotics systems], [URDD, robot description, JSON/YAML modules, Bevy visualization, Three.js viewer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Roshan Klein-Seetharaman, Daniel Rakita"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23135",children:"https://arxiv.org/pdf/2512.23135"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured JSON and YAML modules to reduce redundancy and improve standardization. 2. Provided an open-source toolkit with a Rust implementation to automatically generate URDDs from URDFs, including Bevy-based visualization capabilities. 3. Developed a JavaScript/Three.js viewer for web-based inspection of URDDs, enabling interactive visualization and verification across platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3c8fda99c3364911696d021db2c35cc62671784fcdbf685fd62934de409e5f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f3c8fda99c3364911696d021db2c35cc62671784fcdbf685fd62934de409e5f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of redundant computations and fragmented implementations in robotics due to basic robot specification files like URDF. It proposes the Universal Robot Description Directory (URDD), a modular representation that organizes richer derived information into JSON/YAML files, along with tools for automatic generation and visualization. The work concludes that URDD efficiently encapsulates extensive robot data, enabling shared standards and reducing redundancy across frameworks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond URDF: The Universal Robot Description Directory] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u57fa\u7840\u673a\u5668\u4eba\u63cf\u8ff0\u6587\u4ef6\u4fe1\u606f\u6709\u9650 / Basic robot specs have limited info]\n    B --\x3e B2[\u4e0b\u6e38\u5e94\u7528\u91cd\u590d\u8ba1\u7b97 / Downstream apps re-derive data redundantly]\n    C --\x3e C1[\u63d0\u51faURDD\u6a21\u5757\u5316\u8868\u793a / Propose URDD modular representation]\n    C --\x3e C2[\u4f7f\u7528JSON/YAML\u7ec4\u7ec7\u4fe1\u606f / Use JSON/YAML to organize info]\n    C --\x3e C3[\u63d0\u4f9b\u5f00\u6e90\u751f\u6210\u4e0e\u53ef\u89c6\u5316\u5de5\u5177 / Provide open-source generation & visualization tools]\n    D --\x3e D1[URDD\u9ad8\u6548\u751f\u6210\u4e14\u4fe1\u606f\u4e30\u5bcc / URDD generated efficiently & info-rich]\n    D --\x3e D2[\u652f\u6301\u6838\u5fc3\u5b50\u7a0b\u5e8f\u6784\u5efa / Enables core subroutine construction]\n    D --\x3e D3[\u5efa\u7acb\u7edf\u4e00\u53ef\u6269\u5c55\u6807\u51c6 / Establishes unified, extensible standards]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics], [self-collision matrix, robot self-collision, proximity query, shape representation, interactive visualization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Roshan Klein-Seetharama, Daniel Rakita"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23140",children:"https://arxiv.org/pdf/2512.23140"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An interactive tool for generating and visualizing self-collision matrices that overcomes limitations of static tools like MoveIt Setup Assistant. 2. Support for multiple shape representations (spheres, OBBs, convex hulls, convex decompositions) and dynamic inspection/filtering. 3. Implementation in Rust with Bevy engine for high-quality visualization and export to JSON/YAML for easy integration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea10a80ffc42162cc3aac7a557e3310d2bcf065680b6fdc22c295d40ed8b5ec4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea10a80ffc42162cc3aac7a557e3310d2bcf065680b6fdc22c295d40ed8b5ec4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces a new interactive software tool for generating and visualizing robot self-collision matrices. It supports multiple shape representations and enables dynamic inspection and refinement, leading to faster and more accurate self-collision and self-proximity queries. The tool is implemented in Rust using the Bevy game engine and outputs results in standard formats for integration into planning frameworks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u5de5\u5177\u9759\u6001\u3001\u7f3a\u4e4f\u90bb\u8fd1\u652f\u6301\u3001\u5de5\u4f5c\u6d41\u7e41\u7410/Current tools: static, lack proximity support, tedious workflow]\n    C --\x3e C1[\u4ea4\u4e92\u5f0f\u5de5\u5177\u652f\u6301\u591a\u79cd\u5f62\u72b6\u8868\u793a\u4e0e\u52a8\u6001\u68c0\u67e5/Interactive tool with multi-shape representation & dynamic inspection]\n    C --\x3e C2[\u4f7f\u7528Rust\u4e0eBevy\u5f15\u64ce\u5b9e\u73b0/Implemented in Rust with Bevy engine]\n    D --\x3e D1[\u751f\u6210\u66f4\u5feb\u66f4\u51c6\u786e\u7684\u78b0\u649e\u4e0e\u90bb\u8fd1\u67e5\u8be2/Generates faster & more accurate collision/proximity queries]\n    D --\x3e D2[\u8f93\u51faJSON/YAML\u4fbf\u4e8e\u96c6\u6210/Exports JSON/YAML for easy integration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robot localization], [pole-centric descriptors, contrastive learning, small-scale observations, landmark distinctiveness, pole-at-distance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wuhao Xie, Kanji Tanaka"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Fukui"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23141",children:"https://arxiv.org/pdf/2512.23141"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Construction of the Small Pole Landmark (SPL) dataset via an automated tracking-based pipeline for evaluating landmark identification under sparse observations. 2. Empirical comparative analysis demonstrating that Contrastive Learning (CL) induces a more robust feature space for sparse geometry than Supervised Learning (SL). 3. Systematic robustness breakdown analyzing the trade-off between observation distance and descriptor reliability, identifying effective operational ranges."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a74c5a1b74d238ad9ddaabd43dc96605a8f33839f93907c39978f07db411a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a74c5a1b74d238ad9ddaabd43dc96605a8f33839f93907c39978f07db411a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the degradation of pole landmark identification reliability under long-distance, sparse observations in urban environments. It proposes an evaluation framework using the SPL dataset and compares Contrastive Learning (CL) and Supervised Learning (SL) paradigms, finding that CL achieves superior retrieval performance, especially in the 5-10m range, by learning more robust features."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Pole-centric Descriptors for Robust Robot Localization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Pole-at-Distance (PaD) observations degrade landmark identification reliability]\n    C --\x3e C1[Establish SPL dataset via automated tracking pipeline]\n    C --\x3e C2[Comparative analysis of Contrastive Learning vs. Supervised Learning]\n    D --\x3e D1[CL induces more robust feature space for sparse geometry]\n    D --\x3e D2[Superior retrieval performance in 5-10m range]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [space robotics], [multi-limbed robot, intra-vehicular activity, motion planning, microgravity simulation, autonomous operation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Seiko Piotr Yamaguchi, Kentaro Uno, Yasumaru Fujii, Masazumi Imai, Kazuki Takada, Taku Okawara, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Japan Aerospace Exploration Agency (JAXA), Tohoku University, Hamano Products Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23153",children:"https://arxiv.org/pdf/2512.23153"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A feasibility study and design for a multi-limbed intra-vehicular robot (MLIVR) to automate logistical tasks on the ISS. 2. Development and simulation of 3D motion planning for the robot's transportation capabilities in a space station environment. 3. Execution of prototype ground tests on a 2D table to validate autonomous operation in a simulated microgravity setting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/282a695eb4eaf1576b9e4ce6a2fca61f3d48e62d714d0b32d51e22f5a403fbd6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/282a695eb4eaf1576b9e4ce6a2fca61f3d48e62d714d0b32d51e22f5a403fbd6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the use of an autonomous multi-limbed robot to assist astronauts with logistical tasks on the International Space Station. The method involved simulating 3D motion planning and testing a prototype on a 2D table to mimic microgravity. The results show that such tasks can be performed with minimal human intervention, enhancing operational efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u5b87\u822a\u5458\u65f6\u95f4\u88ab\u7269\u6d41\u4efb\u52a1\u5360\u7528/Astronaut time consumed by logistical tasks")\n    Method --\x3e M1("3D\u8fd0\u52a8\u89c4\u5212\u6a21\u62df/3D motion planning simulation")\n    Method --\x3e M2("2D\u5e73\u53f0\u539f\u578b\u6d4b\u8bd5/2D table prototype testing")\n    Results --\x3e R1("\u9a8c\u8bc1\u6700\u5c0f\u4eba\u529b\u5e72\u9884\u7684\u53ef\u884c\u6027/Validated feasibility of minimal human intervention")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [modular robotics], [coupling mechanism, sequential hermaphrodite, shape-matching, lattice-based, modular self-reconfigurable robot (MSR)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Keigo Torii, Kentaro Uno, Shreya Santra, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23154",children:"https://arxiv.org/pdf/2512.23154"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel shape-matching mechanical coupling mechanism that meets complex requirements for heterogeneous structural modules. 2. A design enabling controlled, sequential transitions between male and female states to facilitate single-sided operations. 3. The capability for single-sided decoupling from both the male and female sides by forcibly switching the opposite mechanism's state."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e13d98b23f8f9371fe7ea6dda54625a0d3bd211a4b265a46a96f7e03a5eba13_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e13d98b23f8f9371fe7ea6dda54625a0d3bd211a4b265a46a96f7e03a5eba13_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the complex design requirements for coupling mechanisms in lattice-based modular robots, such as single-sided operation and flat uncoupled surfaces. It proposes a novel sequential hermaphrodite coupling mechanism that dynamically switches between male and female states to meet these needs. The mechanism is concluded to be applicable to various modular robot systems and tool changers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Sequential Hermaphrodite Coupling Mechanism<br>\u987a\u5e8f\u96cc\u96c4\u540c\u4f53\u8026\u5408\u673a\u5236"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Coupling mechanisms for lattice-based modular robots need to meet multiple complex requirements (single-sided operation, flat surfaces, etc.)<br>\u6676\u683c\u6a21\u5757\u5316\u673a\u5668\u4eba\u7684\u8026\u5408\u673a\u5236\u9700\u6ee1\u8db3\u591a\u79cd\u590d\u6742\u8981\u6c42\uff08\u5355\u4fa7\u64cd\u4f5c\u3001\u5e73\u6574\u8868\u9762\u7b49\uff09"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose a novel shape-matching mechanism with controlled sequential transitions between male and female states<br>\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5f62\u72b6\u5339\u914d\u673a\u5236\uff0c\u53ef\u5728\u96c4\u6027\u548c\u96cc\u6027\u72b6\u6001\u95f4\u8fdb\u884c\u53d7\u63a7\u987a\u5e8f\u8f6c\u6362"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Mechanism satisfies design requirements, enables single-sided coupling/decoupling from both sides<br>\u673a\u5236\u6ee1\u8db3\u8bbe\u8ba1\u8981\u6c42\uff0c\u53ef\u5b9e\u73b0\u4e24\u4fa7\u7684\u5355\u4fa7\u8fde\u63a5/\u5206\u79bb"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [ergodic coverage, spectral multiscale coverage, stochastic perturbation, gradient cancellation, mean-square boundedness]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kooktae Lee, Julian Martinez"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," New Mexico Institute of Mining and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23158",children:"https://arxiv.org/pdf/2512.23158"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Rigorously characterizes the initial conditions and symmetry-induced invariant manifolds that cause directional degeneracy (stalling/axis-constrained motion) in classical Spectral Multiscale Coverage (SMC). 2. Introduces a novel stochastic spectral control method that combines a stochastic perturbation with a contraction term to address the degeneracy. 3. Provides theoretical proofs that the proposed dynamics ensure almost-sure escape from zero-gradient manifolds while maintaining mean-square boundedness of agent trajectories."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d9e67e490796584c8650824889a0abbafdd39b17b5ae994eec026cc68f38_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d9e67e490796584c8650824889a0abbafdd39b17b5ae994eec026cc68f38_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of gradient cancellation and agent stalling in multi-agent ergodic coverage when agents start near symmetry points of a target distribution. The authors propose a stochastic spectral control method that adds perturbation and contraction to the dynamics, proving it escapes degenerate states while keeping trajectories bounded. Simulations confirm the method mitigates stalling and axis-constrained motion effectively."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u68af\u5ea6\u62b5\u6d88\u5bfc\u81f4\u505c\u6ede/Gradient cancellation causes stalling"]\n    Problem --\x3e P2["\u5bf9\u79f0\u70b9\u521d\u59cb\u5316\u95ee\u9898/Initialization near symmetry points"]\n    Method --\x3e M1["\u5f15\u5165\u968f\u673a\u6270\u52a8/Introduce stochastic perturbation"]\n    Method --\x3e M2["\u7ed3\u5408\u6536\u7f29\u9879/Combine with contraction term"]\n    Results --\x3e R1["\u51e0\u4e4e\u5fc5\u7136\u9003\u79bb\u96f6\u68af\u5ea6\u6d41\u5f62/Almost-sure escape from zero-gradient manifolds"]\n    Results --\x3e R2["\u8f68\u8ff9\u5747\u65b9\u6709\u754c/Trajectories mean-square bounded"]\n    Results --\x3e R3["\u7f13\u89e3\u77ac\u6001\u505c\u6ede/Mitigates transient stalling"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic imitation learning], [world model, vision-language-action (VLA) model, inverse dynamics model, surgical robotics, synthetic data generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23162",children:"https://arxiv.org/pdf/2512.23162"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Data scarcity for paired video-action data in surgical robotics]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Build SurgWorld world model to generate synthetic videos; Use inverse dynamics to infer pseudo-kinematics]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Surgical VLA policy trained with augmented data outperforms policy trained only on real data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [cooperative driving, human-machine conflict, intention-aware planning, authority allocation, shared control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qin Wang, Shanmin Pang, Jianwu Fang, Shengye Dong, Fuhao Liu, Jianru Xue, Chen Lv"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xi'an Jiaotong University, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23220",children:"https://arxiv.org/pdf/2512.23220"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/i-Qin/HOCD",children:"https://github.com/i-Qin/HOCD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Human-Oriented Cooperative Driving (HOCD) approach that minimizes human-machine conflict by prioritizing driver intention and state. 2. Designs an intention-aware trajectory planning method at the tactical level, using an intention consistency cost to align the trajectory with driver intention. 3. Develops a reinforcement learning-based control authority allocation strategy at the operational level to achieve consistency between driver state and authority allocation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d5e5ef9e0e93b645fd2997c604be86cc36eb4f1a7f88af7219f0cc6a908523b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d5e5ef9e0e93b645fd2997c604be86cc36eb4f1a7f88af7219f0cc6a908523b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Human-Oriented Cooperative Driving (HOCD) approach to improve human-vehicle interaction by minimizing conflict. The method integrates intention-aware trajectory planning and a reinforcement learning-based authority allocation strategy. Simulation and human-in-the-loop experiments show the approach aligns with driver intention, ensures reasonable authority allocation, and enhances driving performance compared to other methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Human-Oriented Cooperative Driving Approach<br>\u4eba\u673a\u534f\u540c\u9a7e\u9a76\u65b9\u6cd5"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Human-machine conflict in cooperative driving<br>\u4eba\u673a\u534f\u540c\u9a7e\u9a76\u4e2d\u7684\u4eba\u673a\u51b2\u7a81"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>HOCD: Integrates intention & state<br>HOCD: \u96c6\u6210\u9a7e\u9a76\u610f\u56fe\u4e0e\u72b6\u6001"]\n    Method --\x3e SubMethod1["\u6218\u672f\u5c42\u9762/Tactical Level<br>Intention-aware trajectory planning<br>\u610f\u56fe\u611f\u77e5\u8f68\u8ff9\u89c4\u5212"]\n    Method --\x3e SubMethod2["\u64cd\u4f5c\u5c42\u9762/Operational Level<br>RL-based authority allocation<br>\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6743\u9650\u5206\u914d"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Aligns intention, reasonable allocation, enhances performance<br>\u5bf9\u9f50\u610f\u56fe\u3001\u5408\u7406\u5206\u914d\u3001\u63d0\u5347\u6027\u80fd"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [coverage path planning], [UAV Swarms, Multi-Agent, Coverage Path Planning, Remote Sensing, Trajectory Optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Socratis Gkelios, Savvas D. Apostolidis, Pavlos Ch. Kapoutsis, Elias B. Kosmatopoulos, Athanasios Ch. Kapoutsis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Democritus University of Thrace, Information Technologies Institute, Centre for Research & Technology Hellas"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23257",children:"https://arxiv.org/pdf/2512.23257"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," github.com/soc12/mUDAI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formally introduces the Fast Inspection of Scattered Regions (FISR) problem, focusing on inspecting multiple non-connected areas. 2. Proposes the multi-UAV Disjoint Areas Inspection (mUDAI) method, a two-fold optimization for image capture positions and UAV trajectories. 3. Validates the method through simulations and real-world deployments, demonstrating improved operational efficiency while maintaining data quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eadfe6ac5c135ae77cf9cbeeaa979770a93369ca56f253722ad9679e402d746a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eadfe6ac5c135ae77cf9cbeeaa979770a93369ca56f253722ad9679e402d746a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of traditional coverage path planning for inspecting multiple scattered regions using UAVs. It proposes the mUDAI method, which optimizes both image capture positions and UAV trajectories to minimize time and resource use. The method is validated through simulations and real-world tests, showing it enables rapid, efficient inspections."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>\u4f20\u7edf\u8986\u76d6\u8def\u5f84\u89c4\u5212(CPP)\u5728\u68c0\u67e5\u591a\u4e2a\u5206\u6563\u533a\u57df\u65f6\u6548\u7387\u4f4e\u4e0b"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51famUDAI\u65b9\u6cd5\uff0c\u53cc\u91cd\u4f18\u5316\u56fe\u50cf\u91c7\u96c6\u4f4d\u7f6e\u548c\u65e0\u4eba\u673a\u8f68\u8ff9"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u90e8\u7f72\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6548\u7387\u5e76\u4fdd\u6301\u4e86\u6570\u636e\u8d28\u91cf"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual SLAM], [ORB-SLAM3, YOLOv8, dynamic object filtering, point cloud refinement, CUDA acceleration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yuan Ze University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23318",children:"https://arxiv.org/pdf/2512.23318"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3's robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: vSLAM accuracy compromised by dynamic objects]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: ORB-SLAM3 + YOLOv8 segmentation + multi-stage point cloud filtering]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Mixed performance, notable improvement in specific sequences (e.g., Seq04)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [explainable ai (xai)], [inverse kinematics, shapley additive explanations (SHAP), InterpretML, obstacle avoidance, neural network]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yuan Ze University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23312",children:"https://arxiv.org/pdf/2512.23312"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation<br>\u53ef\u89e3\u91ca\u795e\u7ecf\u9006\u8fd0\u52a8\u5b66\u7528\u4e8e\u969c\u788d\u7269\u611f\u77e5\u673a\u5668\u4eba\u64cd\u4f5c"] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B["\u6838\u5fc3\u95ee\u9898/Problem<br>Opaque neural IK models lack transparency and safety for responsible AI.<br>\u9ed1\u76d2\u795e\u7ecfIK\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027"] --\x3e B1["\u6311\u6218/Challenges<br>Debugging failures, safety certification"]\n    C["\u4e3b\u8981\u65b9\u6cd5/Method<br>XAI workflow integrating SHAP and physics simulation.<br>\u96c6\u6210SHAP\u4e0e\u7269\u7406\u4eff\u771f\u7684XAI\u5de5\u4f5c\u6d41"] --\x3e C1["\u6a21\u578b/Variants<br>Improved IKNet, Focused IKNet"]\n    C --\x3e C2["\u5de5\u5177/Tools<br>SHAP, InterpretML, Simulator"]\n    D["\u5173\u952e\u7ed3\u679c/Results<br>Balanced feature attribution correlates with wider safety margins.<br>\u5747\u8861\u7684\u7279\u5f81\u5f52\u56e0\u4e0e\u66f4\u5bbd\u7684\u5b89\u5168\u88d5\u5ea6\u76f8\u5173"] --\x3e D1["\u7ed3\u8bba/Conclusion<br>XAI guides architectural refinement for trustworthy IK.<br>XAI\u6307\u5bfc\u53ef\u4fe1IK\u7684\u67b6\u6784\u6539\u8fdb"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [task allocation, swarm robotics, scalability functions, marginal gains, collective decision-making]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Simay Atasoy Bing\xf6l, Tobias T\xf6pfer, Sven Kosub, Heiko Hamann, Andreagiovanni Reina"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Universit\xe4t Konstanz, Max Planck Institute of Animal Behavior"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23431",children:"https://arxiv.org/pdf/2512.23431"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A computationally efficient algorithm for optimal agent allocation based on marginal performance gains. 2. The algorithm handles tasks with concave scalability functions, including linear, saturating, and retrograde scaling. 3. Validation of the algorithm in a simulated robot swarm performing collective decision-making tasks with varying difficulty."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65eaf636a462023cd662333c71ac9a0b2b73588d8278f1b001d8292a37ecc630_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65eaf636a462023cd662333c71ac9a0b2b73588d8278f1b001d8292a37ecc630_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of optimally allocating a finite number of agents across multiple tasks where performance scales differently. It proposes an efficient algorithm based on marginal gains to handle concave scalability functions, including retrograde scaling where too many agents degrade performance. The method is validated in robot swarm simulations for collective decision-making, showing its utility for future multi-robot systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Optimal Scalability-Aware Allocation of Swarm Robots<br>\u673a\u5668\u4eba\u96c6\u7fa4\u7684\u53ef\u6269\u5c55\u6027\u611f\u77e5\u6700\u4f18\u5206\u914d] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6709\u9650\u667a\u80fd\u4f53\u5206\u914d\u5230\u591a\u4e2a\u4efb\u52a1<br>Limited agents to multiple tasks]\n    B --\x3e B2[\u6027\u80fd\u968f\u667a\u80fd\u4f53\u6570\u91cf\u975e\u7ebf\u6027\u53d8\u5316<br>Nonlinear performance scaling]\n    B --\x3e B3[\u66b4\u529b\u641c\u7d22\u4e0d\u53ef\u884c<br>Brute-force infeasible]\n    C --\x3e C1[\u57fa\u4e8e\u8fb9\u9645\u6027\u80fd\u589e\u76ca\u7684\u7b97\u6cd5<br>Algorithm based on marginal gains]\n    C --\x3e C2[\u5904\u7406\u51f9\u53ef\u6269\u5c55\u6027\u51fd\u6570<br>Handles concave scalability functions]\n    D --\x3e D1[\u5728\u673a\u5668\u4eba\u96c6\u7fa4\u51b3\u7b56\u4e2d\u9a8c\u8bc1<br>Validated in robot swarm decision-making]\n    D --\x3e D2[\u7b97\u6cd5\u6709\u6548\u5206\u914d\u673a\u5668\u4eba<br>Algorithm useful for allocation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [Model Predictive Control, Coverage-Based Testing, Edge-Case Exploration, Multi-Agent Simulation, Behaviour Coverage]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Manuel Franco-Vivo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Bristol"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23445",children:"https://arxiv.org/pdf/2512.23445"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Assessing Behaviour Coverage in a Multi-Agent System Simulation for Autonomous Vehicle Testing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u5982\u4f55\u5168\u9762\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u8986\u76d6\u5ea6\uff1f/How to comprehensively evaluate behaviour coverage of AV systems in simulation?]\nC --\x3e C1[\u5b9a\u4e49\u573a\u666f\u4e0e\u4ea4\u4e92\uff0c\u63d0\u51faMPC\u884c\u4eba\u667a\u80fd\u4f53/Define scenarios & interactions, propose MPC pedestrian agent]\nD --\x3e D1[\u884c\u4e3a\u8986\u76d6\u5ea6\u5bf9\u9a8c\u8bc1\u7cfb\u7edf\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981/Behaviour coverage is crucial for validating system effectiveness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Theory of Mind for Explainable Human-Robot Interaction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [human-robot interaction], [Theory of Mind, Explainable AI, XAI evaluation, human-centered explanation, VXAI framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Hamburg"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23482",children:"https://arxiv.org/pdf/2512.23482"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot's actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM's user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Theory of Mind for Explainable Human-Robot Interaction") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("ToM\u89e3\u91ca\u4e0e\u673a\u5668\u4eba\u5185\u90e8\u63a8\u7406\u4e0d\u4e00\u81f4/ToM explanations may not match robot\'s internal reasoning")\n    Problem --\x3e P2("XAI\u7f3a\u4e4f\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca/XAI lacks user-centered explanations")\n    Method --\x3e M1("\u5c06ToM\u89c6\u4e3aXAI\u7684\u4e00\u79cd\u5f62\u5f0f/Consider ToM as a form of XAI")\n    Method --\x3e M2("\u5728XAI\u6846\u67b6\u5185\u6574\u5408ToM\u539f\u5219/Integrate ToM principles within XAI frameworks")\n    Results --\x3e R1("\u63d0\u51fa\u89c6\u89d2\u8f6c\u53d8\uff0c\u4f18\u5148\u8003\u8651\u7528\u6237\u9700\u6c42/Proposed shift in perspective to prioritize user\'s needs")\n    Results --\x3e R2("\u4e3a\u4f7f\u7528VXAI\u7b49\u6846\u67b6\u8bc4\u4f30ToM\u5960\u5b9a\u57fa\u7840/Laid foundation for evaluating ToM using frameworks like VXAI")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robust control, learning-based control], [robust control, deep learning, safety guarantees, heavy-duty machinery, hierarchical control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mehdi Heydari Shahna"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' Based on the author name "Mehdi Heydari Shahna", a common affiliation for this research area would be a university with a strong robotics program, such as Aalto University, Tampere University, or the University of Oulu in Finland. However, the provided text does not explicitly state the institution. A reasonable inference from the context of heavy-duty machinery and robotics in Europe could be "Tampere University" or "Aalto University", but without explicit data, the safest answer is "Not explicitly stated in provided content".']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23505",children:"https://arxiv.org/pdf/2512.23505"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A generic modular control framework for electrified heavy-duty mobile machines that is energy-source independent and simplifies design. 2. Hierarchical control policies that integrate AI/learning strategies while providing formal guarantees for safety, performance, and stability. 3. Methods to interpret and verify black-box learning components to ensure compliance with international safety standards."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f78ddebe7bd223957b3334fed0e1b9489d7be34c9c804202f8cfb9e774548a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f78ddebe7bd223957b3334fed0e1b9489d7be34c9c804202f8cfb9e774548a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This dissertation addresses the challenge of safely integrating AI into heavy-duty mobile machines undergoing electrification. It proposes a robust, modular control framework that combines deep learning with formal guarantees for performance and stability. The framework is validated through multiple case studies, advancing control methods for safer and more reliable robotic systems in heavy industry."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Deep Learning Control for Heavy-Duty Machinery] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Two Transitions for HDMMs<br>1. \u67f4\u6cb9\u6db2\u538b\u5230\u7535\u52a8<br>Diesel-Hydraulic to Electric<br>2. \u4eba\u5de5\u76d1\u7763\u5230\u81ea\u4e3b\u5316<br>Human Supervision to Autonomy<br>\u5173\u952e\u6311\u6218: \u5b89\u5168\u4e0e\u53ef\u9760\u6027<br>Key Challenge: Safety & Reliability]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51fa\u4e00\u4e2a\u63a7\u5236\u6846\u67b6<br>Proposes a Control Framework<br>1. \u901a\u7528\u6a21\u5757\u5316\u8bbe\u8ba1<br>Generic Modular Design<br>2. \u5206\u5c42\u7b56\u7565\u6574\u5408AI<br>Hierarchical Policy Integrating AI<br>3. \u4fdd\u8bc1\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027<br>Guarantees Performance & Stability]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u6846\u67b6\u5f97\u5230\u9a8c\u8bc1<br>Framework Validated<br>\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76<br>Via Three Case Studies<br>\u4ea7\u51fa\u4e94\u7bc7\u8bba\u6587<br>Five Publications<br>\u652f\u6301\u4e24\u5927\u8f6c\u578b<br>Supports Both Transitions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Act2Goal: From World Model To General Goal-conditioned Policy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [goal-conditioned policy, world model, multi-scale temporal hashing, hindsight goal relabeling, LoRA]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Agibot Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23541",children:"https://arxiv.org/pdf/2512.23541"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://act2goal.github.io/",children:"https://act2goal.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Act2Goal: From World Model To General Goal-conditioned Policy] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u957f\u89c6\u91ce\u64cd\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73/Existing goal-conditioned policies struggle with long-horizon manipulation]\n    C --\x3e C1[\u96c6\u6210\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u4e0e\u591a\u5c3a\u5ea6\u65f6\u5e8f\u63a7\u5236/Integrates goal-conditioned visual world model with multi-scale temporal control]\n    C --\x3e C2[\u5f15\u5165\u591a\u5c3a\u5ea6\u65f6\u5e8f\u54c8\u5e0c(MSTH)\u5206\u89e3\u8f68\u8ff9/Introduces Multi-Scale Temporal Hashing (MSTH) to decompose trajectory]\n    D --\x3e D1[\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u5f3a/Strong zero-shot generalization]\n    D --\x3e D2[\u901a\u8fc7\u5728\u7ebf\u81ea\u9002\u5e94\u663e\u8457\u63d0\u5347\u6210\u529f\u7387/Improves success rates significantly via online adaptation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Soft Robotic Technological Probe for Speculative Fashion Futures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-robot interaction], [soft robotics, wearable technology, speculative design, pneumatic actuation, technological probe]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amy Ingold, Loong Yi Lee, Richard Suphapol Diteesawat, Ajmal Roshan, Yael Zekaria, Edith-Clare Hall, Enrico Werner, Nahian Rahman, Elaine Czech, Jonathan Rossiter"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Bristol"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23570",children:"https://arxiv.org/pdf/2512.23570"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. The design and fabrication of "Sumbrella," a novel soft robotic garment integrating origami-inspired bistable units, fabric pneumatic actuators, and computer vision. 2. The use of Sumbrella as a technological probe in a focus group study to explore public interpretation, interaction, and ethical concerns regarding future soft robotic wearables. 3. The contribution of key considerations for HRI, including kinesic communication, social dynamics, and ethical guidelines, and a reflection on the value of speculative design for evaluating social acceptability.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/961ff66eaf860636f7951016c0465ba6020b516e266f501287c6b78b23a9fafa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/961ff66eaf860636f7951016c0465ba6020b516e266f501287c6b78b23a9fafa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper presents "Sumbrella," a soft robotic garment designed as a speculative fashion probe to explore the social implications of wearable robotics. Through a focus group study, the authors used the prototype to gather insights on how people imagine future interactions with such technology, revealing both expressive potential and significant ethical concerns. The work contributes design considerations and a methodological reflection on using speculative design in Human-Robot Interaction research to address social meaning alongside functionality.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Soft Robotic Technological Probe for Speculative Fashion Futures] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u65b0\u5174\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u9700\u517c\u987e\u529f\u80fd\u4e0e\u793e\u4f1a\u610f\u4e49/Emerging wearable robotics demand design addressing function and social meaning]\n    C --\x3e C1[\u8bbe\u8ba1\u5e76\u5236\u9020Sumbrella\u8f6f\u4f53\u673a\u5668\u4eba\u670d\u88c5/Design and fabricate Sumbrella soft robotic garment]\n    C --\x3e C2[\u4f5c\u4e3a\u6280\u672f\u63a2\u9488\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u7814\u7a76/Use as a technological probe in a focus group study]\n    D --\x3e D1[\u5f15\u53d1\u5bf9\u8868\u8fbe\u6f5c\u529b\u4e0e\u4f26\u7406\u98ce\u9669\u7684\u4e30\u5bcc\u8ba8\u8bba/Surfaced discussions on expressive potential and ethical risks]\n    D --\x3e D2[\u4e3aHRI\u8d21\u732e\u8bbe\u8ba1\u8003\u91cf\u4e0e\u4f26\u7406\u6307\u5357/Contributed HRI design considerations and ethical guidelines]\n    D --\x3e D3[\u53cd\u601d\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6cd5\u7684\u4ef7\u503c/Reflected on the value of speculative design]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Unsupervised Learning for Detection of Rare Driving Scenarios"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [anomaly detection], [Deep Isolation Forest, t-SNE, naturalistic driving data]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dat Le, Thomas Manhardt, Moritz Venator, Johannes Betz"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Munich (TUM), CARIAD SE"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23585",children:"https://arxiv.org/pdf/2512.23585"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an unsupervised learning framework using Deep Isolation Forest for detecting rare driving scenarios without labeled data. 2. Introduces a preprocessing pipeline that extracts structured statistical features from perception data using sliding windows. 3. Incorporates t-SNE for dimensionality reduction and visualization to improve the interpretability of detected anomalies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d1396541ab0766bc09a3986c6b01acba4cc137b0167e70049e1da565e969a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22d1396541ab0766bc09a3986c6b01acba4cc137b0167e70049e1da565e969a3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting rare and hazardous driving scenarios for autonomous systems. It proposes an unsupervised framework using Deep Isolation Forest on naturalistic driving data, which effectively identifies anomalies without relying on labeled datasets. The method provides a scalable solution, though it depends on proxy ground truth and manually defined features."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Unsupervised Learning for Detection of Rare Driving Scenarios"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Detecting rare and hazardous driving scenarios for autonomous systems"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Unsupervised framework using Deep Isolation Forest and t-SNE on naturalistic driving data"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Effectively identifies rare scenarios, offers a scalable anomaly detection solution"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [control systems], [Kalman Filter, Disturbance Observer, Steer-by-Wire, Driver Impedance, Torque Estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nikolai Beving, Jonas Marxen, Steffen Mueller, Johannes Betz"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Technical University of Berlin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23593",children:"https://arxiv.org/pdf/2512.23593"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Designed a Kalman filter-based disturbance observer to estimate high-frequency driver torque without using direct torque sensors. 2. Modeled the driver's passive torque as an extended state using a PT1-lag approximation within both linear and nonlinear system models. 3. Demonstrated that a nonlinear extended Kalman Filter outperforms a linear one in handling frictional nonlinearities during static-to-dynamic friction transitions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae20fc48cc0d99b531523c76958cd606e506aeb5f6e913a69a693c9835f5d2eb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae20fc48cc0d99b531523c76958cd606e506aeb5f6e913a69a693c9835f5d2eb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of high-frequency driver-induced disturbances in Steer-by-Wire systems. It proposes a Kalman filter-based disturbance observer that estimates driver torque using only motor state measurements, eliminating the need for costly direct torque sensors. The method was validated via simulation, showing accurate disturbance reconstruction with minimal delay (~14ms) and improved performance using a nonlinear extended Kalman filter for handling friction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: SbW\u7cfb\u7edf\u6613\u53d7\u9ad8\u9891\u9a7e\u9a76\u5458\u626d\u77e9\u5e72\u6270/SbW systems susceptible to high-frequency driver torque disturbances]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u6270\u52a8\u89c2\u6d4b\u5668/Kalman filter-based disturbance observer]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u51c6\u786e\u91cd\u6784\u5e72\u6270\uff0c\u5ef6\u8fdf\u7ea614ms\uff0c\u975e\u7ebf\u6027EKF\u6027\u80fd\u66f4\u4f18/Accurately reconstructs disturbances, ~14ms delay, nonlinear EKF performs better]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-robot interaction], [surface segmentation, mixed reality interface, task-centric programming, robot trajectory generation, user study]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Christoph Willibald, Lugh Martensen, Thomas Eiband, Dongheui Lee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," German Aerospace Center (DLR), University of L\xfcbeck, TU Wien"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23616",children:"https://arxiv.org/pdf/2512.23616"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel interactive robot programming approach using task-centric workflows for non-experts. 2. A new surface segmentation algorithm that incorporates human input and provides continuous visual feedback for iterative refinement. 3. An optimal mixed reality interface design, validated through user studies, that reduces workload and improves usability for surface finishing tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a561f172d18a77b56aecf8017c7bffbc65ae8d01334f01ff6a1aa8ff7c050bf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a561f172d18a77b56aecf8017c7bffbc65ae8d01334f01ff6a1aa8ff7c050bf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the barrier of complex robot programming for surface finishing tasks in small-scale manufacturing. It proposes a task-centric mixed reality interface where non-experts can intuitively program a robot by interactively segmenting workpiece surfaces and receiving visual feedback, with the system then generating the corresponding robot trajectory. User studies showed that this approach significantly reduces user workload and improves usability for effective task programming."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u673a\u5668\u4eba\u7f16\u7a0b\u8bbe\u7f6e\u590d\u6742\uff0c\u963b\u788d\u5728\u4e2d\u5c0f\u4f01\u4e1a\u4e2d\u90e8\u7f72/Robot programming setup is complex, hindering deployment in SMEs]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u9762\u5411\u4efb\u52a1\u7684\u6df7\u5408\u73b0\u5b9e\u754c\u9762\u4e0e\u4ea4\u4e92\u5f0f\u8868\u9762\u5206\u5272/Task-centric MR interface with interactive surface segmentation]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u964d\u4f4e\u7528\u6237\u5de5\u4f5c\u8d1f\u8377\uff0c\u63d0\u9ad8\u53ef\u7528\u6027/Reduced user workload, improved usability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [aerial robotics, control systems, geometric optimization], [fully-actuated multirotors, topological optimization, isotropy metric, phase locking, N-5 scaling law]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Antonio Franchi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Based on the author name \"Antonio Franchi\", the institution is not explicitly stated in the provided text. Common affiliations for this author include CNRS (Centre national de la recherche scientifique) and LAAS (Laboratoire d'analyse et d'architecture des syst\xe8mes) in Toulouse, France."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23619",children:"https://arxiv.org/pdf/2512.23619"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Formulates the geometric design of fully-actuated multirotors as a topological optimization problem on the product manifold of Projective Lines, moving beyond parametric optimization. 2. Discovers and formulates the "N-5 Scaling Law", an empirical relationship describing the topology of optimal configurations for regular polyhedral chassis. 3. Identifies a design redundancy enabling optimality-preserving morphing, allowing continuous vehicle reconfiguration without loss of isotropic control authority.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/867b87b0001ff54cd94ab728db5acbc6219646d0a24ca04d8913e9c5baa68162_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/867b87b0001ff54cd94ab728db5acbc6219646d0a24ca04d8913e9c5baa68162_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the optimal geometric design of fully-actuated multirotor drones. It formulates the problem as a topological optimization on a manifold and discovers that for symmetric chassis, the optimal solutions form continuous curves following an "N-5 Scaling Law". This reveals a design redundancy that allows the vehicle to morph its shape while maintaining optimal control performance.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Conventional parametric optimization for multirotor design misses the intrinsic structure of the solution space."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Formulate design on manifold (RP\xb2)^N, minimize Log-Volume isotropy metric."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Discovers N-5 Scaling Law & topology enabling optimality-preserving morphing."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [video-to-locomotion, visual motion intent, diffusion policy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23649",children:"https://arxiv.org/pdf/2512.23649"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework's effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RoboMirror: Video to Humanoid Locomotion] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u89c6\u89c9\u7406\u89e3\u4e0e\u63a7\u5236\u5b58\u5728\u9e3f\u6c9f/Gap between visual understanding and control]\n    B --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3/Existing methods lack genuine visual understanding]\n    C --\x3e C1[\u5229\u7528VLM\u63d0\u53d6\u89c6\u89c9\u8fd0\u52a8\u610f\u56fe/Use VLMs to distill visual motion intents]\n    C --\x3e C2[\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u751f\u6210\u52a8\u4f5c/Diffusion-based policy generates actions]\n    C --\x3e C3[\u65e0\u9700\u59ff\u6001\u91cd\u5efa\u6216\u91cd\u5b9a\u5411/No explicit pose reconstruction or retargeting]\n    D --\x3e D1[\u652f\u6301\u7b2c\u4e00\u4e0e\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u63a7\u5236/Supports egocentric & third-person control]\n    D --\x3e D2[\u964d\u4f4e80%\u63a7\u5236\u5ef6\u8fdf/Reduces control latency by 80%]\n    D --\x3e D3[\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53473.7%/3.7% higher task success rate]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [audio-to-locomotion, diffusion policy, retargeting-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," BAAI (Beijing Academy of Artificial Intelligence), University of Sydney, Harbin Institute of Technology, Hong Kong University of Science and Technology, Shanghai Jiao Tong University, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23650",children:"https://arxiv.org/pdf/2512.23650"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes RoboPerform, the first unified framework for directly generating music-driven dance and speech-driven co-speech gestures from audio for humanoid robots. 2. Introduces a novel "motion = content + style" principle, treating audio as implicit style signals to eliminate the need for explicit motion reconstruction and retargeting. 3. Designs a policy architecture integrating a ResMoE teacher for diverse motion patterns and a diffusion-based student for audio style injection, ensuring low latency and high fidelity.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384b6201b72bb7358f39e97637b20f414d5429fd79013e12be43d148e4e91c65_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384b6201b72bb7358f39e97637b20f414d5429fd79013e12be43d148e4e91c65_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of expressive, audio-reactive locomotion in humanoid robots by proposing RoboPerform, a unified framework that directly generates dance and co-speech gestures from audio without explicit motion reconstruction. The method treats audio as a style signal and uses a teacher-student policy with diffusion for style injection. Experiments show the approach achieves physically plausible and audio-aligned motions, enabling responsive robot performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Robots lack expressive, audio-reactive locomotion] --\x3e B1[\u5f53\u524d\u65b9\u6cd5\u5c40\u9650/Limitations<br>Predefined motions, retargeting errors, high latency]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>RoboPerform: Unified audio-to-locomotion framework] --\x3e C1[\u6838\u5fc3\u539f\u7406/Core Principle<br>Motion = Content + Style] --\x3e C2[\u6280\u672f\u67b6\u6784/Architecture<br>ResMoE Teacher + Diffusion Student]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Promising physical plausibility and audio alignment] --\x3e D1[\u4f18\u52bf/Advantages<br>Low latency, high fidelity, retargeting-free]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot navigation], [Artificial Potential Field, local minima, path planning, backfilling mechanism, ramp-based enhancement]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohammed Baziyad, Manal Al Shohna, Tamer Rabie"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Sharjah"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23672",children:"https://arxiv.org/pdf/2512.23672"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),'  1. Proposed the novel "Bulldozer" technique, which introduces a backfilling mechanism to systematically identify and eliminate local minima regions in APF-based navigation by increasing their potential values. 2. Incorporated a ramp-based enhancement to assist robots in escaping trap areas when they start within a local minimum, improving robustness. 3. Provided comprehensive experimental validation on a physical mobile robot, demonstrating superior execution speed and competitive path quality compared to standard APF, adaptive APF, A*, PRM, and RRT.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/102dc24d2d8cfbc8e6d1d7ea2397f8d21537a37a1ed1946ffbedecc40d34a0a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/102dc24d2d8cfbc8e6d1d7ea2397f8d21537a37a1ed1946ffbedecc40d34a0a3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the local minima trap problem in Artificial Potential Field (APF) based robot navigation by proposing the "Bulldozer" technique. The method uses a backfilling mechanism to eliminate local minima and a ramp-based enhancement to escape traps, preserving APF\'s advantages. Experimental results show it effectively solves the local minima issue while achieving fast execution and good path quality suitable for real-world use.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: APF\u65b9\u6cd5\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u9677\u9631/Local Minima Trap in APF]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63a8\u571f\u673a\u6280\u672f\uff1a\u56de\u586b\u673a\u5236\u4e0e\u659c\u5761\u589e\u5f3a/Bulldozer Technique: Backfilling & Ramp Enhancement]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u6709\u6548\u89e3\u51b3\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u6267\u884c\u901f\u5ea6\u5feb\uff0c\u8def\u5f84\u8d28\u91cf\u597d/Resolves Local Minima, Fast Execution, Good Path Quality]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Process Reward Model, Policy-Invariant Reward Shaping, Multi-Perspective Reward Fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wang, Yuheng Ji, Cheng Chi, Yaoxu Lyu, Zhongxia Zhao, Xiansheng Chen, Peterson Co, Shaoxuan Xie, Guocai Yao, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Beijing Academy of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23703",children:"https://arxiv.org/pdf/2512.23703"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://robo-dopamine.github.io",children:"https://robo-dopamine.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Dopamine-Reward, a method for learning a step-aware, general-purpose process reward model (GRM) from multi-view inputs to overcome perceptual limitations. 2. Proposes a theoretically-sound Policy-Invariant Reward Shaping method within the Dopamine-RL framework to enable efficient policy learning without altering the optimal policy. 3. Demonstrates high efficiency and generalization, where a one-shot adapted GRM enables policy learning to achieve 95% success with only 150 online rollouts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbdde8d7dea23f224b530580752926db8c72c9f5768172278573c890a3c6b0c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbdde8d7dea23f224b530580752926db8c72c9f5768172278573c890a3c6b0c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of designing effective reward functions for real-world robotic RL by introducing Robo-Dopamine. It proposes a general, step-aware reward model trained on a large dataset and a robust policy learning framework with theoretically-sound reward shaping. Experiments show the approach achieves state-of-the-art reward accuracy and significantly improves policy learning efficiency with strong generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: RL reward design is hard; existing PRMs lack step-awareness & use single-view, reward shaping is unsound]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Dopamine-Reward (GRM with Step-wise Discretization & Multi-Perspective Fusion) & Dopamine-RL (Policy-Invariant Reward Shaping)]\n    D[\u5173\u952e\u7ed3\u679c/Results: SOTA reward accuracy; High policy learning efficiency (95% success with 150 rollouts); Strong generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2026-01-01",children:"2026-01-01"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Break Out the Silverware -- Semantic Understanding of Stored Household Items"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [commonsense reasoning], [benchmark dataset, vision-language model, hybrid agent pipeline, storage location prediction, semantic understanding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Michaela Levi-Richter, Reuth Mirsky, Oren Glickman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bar Ilan University, Tufts University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23739",children:"https://arxiv.org/pdf/2512.23739"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [monocular depth estimation], [Depth Anything V2, DV-LORA, synthetic-to-real adaptation, SCARED dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ankan Aich, Yangming Lee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Rochester Institute of Technology (RIT)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23786",children:"https://arxiv.org/pdf/2512.23786"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Leveraged the high-fidelity synthetic priors of the Depth Anything V2 model to capture precise geometric details of thin structures in surgical scenes. 2. Efficiently adapted these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA) to minimize parameters and bridge the synthetic-to-real gap. 3. Introduced a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dd5c633cc3115f9e880a9737b5b76f77a1dcd45eb6b882e43363394c4566463_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dd5c633cc3115f9e880a9737b5b76f77a1dcd45eb6b882e43363394c4566463_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of monocular depth estimation in specular, fluid-filled surgical environments by adapting the synthetic-prior-rich Depth Anything V2 model to the medical domain using DV-LORA. It also proposes a new stratified evaluation protocol for high-specularity scenes. The method achieves state-of-the-art results on the SCARED dataset, demonstrating superior robustness in adverse lighting conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Leveraging Synthetic Priors for Monocular Depth Estimation<br>\u5229\u7528\u5408\u6210\u5148\u9a8c\u8fdb\u884c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[Problem: MDE fragile in specular surgical scenes<br>\u95ee\u9898\uff1a\u955c\u9762\u624b\u672f\u573a\u666f\u4e2dMDE\u8106\u5f31]\n    C[Method: Adapt Depth Anything V2 with DV-LORA<br>\u65b9\u6cd5\uff1a\u4f7f\u7528DV-LORA\u9002\u914dDepth Anything V2]\n    D[Results: SOTA on SCARED, 98.1% accuracy<br>\u7ed3\u679c\uff1a\u5728SCARED\u4e0a\u8fbe\u5230SOTA\uff0c98.1%\u51c6\u786e\u7387]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic manipulation], [factor graph, tactile sensing, pose estimation, contact estimation, visuo-tactile]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mark Van der Merwe, Kei Ota, Dmitry Berenson, Nima Fazeli, Devesh K. Jha"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Michigan, Mitsubishi Electric Research Laboratories (MERL)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23856",children:"https://arxiv.org/pdf/2512.23856"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," tacgraph.github.io"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A method for jointly estimating in-hand object pose and extrinsic contacts by integrating local tactile observations with physical constraints. 2. A formalization of the estimation problem as a factor graph that enforces kinematic and force constraints for quasi-static rigid body interactions. 3. Demonstrated superior performance compared to existing geometric and contact-informed pipelines, particularly when using only tactile information."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c89b1fc9ed9356303bd0e39a468b19e73c750a6281633c6145fe3a04e508003_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c89b1fc9ed9356303bd0e39a468b19e73c750a6281633c6145fe3a04e508003_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the ill-posed problem of estimating an object's pose and its contacts with the environment during in-hand manipulation. The proposed method, TacGraph, combines local tactile observations with physical constraints (kinematic and force) within a factor graph framework for efficient joint estimation. Experiments show it outperforms existing methods, especially when relying solely on tactile sensing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u4ec5\u89e6\u89c9\u611f\u77e5\u7684\u4f4d\u59ff\u4e0e\u63a5\u89e6\u4f30\u8ba1\u662f\u75c5\u6001\u95ee\u9898/Tactile-only pose & contact estimation is ill-posed]\nB --\x3e B2[\u89c6\u89c9\u53cd\u9988\u5b58\u5728\u566a\u58f0\u4e0e\u906e\u6321/Visual feedback suffers from noise & occlusion]\nC --\x3e C1[\u7ed3\u5408\u5c40\u90e8\u89e6\u89c9\u89c2\u6d4b\u4e0e\u7269\u7406\u7ea6\u675f/Pair local tactile observations with physical constraints]\nC --\x3e C2[\u6784\u5efa\u56e0\u5b50\u56fe\u786e\u4fdd\u8fd0\u52a8\u5b66\u4e0e\u529b\u7ea6\u675f/Formalize as factor graph for kinematic & force constraints]\nD --\x3e D1[\u4f18\u4e8e\u73b0\u6709\u51e0\u4f55\u4e0e\u63a5\u89e6\u611f\u77e5\u65b9\u6cd5/Outperforms existing geometric & contact-informed pipelines]\nD --\x3e D2[\u4ec5\u89e6\u89c9\u4fe1\u606f\u4e0b\u8868\u73b0\u4f18\u5f02/Especially effective with only tactile information]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic manipulation], [tactile sensing, vision-language-action models, hierarchical perception, world model, contact-rich manipulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Guo Ye, Zexi Zhang, Xu Zhao, Shang Wu, Haoran Lu, Shihan Lu, Han Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northwestern University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23864",children:"https://arxiv.org/pdf/2512.23864"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DreamTacVLA, a framework that grounds VLA models in contact physics using a hierarchical perception scheme with high-resolution tactile images, wrist-camera vision, and third-person vision. 2. Proposes a Hierarchical Spatial Alignment (HSA) loss to align tactile tokens with their spatial counterparts in visual views, creating a unified representation. 3. Finetunes the system with a tactile world model that predicts future tactile signals, enabling the agent to condition actions on anticipated contact dynamics, and constructs a hybrid large-scale dataset from digital twin and real-world sources to overcome data scarcity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a7e7908fa684e60efce68108a909c179516c57fb8ccd7e68353665652c969a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a7e7908fa684e60efce68108a909c179516c57fb8ccd7e68353665652c969a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of Vision-Language-Action (VLA) models in contact-rich manipulation tasks by introducing DreamTacVLA. The method integrates high-resolution tactile sensing with vision using hierarchical spatial alignment and a tactile prediction world model, trained on a hybrid dataset. It demonstrates superior performance, achieving up to 95% success, highlighting the importance of touch-aware reasoning for robust robotic agents."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VLA models are blind to physical contact, struggling with contact-rich tasks.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical perception with tactile, wrist, and third-person vision, aligned via HSA loss and finetuned with a tactile world model.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms VLA baselines, achieving up to 95% success on contact-rich manipulation tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic exploration], [LiDAR exploration, occupancy mapping, hybrid frontier, spherical projection, ray-casting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Liangtao Feng, Zhenchang Liu, Feng Zhang, Xuefeng Ren"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Zhuoyi Intelligent Tech Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23972",children:"https://arxiv.org/pdf/2512.23972"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an observation-quality occupancy map and performs ray-casting on it to handle inconsistent LiDAR point-cloud quality. 2. Introduces a hybrid frontier method to reduce computational burden and address limitations of point-cloud quality in exploration. 3. Designs an outward spherical-projection ray-casting strategy to ensure flight safety and exploration efficiency in open areas."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b4686bcb18a11b605083d99ba1c06ec4e088b72d6979aae06c19946dd29aa3e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b4686bcb18a11b605083d99ba1c06ec4e088b72d6979aae06c19946dd29aa3e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SHIELD, a method for efficient LiDAR-based drone exploration. It addresses challenges like inconsistent point-cloud quality and high computational cost by using an observation-quality occupancy map, a hybrid frontier method, and a spherical-projection ray-casting strategy. Simulations and flight experiments demonstrate its effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[SHIELD: LiDAR-based Drone Exploration] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[LiDAR\u70b9\u4e91\u8d28\u91cf\u4e0d\u4e00\u81f4/Inconsistent Point-Cloud Quality]\n    Problem --\x3e P2[\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u8d1f\u62c5\u91cd/High Computational Burden]\n    Problem --\x3e P3[\u5f00\u653e\u533a\u57df\u5b89\u5168\u4e0e\u6548\u7387/Safety & Efficiency in Open Areas]\n    Method --\x3e M1[\u89c2\u6d4b\u8d28\u91cf\u5360\u636e\u5730\u56fe/Observation-Quality Occupancy Map]\n    Method --\x3e M2[\u6df7\u5408\u8fb9\u754c\u65b9\u6cd5/Hybrid Frontier Method]\n    Method --\x3e M3[\u5916\u5411\u7403\u5f62\u6295\u5f71\u5c04\u7ebf\u6295\u5c04/Outward Spherical-Projection Ray-Casting]\n    Results --\x3e R1[\u4eff\u771f\u9a8c\u8bc1\u6709\u6548/Simulations Prove Effectiveness]\n    Results --\x3e R2[\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1/Flight Experiments Validate]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Evaluation of Impression Difference of a Domestic Mobile Manipulator with Autonomous and/or Remote Control in Fetch-and-Carry Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [human-robot interaction], [autonomous remote control, user-robot-operator triad, mobile manipulation, affinity, fetch-and-carry]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Takashi Yamamoto, Hiroaki Yaguchi, Shohei Kato, Hiroyuki Okada"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Toyota Motor Corporation, Nagoya Institute of Technology, Tamagawa University, Kushinada Tech. Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24029",children:"https://arxiv.org/pdf/2512.24029"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formalized the dual-agency structure of a service robot as a User-Robot-Operator triad in an autonomous remote-control setting. 2. Developed and evaluated an early-stage prototype interface combining natural-language text chat with freehand sketch annotations over a robot's live camera view for remote intervention. 3. Provided empirical evidence from controlled experiments showing systematic, mode-dependent differences in user-rated affinity (autonomous > hybrid > remote) and perceived security."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dd22c33e4c17332f65c8a5900aaa77e84ca89593caec86ae9813719341843f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dd22c33e4c17332f65c8a5900aaa77e84ca89593caec86ae9813719341843f2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how different control modes (autonomous, remote, hybrid) of a domestic mobile manipulator affect user impressions in fetch-and-carry tasks. The authors formalize the robot's dual agency and evaluate a prototype interface for remote intervention. The results show that user affinity is highest for autonomous mode, followed by hybrid and then remote control, offering guidance for designing human-in-the-loop mobile manipulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Evaluation of Impression Difference of a Domestic Mobile Manipulator") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u7528\u6237\u5bf9\u81ea\u4e3b/\u9065\u63a7/\u6df7\u5408\u6a21\u5f0f\u673a\u5668\u4eba\u7684\u5370\u8c61\u5dee\u5f02/User impression differences across robot control modes")\n    Method --\x3e M1("\u5f62\u5f0f\u5316\u7528\u6237-\u673a\u5668\u4eba-\u64cd\u4f5c\u5458\u4e09\u5143\u7ec4/Formalize User-Robot-Operator triad")\n    Method --\x3e M2("\u5f00\u53d1\u6587\u672c\u804a\u5929+\u8349\u56fe\u6807\u6ce8\u8fdc\u7a0b\u5e72\u9884\u539f\u578b/Develop text chat + sketch annotation prototype")\n    Method --\x3e M3("\u5728WRS\u6d4b\u8bd5\u573a\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c/Conduct controlled experiments on WRS test field")\n    Results --\x3e R1("\u4eb2\u548c\u529b\u8bc4\u7ea7: \u81ea\u4e3b > \u6df7\u5408 > \u9065\u63a7/Affinity rating: Autonomous > Hybrid > Remote")\n    Results --\x3e R2("\u611f\u77e5\u5b89\u5168\u6027\u5b58\u5728\u6a21\u5f0f\u5dee\u5f02/Perceived security differs by mode")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [adversarial attack], [diffusion models, monocular depth estimation, physical adversarial attack, Jacobian Vector Product Guidance, Salient Region Selection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yongtao Chen, Yanbo Wang, Wentao Zhao, Guole Shen, Tianchen Deng, Jingchuan Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24111",children:"https://arxiv.org/pdf/2512.24111"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a training-free, diffusion-based generative framework for creating naturalistic adversarial objects to attack Monocular Depth Estimation models. 2. Introduces a Salient Region Selection module to identify MDE-critical areas and a Jacobian Vector Product Guidance mechanism to align adversarial gradients with the diffusion model's capabilities. 3. Demonstrates through extensive experiments that the generated adversarial objects are more effective, stealthy, and physically deployable than prior texture-based attacks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/866f9f80528d36a9ff26173e3b4cfe5a8d9aaa0c968fa621b3f8bc4fe8d6694d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/866f9f80528d36a9ff26173e3b4cfe5a8d9aaa0c968fa621b3f8bc4fe8d6694d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the vulnerability of Monocular Depth Estimation (MDE) in autonomous driving by proposing a novel adversarial attack framework. It uses a guided diffusion model to generate scene-consistent, physically plausible adversarial objects, which are shown to be more effective and stealthy than existing patch-based attacks in both digital and physical experiments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("MDE\u6613\u53d7\u653b\u51fb/MDE is vulnerable")\n    Problem --\x3e P2("\u73b0\u6709\u653b\u51fb\u4e0d\u73b0\u5b9e/Existing attacks lack realism")\n    Method --\x3e M1("\u65e0\u8bad\u7ec3\u751f\u6210\u6846\u67b6/Training-free generative framework")\n    Method --\x3e M2("\u663e\u8457\u533a\u57df\u9009\u62e9/Salient Region Selection")\n    Method --\x3e M3("JVP\u5f15\u5bfc/Jacobian Vector Product Guidance")\n    Results --\x3e R1("\u9ad8\u653b\u51fb\u6709\u6548\u6027/High attack effectiveness")\n    Results --\x3e R2("\u9ad8\u9690\u853d\u6027/High stealthiness")\n    Results --\x3e R3("\u5f3a\u7269\u7406\u53ef\u90e8\u7f72\u6027/Strong physical deployability")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] RflyUT-Sim: A Simulation Platform for Development and Testing of Complex Low-Altitude Traffic Control"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sys], [simulation & testing], [UAV traffic simulation, high-fidelity simulation, RflySim/AirSim, Unreal Engine 5, oblique photogrammetry]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zonghan Li, Tianwen Tao, Rao Fu, Liang Wang, Dongyuan Zhang, Quan Quan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Beijing Jiaotong University, Huazhong University of Science and Technology, Beijing Intelligent Token Technology Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24112",children:"https://arxiv.org/pdf/2512.24112"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an integrated high-fidelity simulation platform for low-altitude UAV traffic that models all network components (control, traffic management, UAV, communication, anomaly modules). 2. Integrates RflySim/AirSim and Unreal Engine 5 to develop full-state UAV models and realistic 3D maps using oblique photogrammetry. 3. Provides a highly flexible and customizable platform with a wide range of interfaces and has released the source code to facilitate research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7213df61d3b3d213985977e50c5d3698d88b078a571db2a07082e7e3d6f4787a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7213df61d3b3d213985977e50c5d3698d88b078a571db2a07082e7e3d6f4787a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RflyUT-Sim, a high-fidelity simulation platform designed to address the challenges of testing complex low-altitude UAV traffic. It integrates RflySim/AirSim and Unreal Engine 5 to create realistic UAV models and 3D environments, simulating the entire traffic network. The platform's flexibility and released source code aim to lower the barrier for conducting low-altitude traffic research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RflyUT-Sim] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u6d4b\u8bd5\u6210\u672c\u9ad8/High cost of large-scale UAV testing]\n    B --\x3e B2[\u4f4e\u7a7a\u4ea4\u901a\u6d4b\u8bd5\u573a\u666f\u590d\u6742/Complexity of low-altitude traffic test scenarios]\n    B --\x3e B3[\u7f3a\u4e4f\u9ad8\u4fdd\u771f\u7efc\u5408\u6d4b\u8bd5\u5e73\u53f0/Lack of high-fidelity comprehensive testing platforms]\n    C --\x3e C1[\u96c6\u6210RflySim/AirSim\u4e0eUE5/Integrate RflySim/AirSim and Unreal Engine 5]\n    C --\x3e C2[\u6a21\u62df\u65e0\u4eba\u673a\u4ea4\u901a\u5168\u7ec4\u4ef6/Simulate all UAV traffic network components]\n    C --\x3e C3[\u63d0\u4f9b\u7075\u6d3b\u53ef\u5b9a\u5236\u63a5\u53e3/Provide flexible and customizable interfaces]\n    D --\x3e D1[\u53d1\u5e03\u5e73\u53f0\u6e90\u4ee3\u7801/Release platform source code]\n    D --\x3e D2[\u4fbf\u4e8e\u4f4e\u7a7a\u4ea4\u901a\u7814\u7a76/Facilitate low-altitude traffic research]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai], [Embodied Reasoning, Action Tokenization, Vision-Language-Action Models, Flow Matching, Discrete Control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yi Liu, Sukai Wang, Dafeng Wei, Xiaowei Cai, Linqing Zhong, Jiange Yang, Guanghui Ren, Jinyu Zhang, Maoqing Yao, Chuankang Li, Xindong He, Liliang Chen, Jianlan Luo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," AgiBot Research, Shanghai Innovation Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24125",children:"https://arxiv.org/pdf/2512.24125"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ERIQ, a large-scale benchmark for decoupled evaluation of embodied reasoning in robotic manipulation. 2. Proposes FACT, a flow-matching-based action tokenizer for high-fidelity discretization of continuous control. 3. Presents GenieReasoner, a unified model that jointly optimizes reasoning and action in a discrete space, outperforming baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1a2e4506fbecb3e90de4ef501197f9125c51ac19b6cfc789fdfcb6e035edf72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1a2e4506fbecb3e90de4ef501197f9125c51ac19b6cfc789fdfcb6e035edf72_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of combining broad generalization with precise execution in general-purpose robotics. It introduces a benchmark (ERIQ) to diagnose reasoning capabilities and a method (FACT) to tokenize actions, leading to a unified model (GenieReasoner) that improves performance on real-world tasks. The work provides a framework to overcome the reasoning-precision trade-off in robotic manipulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>VLA\u6a21\u578b\u96be\u4ee5\u517c\u987e\u6cdb\u5316\u4e0e\u7cbe\u786e\u6267\u884c"] --\x3e P1["\u6cdb\u5316\u4e0e\u7cbe\u5ea6\u6743\u8861<br>Reasoning-Precision Trade-off"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method"] --\x3e M1["\u8bc4\u4f30\u57fa\u51c6: ERIQ<br>Benchmark: ERIQ"]\n    Method --\x3e M2["\u52a8\u4f5c\u5206\u8bcd\u5668: FACT<br>Action Tokenizer: FACT"]\n    Method --\x3e M3["\u7edf\u4e00\u6a21\u578b: GenieReasoner<br>Unified Model: GenieReasoner"]\n    Results["\u5173\u952e\u7ed3\u679c/Results"] --\x3e R1["\u63ed\u793a\u4e86\u63a8\u7406\u80fd\u529b\u4e0e\u6cdb\u5316\u7684\u6b63\u76f8\u5173<br>Revealed positive correlation"]\n    Results --\x3e R2["\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf<br>Outperformed baselines in real-world tasks"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [intelligent transportation systems], [vehicular communications (V2X), social robotics, cooperative automated driving, vulnerable road users (VRU), mixed traffic]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Manuel Bied, John Arockiasamy, Andy Comeca, Maximilian Schrapel, Victoria Yang, Alexey Rolich, Barbara Bruno, Maike Schwammberger, Dieter Fiems, Alexey Vinel"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' Karlsruhe Institute of Technology (KIT), University of Roma "Sapienza", Ghent University']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24129",children:"https://arxiv.org/pdf/2512.24129"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the novel concept of using social robots as moderators between autonomous vehicles and vulnerable road users (VRUs) in mixed traffic scenarios. 2. Identifies and integrates four key technological enablers for this system: advanced perception, vehicular communications, social human-robot interaction, and formal specification. 3. Reports on a first proof-of-concept implementation integrating three of these enablers, demonstrating a social robot advising pedestrians interacting with a cooperative automated e-bike."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af48fdf6ae377ed4fa4165a70f67cab36319c5ee9529ebcd5c9663574a0eac3e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af48fdf6ae377ed4fa4165a70f67cab36319c5ee9529ebcd5c9663574a0eac3e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of safe interaction in mixed traffic involving autonomous vehicles and vulnerable road users. It proposes a system where a social robot acts as a moderator, integrating perception, V2X communication, and human-robot interaction. A proof-of-concept demonstration with an automated e-bike shows the feasibility of this approach for guiding pedestrians."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ROBOPOL: Social Robotics Meets Vehicular Communications] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6df7\u5408\u4ea4\u901a\u4e2d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u4ea4\u4e92/Mixed Traffic AV-VRU Interaction]\n    C --\x3e C1[\u793e\u4f1a\u673a\u5668\u4eba\u4f5c\u4e3a\u534f\u8c03\u8005/Social Robot as Moderator]\n    C1 --\x3e C2[\u56db\u5927\u4f7f\u80fd\u6280\u672f\u96c6\u6210/Four Enabler Integration]\n    C2 --\x3e C3[\u611f\u77e5, V2X\u901a\u4fe1, \u4eba\u673a\u4ea4\u4e92, \u5f62\u5f0f\u5316\u89c4\u8303/Perception, V2X, HRI, Formal Spec]\n    D --\x3e D1[\u6982\u5ff5\u9a8c\u8bc1: \u673a\u5668\u4eba\u5f15\u5bfc\u884c\u4eba\u4e0e\u81ea\u52a8\u7535\u52a8\u81ea\u884c\u8f66/Proof-of-Concept: Robot Advising Pedestrians with E-bike]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [semantic navigation], [zero-shot navigation, monocular camera, in-context learning, 3D foundation models, open-vocabulary]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ming-Ming Yu, Yi Chen, B\xf6rje F. Karlsson, Wenjun Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Beijing Academy of Artificial Intelligence (BAAI), Institute of Automation, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24212",children:"https://arxiv.org/pdf/2512.24212"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RANGER, a zero-shot semantic navigation framework that operates using only a monocular camera, eliminating the dependency on depth and pose sensors. 2. Introduces strong in-context learning capability, allowing the system to quickly adapt to new environments by observing a short video without architectural changes or fine-tuning. 3. Integrates key components like 3D reconstruction, semantic point cloud generation, and VLM-driven exploration into a cohesive framework, validated on benchmarks and real-world tests."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec071dfb8e64cc84b9968fc8e7e295d190c6ad5e4a190eaac70aeb44778d22a5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec071dfb8e64cc84b9968fc8e7e295d190c6ad5e4a190eaac70aeb44778d22a5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes RANGER, a monocular zero-shot semantic navigation framework that uses 3D foundation models to operate without depth or pose data and can quickly adapt to new environments via in-context learning from short videos. Experiments show it achieves competitive navigation performance and superior adaptability without prior 3D mapping."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RANGER: A Monocular Zero-Shot Semantic Navigation Framework] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f9d\u8d56\u6df1\u5ea6\u4e0e\u4f4d\u59ff/Depth & Pose Dependency]\n    B --\x3e B2[\u7f3a\u4e4f\u60c5\u5883\u5b66\u4e60\u80fd\u529b/Lack of ICL Capability]\n    C --\x3e C1[\u4ec5\u5355\u76ee\u76f8\u673a/Monocular Camera Only]\n    C --\x3e C2[3D\u57fa\u7840\u6a21\u578b/3D Foundation Models]\n    C --\x3e C3[\u60c5\u5883\u5b66\u4e60\u9002\u5e94/In-Context Learning Adaptation]\n    D --\x3e D1[\u7ade\u4e89\u6027\u5bfc\u822a\u6027\u80fd/Competitive Navigation Performance]\n    D --\x3e D2[\u4f18\u8d8a\u7684\u60c5\u5883\u5b66\u4e60\u9002\u5e94\u6027/Superior ICL Adaptability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] GR-Dexter Technical Report"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [vision-language-action (VLA), dexterous manipulation, bimanual teleoperation, cross-embodiment datasets, anthropomorphic robotic hand]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruoshi Wen, Guangzeng Chen, Zhongren Cui, Min Du, Yang Gou, Zhigang Han, Liqun Huang, Mingyu Lei, Yunfei Li, Zhuohang Li, Wenlei Liu, Yuxiao Liu, Xiao Ma, Hao Niu, Yutao Ouyang, Zeyu Ren, Haixin Shi, Wei Xu, Haoxiang Zhang, Jiajun Zhang, Xiao Zhang, Liwei Zheng, Weiheng Zhong, Yifei Zhou, Zhengming Zhu, Hang Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24210",children:"https://arxiv.org/pdf/2512.24210"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://byte-dexter.github.io/gr-dexter",children:"https://byte-dexter.github.io/gr-dexter"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Design of a compact, modular 21-DoF anthropomorphic robotic hand (ByteDexter V2) for dexterous manipulation. 2. Development of an intuitive bimanual teleoperation system for efficient real-robot data collection. 3. A training recipe that combines teleoperated trajectories with large-scale vision-language and curated cross-embodiment datasets to train a generalist VLA policy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f23a4ae875e18cbf460f2271eadc97907def817d036490b84d2f33bbe374f4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f23a4ae875e18cbf460f2271eadc97907def817d036490b84d2f33bbe374f4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents GR-Dexter, a framework for scaling vision-language-action models to bimanual robots with high-DoF dexterous hands. It integrates a new compact robotic hand design, a teleoperation system for data collection, and a training method using real and cross-embodiment data. The system demonstrates strong performance and improved generalization in real-world manipulation tasks, advancing towards generalist dexterous-hand robotics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GR-Dexter Technical Report] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[VLA\u6a21\u578b\u96be\u7528\u4e8e\u7075\u5de7\u624b / Scaling VLA to dexterous hands is hard]\n    B1 --\x3e B2[\u52a8\u4f5c\u7a7a\u95f4\u5927 / Large action space]\n    B1 --\x3e B3[\u906e\u6321\u4e25\u91cd / Severe occlusions]\n    B1 --\x3e B4[\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8 / High cost of real-robot data]\n    C --\x3e C1[\u786c\u4ef6 / Hardware]\n    C --\x3e C2[\u6570\u636e / Data]\n    C --\x3e C3[\u6a21\u578b / Model]\n    C1 --\x3e C1a[\u7d27\u51d121-DoF\u7075\u5de7\u624b / Compact 21-DoF dexterous hand]\n    C2 --\x3e C2a[\u53cc\u624b\u9065\u64cd\u4f5c\u7cfb\u7edf / Bimanual teleoperation system]\n    C3 --\x3e C3a[\u8bad\u7ec3\u914d\u65b9: \u9065\u64cd\u6570\u636e+\u5927\u89c4\u6a21VLM+\u8de8\u5177\u8eab\u6570\u636e / Training recipe: Teleop data + large-scale VLM + cross-embodiment data]\n    D --\x3e D1[\u5f3a\u9886\u57df\u5185\u6027\u80fd / Strong in-domain performance]\n    D --\x3e D2[\u5bf9\u672a\u89c1\u7269\u4f53\u548c\u6307\u4ee4\u66f4\u9c81\u68d2 / More robust to unseen objects & instructions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [control theory & optimization], [Heteroscedastic Bayesian Optimization, PID Tuning, UAV Trajectory Tracking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fuqiang Gu, Jiangshan Ai, Xu Lu, Xianlei Long, Yan Li, Tao Jiang, Chao Chen, Huidong Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chongqing University, Macquarie University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24249",children:"https://arxiv.org/pdf/2512.24249"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HBO-PID, a novel control algorithm integrating Heteroscedastic Bayesian Optimization with classical PID for UAV control. 2. Introduces explicit modeling of input-dependent noise variance to improve adaptation to dynamic environments. 3. Adopts a two-stage optimization strategy to accelerate the convergence of finding optimal controller parameters."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f2d1516402e9370a6abddfcf55734a6f894512a452d5f59baa27df4f4b8fcdd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f2d1516402e9370a6abddfcf55734a6f894512a452d5f59baa27df4f4b8fcdd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes HBO-PID, a novel control algorithm that combines Heteroscedastic Bayesian Optimization with PID control to improve UAV trajectory tracking. The method explicitly models noise variance and uses a two-stage optimization for efficiency. Experiments show it significantly outperforms state-of-the-art methods in both position and angular accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898/Paper Title: Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u65e0\u4eba\u673a\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u4e0d\u8db3/UAV Trajectory Tracking Accuracy & Robustness Limited]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5f02\u65b9\u5dee\u8d1d\u53f6\u65af\u4f18\u5316PID/Heteroscedastic Bayesian Optimization PID (HBO-PID)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u4f4d\u7f6e\u7cbe\u5ea6\u63d0\u534724.7%-42.9%, \u89d2\u5ea6\u7cbe\u5ea6\u63d0\u534740.9%-78.4%/Position Accuracy \u219124.7%-42.9%, Angular Accuracy \u219140.9%-78.4%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Local Path Optimization in The Latent Space Using Learned Distance Gradient"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic motion planning], [latent space, distance gradient, path optimization, constrained motion planning, manifold approximation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiawei Zhang, Chengchao Bai, Wei Pan, Tianhang Liu, Jifeng Guo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, The University of Manchester"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24272",children:"https://arxiv.org/pdf/2512.24272"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes training a neural network to predict robot-obstacle minimum distance directly from latent vectors. 2. Uses the learned distance gradient to compute movement directions in latent space for obstacle avoidance. 3. Integrates a local path optimization algorithm in the latent space with path validity checking to reduce replanning time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc41ceb1fda6fabc210e6299afabd747155a464bf4609e8d4a3646d04b8f2c52_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc41ceb1fda6fabc210e6299afabd747155a464bf4609e8d4a3646d04b8f2c52_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of slow replanning in data-driven constrained motion planning due to manifold approximation errors. It proposes a method that learns a distance gradient in the latent space to locally optimize paths and avoid obstacles, which is then integrated with validity checking to reduce overall planning time. The proposed method demonstrates faster planning speed compared to state-of-the-art algorithms in various scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Local Path Optimization in The Latent Space Using Learned Distance Gradient<br/>\u57fa\u4e8e\u5b66\u4e60\u8ddd\u79bb\u68af\u5ea6\u7684\u6f5c\u5728\u7a7a\u95f4\u5c40\u90e8\u8def\u5f84\u4f18\u5316"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u6f5c\u5728\u7a7a\u95f4\u78b0\u649e\u68c0\u6d4b\u56f0\u96be<br/>Difficulty in Latent Space Collision Detection"]\n    Problem --\x3e P2["\u8def\u5f84\u91cd\u89c4\u5212\u8017\u65f6<br/>Time-Consuming Path Replanning"]\n    Method --\x3e M1["\u5b66\u4e60\u8ddd\u79bb\u68af\u5ea6\u7f51\u7edc<br/>Learn Distance Gradient Network"]\n    Method --\x3e M2["\u6f5c\u5728\u7a7a\u95f4\u5c40\u90e8\u4f18\u5316<br/>Local Optimization in Latent Space"]\n    Method --\x3e M3["\u4e0e\u6709\u6548\u6027\u68c0\u67e5\u96c6\u6210<br/>Integrate with Validity Check"]\n    Results --\x3e R1["\u89c4\u5212\u901f\u5ea6\u6700\u5feb<br/>Fastest Planning Speed"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Safe Sliding Mode Control for Marine Vessels Using High-Order Control Barrier Functions and Fast Projection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics and control], [Sliding Mode Control, High-Order Control Barrier Functions, Fast Projection, Marine Vessel, Safety-Critical Control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Spyridon Syntakas, Kostas Vlachos"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' (Institution not explicitly stated in provided content; inferred from author names as potentially Greek, but no affiliation/email provided. Output is "Unknown")']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24281",children:"https://arxiv.org/pdf/2512.24281"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Integration of Sliding Mode Control (SMC) with High-Order Control Barrier Functions (HOCBFs) for robust and safe navigation of marine vessels under disturbances. 2. Introduction of a state-dependent adaptive HOCBF and a fast half-space projection method to replace the standard Quadratic Program (QP), enhancing computational efficiency. 3. Full implementation and evaluation on a complex, nonlinear 3-DOF marine dynamics model with thruster allocation, demonstrating real-time suitability for embedded systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e93241ece43aa775d49cb27eed10590c156c12c0d008f871757073e5824e3a5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e93241ece43aa775d49cb27eed10590c156c12c0d008f871757073e5824e3a5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new safe control framework for marine vessels that combines robust Sliding Mode Control (SMC) with safety-guaranteeing High-Order Control Barrier Functions (HOCBFs). A fast projection method is used to efficiently adjust the control input to avoid obstacles, replacing heavier optimization. The method is validated in simulations, showing robust, collision-free navigation suitable for real-time use on computationally constrained marine robots."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Safe Sliding Mode Control for Marine Vessels] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Marine vessel safe navigation under disturbances)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: SMC + HOCBF + Fast Projection)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Robust, safe, computationally efficient)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [temporal graph attention, hierarchical graph pooling, multi-modal fusion, UGV navigation, deep reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruitong Li, Lin Zhang, Yuenan Zhao, Chengxin Liu, Ran Song, Wei Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The affiliations are not explicitly provided in the given content. Based on the author names, it is not possible to reliably infer the main research institution(s)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24284",children:"https://arxiv.org/pdf/2512.24284"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a DRL-based navigation framework (DRL-TH) that integrates historical observations and adaptively fuses multi-modal information. 2. Introduced a Temporal-Guided Graph Attention Network (TG-GAT) to capture temporal context and scene evolution between consecutive frames. 3. Designed a Graph Hierarchical Abstraction Module (GHAM) to dynamically and balance multi-scale representations from RGB and LiDAR features."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83abe1de24f9a7e490d93db45dec754f2a2ff7f3de84d6e6983a358e1d9dff40_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83abe1de24f9a7e490d93db45dec754f2a2ff7f3de84d6e6983a358e1d9dff40_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DRL-TH, a deep reinforcement learning framework for UGV navigation in crowded environments. It addresses limitations of single-frame observation and simple fusion by introducing a temporal graph attention network and a hierarchical graph pooling module for adaptive multi-modal feature integration. Experiments and real-world deployment show that DRL-TH outperforms existing methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[DRL-TH: UGV\u5bfc\u822a\u6846\u67b6] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u5355\u5e27\u89c2\u6d4b/Single-frame observation]\n    Problem --\x3e P2[\u7b80\u5355\u591a\u6a21\u6001\u878d\u5408/Simple multi-modal fusion]\n    P1 --\x3e P1_Sub[\u9650\u5236\u52a8\u6001\u9002\u5e94\u6027/Limits dynamic adaptability]\n    P2 --\x3e P2_Sub[\u96be\u4ee5\u6355\u6349\u65f6\u5e8f\u4e0a\u4e0b\u6587/Hard to capture temporal context]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u65f6\u5e8f\u5f15\u5bfc\u56fe\u6ce8\u610f\u529b\u7f51\u7edc/Temporal-Guided GAT (TG-GAT)]\n    Method --\x3e M2[\u56fe\u5c42\u6b21\u62bd\u8c61\u6a21\u5757/Graph Hierarchical Abstraction Module (GHAM)]\n    M1 --\x3e M1_Sub[\u6355\u6349\u8fde\u7eed\u5e27\u5173\u8054/Captures correlations between consecutive frames]\n    M2 --\x3e M2_Sub[\u52a8\u6001\u878d\u5408RGB\u4e0eLiDAR\u7279\u5f81/Dynamically fuses RGB & LiDAR features]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Outperforms existing methods]\n    Results --\x3e R2[\u771f\u5b9eUGV\u4e0a\u8868\u73b0\u826f\u597d/Performs well on real UGV]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Real-world Reinforcement Learning from Suboptimal Interventions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [human-in-the-loop RL, constrained RL, state-wise Lagrangian, suboptimal interventions, robotic manipulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yinuo Zhao, Huiqian Jin, Lechun Jiang, Xinyi Zhang, Kun Wu, Pei Ren, Zhiyuan Xu, Zhengping Che, Lei Sun, Dapeng Wu, Chi Harold Liu, Jian Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Innovation Center of Humanoid Robotics, City University of Hong Kong, Nankai University, Beijing Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24288",children:"https://arxiv.org/pdf/2512.24288"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SiLRI, a state-wise Lagrangian RL algorithm that formulates the online manipulation problem as a constrained RL optimization where constraint bounds are determined by the uncertainty of human interventions. 2. Introduces a state-wise Lagrange multiplier and solves the problem via a min-max optimization to jointly optimize the policy and the multiplier, enabling exploitation of suboptimal interventions without being constrained by them. 3. Demonstrates through real-world experiments that SiLRI significantly accelerates learning, reducing time to 90% success rate by at least 50% compared to prior methods and achieving 100% success on long-horizon tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f15ba64d7b2d7760e2411270a47d97305fe1e21ad798c2423cb89a0e5cb5750_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f15ba64d7b2d7760e2411270a47d97305fe1e21ad798c2423cb89a0e5cb5750_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of leveraging suboptimal human interventions to accelerate real-world robotic RL without being limited by them. It proposes SiLRI, a state-wise Lagrangian RL algorithm that treats interventions as state-dependent constraints and solves a min-max optimization. Real-world experiments show SiLRI cuts learning time by over 50% and achieves perfect success on complex tasks where other methods fail."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Real-world Reinforcement Learning from Suboptimal Interventions] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u5229\u7528\u53ef\u80fd\u6b21\u4f18\u7684\u4eba\u7c7b\u5e72\u9884\u52a0\u901f\u5b66\u4e60\u800c\u4e0d\u53d7\u5176\u9650\u5236/How to leverage potentially suboptimal human interventions to accelerate learning without being constrained by them]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faSiLRI\uff0c\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u62c9\u683c\u6717\u65e5\u7684RL\u7b97\u6cd5\uff0c\u5c06\u5e72\u9884\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u7ea6\u675f/Propose SiLRI, a state-wise Lagrangian RL algorithm treating intervention uncertainty as constraints]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5b66\u4e60\u901f\u5ea6\u63d0\u5347>50%\uff0c\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u8fbe\u5230100%\u6210\u529f\u7387/Learning speed improved >50%, achieved 100% success rate on long-horizon tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic manipulation], [dexterous hand manipulation, human-centric data, multi-modal dataset, policy generalization, motion capture]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," TARS Robotics, Yuhang Zheng, Jichao Peng, Weize Li, Yupeng Zheng, Xiang Li, Yujie Jin, Julong Wei, Guanhua Zhang, Ruiling Zheng, Ming Cao, Songen Gu, Zhenhong Zou, Kaige Li, Ke Wu, Mingmin Yang, Jiahao Liu, Pengfei Li, Hengjie Si, Feiyu Zhu, Wang Fu, Likun Wang, Ruiwen Yao, Jieru Zhao, Yilun Chen, Wenchao Din"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TARS Robotics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24310",children:"https://arxiv.org/pdf/2512.24310"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/tars-robotics/World-In-Your-Hands",children:"https://github.com/tars-robotics/World-In-Your-Hands"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture. 2. Presents the WiYH Dataset, a large-scale, multi-modal dataset with over 1,000 hours of human manipulation data across diverse real-world scenarios and hundreds of skills. 3. Provides extensive annotations and benchmarks to support a wide range of tasks from perception to action, forming an open-source ecosystem."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0db53b26bed90b683311e986846432d80dee2b3576c54541403be5222bf2b65f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0db53b26bed90b683311e986846432d80dee2b3576c54541403be5222bf2b65f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces World In Your Hands (WiYH), a large-scale open-source ecosystem for learning human-centric manipulation. It addresses the lack of diverse and large-scale data for dexterous hand policies by providing a novel data collection system, a massive multi-modal dataset, and supporting benchmarks. Experiments show that using WiYH's data significantly improves the generalization and robustness of manipulation policies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["World In Your Hands (WiYH)"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Limited scale & diversity of dexterous hand manipulation data"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Large-scale open-source ecosystem with data collection kit, dataset, and benchmarks"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Enhanced policy generalization and robustness"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai / robot learning], [multimodal motion generation, action streaming, discrete codebook, FSQ (Finite Scalar Quantization), zero-shot tracking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Beijing Institute for General Artificial Intelligence (BIGAI), Huazhong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24321",children:"https://arxiv.org/pdf/2512.24321"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://jnnan.github.io/uniact/",children:"https://jnnan.github.io/uniact/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified two-stage framework (UniAct) that integrates a fine-tuned MLLM with a causal streaming pipeline for low-latency (<500ms) multimodal instruction execution. 2. The use of a shared discrete codebook via FSQ to unify heterogeneous inputs (language, music, trajectory, motion) and constrain motions to a physically grounded manifold. 3. Introduction of the UniMoCap benchmark and demonstration of robust generalization, including a 19% improvement in zero-shot tracking success rate."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d71f14808bc6cc1375b432715e3356f7f970e4a056f6960261c0aed51ae956aa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d71f14808bc6cc1375b432715e3356f7f970e4a056f6960261c0aed51ae956aa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes UniAct, a unified framework for generating and streaming motions to humanoid robots from diverse multimodal instructions like language and music. It uses a fine-tuned MLLM and a shared discrete codebook to translate instructions into actions with low latency. The method shows improved zero-shot motion tracking and robust generalization on a new benchmark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Bridging high-level multimodal perception with whole-body execution for humanoid robots]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Two-stage framework with fine-tuned MLLM and causal streaming pipeline, using shared discrete codebook (FSQ)]\n    D[\u5173\u952e\u7ed3\u679c/Results: Sub-500 ms latency, 19% improvement in zero-shot tracking, robust generalization on UniMoCap benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] 3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [autonomous systems guidance and control], [nonlinear model predictive control, path-following guidance, fixed-wing UAS, system identification, model predictive contouring control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Camron Alexander Hirst, Chris Reale, Eric Frew"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, Charles Stark Draper Laboratory Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24326",children:"https://arxiv.org/pdf/2512.24326"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Presents two novel 3D path-following guidance algorithms based on nonlinear MPC for fixed-wing small UAS. 2. Introduces a control-augmented model and system identification for the RAAVEN aircraft to enable MPC. 3. Formulates one MPC to schedule a static path rate and another, inspired by contouring control, to dynamically optimize the path rate for a trade-off between path progression and tracking error."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c481097117513c6e7ec0ae58ca10d3f59dece31fd2874b432bc9d53ad5df59_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c481097117513c6e7ec0ae58ca10d3f59dece31fd2874b432bc9d53ad5df59_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of high-performance 3D path-following for fixed-wing uncrewed aircraft by proposing two nonlinear model predictive control (MPC) guidance algorithms. One uses a static path rate, while the other dynamically optimizes it, allowing a trade-off between speed and accuracy. Flight tests demonstrate the real-world feasibility and superior performance of these MPC methods compared to a baseline lookahead guidance law."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["3D Path-Following Guidance via Nonlinear MPC for FW-sUAS<br>\u57fa\u4e8e\u975e\u7ebf\u6027MPC\u7684\u56fa\u5b9a\u7ffc\u5c0f\u578b\u65e0\u4eba\u673a\u4e09\u7ef4\u8def\u5f84\u8ddf\u8e2a\u5236\u5bfc"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>High-performance 3D path-following for fixed-wing UAS is challenging<br>\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u9ad8\u6027\u80fd\u4e09\u7ef4\u8def\u5f84\u8ddf\u8e2a\u5177\u6709\u6311\u6218\u6027"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Two Nonlinear MPC guidance algorithms<br>\u4e24\u79cd\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5236\u5bfc\u7b97\u6cd5"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Real-world feasibility & superior performance shown in flight tests<br>\u98de\u884c\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\u4e0e\u4f18\u8d8a\u6027\u80fd"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] New Insights into Cascaded Geometric Flight Control: From Performance Guarantees to Practical Pitfalls"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [flight control, nonlinear control, geometric control], [cascaded geometric control, sliding variables, quaternion-based control, trajectory tracking, stability proof]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Brett T. Lopez"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24377",children:"https://arxiv.org/pdf/2512.24377"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A new stability proof for cascaded geometric flight control using sliding variables, demonstrating exponential convergence for position trajectory tracking., 2. Analysis revealing the influence of attitude tracking error on the position control loop, elucidating the complex coupling between the loops., 3. Investigation of the control architecture's robustness to model uncertainties and identification of its practical pitfalls."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5000c178eda34f5f3aa41a7fd18d3b45973e49250588ed5efd005f8ad62f0ae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5000c178eda34f5f3aa41a7fd18d3b45973e49250588ed5efd005f8ad62f0ae_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a new stability analysis for cascaded geometric controllers used by aerial vehicles. By integrating a quaternion-based sliding controller into the cascaded architecture, the authors prove exponential convergence for position trajectory tracking and gain new insights into the coupling between control loops and system robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[New Insights into Cascaded Geometric Flight Control] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Precise trajectory tracking for aerial vehicles with nonlinear, coupled dynamics)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: New stability proof combining cascaded geometric control with sliding variables and a quaternion-based sliding controller)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Proves exponential convergence; reveals attitude-position error coupling and practical pitfalls)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Geometric Multi-Session Map Merging with Learned Local Descriptors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [SLAM (Simultaneous Localization and Mapping)], [map merging, learned local descriptors, geometric transformer, factor-graph optimization, loop closure detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA), DEVCOM Army Research Laboratory (ARL)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24384",children:"https://arxiv.org/pdf/2512.24384"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GMLD, a learning-based local descriptor framework for multi-session point cloud map merging. 2. Introduces a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for robust loop closure detection and pose estimation. 3. Incorporates inter-session scan matching cost factors into factor-graph optimization to enhance global map consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c330f606af1c67ed97c26c40d98ecfd5f18fd1d10de890504cf057e102199d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c330f606af1c67ed97c26c40d98ecfd5f18fd1d10de890504cf057e102199d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of merging point cloud maps from multiple sessions or agents in large-scale environments. The proposed GMLD framework uses learned local descriptors and a geometric transformer for feature extraction, combined with factor-graph optimization for global consistency. Experimental results on public and self-collected datasets demonstrate accurate and robust map merging performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Geometric Multi-Session Map Merging with Learned Local Descriptors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Multi-session map merging in large-scale environments)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: GMLD framework with learned descriptors & geometric transformer)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Accurate and robust map merging with low error)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-modal perception], [spatial intelligence, multi-modal pre-training, 3D object detection, semantic occupancy prediction, foundation models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, National University of Singapore, Nanjing University of Aeronautics and Astronautics, Alibaba Group, Singapore Management University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24385",children:"https://arxiv.org/pdf/2512.24385"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/worldbench/awesome-spatial-intelligence",children:"https://github.com/worldbench/awesome-spatial-intelligence"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formulates a unified taxonomy for multi-modal pre-training paradigms, from single-modality to unified frameworks. 2. Investigates the integration of textual inputs and occupancy representations for open-world perception and planning. 3. Identifies critical bottlenecks (e.g., computational efficiency) and proposes a roadmap towards general-purpose multi-modal foundation models for robust Spatial Intelligence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d3c33bdbff1adb3e8892d63c2303634ba223ae7e14138e8573b929718fecafc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d3c33bdbff1adb3e8892d63c2303634ba223ae7e14138e8573b929718fecafc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of integrating diverse sensor data (e.g., cameras, LiDAR) to achieve Spatial Intelligence for autonomous systems. It proposes a comprehensive framework and taxonomy for multi-modal pre-training, analyzing techniques for unified representation learning and identifying future research directions. The main conclusion is a roadmap towards building general-purpose multi-modal foundation models capable of robust real-world perception and planning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems<br>\u6253\u9020\u7a7a\u95f4\u667a\u80fd\uff1a\u81ea\u4e3b\u7cfb\u7edf\u591a\u6a21\u6001\u6570\u636e\u9884\u8bad\u7ec3\u8def\u7ebf\u56fe"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Integrating multi-modal sensor data for unified Spatial Intelligence<br>\u878d\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u4ee5\u5b9e\u73b0\u7edf\u4e00\u7684\u7a7a\u95f4\u667a\u80fd"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Comprehensive framework & unified taxonomy for multi-modal pre-training<br>\u5168\u9762\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\u4e0e\u7edf\u4e00\u5206\u7c7b\u6cd5"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Roadmap for general-purpose multi-modal foundation models<br>\u901a\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u8def\u7ebf\u56fe"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [Functional Mockup Unit (FMU), fault injection, Continuous Integration/Continuous Delivery (CI/CD)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Giovanni Lambertini, Matteo Pini, Eugenio Mascaro, Francesco Moretti, Ayoub Raji, Marko Bertogna"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Modena and Reggio Emilia"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24402",children:"https://arxiv.org/pdf/2512.24402"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An automated simulation and reporting pipeline for an autonomous racing stack that can execute up to three times faster than real-time, locally or on GitHub for CI/CD. 2. A fault injection module capable of introducing sensor delays, perturbations, and modifying outputs of any node in the software stack. 3. A design for an automated reporting process aimed at maximizing the effectiveness of simulation analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d172287ab1ffde29e52c3f7cde1a986f7a5d73ac371395320ced490d39f5dac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d172287ab1ffde29e52c3f7cde1a986f7a5d73ac371395320ced490d39f5dac_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an automated simulation and reporting pipeline for validating an autonomous racing stack. The method uses a high-fidelity vehicle model as a Functional Mockup Unit (FMU) and includes a fault injection module to test system robustness. The pipeline enables fast, realistic scenario testing and automated reporting, which is crucial for efficiently validating critical autonomous driving functions like high-speed overtaking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Need for efficient validation of autonomous racing stack modules, especially for high-speed maneuvers and localization."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Automated simulation pipeline using high-fidelity FMU model, scenario initialization, and fault injection."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Pipeline executes up to 3x faster than real-time, supports CI/CD, and includes automated reporting."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Subsecond 3D Mesh Generation for Robot Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [3D mesh generation, open-vocabulary segmentation, point cloud registration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qian Wang, Omar Abdellall, Tony Gao, Xiatao Sun, Daniel Rakita"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24428",children:"https://arxiv.org/pdf/2512.24428"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An end-to-end system for generating a contextually grounded 3D mesh from a single RGB-D image in under one second. 2. Integration of open-vocabulary segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration into a single optimized pipeline. 3. Demonstration of the system's effectiveness in enabling real-world robot manipulation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c21c85b85a842e2b91805631063ff03077c9e225bd56663524ef2ce7b37de98b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c21c85b85a842e2b91805631063ff03077c9e225bd56663524ef2ce7b37de98b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a fast, end-to-end system that generates a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. The method integrates open-vocabulary segmentation, accelerated diffusion-based mesh generation, and point cloud registration. The system enables the practical use of meshes for real-time robotic perception and planning, as demonstrated in a manipulation task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Subsecond 3D Mesh Generation for Robot Manipulation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u751f\u6210\u6162\u4e14\u7f3a\u4e4f\u573a\u666f\u5173\u8054/Slow generation & lack of contextual grounding]\n    C --\x3e C1[\u5f00\u653e\u8bcd\u6c47\u5206\u5272/Open-vocabulary segmentation]\n    C --\x3e C2[\u52a0\u901f\u6269\u6563\u7f51\u683c\u751f\u6210/Accelerated diffusion mesh generation]\n    C --\x3e C3[\u70b9\u4e91\u914d\u51c6/Point cloud registration]\n    D --\x3e D1[<1\u79d2\u751f\u6210/<1 second generation]\n    D --\x3e D2[\u652f\u6301\u673a\u5668\u4eba\u64cd\u4f5c/Enables robot manipulation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [autonomous driving], [counterfactual reasoning, vision-language-action, self-reflective model, adaptive reasoning, trajectory generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"}),' Zhenghao "Mark" Peng, Wenhao Ding, Yurong You, Yuxiao Chen, Wenjie Luo, Thomas Tian, Yulong Cao, Apoorva Sharma, Danfei Xu, Boris Ivanovic, Boyi Li, Bolei Zhou, Yan Wang, Marco Pavone']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA, UCLA, Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24426",children:"https://arxiv.org/pdf/2512.24426"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Counterfactual VLA (CF-VLA), a self-reflective framework that enables an autonomous driving model to reason about and revise its planned actions before execution through counterfactual reasoning. 2. Proposes a rollout-filter-label pipeline to efficiently mine high-value scenes and label counterfactual reasoning traces for training, enabling the acquisition of self-reflective capabilities. 3. Demonstrates significant improvements in trajectory accuracy (up to 17.6%) and safety metrics (20.5%) on large-scale driving datasets, with adaptive reasoning that activates primarily in challenging scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e3c4319bff8eb33d06cd671495be918c900f2aeecaabdc7cd1ff11f2ddb7bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e3c4319bff8eb33d06cd671495be918c900f2aeecaabdc7cd1ff11f2ddb7bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of current reasoning-augmented Vision-Language-Action (VLA) models, which lack self-reflection on action safety. It proposes Counterfactual VLA (CF-VLA), a framework that first generates meta-actions, then performs counterfactual reasoning to simulate outcomes and correct unsafe plans before final trajectory generation. Experiments show CF-VLA significantly improves trajectory accuracy and safety while adaptively engaging reasoning only in complex scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Counterfactual VLA] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709VLA\u7f3a\u4e4f\u81ea\u6211\u53cd\u601d/Existing VLAs lack self-reflection]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u751f\u6210\u5143\u52a8\u4f5c/Generate Meta-actions]\n    Method --\x3e M2[\u53cd\u4e8b\u5b9e\u63a8\u7406/Counterfactual Reasoning]\n    M2 --\x3e M2_1[\u6a21\u62df\u7ed3\u679c/Simulate Outcomes]\n    M2 --\x3e M2_2[\u4fee\u6b63\u8ba1\u5212/Correct Plans]\n    Method --\x3e M3[\u8bad\u7ec3\u6d41\u7a0b: rollout-filter-label/Training Pipeline]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u63d0\u5347\u8f68\u8ff9\u7cbe\u5ea6/Improves Trajectory Accuracy]\n    Results --\x3e R2[\u589e\u5f3a\u5b89\u5168\u6307\u6807/Enhances Safety Metrics]\n    Results --\x3e R3[\u81ea\u9002\u5e94\u63a8\u7406/Adaptive Reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-language model (VLM), fallback maneuver, semantic hazard detection, autonomous surface vessel (ASV), IMO MASS Code]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NTNU (Norwegian University of Science and Technology), Stanford University, NVIDIA Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24470",children:"https://arxiv.org/pdf/2512.24470"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector for maritime autonomy. 2. Introduces a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback to meet IMO MASS Code requirements. 3. Demonstrates that sub-10-second VLM models retain semantic awareness and outperform geometry-only baselines in hazard scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd093268c75343899a9f72b23f4d22948c1b634539b5e39f8d26296c9f122886_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd093268c75343899a9f72b23f4d22948c1b634539b5e39f8d26296c9f122886_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of semantic hazard detection for autonomous maritime vessels, which is required by the draft IMO MASS Code. It proposes Semantic Lookout, a vision-language model (VLM) based system that selects safe fallback maneuvers by understanding scene semantics. The results show that this approach is effective within practical latency budgets and outperforms traditional geometry-only methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Foundation models on the bridge / \u8bba\u6587\u6807\u9898"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u81ea\u4e3b\u8239\u8236\u9700\u68c0\u6d4b\u8bed\u4e49\u5371\u5bb3 / Autonomous vessels need semantic hazard detection"]\n    Problem --\x3e P2["\u4f20\u7edf\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u8bed\u4e49\u5f02\u5e38 / Classical stacks struggle with semantic OOD situations"]\n    Method --\x3e M1["\u5f15\u5165Semantic Lookout / Introduce Semantic Lookout"]\n    Method --\x3e M2["\u57fa\u4e8eVLM\u7684\u5907\u7528\u673a\u52a8\u9009\u62e9\u5668 / VLM-based fallback maneuver selector"]\n    Method --\x3e M3["\u5feb\u901f-\u6162\u901f\u5f02\u5e38\u5904\u7406\u6d41\u7a0b / Fast-slow anomaly pipeline"]\n    Results --\x3e R1["10\u79d2\u5185\u6a21\u578b\u4fdd\u6301\u8bed\u4e49\u611f\u77e5 / Sub-10 s models retain semantic awareness"]\n    Results --\x3e R2["\u4f18\u4e8e\u51e0\u4f55\u57fa\u7ebf / Outperforms geometry-only baselines"]\n    Results --\x3e R3["\u652f\u6301IMO MASS\u6cd5\u89c4 / Supports draft IMO MASS Code"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Energy-Aware Bayesian Control Barrier Functions for Physics-Informed Gaussian Process Dynamics"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [safe control], [Control Barrier Functions, Gaussian Processes, Port-Hamiltonian Systems, Bayesian Inference, Energy-Aware Safety]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chi Ho Leung, Philip E. Par\xe9"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Purdue University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24493",children:"https://arxiv.org/pdf/2512.24493"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed a Bayesian-CBF framework for systems with learned dynamics, 2. Instantiated the framework with Energy-aware Bayesian CBFs (EB-CBFs) that construct safety barriers directly from Hamiltonian and vector-field posteriors, 3. Provided probabilistic safety guarantees for energy-constrained systems under GP-learned dynamics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/accb1ff5f1fd393d69f125ce4556fd07682e69ca503aa2b01d602950d0a62c50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/accb1ff5f1fd393d69f125ce4556fd07682e69ca503aa2b01d602950d0a62c50_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses safe control for dynamical systems with learned Gaussian Process dynamics, focusing on mechanical and port-Hamiltonian systems. It proposes a novel Bayesian Control Barrier Function framework, specifically Energy-aware Bayesian CBFs (EB-CBFs), which use the GP posterior over the Hamiltonian to construct energy-based safety filters. Numerical simulations on a mass-spring system demonstrate that the method achieves high-probability safety guarantees."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Energy-Aware Bayesian Control Barrier Functions] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: How to design an energy-aware CBF with high-probability safety guarantees for systems with GP-learned dynamics?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Develop Bayesian-CBF framework and instantiate with Energy-aware Bayesian CBFs (EB-CBFs) using Hamiltonian/vector-field posteriors]\n    Results[\u5173\u952e\u7ed3\u679c/Results: EB-CBFs achieve high-probability safety as a minimal-modification safety filter]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [world models], [joint-embedding predictive architecture, representation space planning, model-based reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meta FAIR, INRIA Paris, Ecole normale sup\xe9rieure/PSL, New York University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24497",children:"https://arxiv.org/pdf/2512.24497"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/facebookresearch/jepa-wms",children:"https://github.com/facebookresearch/jepa-wms"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a comprehensive characterization and study of Joint-Embedding Predictive Architecture World Models (JEPA-WMs) for physical planning. 2. Systematically investigates the impact of model architecture, training objective, and planning algorithm on planning success in simulated and real-world robotic tasks. 3. Combines the findings to propose a new model that outperforms established baselines (DINO-WM and V-JEPA-2-AC) in navigation and manipulation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the key factors for successful physical planning using Joint-Embedding Predictive World Models (JEPA-WMs). It conducts a systematic study of architectural and algorithmic choices within this family of methods and proposes a new model that achieves superior performance on navigation and manipulation tasks compared to existing baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[What Drives Success in Physical Planning with JEPA-WMs?] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>How to build agents that generalize to new physical tasks?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Study JEPA-WMs: architecture, objective, planning algorithm]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>Proposed model outperforms baselines (DINO-WM, V-JEPA-2-AC)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic grasping], [surface fitting, grasp planning, contact stability, center of mass, iterative optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tomoya Yamanokuchi, Alberto Bacchin, Emilio Olivastri, Ryotaro Arifuku, Takamitsu Matsubara, Emanuele Menegatti"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nara Institute of Science and Technology, University of Padua"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24550",children:"https://arxiv.org/pdf/2512.24550"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel disentangled iterative surface fitting algorithm that integrates contact stability with geometric compatibility. 2. A three-step grasp pose optimization method inspired by human grasping, sequentially optimizing rotation, translation, and gripper aperture. 3. Comprehensive validation in simulation and real-world experiments, demonstrating improved grasp success by reducing CoM misalignment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dba548fac0cf4df4540c9b30eb28e047af13ce7f6fd890664a639281626ce49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dba548fac0cf4df4540c9b30eb28e047af13ce7f6fd890664a639281626ce49_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unstable grasps in surface fitting-based grasp planning by proposing DISF, a method that disentangles grasp pose optimization into sequential steps for rotation, translation, and aperture adjustment to improve contact stability and align with the object's center of mass. The approach is validated in simulation and real-world experiments, showing higher grasp success rates compared to baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[DISF: Disentangled Iterative Surface Fitting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u8868\u9762\u62df\u5408\u6293\u53d6\u89c4\u5212\u5ffd\u7565\u63a5\u89e6\u7a33\u5b9a\u6027/Surface-fitting grasp planning overlooks contact stability]\nC --\x3e C1[\u89e3\u8026\u4e09\u6b65\u4f18\u5316/Disentangled three-step optimization]\nC1 --\x3e C1_1[1. \u65cb\u8f6c\u4f18\u5316:\u5bf9\u9f50\u6cd5\u7ebf/Rotation: Align normals]\nC1 --\x3e C1_2[2. \u5e73\u79fb\u4f18\u5316:\u5bf9\u9f50\u8d28\u5fc3/Translation: Align to CoM]\nC1 --\x3e C1_3[3. \u5f00\u5408\u8c03\u6574:\u4f18\u5316\u63a5\u89e6\u70b9/Aperture: Optimize contact points]\nD --\x3e D1[\u51cf\u5c11\u8d28\u5fc3\u504f\u5dee\uff0c\u4fdd\u6301\u51e0\u4f55\u517c\u5bb9\u6027/Reduced CoM misalignment, preserved geometric compatibility]\nD --\x3e D2[\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u6293\u53d6\u6210\u529f\u7387\u66f4\u9ad8/Higher grasp success in sim & real-world]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot manipulation], [state ambiguity, working memory recoding, visuomotor policy, imitation learning, temporal disambiguation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qingda Hu, Ziheng Qiu, Zijun Xu, Kaizhao Zhang, Xizhou Bu, Zuolei Sun, Bo Zhang, Jieru Zhao, Zhongxue Gan, Wenchao Ding"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fudan University, Westwell, Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24638",children:"https://arxiv.org/pdf/2512.24638"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://tinda24.github.io/pam/",children:"https://tinda24.github.io/pam/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PAM, a novel visuomotor policy with adaptive working memory that efficiently handles long history windows (up to 300 frames) for resolving state ambiguity in robot manipulation. 2. Proposes a two-stage training method with a hierarchical feature extractor and a context router with range-specific queries to produce compact historical representations while maintaining high inference speed. 3. Designs an auxiliary reconstruction objective to ensure the context router acts as an effective bottleneck and validates the method on 7 meticulously designed tasks covering multiple state ambiguity scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0abbd5ee0b650d8572ebdfda2698f00d007435b92fab47409698edf25a778e24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0abbd5ee0b650d8572ebdfda2698f00d007435b92fab47409698edf25a778e24_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of state ambiguity in robot manipulation, where identical observations can correspond to multiple valid trajectories. The authors propose PAM, a visuomotor policy with adaptive working memory recoding, which efficiently utilizes long-term history (up to 10 seconds) to disambiguate states while maintaining high inference speeds above 20Hz. The method is validated on multiple tasks, demonstrating its ability to handle diverse ambiguity scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u72b6\u6001\u6a21\u7cca\u6027/State Ambiguity]\n    B --\x3e B2[\u957f\u5386\u53f2\u5efa\u6a21\u56f0\u96be/Long-History Modeling Challenge]\n    C --\x3e C1[\u81ea\u9002\u5e94\u5de5\u4f5c\u8bb0\u5fc6\u7b56\u7565/PAM Policy]\n    C --\x3e C2[\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u5668/Hierarchical Feature Extractor]\n    C --\x3e C3[\u4e0a\u4e0b\u6587\u8def\u7531\u5668/Context Router]\n    D --\x3e D1[\u5904\u74067\u79cd\u4efb\u52a1/Handles 7 Tasks]\n    D --\x3e D2[300\u5e27\u5386\u53f2\u7a97\u53e3/300-Frame History]\n    D --\x3e D3[\u63a8\u7406\u901f\u5ea6>20Hz/Inference >20Hz]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot navigation], [hybrid motion planning, deep reinforcement learning, entity-aware reward, graph-based global planner, collision avoidance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yury Kolomeytsev, Dmitry Golembiovsky"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lomonosov Moscow State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24651",children:"https://arxiv.org/pdf/2512.24651"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HMP-DRL, a hybrid framework integrating a graph-based global planner with a local DRL policy via checkpoints. 2. Introduces an entity-aware reward structure for the local planner to ensure social compliance by adjusting safety based on agent type. 3. Validates the method in a realistic simulation, showing superior performance in success rate, collision rate, and time to goal."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes HMP-DRL, a hybrid motion planning framework that combines a graph-based global planner for long-range pathfinding with a local Deep Reinforcement Learning policy for reactive, socially-compliant navigation. The method uses checkpoints to integrate the global path and an entity-aware reward function to dynamically adjust to different moving agents. Experiments in realistic simulation show it outperforms other methods in key navigation metrics, enhancing safety and reliability in complex environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u56fe\u89c4\u5212\u5668\u7f3a\u4e4f\u53cd\u5e94\u6027/Traditional graph planners lack reactivity]\n    B --\x3e B2[\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587/DRL methods lack global context]\n    C --\x3e C1[\u6df7\u5408\u6846\u67b6HMP-DRL/Hybrid framework HMP-DRL]\n    C1 --\x3e C2[\u56fe\u89c4\u5212\u5668\u751f\u6210\u8def\u5f84/Graph planner generates path]\n    C1 --\x3e C3[\u5c40\u90e8DRL\u7b56\u7565\u4f7f\u7528\u68c0\u67e5\u70b9\u548c\u5b9e\u4f53\u611f\u77e5\u5956\u52b1/Local DRL policy uses checkpoints & entity-aware reward]\n    D --\x3e D1[\u66f4\u9ad8\u7684\u6210\u529f\u7387/Higher success rate]\n    D --\x3e D2[\u66f4\u4f4e\u7684\u78b0\u649e\u7387/Lower collision rate]\n    D --\x3e D3[\u66f4\u77ed\u7684\u5230\u8fbe\u65f6\u95f4/Shorter time to goal]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics, dexterous manipulation], [Bowden cable, antagonistic actuation, rolling-contact joint, lightweight hand, remote actuation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sungjae Min, Hyungjoo Kim, David Hyunchul Shim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24657",children:"https://arxiv.org/pdf/2512.24657"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel lightweight robotic hand design combining rolling-contact joint optimization with antagonistic Bowden-cable actuation for single-motor-per-joint control. 2. A system architecture that relocates the actuator module to the robot's torso, drastically reducing the distal mass of the hand. 3. Demonstration of high performance, including fingertip force exceeding 18N and the ability to lift payloads over one hundred times its own mass."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68209284d19343b15573d618c7b75854112d070151b024eca7b4729486140f7e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68209284d19343b15573d618c7b75854112d070151b024eca7b4729486140f7e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of designing a dexterous robotic hand that is both lightweight and strong for payload-constrained humanoids. It proposes a hand actuated by antagonistic Bowden cables with optimized rolling-contact joints, enabling remote actuation from the torso to minimize weight. The resulting hand, weighing only 236g, demonstrated high force output and robust dexterous manipulation capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Conflicting requirements for robotic hands (lightweight, strong, fast, dexterous) limit humanoid robot payload."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: Lightweight hand with antagonistic Bowden-cable actuation and rolling-contact joints; actuators relocated to torso."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: Hand mass 236g; >18N fingertip force; lifts >100x its mass; validated dexterous tasks."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai / robotic manipulation], [imitation learning, offline reinforcement learning, multimodal dataset, bimanual manipulation, sim-to-real transfer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chengkai Hou, Kun Wu, Jiaming Liu, Zhengping Che, Di Wu, Fei Liao, Guangrun Li, Jingyang He, Qiuxuan Feng, Zhao Jin, Chenyang Gu, Zhuoyang Liu, Nuowei Han, Xiangju Mi, Yaoxu Lv, Yankai Fu, Gaole Dai, Langzhe Gu, Tao Li, Yuheng Zhang, Yixue Zhang, Xinhua Wang, Shichao Fan, Meng Li, Zhen Zhao, Ning Liu, Zhiyuan Xu, Pei Ren, Junjie Ji, Haonan Liu, Kuan Cheng, Shanghang Zhang, Jian Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Innovation Center of Humanoid Robotics, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24653",children:"https://arxiv.org/pdf/2512.24653"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RoboMIND 2.0, a large-scale, multimodal real-world dataset with over 310K dual-arm manipulation trajectories across diverse robots and tasks, including tactile and mobile manipulation data. 2. Provides high-fidelity digital twins and a complementary 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer research. 3. Proposes the MIND-2 system, a hierarchical framework optimized via offline RL that integrates a high-level semantic planner and a low-level vision-language-action executor for complex task decomposition and execution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1a0e9601e4aa64a92e289971771ed61ad7545ed91ce3194cf342f87d6636d96_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1a0e9601e4aa64a92e289971771ed61ad7545ed91ce3194cf342f87d6636d96_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the scarcity of diverse, large-scale real-world robotic manipulation data by introducing the RoboMIND 2.0 dataset, which includes hundreds of thousands of bimanual and mobile manipulation trajectories. To leverage this data, the authors propose the MIND-2 system, a hierarchical framework that uses offline reinforcement learning to integrate high-level planning with low-level control. The work aims to significantly advance the generalization capabilities of embodied AI agents in long-horizon, contact-rich, and unstructured environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RoboMIND 2.0 \u8bba\u6587 / RoboMIND 2.0 Paper] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898 / Problem] --\x3e B1[\u6570\u636e\u7a00\u7f3a / Scarcity of large-scale, diverse real-world robotic demonstrations]\n    B --\x3e B2[\u6cdb\u5316\u80fd\u529b\u6709\u9650 / Limited generalization in long-horizon bimanual and mobile manipulation]\n    C[\u4e3b\u8981\u65b9\u6cd5 / Method] --\x3e C1[\u53d1\u5e03RoboMIND 2.0\u6570\u636e\u96c6 / Release RoboMIND 2.0 Dataset]\n    C1 --\x3e C1_1[31\u4e07+\u771f\u5b9e\u8f68\u8ff9 / 310K+ real trajectories]\n    C1 --\x3e C1_2[\u89e6\u89c9\u4e0e\u79fb\u52a8\u6570\u636e / Tactile & mobile data]\n    C1 --\x3e C1_3[\u6570\u5b57\u5b6a\u751f\u4e0e\u4eff\u771f\u6570\u636e / Digital twins & simulated data]\n    C --\x3e C2[\u63d0\u51faMIND-2\u7cfb\u7edf / Propose MIND-2 System]\n    C2 --\x3e C2_1[\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u5668 / High-level semantic planner (MIND-2-VLM)]\n    C2 --\x3e C2_2[\u4f4e\u5c42VLA\u6267\u884c\u5668 / Low-level VLA executor (MIND-2-VLA)]\n    C2 --\x3e C2_3[\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316 / Optimized via offline RL]\n    D[\u5173\u952e\u7ed3\u679c / Results] --\x3e D1[\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6 / Large-scale multimodal dataset]\n    D --\x3e D2[\u4fc3\u8fdb\u6cdb\u5316\u7814\u7a76 / Facilitates research on generalization]\n    D --\x3e D3[\u5c42\u6b21\u5316\u7cfb\u7edf\u6846\u67b6 / Hierarchical system framework]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic planning under uncertainty], [mutual information, belief tree search, sigma point approximation, hierarchical particle structure, reusable rollout]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kangjie Zhou, Zhaoyang Li, Han Gao, Yao Su, Hangxin Liu, Junzhi Yu, Chang Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Chinese Academy of Sciences, Beijing Institute for General Artificial Intelligence (BIGAI)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24680",children:"https://arxiv.org/pdf/2512.24680"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel sigma point-based approximation method for fast and accurate mutual information reward estimation under non-Gaussian belief distributions. 2. A hierarchical particle structure to handle significant prior uncertainty, extracting critical particles for global guidance and adapting particle numbers for efficiency. 3. A reusable belief tree search approach that reuses rollout evaluations to build a policy tree for efficient online trajectory planning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc3dba762a4438b5f8c9012aee9dfd323191b47a56dd7b072cb478c53eb47f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc3dba762a4438b5f8c9012aee9dfd323191b47a56dd7b072cb478c53eb47f6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ReSPIRe, a trajectory planning method for robot search and tracking in unknown environments with poor prior information. It introduces a sigma point-based MI approximation, a hierarchical particle structure, and a reusable belief tree search to improve planning efficiency and accuracy. Experiments show it outperforms benchmarks in search efficiency, tracking stability, and computational performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ReSPIRe: Informative and Reusable Belief Tree Search] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Robot Search and Tracking in Unknown Cluttered Environments with Inaccurate Priors]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Sigma Point MI Approximation, Hierarchical Particle Structure, Reusable Belief Tree Search]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms Benchmarks in MI Error, Search Efficiency, Tracking Stability, and Computational Efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [VLA models, action chunking, trajectory smoothing, asynchronous inference, robot motion control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yongsheng Zhao, Lei Zhao, Baoping Cheng, Gongxin Yao, Xuanzhang Wen, Han Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," China Mobile (Hangzhou) Information Technology Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24673",children:"https://arxiv.org/pdf/2512.24673"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VLA-RAIL, a framework for asynchronous inference and motion control to enable smooth, continuous robot action execution. 2. Introduces a Trajectory Smoother using polynomial fitting to filter noise and jitter within an action chunk. 3. Designs a Chunk Fuser to ensure position, velocity, and acceleration continuity between successive action chunks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/886e2a128c843b55a605e8cbe2a7b174fc4b0e84225f1908b0b180891d09bdad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/886e2a128c843b55a605e8cbe2a7b174fc4b0e84225f1908b0b180891d09bdad_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of motion jitter, stalling, and pauses when deploying Vision-Language-Action (VLA) models on robots due to sequential inference and execution. It proposes VLA-RAIL, a framework that decouples model inference from robot control via a Trajectory Smoother and Chunk Fuser. Experiments show it reduces jitter, increases speed, and improves task success rates for robotic manipulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VLA-RAIL: A Real-Time Asynchronous Inference Linker] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5bfc\u81f4\u673a\u5668\u4eba\u52a8\u4f5c\u6296\u52a8\u3001\u5361\u987f/Existing methods cause jitter, stalling in robot actions]\n    C --\x3e C1[\u5f02\u6b65\u63a8\u7406\u4e0e\u8fd0\u52a8\u63a7\u5236/Asynchronous inference & motion control]\n    C1 --\x3e C2[\u8f68\u8ff9\u5e73\u6ed1\u5668/Trajectory Smoother]\n    C1 --\x3e C3[\u5757\u878d\u5408\u5668/Chunk Fuser]\n    D --\x3e D1[\u51cf\u5c11\u8fd0\u52a8\u6296\u52a8/Reduces motion jitter]\n    D --\x3e D2[\u63d0\u5347\u6267\u884c\u901f\u5ea6/Enhances execution speed]\n    D --\x3e D3[\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387/Improves task success rates]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-robot localization], [relative pose estimation, sensor fusion, hierarchical estimator, bearing outlier rejection, robocentric kinematics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhehan Li, Zheng Wang, Jiadong Lu, Qi Liu, Zhiren Xun, Yue Wang, Fei Gao, Chao Xu, Yanjun Cao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24688",children:"https://arxiv.org/pdf/2512.24688"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A compact hardware design integrating IR LEDs, an IR camera, UWB, and IMU into a small cube for multi-robot relative sensing., 2. A two-stage hierarchical estimator comprising a fast single-frame estimator with closed-form solution and outlier rejection, and an accurate multi-frame estimator with IMU pre-integration and optimization., 3. Demonstrated robustness to up to 90% bearing outliers and high accuracy (0.073m, 1.817\xb0 RMSE) in real-world experiments without requiring global information."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/305ab0f1d9896dda630bd4d638581b98363b86a626acdea82d5357417610e8b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/305ab0f1d9896dda630bd4d638581b98363b86a626acdea82d5357417610e8b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents CREPES-X, a hierarchical framework for direct cooperative relative pose estimation in multi-robot systems. It uses a compact sensor module and a two-stage estimator to fuse bearing, distance, and inertial measurements without environmental features, achieving high speed, accuracy, and robustness to outliers. Experiments validate its effectiveness in challenging conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u591a\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d\u6311\u6218 / Multi-robot relative localization challenges]\n    B1 --\x3e B2[\u4f9d\u8d56\u73af\u5883\u7279\u5f81\u6216\u60ef\u6027\u5047\u8bbe / Rely on environmental features or inertial assumptions]\n    B1 --\x3e B3[\u975e\u89c6\u8ddd\u4e0e\u5f02\u5e38\u503c / NLOS degradation and outliers]\n    C --\x3e C1[\u7d27\u51d1\u786c\u4ef6\u8bbe\u8ba1 / Compact hardware design]\n    C1 --\x3e C2[IR LED, \u76f8\u673a, UWB, IMU / IR LED, camera, UWB, IMU]\n    C --\x3e C3[\u4e24\u7ea7\u5206\u5c42\u4f30\u8ba1\u5668 / Two-stage hierarchical estimator]\n    C3 --\x3e C4[\u5355\u5e27\u4f30\u8ba1\u5668: \u95ed\u5f0f\u89e3, \u5f02\u5e38\u503c\u5254\u9664 / Single-frame: closed-form, outlier rejection]\n    C3 --\x3e C5[\u591a\u5e27\u4f30\u8ba1\u5668: IMU\u9884\u79ef\u5206, \u4f18\u5316 / Multi-frame: IMU pre-integration, optimization]\n    D --\x3e D1[\u9c81\u68d2\u6027: 90% \u65b9\u4f4d\u5f02\u5e38\u503c / Robustness: 90% bearing outliers]\n    D --\x3e D2[\u51c6\u786e\u6027: 0.073m, 1.817\xb0 RMSE / Accuracy: 0.073m, 1.817\xb0 RMSE]\n    D --\x3e D3[\u65e0\u5168\u5c40\u4fe1\u606f / No global information required]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [model homotopy, continuation learning, single rigid body model, policy transfer, legged locomotion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dongyun Kang, Min-Gyu Kim, Tae-Gyu Song, Hajun Kim, Sehoon Ha, Hae-Won Park"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), Georgia Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24698",children:"https://arxiv.org/pdf/2512.24698"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a continuation-based learning framework combining simplified model pretraining and model homotopy transfer for efficient policy learning. 2. Introduces a model homotopy path defined by gradually redistributing mass and inertia from a Single Rigid Body (SRB) model to a full-body model. 3. Demonstrates faster convergence and superior transfer stability for complex dynamic tasks (e.g., flips, wall maneuvers) and successful real-robot deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f290446d6dbc75a7566abaf855c8d703414e58780e4d737ff33bd9bcd9c33512_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f290446d6dbc75a7566abaf855c8d703414e58780e4d737ff33bd9bcd9c33512_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of transferring policies from simplified to full-body dynamics for legged robots. It proposes a framework that first pretrains a policy using a Single Rigid Body model and then uses a model homotopy to gradually transfer it to the full-body environment. The method achieves faster convergence, stable transfer, and is validated on dynamic tasks with a real quadruped robot."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Dynamic Policy Learning for Legged Robot] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u6a21\u578b\u5dee\u5f02/Model discrepancy]\n    P1 --\x3e P2[\u7b80\u5316\u6a21\u578b\u5230\u5168\u8eab\u6a21\u578b\u7b56\u7565\u8fc1\u79fb\u56f0\u96be/Difficult policy transfer from simplified to full-body model]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u7b80\u5316\u6a21\u578b\u9884\u8bad\u7ec3/Simplified model pretraining]\n    M1 --\x3e M2[\u4f7f\u7528\u5355\u521a\u4f53\u6a21\u578b/Using Single Rigid Body model]\n    Method --\x3e M3[\u6a21\u578b\u540c\u4f26\u8fc1\u79fb/Model homotopy transfer]\n    M3 --\x3e M4[\u6e10\u8fdb\u8d28\u91cf\u4e0e\u60ef\u91cf\u91cd\u5206\u914d/Gradual mass & inertia redistribution]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u66f4\u5feb\u6536\u655b/Faster convergence]\n    Results --\x3e R2[\u8fc1\u79fb\u7a33\u5b9a/Superior transfer stability]\n    Results --\x3e R3[\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u6210\u529f/Successful real-robot deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [latent semantic rule encoding, recurrent world model, language-guided latent classification, semantic risk detection, autonomous driving]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qian Cheng, Weitao Zhou, Cheng Jing, Nanshan Deng, Junze Wen, Zhaoyang Liu, Kun Jiang, Diange Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24712",children:"https://arxiv.org/pdf/2512.24712"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LSRE, a framework that encodes sparse VLM judgments into decision boundaries within a recurrent world model's latent space for real-time semantic risk assessment. 2. Demonstrated that LSRE achieves detection accuracy comparable to a per-frame VLM baseline while enabling earlier hazard anticipation and operating at 10 Hz. 3. Showed that the learned latent classifier generalizes to rarely seen, semantically similar test cases, indicating its effectiveness for semantic safety monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d95e05edf41980725845c43ad581e04cc0d1877fc56167a5c4d2c47f7e9516_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d95e05edf41980725845c43ad581e04cc0d1877fc56167a5c4d2c47f7e9516_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of real-time semantic rule compliance in autonomous driving, where explicit encoding of complex social rules is difficult. It proposes LSRE, a framework that uses sparsely sampled VLM outputs to train a lightweight latent classifier within a recurrent world model, enabling efficient semantic risk detection. The method achieves accuracy comparable to a VLM baseline with much lower latency and better anticipation, showing promise for deployable semantic safety systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Real-time semantic rule compliance in autonomous driving is difficult to encode explicitly and VLM inference is too slow.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: LSRE converts sparse VLM judgments into decision boundaries in a recurrent world model's latent space.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Achieves VLM-comparable accuracy, earlier anticipation, 10 Hz operation, and generalization to unseen cases.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [reinforcement learning, quantization, domain randomization, gait scheduling, edge ML]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yichen Liu, Kesava Viswanadha, Zhongyu Li, Nelson Lojo, Kristofer S. J. Pister"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24740",children:"https://arxiv.org/pdf/2512.24740"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and implements an edge ML pipeline for microrobot locomotion, training a compact RL policy in simulation and deploying it on an ultra-low-power ARM Cortex-M0 SoC. 2. Introduces a resource-aware gait scheduling framework that selects the optimal gait mode (e.g., trot, gallop) based on the hardware's power budget and achievable inference frequency to maximize expected reward. 3. Demonstrates the use of domain randomization and integer quantization (Int8) to enhance policy robustness and enable higher update rates on severely resource-constrained hardware."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59e857eb5b5c4d1e8a552ef8542246a2d1db88ba891759753679922c30b963c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b59e857eb5b5c4d1e8a552ef8542246a2d1db88ba891759753679922c30b963c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of on-device, low-latency control for microrobots under strict compute and power constraints. The method involves training a compact reinforcement learning policy with domain randomization in simulation, quantizing it to Int8 for efficient inference on a 5 MHz microcontroller, and proposing a power-budget-aware gait scheduler. The main conclusion is that this edge ML approach enables autonomous locomotion control on ultra-small hardware, with domain randomization improving out-of-distribution stability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Control of Microrobots with RL under On-Device Compute Constraints<br>\u5fae\u673a\u5668\u4eba\u5728\u8bbe\u5907\u8ba1\u7b97\u7ea6\u675f\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[On-device autonomy for microrobots<br>\u5fae\u673a\u5668\u4eba\u7684\u8bbe\u5907\u7aef\u81ea\u4e3b\u6027]\n    B --\x3e B2[Severe compute/power constraints<br>\u4e25\u82db\u7684\u8ba1\u7b97/\u529f\u8017\u7ea6\u675f]\n    C --\x3e C1[Train compact RL policy with domain randomization<br>\u4f7f\u7528\u57df\u968f\u673a\u5316\u8bad\u7ec3\u7d27\u51d1RL\u7b56\u7565]\n    C --\x3e C2[Quantize policy (Int8) for efficient inference<br>\u91cf\u5316\u7b56\u7565(Int8)\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406]\n    C --\x3e C3[Resource-aware gait scheduling<br>\u8d44\u6e90\u611f\u77e5\u7684\u6b65\u6001\u8c03\u5ea6]\n    D --\x3e D1[Deployment on ultra-small SoC (Cortex-M0)<br>\u5728\u8d85\u5c0f\u578bSoC\u4e0a\u90e8\u7f72]\n    D --\x3e D2[Connects power budget to feasible update rate<br>\u8fde\u63a5\u529f\u8017\u9884\u7b97\u4e0e\u53ef\u884c\u66f4\u65b0\u9891\u7387]\n    D --\x3e D3[Improved OOD stability via domain randomization<br>\u901a\u8fc7\u57df\u968f\u673a\u5316\u63d0\u5347OOD\u7a33\u5b9a\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic manipulation], [3D object flow, video generation, zero-shot manipulation, trajectory optimization, reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24766",children:"https://arxiv.org/pdf/2512.24766"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Dream2Flow, a framework that bridges video generation and robotic control using 3D object flow as an intermediate representation. 2. Demonstrates the ability to reconstruct 3D object motions from generated videos and formulate manipulation as object trajectory tracking, overcoming the embodiment gap. 3. Shows that the method enables zero-shot guidance from pre-trained video models to manipulate diverse object categories (rigid, articulated, deformable, granular) without task-specific demonstrations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Dream2Flow, a framework that uses 3D object flow extracted from videos generated by off-the-shelf models as an interface for robotic manipulation. It translates these generated motions into executable robot actions via trajectory optimization or reinforcement learning, enabling zero-shot manipulation of diverse objects in open-world settings. The results demonstrate 3D object flow as a general and scalable bridge between video generation models and robotic control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Translating human-like motions from video models into low-level robot actions)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Use 3D object flow as intermediate representation, reconstruct motions from videos, track trajectories)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Enables zero-shot manipulation of diverse objects, bridges video generation to robot control)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [human-robot interaction], [object rearrangement, human preference modeling, Monte Carlo Tree Search, psychological constructs, user study]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Emmanuel Fashae, Michael Burke, Leimin Tian, Lingheng Meng, Pamela Carreno-Medrano"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Monash University, CSIRO Robotics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24829",children:"https://arxiv.org/pdf/2512.24829"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel, interpretable formulation of human object arrangement preferences based on four psychological constructs (spatial practicality, habitual convenience, semantic coherence, commonsense appropriateness). 2. Designs and validates a self-report questionnaire to capture these constructs through a 63-participant online study. 3. Demonstrates the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner to generate arrangements that align with human preferences."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0c9201484194f4f746f05eae7a288356c0e53c3a3a9d4683db5313924455a5a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0c9201484194f4f746f05eae7a288356c0e53c3a3a9d4683db5313924455a5a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of interpretability in robotic object rearrangement models by identifying four explicit psychological constructs that guide human organizational preferences. The authors designed a questionnaire to measure these constructs and integrated them into a Monte Carlo Tree Search planner. The results show that the planner, guided by these interpretable preferences, can generate arrangements closely matching those created by human participants."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Explaining Why Things Go Where They Go<br>\u89e3\u91ca\u7269\u54c1\u4e3a\u4f55\u5f52\u4f4d] --\x3e B(Problem: \u673a\u5668\u4eba\u91cd\u6392\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027<br>Problem: Robotic rearrangement models lack interpretability)\n    A --\x3e C(Method: \u63d0\u51fa\u56db\u4e2a\u53ef\u89e3\u91ca\u504f\u597d\u6784\u5ff5\u4e0e\u95ee\u5377<br>Method: Four interpretable preference constructs & questionnaire)\n    A --\x3e D(Results: \u57fa\u4e8eMCTS\u7684\u89c4\u5212\u5668\u80fd\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u5e03\u5c40<br>Results: MCTS planner generates human-aligned arrangements)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multiple-object tracking], [appearance-motion association, re-identification, exponential moving average]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Ahmed Al Muzaddid, Jordan A. James, William J. Beksi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Texas at Arlington"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24838",children:"https://arxiv.org/pdf/2512.24838"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A reranking-enhanced appearance association module to improve feature matching. 2. A one-to-many association strategy with an appearance-based conflict resolution mechanism. 3. An exponential moving average prototype feature bank for robust appearance modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408f95352ab182fdf244f790c589fbdfab17f28b86d4a84534385d1d25eb2fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408f95352ab182fdf244f790c589fbdfab17f28b86d4a84534385d1d25eb2fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes CropTrack, a novel multiple-object tracking framework for agricultural environments that combines appearance and motion information to address challenges like occlusions and similar object appearances. It introduces techniques like reranking-enhanced association and a prototype feature bank to improve identity preservation. The method outperforms traditional motion-based trackers and achieves state-of-the-art association accuracy on agricultural MOT datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u519c\u4e1aMOT\u6311\u6218: \u906e\u6321, \u5916\u89c2\u76f8\u4f3c/MOT Challenges: Occlusions, Similar Appearance]\n    C --\x3e C1[\u7ed3\u5408\u5916\u89c2\u4e0e\u8fd0\u52a8\u4fe1\u606f/Combine Appearance & Motion]\n    C --\x3e C2[\u91cd\u6392\u5e8f\u589e\u5f3a\u5173\u8054/Reranking-enhanced Association]\n    C --\x3e C3[\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7279\u5f81\u5e93/Exponential Moving Average Feature Bank]\n    D --\x3e D1[\u66f4\u4f18\u7684\u8eab\u4efd\u4fdd\u6301/Better Identity Preservation]\n    D --\x3e D2[\u66f4\u9ad8\u7684\u5173\u8054\u51c6\u786e\u7387/Higher Association Accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [scene understanding], [3D scene graph, articulated objects, human demonstration, functional memory, open-vocabulary]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qiuyi Gu, Yuze Sheng, Jincheng Yu, Jiahao Tang, Xiaolong Shan, Zhaoyang Shen, Tinghao Yi, Xiaodan Liang, Xinlei Chen, Yu Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, University of Science and Technology of China, Pengcheng Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24845",children:"https://arxiv.org/pdf/2512.24845"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A framework (ArtiSG) that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. 2. A robust articulation data collection pipeline using a portable setup to estimate 6-DoF articulation trajectories and axes under camera ego-motion. 3. Integration of kinematic priors into a hierarchical graph and use of interaction data to discover functional elements missed by visual perception."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c33a51675e9901df263079c00c28d088fa6727c755e5d0709633ed22d655d18_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c33a51675e9901df263079c00c28d088fa6727c755e5d0709633ed22d655d18_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents ArtiSG, a framework that constructs functional 3D scene graphs by learning from human manipulation demonstrations to understand object articulation and discover functional elements. It introduces a portable data collection pipeline to estimate articulation parameters and integrates these into an open-vocabulary graph. Experiments show it outperforms baselines in functional recall and articulation precision, and the resulting graph effectively guides robots in language-directed manipulation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ArtiSG: Functional 3D Scene Graph Construction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[3D\u573a\u666f\u56fe\u7f3a\u4e4f\u529f\u80fd\u4fe1\u606f/3D Scene Graphs Lack Functional Info]\n    B --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u6a21\u7cca\u6027\u6216\u4f9d\u8d56\u53d7\u9650\u8bbe\u7f6e/Existing Methods Have Visual Ambiguity or Rely on Constrained Settings]\n    B --\x3e B3[\u7ec6\u5c0f\u529f\u80fd\u5143\u7d20\u5e38\u88ab\u6f0f\u68c0/Fine-grained Functional Elements Often Missed]\n    C --\x3e C1[\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u6784\u5efa\u529f\u80fd\u8bb0\u5fc6/Use Human Demonstration to Build Functional Memory]\n    C --\x3e C2[\u9c81\u68d2\u7684\u4fbf\u643a\u5f0f\u5173\u8282\u6570\u636e\u91c7\u96c6/Robust Portable Articulation Data Collection]\n    C --\x3e C3[\u5206\u5c42\u5f00\u653e\u8bcd\u6c47\u56fe\u96c6\u6210\u5148\u9a8c/Hierarchical Open-vocabulary Graph with Priors]\n    D --\x3e D1[\u529f\u80fd\u5143\u7d20\u53ec\u56de\u4e0e\u5173\u8282\u4f30\u8ba1\u7cbe\u5ea6\u663e\u8457\u63d0\u5347/Significant Improvement in Functional Recall & Articulation Precision]\n    D --\x3e D2[\u56fe\u4f5c\u4e3a\u53ef\u9760\u8bb0\u5fc6\u6307\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c/Graph as Reliable Memory Guides Robot Manipulation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai], [Vision-and-Language Navigation, Multimodal Large Language Models, evaluation framework, spatial reasoning, sequential decision-making]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xunyi Zhao, Gengze Zhou, Qi Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Adelaide University, Australian Institute of Machine Learning"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24851",children:"https://arxiv.org/pdf/2512.24851"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced VLN-MME, a unified and extensible evaluation framework for probing MLLMs as zero-shot agents in Vision-and-Language Navigation. 2. Provided a highly modular and accessible design that enables structured comparisons and component-level ablations across diverse MLLM architectures and agent designs. 3. Discovered that enhancing agents with Chain-of-Thought reasoning and self-reflection leads to performance degradation, revealing MLLMs' poor context awareness and low 3D spatial reasoning fidelity in embodied navigation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15acf4c91a5dc52a1564da96c7602f92c4733b8ba50a6e941b2f51f2a00f6501_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15acf4c91a5dc52a1564da96c7602f92c4733b8ba50a6e941b2f51f2a00f6501_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the performance of Multimodal Large Language Models (MLLMs) as embodied agents for Vision-and-Language Navigation (VLN). It proposes a unified evaluation framework called VLN-MME to benchmark MLLMs in a zero-shot setting. The key finding is that, contrary to expectations, advanced reasoning techniques like Chain-of-Thought degrade navigation performance, indicating MLLMs have significant limitations in 3D spatial reasoning and sequential decision-making for embodied tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8bc4\u4f30MLLMs\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u6027\u80fd/Evaluating MLLMs as Embodied Agents]\n    C --\x3e C1[\u63d0\u51fa\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6VLN-MME/Propose Unified Evaluation Framework VLN-MME]\n    D --\x3e D1[CoT\u4e0e\u53cd\u601d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d/CoT & Self-Reflection Degrade Performance]\n    D --\x3e D2[MLLMs\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5f31/MLLMs Have Poor Spatial Reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Lyapunov certificates, exponential stability, multi-step learning, actor-critic, maximum entropy RL]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24955",children:"https://arxiv.org/pdf/2512.24955"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework (MSACL) that integrates exponential stability theory with maximum entropy RL via multi-step Lyapunov certificate learning, using off-policy data to learn certificates that satisfy theoretical stability conditions. 2. Introduces Exponential Stability Labels (ESL) and a \u03bb-weighted aggregation mechanism to effectively balance the bias-variance trade-off in multi-step learning. 3. Guides policy optimization with a stability-aware advantage function to ensure the learned policy promotes rapid Lyapunov descent, achieving provable stability and robustness under simple rewards."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes MSACL, a model-free reinforcement learning framework that ensures provable exponential stability by learning Lyapunov certificates from multi-step data and guiding policy optimization with a stability-aware advantage. It demonstrates superior performance over baseline and state-of-the-art Lyapunov-based RL methods across six benchmarks, achieving rapid convergence and robustness with simple rewards. The work establishes a link between Lyapunov theory and actor-critic frameworks for verifiably safe control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Provable Stability in Model-Free RL / \u6a21\u578b\u65e0\u5173RL\u7684\u53ef\u8bc1\u660e\u7a33\u5b9a\u6027]\n    C --\x3e C1[Multi-Step Lyapunov Certificate Learning / \u591a\u6b65\u674e\u96c5\u666e\u8bfa\u592b\u8bc1\u4e66\u5b66\u4e60]\n    C --\x3e C2[Stability-Aware Advantage Function / \u7a33\u5b9a\u6027\u611f\u77e5\u4f18\u52bf\u51fd\u6570]\n    D --\x3e D1[Superiority over SOTA / \u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5]\n    D --\x3e D2[Exponential Stability & Robustness / \u6307\u6570\u7a33\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [deformable object manipulation], [hierarchical deformation planning, neural model predictive control, path-set-guided optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yunxi Tang, Tianqi Yang, Jing Huang, Xiangyu Chu, Kwok Wai Samuel Au"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Multi-scale Medical Robotics Centre"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24974",children:"https://arxiv.org/pdf/2512.24974"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A hierarchical deformation planning framework that first generates a spatial path set satisfying homotopic constraints and then synthesizes an optimal temporal deformation sequence. 2. A neural model predictive control approach for accurate tracking of the planned deformation, leveraging a data-driven deformation model. 3. A novel integrated framework combining global planning and local tracking, validated in extensive constrained DLO manipulation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86d851499789c822d47697a25d36a0162ba4f39e7f0421165d2bee0e6a9f0ee2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86d851499789c822d47697a25d36a0162ba4f39e7f0421165d2bee0e6a9f0ee2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of manipulating deformable linear objects (DLOs) in cluttered environments. It proposes a new framework that combines hierarchical deformation planning with neural tracking, using a path-set-guided optimization for planning and a data-driven neural MPC for execution. The method is shown to be effective in complex constrained manipulation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: DLO manipulation in constrained environments is challenging due to high-dimensional state space and obstacles."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Combines hierarchical deformation planning (path-set generation & optimization) with neural tracking (data-driven MPC)."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Framework validated as effective in extensive constrained DLO manipulation tasks."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [embodied vision-language reasoning], [low-light vision, embodied question answering, vision-language models, image enhancement, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24985",children:"https://arxiv.org/pdf/2512.24985"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Coordinated Humanoid Manipulation with Choice Policies"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [imitation learning], [humanoid robot, teleoperation, choice policy, multimodal behavior, whole-body coordination]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UC Berkeley"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25072",children:"https://arxiv.org/pdf/2512.25072"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://choice-policy.github.io",children:"https://choice-policy.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A modular teleoperation interface that decomposes humanoid control into intuitive submodules (e.g., hand-eye coordination, locomotion) for efficient, high-quality data collection. 2. The Choice Policy, a novel imitation learning architecture that generates multiple candidate actions and learns to score them, enabling fast inference and effective modeling of multimodal behaviors. 3. Empirical validation on real-world tasks (dishwasher loading, whiteboard wiping) showing superior performance over diffusion policies and behavior cloning, and highlighting the critical role of hand-eye coordination."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper tackles the challenge of achieving robust whole-body coordination for humanoid robots in unstructured environments. It proposes a system combining a modular teleoperation interface for data collection with a novel "Choice Policy" for imitation learning, which scores multiple candidate actions. Experiments on real-world tasks demonstrate that this approach outperforms baseline methods and that hand-eye coordination is crucial for success in long-horizon manipulation.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Coordinated Humanoid Manipulation with Choice Policies] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5934\u3001\u624b\u3001\u817f\u7684\u9c81\u68d2\u5168\u8eab\u534f\u8c03/Robust whole-body coordination for humanoids"]\n    C --\x3e C1["\u6a21\u5757\u5316\u9065\u64cd\u4f5c\u63a5\u53e3/Modular teleoperation interface"]\n    C --\x3e C2["\u9009\u62e9\u7b56\u7565\uff1a\u751f\u6210\u5e76\u8bc4\u4f30\u5019\u9009\u52a8\u4f5c/Choice Policy: generate & score candidate actions"]\n    D --\x3e D1["\u5728\u6d17\u7897\u673a\u88c5\u8f7d\u3001\u767d\u677f\u64e6\u62ed\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baselines on dishwasher loading & whiteboard wiping"]\n    D --\x3e D2["\u624b\u773c\u534f\u8c03\u5bf9\u957f\u65f6\u57df\u4efb\u52a1\u81f3\u5173\u91cd\u8981/Hand-eye coordination is critical for long-horizon tasks"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [video diffusion model, space-time disentanglement, temporal-warping training, camera-conditioning, generative rendering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Cambridge, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25075",children:"https://arxiv.org/pdf/2512.25075"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zheninghuang/Space-Time-Pilot",children:"https://github.com/zheninghuang/Space-Time-Pilot"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced an animation time-embedding mechanism for explicit motion sequence control in video diffusion. 2. Proposed a temporal-warping training scheme to repurpose multi-view datasets for learning temporal variations. 3. Created the CamxTime synthetic dataset and an improved camera-conditioning mechanism for precise dual space-time control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SpaceTimePilot, a video diffusion model that independently controls camera viewpoint and motion sequence to re-render dynamic scenes from a monocular video. The method uses a novel time-embedding mechanism and a temporal-warping training strategy to achieve robust space-time disentanglement. Experiments show the model enables continuous exploration across space and time, outperforming prior work."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u4ee5\u8fdb\u884c\u53ef\u63a7\u751f\u6210\u6e32\u67d3/How to disentangle space and time from a monocular video for controllable generative rendering]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5f15\u5165\u52a8\u753b\u65f6\u95f4\u5d4c\u5165\u673a\u5236\u548c\u65f6\u57df\u626d\u66f2\u8bad\u7ec3\u65b9\u6848/Introduce animation time-embedding and temporal-warping training scheme]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5b9e\u73b0\u9c81\u68d2\u7684\u65f6\u7a7a\u89e3\u8026\u4e0e\u8fde\u7eed\u53ef\u63a7\u6e32\u67d3/Achieve robust space-time disentanglement and continuous controllable rendering]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);