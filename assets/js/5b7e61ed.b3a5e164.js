"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3410],{3086:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_RO/20251229-20260104","title":"20251229-20260104 (cs.RO)","description":"2025-12-29","source":"@site/docs/daily/cs_RO/20251229-20260104.md","sourceDirName":"daily/cs_RO","slug":"/daily/csro/20251229-20260104","permalink":"/ai_toutiao/daily/csro/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766994315000,"frontMatter":{"slug":"/daily/csro/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20251222-20251228"},"next":{"title":"cs.SC","permalink":"/ai_toutiao/category/cssc"}}');var a=i(4848),r=i(8453);const t={slug:"/daily/csro/20251229-20260104"},o="20251229-20260104 (cs.RO)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251229-20260104-csro",children:"20251229-20260104 (cs.RO)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic perception and planning], [Interfered Fluid Dynamical System (IFDS), Model Predictive Control (MPC), Dynamic Flight Altitude Adjustment (DFAA)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, North China University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21375",children:"https://arxiv.org/pdf/2512.21375"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --\x3e B\nA --\x3e C\nA --\x3e D\nB[\u6838\u5fc3\u95ee\u9898/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]\nC[\u4e3b\u8981\u65b9\u6cd5/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]\nD[\u5173\u952e\u7ed3\u679c/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot navigation], [language-conditioned planning, map forecasting, Log-MPPI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Moorthy Mahesh, Oguzhan Goktug Poyrazoglu, Yukang Cao, Volkan Isler"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' University of Minnesota (inferred from author "Volkan Isler")']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21398",children:"https://arxiv.org/pdf/2512.21398"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PaceForecaster, a novel architecture that integrates co-pilot language instructions into local motion planning. 2. Predicts a forecasted map (Level-2) of occluded areas and an instruction-conditioned subgoal within it. 3. Demonstrates a 36% improvement in navigation performance by integrating PaceForecaster with a Log-MPPI controller."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e1b55661a25fc48b61cda55eadcd0e6f2f90bf3fac6514d8b38c5f3883d102b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the speed-safety trade-off in robot navigation in occluded environments by introducing PaceForecaster, a method that uses language instructions to forecast occluded map regions and generate conditioned subgoals. Integrating this with a Log-MPPI controller allows for more decisive and goal-directed planning. The approach improves navigation performance by 36% over a baseline that uses only the local sensor map."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b89\u5168\u4e0e\u901f\u5ea6\u7684\u6743\u8861<br/>Safety-Speed Trade-off]\n    B1 --\x3e B2[\u906e\u6321\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027<br/>Uncertainty from Occlusions]\n    C --\x3e C1[PaceForecaster \u67b6\u6784<br/>PaceForecaster Architecture]\n    C1 --\x3e C2[\u8f93\u5165: \u4f20\u611f\u5668\u8db3\u8ff9\u4e0e\u6307\u4ee4<br/>Input: Sensor Footprint & Instruction]\n    C2 --\x3e C3[\u8f93\u51fa: \u9884\u6d4b\u5730\u56fe\u4e0e\u5b50\u76ee\u6807<br/>Output: Forecasted Map & Subgoal]\n    C3 --\x3e C4[\u96c6\u6210 Log-MPPI \u63a7\u5236\u5668<br/>Integrated with Log-MPPI Controller]\n    D --\x3e D1[\u6027\u80fd\u63d0\u5347 36%<br/>36% Performance Improvement]\n    D1 --\x3e D2[\u8d85\u8d8a\u4ec5\u4f7f\u7528\u5c40\u90e8\u5730\u56fe\u7684\u57fa\u7ebf<br/>Over Local-Map-Only Baseline]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [Transportation Systems, Robotics], [Fundamental Diagram, Urban Air Mobility, Traffic Flow Theory, Drone Control, Physical Experiments]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hang Zhou, Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang, Xiaowei Shi, Xiaopeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison, University of Illinois Urbana-Champaign, University of Wisconsin-Milwaukee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21425",children:"https://arxiv.org/pdf/2512.21425"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/CATS-Lab/UAM-FD",children:"https://github.com/CATS-Lab/UAM-FD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework integrating theory and physical experiments to construct a Fundamental Diagram for Urban Air Mobility traffic. 2. Develops and validates the first UAM Fundamental Diagram using real-world physical test data from a reduced-scale drone testbed. 3. Creates and releases the UAMTra2Flow dataset containing simulation and physical test trajectory data for UAM traffic analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fcb69b60c91e88b8ff8fdb72d4cfe7a72f7f1c82ac3f0ed8afd4f7ca60297d6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study addresses the lack of understanding of Urban Air Mobility (UAM) traffic flow by proposing a framework to construct its Fundamental Diagram (FD) through theoretical modeling and physical experiments using drones. The results show that classical ground traffic FD structures are applicable to UAM, but physical experiments reveal deviations from simulation, underscoring the need for experimental validation. The findings and a public dataset provide practical insights for future UAM system design."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\nRoot("Developing a Fundamental Diagram for Urban Air Mobility Traffic Flow / \u6784\u5efa\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u672c\u56fe")\nRoot --\x3e Problem("UAM\u4ea4\u901a\u6d41\u7279\u6027\u672a\u77e5 / UAM Traffic Flow Poorly Understood")\nRoot --\x3e Method("\u7406\u8bba\u5206\u6790+\u7269\u7406\u5b9e\u9a8c\u6846\u67b6 / Theory + Physical Experiment Framework")\nRoot --\x3e Results("\u7ecf\u5178FD\u7ed3\u6784\u9002\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5173\u952e / Classical FD Applicable, Validation Crucial")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EVE: A Generator-Verifier System for Generative Policies"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [generative policies, test-time compute, vision-language models, verifier agents, embodied control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yusuf Ali, Gryphon Patlin, Karthik Kothuri, Muhammad Zubair Irshad, Wuwei Liang, Zsolt Kira"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology, Toyota Research Institute, Symbotic Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21430",children:"https://arxiv.org/pdf/2512.21430"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes EVE, a modular generator-verifier framework that improves pretrained generative visuomotor policies at test time without additional training. 2. Introduces a system of multiple zero-shot VLM-based verifier agents that propose action refinements, coupled with an action incorporator to fuse these suggestions. 3. Provides a systematic analysis and practical guidelines for designing scalable generator-verifier systems for embodied control through extensive ablations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a88f407920b463e24f81294c5ecb39e2cf549c376fd258eaa250ec03d1cdbed1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that generative visuomotor policies degrade under distribution shifts and lack recovery capabilities. It proposes EVE, a framework that uses additional inference-time compute to refine a frozen base policy's actions via multiple zero-shot VLM verifiers. The method consistently improves task success rates across diverse manipulation tasks without any retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[EVE: A Generator-Verifier System for Generative Policies] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Generative policies degrade under distribution shifts and lack recovery.] --\x3e ProblemDetail[\u95ee\u9898\u7ec6\u8282/Problem Detail: Costly to finetune; limited test-time robustness.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: EVE framework] --\x3e MethodDetail1[\u65b9\u6cd5\u7ec6\u8282/Method Detail 1: Wraps frozen base policy with zero-shot VLM verifiers.]\n    Method --\x3e MethodDetail2[\u65b9\u6cd5\u7ec6\u8282/Method Detail 2: Verifiers propose action refinements.]\n    Method --\x3e MethodDetail3[\u65b9\u6cd5\u7ec6\u8282/Method Detail 3: Action incorporator fuses verifier outputs.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Boosts performance without training.] --\x3e ResultsDetail[\u7ed3\u679c\u7ec6\u8282/Results Detail: Improves task success rates across diverse manipulation tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Planetary Terrain Datasets and Benchmarks for Rover Path Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [path planning], [global path planning, digital terrain models, autonomous navigation, rover exploration, benchmark datasets]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Marvin Chanc\xe1n, Avijit Banerjee, George Nikolakopoulos"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lule\xe5 University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21438",children:"https://arxiv.org/pdf/2512.21438"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/mchancan/PlanetaryPathBench",children:"https://github.com/mchancan/PlanetaryPathBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the first two large, space mission-derived planetary benchmark datasets for rover path planning (MarsPlanBench and MoonPlanBench). 2. Establishes a unified framework for evaluating both classical and learning-based path planning algorithms on these planetary terrains. 3. Provides new empirical insights into algorithm performance, showing classical methods achieve high success rates on challenging terrains while learning-based methods struggle to generalize."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbb45e523b7fc6a07c19646449f84300acdad583bbecaad64993aae93fddcf33_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of standardized planetary datasets and benchmarks for rover path planning by introducing MarsPlanBench and MoonPlanBench, two large datasets derived from high-resolution digital terrain models of Mars and the Moon. The authors evaluate classical and learning-based path planning algorithms in a unified framework on these new benchmarks. The key finding is that classical algorithms achieve very high success rates (up to 100%) on challenging planetary terrains, explaining their practical use by agencies like NASA, while learning-based models still face generalization difficulties in these domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Planetary Terrain Datasets and Benchmarks for Rover Path Planning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u7f3a\u4e4f\u7528\u4e8e\u8def\u5f84\u89c4\u5212\u7684\u884c\u661f\u6570\u636e\u96c6\u4e0e\u57fa\u51c6/Lack of planetary datasets & benchmarks for path planning]\nC --\x3e C1[\u63d0\u51fa\u706b\u661f\u4e0e\u6708\u7403\u57fa\u51c6\u6570\u636e\u96c6/Propose MarsPlanBench & MoonPlanBench datasets]\nC --\x3e C2[\u5efa\u7acb\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6/Establish unified evaluation framework]\nD --\x3e D1[\u7ecf\u5178\u7b97\u6cd5\u6210\u529f\u7387\u9ad8\u8fbe100%/Classical algorithms achieve up to 100% success rate]\nD --\x3e D2[\u5b66\u4e60\u6a21\u578b\u6cdb\u5316\u56f0\u96be/Learning-based models struggle to generalize]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [control theory], [spatiotemporal tube, probabilistic safety, real-time control, uncertain dynamic environment, reach-avoid-stay]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siddhartha Upadhyay, Ratnangshu Das, Pushpak Jagtap"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Robert Bosch Centre for Cyber-Physical Systems, Indian Institute of Science (IISc), Bengaluru"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21497",children:"https://arxiv.org/pdf/2512.21497"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extension of the Spatiotemporal Tube (STT) framework to handle Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in environments with time-varying uncertain obstacles. 2. Development of a real-time tube synthesis procedure that provides formal probabilistic safety guarantees. 3. Derivation of a closed-form, model-free, approximation-free, and optimization-free control law that confines the system within the tube for efficient real-time execution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90c986ba71dbe6e082ded217bbbe89ccda980dde23fb1eaaed32b9bf8e4aa780_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an extension of the Spatiotemporal Tube framework to solve Probabilistic Temporal Reach-Avoid-Stay tasks in uncertain dynamic environments. The method synthesizes a time-varying safe tube online and provides a closed-form control law to keep the system inside it, offering formal probabilistic safety and task completion guarantees. The approach is validated through simulations and hardware experiments on various robotic platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Paper Title: Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Probabilistic Temporal Reach-Avoid-Stay in uncertain dynamic environments)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Real-time Spatiotemporal Tube synthesis with closed-form control)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Formal probabilistic safety guarantees & efficient real-time execution)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [soft robotics, variable stiffness actuators], [electrostatic layer jamming, helical winding, variable stiffness robotic finger, voltage-driven control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Congrui Bai, Zhenting Du, Weibang Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ShanghaiTech University, King's College London"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21534",children:"https://arxiv.org/pdf/2512.21534"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) mechanism for variable stiffness., 2. Demonstrates that the helical configuration provides exponentially greater stiffness adjustment and a reduced footprint compared to conventional planar designs., 3. Develops and validates a functional robotic finger prototype that confirms the feasibility of voltage-driven stiffness modulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/287c4eb37a5ab5af01f325e34fecea5969ff071b0b31c3e96ae601a3ad2f8fad_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel variable stiffness mechanism for robotics called Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ). It uses a helical configuration and electrostatic attraction to achieve tunable stiffness, offering superior performance and a smaller footprint than traditional planar designs. The work is validated through experiments and a functional robotic finger prototype, confirming its feasibility."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Robotic joints need variable stiffness for adaptability and safety.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Proposes HWS-ELJ, using helical winding and electrostatic attraction for tunable stiffness.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: HWS-ELJ shows superior stiffness enhancement and smaller footprint; a functional robotic finger prototype validates feasibility.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] World-Coordinate Human Motion Retargeting via SAM 3D Body"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion capture and retargeting], [SAM 3D Body, Momentum HumanRig, contact-aware optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhangzheng Tum, Kailun Su, Shaolong Zhu, Yukun Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian University of Technology, Shenzhen University, Harbin Institute of Technology, Shenzhen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21573",children:"https://arxiv.org/pdf/2512.21573"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight framework using a frozen SAM 3D Body backbone and Momentum HumanRig representation for world-coordinate human motion recovery from monocular video. 2. Introduces temporal consistency enforcement via identity/scale locking and efficient sliding-window smoothing in a low-dimensional latent space. 3. Recovers physically plausible global root trajectories using a differentiable soft foot-ground contact model and contact-aware optimization for reliable robot retargeting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ddf0876edffc654fb4ec059aced2eb58f78ddcb89551be7a907e2f5bdee145c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a lightweight framework for recovering world-coordinate human motion from monocular video and retargeting it to a humanoid robot. The method leverages SAM 3D Body as a frozen backbone, enforces temporal consistency, smooths predictions, and uses a contact model for plausible global trajectories. Results show the method produces stable, robot-ready motion from monocular input."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[World-Coordinate Human Motion Retargeting via SAM 3D Body] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u4ece\u5355\u76ee\u89c6\u9891\u6062\u590d\u4e16\u754c\u5750\u6807\u7cfb\u4eba\u4f53\u8fd0\u52a8\u5e76\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684SAM 3D Body\uff0cMHR\u8868\u793a\uff0c\u65f6\u5e8f\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6ed1\u52a8\u7a97\u53e3\u5e73\u6ed1\uff0c\u63a5\u89e6\u611f\u77e5\u5168\u5c40\u4f18\u5316]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u7a33\u5b9a\u7684\u4e16\u754c\u8f68\u8ff9\uff0c\u53ef\u9760\u7684\u673a\u5668\u4eba\u91cd\u5b9a\u5411\uff0c\u4ece\u5355\u76ee\u8f93\u5165\u751f\u6210\u673a\u5668\u4eba\u5c31\u7eea\u7684\u8fd0\u52a8]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21618",children:"https://arxiv.org/pdf/2512.21618"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f\u4ea4\u901a\u7f16\u8f91 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u5bf9\u79f0\u81ea\u56de\u5f52\u5728\u7ebf\u4fee\u590d\u8303\u5f0f\u4e0e\u514d\u8bad\u7ec3\u534f\u8c03\u673a\u5236 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u65b0\u89c6\u89d2\u589e\u5f3a\u548c3D\u8f66\u8f86\u63d2\u5165\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AstraNav-Memory: Contexts Compression for Long Memory"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [visual context compression, image-centric memory, lifelong embodied navigation, DINOv3, Qwen2.5-VL]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Botao Ren, Junjun Hu, Xinda Xue, Minghua Luo, Jintao Chen, Haochen Bai, Liangliang You, Mu Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Alibaba Group (Amap), Tsinghua University, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21627",children:"https://arxiv.org/pdf/2512.21627"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://astra-amap.github.io/AstraNav-Memory.github.io/",children:"https://astra-amap.github.io/AstraNav-Memory.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed an image-centric memory framework for lifelong embodied navigation that uses an efficient visual context compression module to achieve long-term implicit memory. 2. Introduced a configurable visual tokenizer built on a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, enabling high compression rates (e.g., 16x) to expand context capacity. 3. Demonstrated state-of-the-art navigation performance on benchmarks (GOAT-Bench, HM3D-OVON), showing improved exploration in unfamiliar environments and shorter paths in familiar ones, with ablation studies validating the efficiency-accuracy trade-off."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce3c5628c478fa221373bddffcb2cb72bd35c261cc9b555152b34063fccbb9f2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of building long-term memory for lifelong embodied navigation. It proposes AstraNav-Memory, an image-centric framework that compresses visual contexts using a configurable tokenizer to efficiently store hundreds of images, coupled with a Qwen2.5-VL-based navigation policy. The method achieves state-of-the-art performance, balancing efficiency and accuracy for scalable lifelong agents."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AstraNav-Memory: Contexts Compression for Long Memory] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Lifelong embodied navigation requires efficient long-term spatial-semantic memory. Object-centric methods are limited by robustness and scalability.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose an image-centric memory framework with a visual context compression module (ViT+DINOv3+PixelUnshuffle) coupled with a Qwen2.5-VL navigation policy.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA on GOAT-Bench and HM3D-OVON. Improves exploration in unfamiliar environments and shortens paths in familiar ones. Moderate compression offers best balance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [swarm intelligence], [Ant Colony Optimization, structural prior, load-aware objective, overlap suppression, multi-robot path planning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kyungpook National University, Kumoh National Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21654",children:"https://arxiv.org/pdf/2512.21654"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. \u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55/Traditional methods struggle to scale.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: A structure-induced ACO framework. \u5229\u7528\u7ed3\u6784\u5148\u9a8c\u3001\u8d1f\u8f7d\u611f\u77e5\u76ee\u6807\u548c\u91cd\u53e0\u6291\u5236/Uses structural prior, load-aware objective, and overlap suppression.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Improves route compactness, stability, and workload distribution. \u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6846\u67b6/Provides a scalable framework.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [socially compliant navigation], [meta-cognitive prompt, vision language model, multi-action generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zishuo Wang, Xinyu Zhang, Zhuonan Liu, Tomohito Kawabata, Daeun Song, Xuesu Xiao, Ling Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hokkaido University, George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21722",children:"https://arxiv.org/pdf/2512.21722"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MAction-SocialNav, a vision language model for socially compliant navigation that generates multiple plausible actions to handle real-world ambiguity. 2. Introduces a novel meta-cognitive prompt (MCP) method to enhance the model's reasoning capability. 3. Curates a new multi-action socially compliant navigation dataset with diverse conditions and dual human annotations, and designs five evaluation metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08864b9cd92dd7e9f4041ef22b9caf54fe559ad301e22a19b24957d3cb331538_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of action ambiguity in socially compliant robot navigation by proposing MAction-SocialNav, an efficient vision language model that generates multiple plausible actions per scenario using a novel meta-cognitive prompt tuning method. The method is evaluated on a newly curated dataset and shows superior decision quality, safety alignment, and real-time efficiency compared to large language models like GPT-4o and Claude."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Social norms are ambiguous; multiple actions may be acceptable, but existing methods assume a single correct action."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes an efficient VLM with a novel meta-cognitive prompt (MCP) to generate multiple plausible actions."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves higher decision quality and safety than GPT-4o/Claude while maintaining real-time efficiency."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," MIRAI, Cognitive AI Systems Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21723",children:"https://arxiv.org/pdf/2512.21723"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [space robotics], [modular robot, reconfigurable robot, lunar construction, field demonstration, connector design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21853",children:"https://arxiv.org/pdf/2512.21853"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MoonBot: \u9762\u5411\u6708\u7403\u57fa\u5730\u5efa\u8bbe\u7684\u6a21\u5757\u5316\u6309\u9700\u53ef\u91cd\u6784\u673a\u5668\u4eba] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6708\u7403\u63a2\u7d22\u4e0e\u57fa\u5730\u5efa\u8bbe\u9700\u6c42 / Lunar Exploration & Base Construction Needs]\n    C --\x3e C1[\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf / Modular & Reconfigurable Robotic System]\n    C --\x3e C2[\u6982\u5ff5\u9a8c\u8bc1\u4e0e\u73b0\u573a\u6f14\u793a / Proof-of-Concept & Field Demonstration]\n    D --\x3e D1[\u6210\u529f\u6267\u884c\u6a21\u62df\u4efb\u52a1 / Successfully Executed Simulated Tasks]\n    D --\x3e D2[\u603b\u7ed3\u4e86\u8fde\u63a5\u5668\u8bbe\u8ba1\u7b49\u7ecf\u9a8c\u6559\u8bad / Summarized Lessons (e.g., Connector Design)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [space robotics], [trajectory planning, nonlinear optimization, dynamic keep-out sphere, ON/OFF thrusters, rendezvous and docking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kenta Iizuka, Akiyoshi Uchida, Kentaro Uno, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21882",children:"https://arxiv.org/pdf/2512.21882"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A trajectory planning method based on nonlinear optimization for close-range rendezvous with a tumbling target. 2. The introduction of a dynamic keep-out sphere that adapts to approach conditions for safer access. 3. A control strategy to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0619d27a582b3a86da2ec2f90d52c42f5327490ba9737fc6b241e03569d2be3b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of safely approaching a tumbling space debris object for capture. It proposes a trajectory planning method using nonlinear optimization with a dynamic safety boundary and a control strategy for ON/OFF thrusters. The method enables closer and safer access as a preliminary step for robotic capture."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u5b89\u5168\u63a5\u8fd1\u7ffb\u6eda\u76ee\u6807/Safely approaching a tumbling target")\n    Method --\x3e M1("\u975e\u7ebf\u6027\u4f18\u5316\u8f68\u8ff9\u89c4\u5212/Nonlinear optimization-based trajectory planning")\n    Method --\x3e M2("\u52a8\u6001\u7981\u5165\u7403\u4f53/Dynamic keep-out sphere")\n    Method --\x3e M3("ON/OFF\u63a8\u8fdb\u5668\u63a7\u5236/ON/OFF thruster control strategy")\n    Results --\x3e R1("\u66f4\u8fd1\u66f4\u5b89\u5168\u7684\u8bbf\u95ee/Closer and safer access")\n    Results --\x3e R2("\u8003\u8651\u5b9e\u9645\u7ea6\u675f/Practical implementation considered")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [robotics, dynamics and control], [inertia parameter estimation, online identification, momentum conservation, floating-base robots, space robotics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Akiyoshi Uchida, Antonine Richard, Kentaro Uno, Miguel Olivares-Mendez, Kazuya Yoshida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University, University of Luxembourg"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21886",children:"https://arxiv.org/pdf/2512.21886"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extended an existing online inertia identification method to be applicable to floating-base robots by incorporating momentum conservation. 2. Validated the proposed method through numerical simulations for space applications like on-orbit servicing. 3. Demonstrated accurate estimation of unknown object inertia parameters during manipulation in a simulated microgravity environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56413666705f0b959b1c7926a7846dc2d7421e2c52ee8ac46620538562104026_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of estimating the inertia parameters of an unknown object grasped by a manipulator on a free-floating space robot. The authors extend an existing online identification method by incorporating momentum conservation to handle the floating base. Numerical simulations validate the method, showing accurate identification and highlighting its potential for on-orbit servicing missions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nRoot(Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\nRoot --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\nRoot --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\nProblem --\x3e P1(\u4f30\u8ba1\u672a\u77e5\u88ab\u6293\u53d6\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570/Estimate inertia parameters of unknown grasped object)\nProblem --\x3e P2(\u9762\u5411\u81ea\u7531\u6f02\u6d6e\u57fa\u5ea7\u7684\u7a7a\u95f4\u673a\u5668\u4eba/For floating-base space robots)\nMethod --\x3e M1(\u5e94\u7528\u5e76\u6269\u5c55\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5/Apply and extend online identification method)\nMethod --\x3e M2(\u7ed3\u5408\u52a8\u91cf\u5b88\u6052\u5b9a\u5f8b/Incorporate momentum conservation)\nResults --\x3e R1(\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1/Validated via numerical simulation)\nResults --\x3e R2(\u53c2\u6570\u8bc6\u522b\u51c6\u786e/Accurate parameter identification)\nResults --\x3e R3(\u9002\u7528\u4e8e\u5728\u8f68\u670d\u52a1/Applicable to on-orbit servicing)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual navigation], [world model, future frame projection, 4-dof uav, long-horizon visual generation, aerial navigation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21887",children:"https://arxiv.org/pdf/2512.21887"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[UAV\u5bfc\u822a\u7f3a\u4e4f\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u80fd\u529b/UAV navigation lacks high-level semantic planning]\n    B --\x3e B2[\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u7406\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u4e0e\u957f\u8ddd\u79bb\u89c6\u89c9\u751f\u6210/Existing models struggle with complex action space & long-horizon visual generation]\n    C --\x3e C1[\u63d0\u51faANWM\u4e16\u754c\u6a21\u578b/Propose ANWM world model]\n    C --\x3e C2[\u5f15\u5165\u672a\u6765\u5e27\u6295\u5f71\u6a21\u5757/Introduce Future Frame Projection module]\n    D --\x3e D1[\u957f\u8ddd\u79bb\u89c6\u89c9\u9884\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347/Significantly outperforms in long-distance visual forecasting]\n    D --\x3e D2[\u63d0\u9ad8\u5927\u89c4\u6a21\u73af\u5883\u5bfc\u822a\u6210\u529f\u7387/Improves UAV navigation success rates in large-scale environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Flexible Multitask Learning with Factorized Diffusion Policy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [diffusion policy, modular architecture, multitask learning, imitation learning, mixture-of-experts]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chaoqi Liu, Haonan Chen, Sigmund H. H\xf8eg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21898",children:"https://arxiv.org/pdf/2512.21898"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Flexible Multitask Learning with Factorized Diffusion Policy] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u673a\u5668\u4eba\u591a\u4efb\u52a1\u5b66\u4e60/Robot Multitask Learning]\n    B1 --\x3e B2[\u52a8\u4f5c\u5206\u5e03\u590d\u6742\u591a\u6a21\u6001/Action Distribution Highly Multimodal]\n    B2 --\x3e B3[\u5355\u4f53\u6a21\u578b\u6b20\u62df\u5408\u4e0e\u4e0d\u7075\u6d3b/Monolithic Models Underfit & Inflexible]\n    C --\x3e C1[\u56e0\u5b50\u5316\u6269\u6563\u7b56\u7565/Factorized Diffusion Policy (FDP)]\n    C1 --\x3e C2[\u6a21\u5757\u5316\u6269\u6563\u4e13\u5bb6/Modular Diffusion Experts]\n    C2 --\x3e C3[\u57fa\u4e8e\u89c2\u5bdf\u7684\u8def\u7531\u5668/Observation-Conditioned Router]\n    C3 --\x3e C4[\u8fde\u7eed\u5206\u6570\u805a\u5408/Continuous Score Aggregation]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf/Outperforms Baselines]\n    D1 --\x3e D2[\u4eff\u771f\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u9a8c\u8bc1/Simulation & Real-World Validation]\n    D2 --\x3e D3[\u652f\u6301\u7075\u6d3b\u7b56\u7565\u9002\u5e94/Enables Flexible Policy Adaptation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [robotic vision], [stereo vision, vision-language-action models, geometric-semantic fusion, depth estimation, robotic manipulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, The University of Hong Kong, Institute of Automation, Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21970",children:"https://arxiv.org/pdf/2512.21970"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://shengliangd.github.io/StereoVLA-Webpage",children:"https://shengliangd.github.io/StereoVLA-Webpage"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed StereoVLA, a novel Vision-Language-Action model that leverages stereo vision for enhanced spatial perception. 2. Introduced a Geometric-Semantic Feature Extraction module to fuse geometric cues from stereo differences with semantic features from a monocular view. 3. Designed an auxiliary Interaction-Region Depth Estimation task to improve spatial understanding and accelerate model convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fbd07a25a843958488215748e8162f92542370bba173316326de44d4be3c65f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of single-view input in Vision-Language-Action (VLA) models for robotic manipulation by introducing StereoVLA, which utilizes stereo vision. The core method involves a novel module to extract and fuse geometric and semantic features, along with an auxiliary depth estimation task. Experiments show the model significantly outperforms baselines in stereo-based tasks and is robust to camera pose variations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u5355\u76eeVLA\u6a21\u578b\u7f3a\u4e4f\u7cbe\u786e\u7684\u51e0\u4f55\u611f\u77e5/Single-view VLAs lack accurate geometry perception)\n    C --\x3e C1(\u63d0\u51faStereoVLA\u6a21\u578b/Propose StereoVLA model)\n    C1 --\x3e C2(\u51e0\u4f55-\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6a21\u5757/Geometric-Semantic Feature Extraction)\n    C2 --\x3e C3(\u4ece\u7acb\u4f53\u89c6\u56fe\u63d0\u53d6\u51e0\u4f55\u7279\u5f81/Extract geometric features from stereo views)\n    C2 --\x3e C4(\u4ece\u5355\u76ee\u89c6\u56fe\u63d0\u53d6\u8bed\u4e49\u7279\u5f81/Extract semantic features from monocular view)\n    C1 --\x3e C5(\u8f85\u52a9\u4ea4\u4e92\u533a\u57df\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1/Auxiliary Interaction-Region Depth Estimation task)\n    D --\x3e D1(\u5728\u7acb\u4f53\u8bbe\u7f6e\u4e0b\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf/Large margin outperforms baselines under stereo setting)\n    D --\x3e D2(\u5bf9\u76f8\u673a\u4f4d\u59ff\u53d8\u5316\u5177\u6709\u5f3a\u9c81\u68d2\u6027/Strong robustness to camera pose variations)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical robotics], [robotic intubation, closed-loop control, shape sensing, depth estimation, human-in-the-loop]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Saksham Gupta, Sarthak Mishra, Arshad Ayub, Kamran Farooque, Spandan Roy, Babita Gupta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," International Institute of Information Technology Hyderabad (IIIT-H), All India Institute of Medical Sciences, New Delhi (AIIMS)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21983",children:"https://arxiv.org/pdf/2512.21983"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A compact, integrated robotic platform (BRIS) for fiberoptic-guided intubation, featuring a steerable bronchoscope, an independent tube advancement mechanism, and a camera-augmented mouthpiece. 2. A learning-enabled closed-loop control framework that uses real-time shape sensing to map joystick inputs to precise bronchoscope tip motion, ensuring stable teleoperation despite tendon nonlinearities. 3. The use of monocular endoscopic depth estimation to classify airway regions and provide anatomy-aware guidance for safe endotracheal tube positioning relative to the carina."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c843dca48b820a7b40108d200bec7f3af89493cac93374b9ac37ec6874acfd9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the Bab Sak Robotic Intubation System (BRIS), a human-in-the-loop platform designed to assist with safe fiberoptic endotracheal intubation. It integrates a learning-enabled control framework for stable scope navigation and uses monocular depth estimation for anatomy-aware tube placement guidance. The system was validated on airway mannequins, demonstrating reliable performance as a step toward safer and more consistent robotic airway management."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[BRIS: \u5b89\u5168\u5149\u7ea4\u63d2\u7ba1\u7cfb\u7edf / BRIS: Safe Fiberoptic Intubation System] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u63d2\u7ba1\u6280\u672f\u96be\u5ea6\u9ad8\uff0c\u73b0\u6709\u7cfb\u7edf\u4e0d\u5b8c\u5584 / Problem: Intubation is difficult, existing systems are limited]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u96c6\u6210\u673a\u5668\u4eba\u5e73\u53f0\u4e0e\u5b66\u4e60\u63a7\u5236\u6846\u67b6 / Method: Integrated robotic platform with learning-enabled control]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4eba\u4f53\u6a21\u578b\u4e0a\u9a8c\u8bc1\u53ef\u9760 / Results: Validated as reliable on mannequins]\n    B --\x3e B1[\u5e76\u53d1\u75c7\u98ce\u9669\u9ad8 / High risk of complications]\n    B --\x3e B2[\u7f3a\u4e4f\u96c6\u6210\u7ba1\u63a8\u8fdb\u4e0e\u6df1\u5ea6\u9a8c\u8bc1 / Lacks integrated tube control & depth verification]\n    C --\x3e C1[\u95ed\u73af\u63a7\u5236\u4e0e\u5f62\u72b6\u611f\u77e5 / Closed-loop control & shape sensing]\n    C --\x3e C2[\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5f15\u5bfc / Monocular depth estimation guidance]\n    D --\x3e D1[\u53ef\u9760\u5bfc\u822a / Reliable navigation]\n    D --\x3e D2[\u53ef\u63a7\u7f6e\u7ba1 / Controlled tube placement]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);