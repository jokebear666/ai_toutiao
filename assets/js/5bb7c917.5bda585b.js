"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[555],{8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},9043:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CV/20251229-20260104","title":"20251229-20260104 (cs.CV)","description":"2025-12-29","source":"@site/docs/daily/cs_CV/20251229-20260104.md","sourceDirName":"daily/cs_CV","slug":"/daily/cscv/20251229-20260104","permalink":"/ai_toutiao/daily/cscv/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767007855000,"frontMatter":{"slug":"/daily/cscv/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.CV)","permalink":"/ai_toutiao/daily/cscv/20251222-20251228"},"next":{"title":"cs.CY","permalink":"/ai_toutiao/category/cscy"}}');var a=i(4848),r=i(8453);const t={slug:"/daily/cscv/20251229-20260104"},o="20251229-20260104 (cs.CV)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251229-20260104-cscv",children:"20251229-20260104 (cs.CV)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani; GenimeLabs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21402",children:"https://arxiv.org/pdf/2512.21402"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u89c2\u4f17\u53c2\u4e0e\u5ea6/Traditional metrics fail to assess real viewer engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u4f7f\u7528VLM\u63d0\u53d6\u65e0\u76d1\u7763\u89c6\u542c\u7279\u5f81/Use VLM to extract unsupervised audiovisual features]\n    Method --\x3e M2[\u805a\u7c7b\u7279\u5f81\u5e76\u8bad\u7ec3\u56de\u5f52\u8bc4\u4f30\u5668/Cluster features and train regression evaluator]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9884\u6d4b\u4e0e\u771f\u5b9e\u53c2\u4e0e\u5ea6\u5f3a\u76f8\u5173/Strong correlation between predicted and actual engagement]\n    Results --\x3e R2[\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30/Provides interpretable and scalable assessment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," California Institute of Technology, Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21414",children:"https://arxiv.org/pdf/2512.21414"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/christinaliu2020/tool-bottleneck-framework",children:"https://github.com/christinaliu2020/tool-bottleneck-framework"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Tool Bottleneck Framework for Medical Image Understanding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Text-based tool composition fails for medical images with localized features]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches or beats baselines, more interpretable, data-efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scalable Deep Subspace Clustering Network"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Quebec at Montreal"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21434",children:"https://arxiv.org/pdf/2512.21434"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n\xd7n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Deep Subspace Clustering Network"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: O(n^3) \u8ba1\u7b97\u590d\u6742\u5ea6 / O(n^3) Computational Complexity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u5730\u6807\u8fd1\u4f3c\u4e0e\u8054\u5408\u4f18\u5316 / Landmark Approximation & Joint Optimization"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u7ebf\u6027\u590d\u6742\u5ea6\u4e0e\u53ef\u6bd4\u6027\u80fd / Linear Complexity & Comparable Performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Simon Fraser University, AIP Labs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21472",children:"https://arxiv.org/pdf/2512.21472"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," /githubsfu-mial/IMAplusplus"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21452",children:"https://arxiv.org/pdf/2512.21452"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Intelligent recognition of GPR road hidden defect images <br/> GPR\u9053\u8def\u9690\u853d\u75c5\u5bb3\u56fe\u50cf\u667a\u80fd\u8bc6\u522b") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n\n    Problem --\x3e P1("Subjective & inefficient GPR interpretation <br/> GPR\u56fe\u50cf\u89e3\u91ca\u4e3b\u89c2\u4e14\u4f4e\u6548")\n    Problem --\x3e P2("Data scarcity <br/> \u6570\u636e\u7a00\u7f3a")\n\n    Method --\x3e M1("DCGAN-based Data Augmentation <br/> \u57fa\u4e8eDCGAN\u7684\u6570\u636e\u589e\u5f3a")\n    Method --\x3e M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net\u7f51\u7edc")\n    Method --\x3e M3("MS COCO Transfer Learning <br/> MS COCO\u8fc1\u79fb\u5b66\u4e60")\n\n    Results --\x3e R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> \u9ad8\u6027\u80fd")\n    Results --\x3e R2("Robust to noise & weak signals <br/> \u5bf9\u566a\u58f0\u548c\u5f31\u4fe1\u53f7\u9c81\u68d2")\n    Results --\x3e R3("New paradigm for automated detection <br/> \u81ea\u52a8\u5316\u68c0\u6d4b\u65b0\u8303\u5f0f")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21459",children:"https://arxiv.org/pdf/2512.21459"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/chloeqxq/CCAD",children:"https://github.com/chloeqxq/CCAD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5f02\u5e38\u68c0\u6d4b\u5728\u6709\u9650\u5f02\u5e38\u6570\u636e\u4e0b\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u6548\u7387\u548c\u7ea6\u675f\u4e0a\u7684\u4e0d\u8db3]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faCCAD\uff0c\u878d\u5408\u91cd\u5efa\u4e0e\u8868\u5f81\u65b9\u6cd5\uff0c\u4f7f\u7528\u538b\u7f29\u7684\u5168\u5c40\u7279\u5f81\u4f5c\u4e3a\u91cd\u5efa\u6a21\u578b\u7684\u6761\u4ef6]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728AUC\u4e0a\u8d85\u8d8aSOTA\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u8d21\u732e\u4e86\u91cd\u65b0\u6807\u6ce8\u7684DAGM 2007\u6570\u636e\u96c6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21476",children:"https://arxiv.org/pdf/2512.21476"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JeremyXSC/GPF-Net",children:"https://github.com/JeremyXSC/GPF-Net"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Coarse high-level features lead to inferior results for small polyps]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Generative Multi-Focus Image Fusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Donghai Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21495",children:"https://arxiv.org/pdf/2512.21495"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Xinzhe99/StackMFF-Series",children:"https://github.com/Xinzhe99/StackMFF-Series"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Generative Multi-Focus Image Fusion] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u603b\u6709\u5bf9\u7126\u56fe\u50cf/Existing methods assume always an in-focus image]\n    B --\x3e B2[\u8fb9\u7f18\u4f2a\u5f71/Edge artifacts]\n    C --\x3e C1[\u9636\u6bb5\u4e00: \u786e\u5b9a\u6027\u878d\u5408/Stage 1: Deterministic Fusion (StackMFF V4)]\n    C --\x3e C2[\u9636\u6bb5\u4e8c: \u751f\u6210\u5f0f\u6062\u590d/Stage 2: Generative Restoration (IFControlNet)]\n    D --\x3e D1[SOTA\u6027\u80fd/State-of-the-art performance]\n    D --\x3e D2[\u5904\u7406\u590d\u6742\u591a\u7126\u5185\u5bb9/Handles complex multi-focal content]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21510",children:"https://arxiv.org/pdf/2512.21510"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art IMVC performance and superior robustness."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Ashik Khan, Arafat Alam Jion"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21512",children:"https://arxiv.org/pdf/2512.21512"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21507",children:"https://arxiv.org/pdf/2512.21507"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Gloria2tt/SVBench-Evaluation",children:"https://github.com/Gloria2tt/SVBench-Evaluation"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SVBench: \u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u793e\u4f1a\u63a8\u7406\u8bc4\u4f30<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u793e\u4f1a\u63a8\u7406\u80fd\u529b<br>Current models lack social reasoning]\n    C --\x3e C1[\u6784\u5efa\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u8303\u5f0f\u7684\u57fa\u51c6<br>Build benchmark based on social psychology paradigms]\n    C --\x3e C2[\u4f7f\u7528\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u514d\u8d39\u6d41\u7a0b\u8fdb\u884c\u8bc4\u4f30<br>Use training-free agent-based pipeline for evaluation]\n    D --\x3e D1[\u6a21\u578b\u5728\u8868\u9762\u5408\u7406\u6027\u4e0a\u8868\u73b0\u826f\u597d<br>Models perform well on surface-level plausibility]\n    D --\x3e D2[\u6a21\u578b\u5728\u793e\u4f1a\u63a8\u7406\u7ef4\u5ea6\u4e0a\u7cfb\u7edf\u6027\u5931\u8d25<br>Models fail systematically on social reasoning dimensions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Ashik Khan, Md Nahid Siddique"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Kharagpur, Florida International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21508",children:"https://arxiv.org/pdf/2512.21508"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods\u2014degraded model calibration (ECE: 0.29-0.34)\u2014and suggested post-hoc calibration as a solution for clinical deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u591a\u6a21\u6001\u80f8\u90e8X\u5149\u5206\u6790\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6cdb\u5316\u6027\u5dee / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u51bb\u7ed3\u7f16\u7801\u5668\u548c\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]\n    D[\u5173\u952e\u7ed3\u679c/Results: PET\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u5168\u5fae\u8c03\uff0c\u4f46\u6821\u51c6\u6027\u5dee / PET methods outperform full fine-tuning but have degraded calibration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21514",children:"https://arxiv.org/pdf/2512.21514"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DiverseGRPO: Mitigating Mode Collapse] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[GRPO\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83/GRPO causes mode collapse]\n    B1 --\x3e B2[\u7f3a\u4e4f\u89c6\u89c9\u591a\u6837\u6027/Lacks visual diversity]\n    C --\x3e C1[\u5956\u52b1\u5c42\u9762: \u5206\u5e03\u521b\u9020\u529b\u5956\u52b1/Reward Level: Distributional Creativity Bonus]\n    C --\x3e C2[\u751f\u6210\u5c42\u9762: \u7ed3\u6784\u611f\u77e5\u6b63\u5219\u5316/Generation Level: Structure-Aware Regularization]\n    C1 --\x3e C3[\u57fa\u4e8e\u8bed\u4e49\u5206\u7ec4\u7684\u8c31\u805a\u7c7b/Spectral Clustering for Semantic Grouping]\n    D --\x3e D1[\u8bed\u4e49\u591a\u6837\u6027\u63d0\u534713%-18%/13%-18% Semantic Diversity Improvement]\n    D --\x3e D2[\u5efa\u7acb\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf/Establishes New Pareto Frontier]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fuzhou University, Research Institute of Highway, Ministry of Transport"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21513",children:"https://arxiv.org/pdf/2512.21513"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/WangPuyun/MuS-Polar3D",children:"https://github.com/WangPuyun/MuS-Polar3D"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u7684\u6563\u5c04\u548c\u89c2\u6d4b\u6761\u4ef6/Existing datasets lack diversity in scattering and observation conditions)\n    C --\x3e C1(\u6784\u5efa\u5305\u542b42\u4e2a\u7269\u4f53\u30017\u79cd\u6563\u5c04\u6761\u4ef6\u30015\u4e2a\u89c6\u89d2\u7684\u504f\u632f\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)\n    C --\x3e C2(\u63d0\u51fa\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u6210\u50cf\u6d41\u7a0b\uff1a\u53bb\u6563\u5c04\u540e3D\u91cd\u5efa/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)\n    D --\x3e D1(\u5b9e\u73b0\u6700\u4f73\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee15.49\u5ea6/Achieve best mean angular error of 15.49 degrees)\n    D --\x3e D2(\u9996\u4e2a\u516c\u5f00\u7684\u5b9a\u91cf\u6d51\u6d4a\u6c34\u4e0b\u504f\u632f3D\u6210\u50cf\u57fa\u51c6\u6570\u636e\u96c6/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21516",children:"https://arxiv.org/pdf/2512.21516"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>\u5168\u5c40-\u5c40\u90e8\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Rare-paired & Mis-paired Samples<br/>\u6837\u672c\u914d\u5bf9\u7a00\u5c11\u4e0e\u9519\u8bef] --\x3e B1[Incomplete & Noise Multi-View Data<br/>\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u591a\u89c6\u56fe\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Unified Contrastive Learning Framework<br/>\u7edf\u4e00\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6] --\x3e C1[Global-Graph Guided CL<br/>\u5168\u5c40\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60]\n    C --\x3e C2[Local-Graph Weighted CL<br/>\u5c40\u90e8\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60]\n    C1 --\x3e C1a[Construct Global Affinity Graph<br/>\u6784\u5efa\u5168\u5c40\u4eb2\u548c\u529b\u56fe]\n    C2 --\x3e C2a[Generate Adaptive Weights<br/>\u751f\u6210\u81ea\u9002\u5e94\u6743\u91cd]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Superior Clustering Performance<br/>\u4f18\u8d8a\u7684\u805a\u7c7b\u6027\u80fd] --\x3e D1[Outperforms SOTA Methods<br/>\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5]\n    D --\x3e D2[Effective on Incomplete & Noise Data<br/>\u5728\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u6570\u636e\u4e0a\u6709\u6548]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Washington, Intel"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21529",children:"https://arxiv.org/pdf/2512.21529"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("VLMs\u9002\u5e94\u5c42\u7ea7\u5206\u7c7b\u6548\u7387\u4f4e/VLMs inefficient for hierarchical classification")\n    Problem --\x3e P2("\u6807\u51c6\u65b9\u6cd5\u9884\u6d4b\u4e0d\u4e00\u81f4/Standard methods produce inconsistent predictions")\n    Method --\x3e M1("\u63d0\u51fa\u5c42\u7ea7\u611f\u77e5\u5fae\u8c03\u6846\u67b6/Propose hierarchy-aware fine-tuning framework")\n    Method --\x3e M2("\u7ed3\u5408TP-KL\u4e0eHiSCE\u635f\u5931/Combine TP-KL and HiSCE losses")\n    Method --\x3e M3("\u96c6\u6210\u8f7b\u91cf\u7ea7LoRA\u9002\u914d/Integrate lightweight LoRA adaptation")\n    Results --\x3e R1("\u63d0\u5347\u5168\u8def\u5f84\u7cbe\u5ea6/Improves Full-Path Accuracy")\n    Results --\x3e R2("\u964d\u4f4e\u4e0d\u4e00\u81f4\u6027\u9519\u8bef/Reduces Tree-based Inconsistency Error")\n    Results --\x3e R3("\u53c2\u6570\u5f00\u9500\u6700\u5c0f/Minimal parameter overhead")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Vision Transformers are Circulant Attention Learners"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21542",children:"https://arxiv.org/pdf/2512.21542"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/LeapLabTHU/Circulant-Attention",children:"https://github.com/LeapLabTHU/Circulant-Attention"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Vision Transformers are Circulant Attention Learners] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Self-attention quadratic complexity O(N\xb2) is computationally heavy for high-resolution vision tasks.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21560",children:"https://arxiv.org/pdf/2512.21560"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Toward Intelligent Scene Augmentation<br>\u667a\u80fd\u573a\u666f\u589e\u5f3a] --\x3e B(Problem: Existing image editing lacks contextual appropriateness<br>\u6838\u5fc3\u95ee\u9898: \u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u5408\u7406\u6027)\n    A --\x3e C(Method: Introduce two new tasks & build datasets<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51fa\u4e24\u4e2a\u65b0\u4efb\u52a1\u5e76\u6784\u5efa\u6570\u636e\u96c6)\n    A --\x3e D(Results: Identifies research gap for joint reasoning & generation<br>\u5173\u952e\u7ed3\u679c: \u6307\u51fa\u4e86\u8054\u5408\u63a8\u7406\u4e0e\u751f\u6210\u7684\u7814\u7a76\u7a7a\u767d)\n    B --\x3e E(Task 1: Context-aware object insertion<br>\u4efb\u52a11: \u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u4f53\u63d2\u5165)\n    B --\x3e F(Task 2: Sponsor-product logo augmentation<br>\u4efb\u52a12: \u8d5e\u52a9\u5546\u4ea7\u54c1\u5546\u6807\u589e\u5f3a)\n    C --\x3e G(Utilize VLMs and diffusion models<br>\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b)\n    C --\x3e H(Build annotated datasets<br>\u6784\u5efa\u5e26\u6807\u6ce8\u7684\u6570\u636e\u96c6)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Seoul National University, OGQ"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21545",children:"https://arxiv.org/pdf/2512.21545"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Exploration of Reproducible Generated Image Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yihang Duan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21562",children:"https://arxiv.org/pdf/2512.21562"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Exploration of Reproducible Generated Image Detection] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Poor Reproducibility & Generalizability]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Literature Review, Dataset Construction, Method Reproduction]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Zhongguancun Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21576",children:"https://arxiv.org/pdf/2512.21576"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Long-window Anchoring in Vision-Language Model Distillation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Small distilled VLMs have limited effective context windows]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: LAid - Progressive attention matching & learnable RoPE modulation]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves up to 3.2x longer context, maintains benchmark performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Keio University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21582",children:"https://arxiv.org/pdf/2512.21582"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("LLM-Free Image Captioning Evaluation in Reference-Flexible Settings") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("LLM-based metrics lack neutrality/LLM\u6307\u6807\u7f3a\u4e4f\u4e2d\u7acb\u6027")\n    Problem --\x3e P2("LLM-free metrics lack performance/\u65e0LLM\u6307\u6807\u6027\u80fd\u4e0d\u8db3")\n    Method --\x3e M1("Propose Pearl metric/\u63d0\u51faPearl\u6307\u6807")\n    Method --\x3e M2("Learn image-caption & caption-caption similarity/\u5b66\u4e60\u56fe\u50cf-\u63cf\u8ff0\u4e0e\u63cf\u8ff0-\u63cf\u8ff0\u76f8\u4f3c\u6027")\n    Method --\x3e M3("Construct large human-annotated dataset/\u6784\u5efa\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6")\n    Results --\x3e R1("Outperforms LLM-free metrics on benchmarks/\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5176\u4ed6\u65e0LLM\u6307\u6807")\n    Results --\x3e R2("Works in reference-based & reference-free settings/\u9002\u7528\u4e8e\u6709\u53c2\u8003\u548c\u65e0\u53c2\u8003\u8bbe\u7f6e")\n    Results --\x3e R3("Fast & aligned with human judgment/\u5feb\u901f\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21584",children:"https://arxiv.org/pdf/2512.21584"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight Global\u2013Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Southwestern University of Finance and Economics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21598",children:"https://arxiv.org/pdf/2512.21598"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21599",children:"https://arxiv.org/pdf/2512.21599"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yusuf Brima, Marcellin Atemkeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Osnabr\xfcck University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21602",children:"https://arxiv.org/pdf/2512.21602"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Imbalanced clinical data in emergency/critical care<br>\u6025\u8bca/\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>\u7cfb\u7edf\u8bc4\u4f30\u6811\u6a21\u578b\u3001TabNet\u53ca\u63d0\u51fa\u7684TabResNet]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost\u6700\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u4e0b\u6027\u80fd\u4e0b\u964d]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21616",children:"https://arxiv.org/pdf/2512.21616"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TAMEing Long Contexts in Personalization<br>\u8bba\u6587\u6807\u9898] --\x3e B[Problem: Existing MLLM personalization lacks long-context support<br>\u6838\u5fc3\u95ee\u9898: \u73b0\u6709MLLM\u4e2a\u6027\u5316\u65b9\u6cd5\u7f3a\u4e4f\u957f\u4e0a\u4e0b\u6587\u652f\u6301]\n    A --\x3e C[Method: Proposes TAME framework with double memory & RA2G<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51faTAME\u6846\u67b6, \u5305\u542b\u53cc\u91cd\u8bb0\u5fc6\u548cRA2G\u8303\u5f0f]\n    A --\x3e D[Results: TAME achieves best performance on LCMP benchmark<br>\u5173\u952e\u7ed3\u679c: TAME\u5728LCMP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21618",children:"https://arxiv.org/pdf/2512.21618"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f\u4ea4\u901a\u7f16\u8f91 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u5bf9\u79f0\u81ea\u56de\u5f52\u5728\u7ebf\u4fee\u590d\u8303\u5f0f\u4e0e\u514d\u8bad\u7ec3\u534f\u8c03\u673a\u5236 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u65b0\u89c6\u89d2\u589e\u5f3a\u548c3D\u8f66\u8f86\u63d2\u5165\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiwen Yang, Jinglin Xu, Yuxin Pen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, University of Science and Technology Beijing"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21617",children:"https://arxiv.org/pdf/2512.21617"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM",children:"https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]\n    Root --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[Support set as confounder/\u652f\u6301\u96c6\u4f5c\u4e3a\u6df7\u6dc6\u53d8\u91cf]\n    Problem --\x3e P2[Biased data distribution/\u6709\u504f\u6570\u636e\u5206\u5e03]\n    Problem --\x3e P3[Spurious correlations/\u865a\u5047\u5173\u8054]\n    Method --\x3e M1[Causal Intervention/\u56e0\u679c\u5e72\u9884]\n    Method --\x3e M2[IMSE: Sample-level/IMSE: \u6837\u672c\u5c42\u9762]\n    Method --\x3e M3[IMFR: Feature-level/IMFR: \u7279\u5f81\u5c42\u9762]\n    Results --\x3e R1[SOTA Performance/\u6700\u4f18\u6027\u80fd]\n    Results --\x3e R2[Datasets: CUB, Dogs, Cars/\u6570\u636e\u96c6: CUB, Dogs, Cars]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Universitas Brawijaya"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21637",children:"https://arxiv.org/pdf/2512.21637"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Attribute Entanglement in Text-Driven Image Editing)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Sparse Latent Constraints via L1 Regularization)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: More Focused Edits, Reduced Unintended Changes)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21643",children:"https://arxiv.org/pdf/2512.21643"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Zhouzone/OmniWeather",children:"https://github.com/Zhouzone/OmniWeather"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Omni-Weather: \u7edf\u4e00\u591a\u6a21\u6001\u5929\u6c14\u57fa\u7840\u6a21\u578b / Unified Multimodal Foundation Model for Weather] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898 / Problem: \u5929\u6c14\u5efa\u6a21\u4e2d\u751f\u6210\u4e0e\u7406\u89e3\u4efb\u52a1\u5206\u79bb / Separation of generation and understanding in weather modeling]\n    C[\u4e3b\u8981\u65b9\u6cd5 / Method: \u7edf\u4e00\u67b6\u6784\uff0c\u5171\u4eab\u81ea\u6ce8\u610f\u529b\uff0c\u601d\u7ef4\u94fe\u6570\u636e\u96c6 / Unified architecture, shared self-attention, Chain-of-Thought dataset]\n    D[\u5173\u952e\u7ed3\u679c / Results: SOTA\u6027\u80fd\uff0c\u4efb\u52a1\u4e92\u589e\u5f3a / State-of-the-art performance, mutual enhancement of tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Fudan University, Huawei Technologies Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21641",children:"https://arxiv.org/pdf/2512.21641"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TrackTeller: Temporal Multimodal 3D Grounding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u52a8\u60013D\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u4f9d\u8d56\u8bed\u8a00\u6307\u4ee3/Dynamic 3D Behavior-Dependent Language Grounding]\n    C --\x3e C1[\u7edf\u4e00\u591a\u6a21\u6001\u65f6\u5e8f\u6846\u67b6/Unified Temporal Multimodal Framework]\n    C1 --\x3e C2[LiDAR-\u56fe\u50cf\u878d\u5408\u4e0e\u8bed\u8a00\u89e3\u7801/LiDAR-Image Fusion & Language Decoding]\n    C1 --\x3e C3[\u6784\u5efaUniScene\u8868\u793a/Build UniScene Representation]\n    C1 --\x3e C4[\u5229\u7528\u8fd0\u52a8\u5386\u53f2\u63a8\u7406/Reason with Motion History]\n    D --\x3e D1[\u5728NuPrompt\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd/Significant Improvement on NuPrompt]\n    D1 --\x3e D2[AMOTA\u63d0\u534770%/70% AMOTA Gain]\n    D1 --\x3e D3[\u8bef\u62a5\u7387\u964d\u4f4e3.15-3.4\u500d/3.15-3.4x FA Reduction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Subramanyam Sahoo, Jared Junkin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley, Johns Hopkins University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21670",children:"https://arxiv.org/pdf/2512.21670"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective",children:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u662f\u9ed1\u76d2\u6a21\u578b/Deepfake detectors are black boxes]\n    C --\x3e C1[\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790/Sparse Autoencoder (SAE) Analysis]\n    C --\x3e C2[\u6cd5\u8bc1\u6d41\u5f62\u5206\u6790/Forensic Manifold Analysis]\n    D --\x3e D1[\u6f5c\u5728\u7279\u5f81\u7a00\u758f\u4f7f\u7528/Latent features are sparsely used]\n    D --\x3e D2[\u6d41\u5f62\u51e0\u4f55\u7279\u6027\u63ed\u793a\u4f2a\u5f71/Manifold geometry reveals artifacts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jalal Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," United Arab Emirates University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21673",children:"https://arxiv.org/pdf/2512.21673"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd/Evaluate DL model performance for AV perception]\nC --\x3e C1[\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6bd4\u8f83YOLO-NAS\u4e0eYOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\nD --\x3e D1[YOLOv8s\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1175%/YOLOv8s saves 75% training time]\nD --\x3e D2[YOLOv8s\u51c6\u786e\u7387\u66f4\u9ad8(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21675",children:"https://arxiv.org/pdf/2512.21675"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/thunderbolt215/UniPercept",children:"https://github.com/thunderbolt215/UniPercept"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["UniPercept: \u7edf\u4e00\u611f\u77e5\u7ea7\u56fe\u50cf\u7406\u89e3 / Unified Perceptual-Level Image Understanding"] --\x3e Problem["MLLMs\u611f\u77e5\u80fd\u529b\u6709\u9650 / MLLMs\' Perceptual Ability is Limited"]\n    Root --\x3e Method["\u63d0\u51fa\u7edf\u4e00\u57fa\u51c6\u4e0e\u6a21\u578b / Proposes Unified Benchmark & Model"]\n    Root --\x3e Results["\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6a21\u578b / Outperforms Existing Models"]\n    Problem --\x3e P1["\u611f\u77e5\u7ea7\u7279\u5f81\u7406\u89e3\u4e0d\u8db3 / Limited Perceptual-Level Feature Understanding"]\n    Method --\x3e M1["UniPercept-Bench\u57fa\u51c6 / UniPercept-Bench Benchmark"]\n    Method --\x3e M2["DAPT\u4e0eTask-Aligned RL\u8bad\u7ec3 / DAPT & Task-Aligned RL Training"]\n    Results --\x3e R1["\u5728VR\u4e0eVQA\u4efb\u52a1\u4e0a\u6cdb\u5316 / Generalizes on VR & VQA Tasks"]\n    Results --\x3e R2["\u53ef\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u5956\u52b1\u6a21\u578b / Serves as Reward Model for Generation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Rensselaer Polytechnic Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21684",children:"https://arxiv.org/pdf/2512.21684"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VLM\u8bed\u4e49\u8f93\u51fa\u96be\u4ee5\u9a8c\u8bc1\u3001\u590d\u73b0\u548c\u5ba1\u8ba1\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6eaf\u6e90\u6846\u67b6\uff0c\u4ece\u5e7b\u706f\u7247\u4e2d\u63d0\u53d6\u6982\u5ff5\u548c\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u54c8\u5e0c\u4e0a\u94fe / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63ed\u793a\u6a21\u578b\u95f4\u663e\u8457\u5dee\u5f02\uff0c\u5b9e\u73b0\u5b8c\u7f8e\u7be1\u6539\u68c0\u6d4b\u548c\u786e\u5b9a\u6027\u590d\u73b0\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b8c\u6574\u6027 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology, University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21683",children:"https://arxiv.org/pdf/2512.21683"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/primebo1/C-Graph",children:"https://github.com/primebo1/C-Graph"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u8fc7\u6ee4\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u635f\u5bb3\u6027\u80fd/Existing methods filter domain-specific info, harming performance]\n    C --\x3e C1[\u5229\u7528\u7ed3\u6784\u4e00\u81f4\u6027\u4f5c\u4e3a\u5148\u9a8c/Use structural consistency as prior]\n    C --\x3e C2[\u56fe\u5efa\u6a21: SPG\u5c42, SMD\u89e3\u7801, CNC\u635f\u5931/Graph Modeling: SPG layer, SMD decoding, CNC loss]\n    D --\x3e D1[\u8de8\u57df\u6027\u80fdSOTA/Cross-domain SOTA performance]\n    D --\x3e D2[\u4fdd\u6301\u6e90\u57df\u7cbe\u5ea6/Preserves source-domain accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21691",children:"https://arxiv.org/pdf/2512.21691"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[VGGT\u4e2d\u7684\u6ce8\u610f\u529b\u5d29\u6e83\u73b0\u8c61/Attention Collapse in VGGT]\n    C --\x3e C1[\u5c06\u6ce8\u610f\u529b\u8fed\u4ee3\u5efa\u6a21\u4e3a\u9000\u5316\u6269\u6563\u8fc7\u7a0b/Model Attention Iteration as Degenerate Diffusion]\n    D --\x3e D1[\u63a8\u5bfc\u51fa\u9884\u6d4b\u5d29\u6e83\u7684\u5747\u503c\u573aPDE/Derive Mean-Field PDE Predicting Collapse]\n    D --\x3e D2[\u7406\u8bba\u89e3\u91ca\u4ee4\u724c\u5408\u5e76\u7684\u7f13\u89e3\u4f5c\u7528/Theory Explains Token-Merging Remedy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Albert Barreiro, Roger Mar\xed, Rafael Redondo, Gloria Haro, Carles Bosch"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Eurecat, Centre Tecnol\xf2gic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21692",children:"https://arxiv.org/pdf/2512.21692"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u5404\u5411\u5f02\u6027\u9ad8\u5149/Existing methods struggle with anisotropic specular reflections]\n    C --\x3e C1[\u63d0\u51faShinyNeRF\u6846\u67b6/Propose ShinyNeRF framework]\n    C1 --\x3e C2[\u4f7f\u7528\u5404\u5411\u540c\u6027vMF\u6df7\u5408\u8fd1\u4f3c\u51fa\u5c04\u8f90\u5c04\u5ea6/Use isotropic vMF mixture to approximate outgoing radiance]\n    C2 --\x3e C3[\u8054\u5408\u4f30\u8ba1\u6cd5\u7ebf\u3001\u5207\u7ebf\u3001\u9ad8\u5149\u53c2\u6570/Jointly estimate normals, tangents, specular parameters]\n    D --\x3e D1[\u5b9e\u73b0SOTA\u6027\u80fd/Achieves SOTA performance]\n    D --\x3e D2[\u63d0\u4f9b\u7269\u7406\u89e3\u91ca\u548c\u6750\u8d28\u7f16\u8f91/Provides physical interpretation and material editing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Li Yang, Yuting Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Wannan Medical College"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21693",children:"https://arxiv.org/pdf/2512.21693"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Prior-AttUNet: \u89c6\u7f51\u819cOCT\u6db2\u4f53\u5206\u5272 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6a21\u7cca\u8fb9\u754c\u4e0e\u8bbe\u5907\u5dee\u5f02 / Ambiguous Boundaries & Device Heterogeneity]\n    C --\x3e C1[\u53cc\u8def\u5f84\u67b6\u6784 / Dual-Path Architecture]\n    C --\x3e C2[\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5148\u9a8c / VAE Priors]\n    C --\x3e C3[\u4e09\u91cd\u6ce8\u610f\u529b\u673a\u5236 / Triple-Attention Mechanism]\n    D --\x3e D1[\u9ad8Dice\u5206\u6570 / High Dice Scores]\n    D --\x3e D2[\u4f4e\u8ba1\u7b97\u6210\u672c / Low Computational Cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21694",children:"https://arxiv.org/pdf/2512.21694"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[BeHGAN: Bengali Handwritten Word Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[HTG is challenging & understudied for Bengali<br/>\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u751f\u6210\u7814\u7a76\u4e0d\u8db3\u4e14\u56f0\u96be]\n    C --\x3e C1[Propose GAN-based method<br/>\u63d0\u51fa\u57fa\u4e8eGAN\u7684\u65b9\u6cd5]\n    C --\x3e C2[Use self-collected dataset<br/>\u4f7f\u7528\u81ea\u6536\u96c6\u6570\u636e\u96c6]\n    C --\x3e C3[Pre-process images<br/>\u9884\u5904\u7406\u56fe\u50cf]\n    D --\x3e D1[Generates diverse handwritten words<br/>\u751f\u6210\u591a\u6837\u5316\u624b\u5199\u8bcd]\n    D --\x3e D2[Contributes to Bengali HTG research<br/>\u63a8\u52a8\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u751f\u6210\u7814\u7a76]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content. Affiliation/email domain not present."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21695",children:"https://arxiv.org/pdf/2512.21695"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Reliable detection of AI-generated images"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: SOTA on Chameleon, strong generalization across datasets"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology, Nanjing Forestry University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21707",children:"https://arxiv.org/pdf/2512.21707"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/alanyz106/ST-MoE",children:"https://github.com/alanyz106/ST-MoE"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u4e0d\u7075\u6d3b\u7684\u65f6\u7a7a\u8868\u793a/Inflexible spatiotemporal representation")\n    Problem --\x3e P2("\u9ad8\u8ba1\u7b97\u6210\u672c/High computational cost")\n    Method --\x3e M1("\u63d0\u51faST-MoE\u6a21\u578b/Propose ST-MoE model")\n    M1 --\x3e M2("\u56db\u79cd\u65f6\u7a7a\u4e13\u5bb6/Four spatiotemporal experts")\n    M2 --\x3e M3("\u53cc\u5411\u65f6\u7a7aMamba/Bidirectional spatiotemporal Mamba")\n    Results --\x3e R1("\u7cbe\u5ea6\u8d85\u8d8aSOTA/Outperforms SOTA in accuracy")\n    Results --\x3e R2("\u53c2\u6570\u51cf\u5c1141.38%/Reduces parameters by 41.38%")\n    Results --\x3e R3("\u8bad\u7ec3\u52a0\u901f3.6\u500d/3.6x training speedup")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21710",children:"https://arxiv.org/pdf/2512.21710"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Thelegendzz/RAPTOR",children:"https://github.com/Thelegendzz/RAPTOR"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[RAPTOR: \u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u89c6\u9891\u9884\u6d4b<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u89c6\u9891\u9884\u6d4b\u7684"\u4e09\u96be\u56f0\u5883": \u901f\u5ea6\u3001\u5206\u8fa8\u7387\u3001\u8d28\u91cf<br>Video Prediction Trilemma: Speed, Resolution, Quality]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u9ad8\u6548\u89c6\u9891\u6ce8\u610f\u529b (EVA) \u6a21\u5757<br>Efficient Video Attention (EVA) Module]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u9996\u4e2a\u5728Jetson\u4e0a512^2\u89c6\u9891>30 FPS<br>First >30 FPS for 512^2 video on Jetson]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Amap Alibaba, Peking University (PKU), Tsinghua University (THU)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21714",children:"https://arxiv.org/pdf/2512.21714"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://astra-amap.github.io/AstraNav-World.github.io/",children:"https://astra-amap.github.io/AstraNav-World.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AstraNav-World] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21734",children:"https://arxiv.org/pdf/2512.21734"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://humanaigc.github.io/knot_forcing_demo_page/",children:"https://humanaigc.github.io/knot_forcing_demo_page/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A "running ahead" mechanism that dynamically updates the reference frame\'s temporal coordinate to maintain long-term coherence.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: \u9a6f\u670d\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>\u5b9e\u65f6\u8096\u50cf\u52a8\u753b\u9700\u8981\u4f4e\u5ef6\u8fdf\u548c\u4e00\u81f4\u6027\uff0c\u4f46\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u8fd0\u52a8\u4e0d\u8fde\u7eed\u95ee\u9898\u3002"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and \'Running Ahead\' mechanism.<br>\u5206\u5757\u751f\u6210\u4e0eKV\u7f13\u5b58\uff0c\u7528\u4e8e\u5e73\u6ed1\u8fc7\u6e21\u7684\u65f6\u5e8f\u7ed3\u6a21\u5757\uff0c\u4ee5\u53ca\'\u8d85\u524d\u8fd0\u884c\'\u673a\u5236\u3002"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u3001\u4ea4\u4e92\u5f0f\u7684\u5b9e\u65f6\u8096\u50cf\u52a8\u753b\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21736",children:"https://arxiv.org/pdf/2512.21736"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://humanaigc.github.io/sync_anyone_demo_page/",children:"https://humanaigc.github.io/sync_anyone_demo_page/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[SyncAnyone] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u73b0\u6709\u65b9\u6cd5\u7834\u574f\u65f6\u7a7a\u4e0a\u4e0b\u6587 / Existing methods disrupt spatiotemporal context"]\n    C --\x3e C1["\u9636\u6bb51: \u57fa\u4e8e\u63a9\u7801\u7684\u6269\u6563\u53d8\u6362\u5668\u8bad\u7ec3 / Stage 1: Mask-based DiT training"]\n    C --\x3e C2["\u9636\u6bb52: \u65e0\u63a9\u7801\u8c03\u4f18\u4e0e\u81ea\u6821\u6b63 / Stage 2: Mask-free tuning & self-correction"]\n    D --\x3e D1["SOTA\u89c6\u89c9\u8d28\u91cf\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027 / SOTA visual quality & temporal coherence"]\n    D --\x3e D2["\u66f4\u597d\u7684\u8eab\u4efd\u4e0e\u80cc\u666f\u4fdd\u6301 / Better identity & background preservation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hengyi Wu, Zhenyi Wang, Heng Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Maryland, College Park, University of Central Florida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21743",children:"https://arxiv.org/pdf/2512.21743"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --\x3e P1[\u9ad8\u71b5\u5c42\u6b20\u62df\u5408/High-entropy layers underfit]\n    Problem --\x3e P2[\u4f4e\u71b5\u5c42\u8fc7\u62df\u5408/Low-entropy layers overfit]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Entropy-aware dynamic feedback for layer-wise control] --\x3e M1[\u51cf\u5c11\u9ad8\u71b5\u5c42\u71b5\u503c/Reduce entropy in high-entropy layers]\n    Method --\x3e M2[\u589e\u52a0\u4f4e\u71b5\u5c42\u71b5\u503c/Increase entropy in low-entropy layers]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Improved generalization and performance] --\x3e R1[\u6536\u655b\u5230\u66f4\u5bbd\u7684\u5c40\u90e8\u6781\u5c0f\u503c/Converge to wider local minima]\n    Results --\x3e R2[\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5/Outperforms state-of-the-art baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Roorkee, OP Jindal University, Lule\xe5 University of Technology, Indian Institute of Technology Dhanbad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21747",children:"https://arxiv.org/pdf/2512.21747"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Driver drowsiness detection for road safety)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved stability on SEED-VIG, SOTA on STEW)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Anna University, National Institute of Technology Tiruchirappalli"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21760",children:"https://arxiv.org/pdf/2512.21760"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Scarcity of paired & aligned multimodal medical datasets]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Evgeny Alves Limarenko, Anastasiia Studenikina"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Moscow Institute of Physics and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21769",children:"https://arxiv.org/pdf/2512.21769"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("BertsWin: 3D MAE\u4f18\u5316") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("3D MAE\u62d3\u6251\u7a00\u758f\u6027/Topological Sparsity in 3D MAE")\n    Problem --\x3e P2("\u7834\u574f\u7a7a\u95f4\u5173\u7cfb/Destroys Spatial Context")\n    Method --\x3e M1("BertsWin\u6df7\u5408\u67b6\u6784/BertsWin Hybrid Architecture")\n    Method --\x3e M2("\u5b8c\u65743D\u4ee4\u724c\u7f51\u683c/Full 3D Token Grid")\n    Method --\x3e M3("Swin\u7a97\u53e3 & \u7ed3\u6784\u635f\u5931/Swin Windows & Structural Loss")\n    Results --\x3e R1("5.8x\u8bed\u4e49\u6536\u655b\u52a0\u901f/5.8x Faster Convergence")\n    Results --\x3e R2("15\u500d\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c11/15x Fewer Epochs")\n    Results --\x3e R3("FLOPs\u6301\u5e73\uff0c\u603b\u8d44\u6e90\u51cf\u5c11/FLOP Parity, Net Resource Reduction")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Inference-based GAN Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jingbo Yang, Adrian G. Bors"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of York"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21776",children:"https://arxiv.org/pdf/2512.21776"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Inference-based GAN Video Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u751f\u6210\u957f\u89c6\u9891/Existing models struggle with long video generation]\n    P1 --\x3e P2[\u89c6\u9891\u957f\u5ea6\u589e\u52a0\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d/Increased length degrades quality]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faVAE-GAN\u6df7\u5408\u89c6\u9891\u751f\u6210\u5668/Propose VAE-GAN hybrid video generator]\n    M1 --\x3e M2[\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u6269\u5c55/Extend with Markov chain framework]\n    M2 --\x3e M3[\u72b6\u6001\u4ee3\u8868\u77ed\u89c6\u9891\u751f\u6210\u5668/Each state is a short video generator]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u80fd\u591f\u751f\u6210\u957f\u89c6\u9891\u5e8f\u5217/Can generate long video sequences]\n    R1 --\x3e R2[\u786e\u4fdd\u65f6\u5e8f\u8fde\u7eed\u6027\u4e0e\u4e00\u81f4\u6027/Ensures temporal continuity and consistency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Ben-Gurion University, Amazon Prime Video, Tel-Aviv University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21778",children:"https://arxiv.org/pdf/2512.21778"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Scene-VLM: Multimodal Video Scene Segmentation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u73b0\u6709\u65b9\u6cd5\u9650\u5236/Limitations of existing methods")\n    P1 --\x3e P1_1("\u89c6\u89c9\u4e2d\u5fc3\u504f\u89c1/Visual-centric bias")\n    P1 --\x3e P1_2("\u5b64\u7acb\u5206\u7c7b/Isolated shot classification")\n    P1 --\x3e P1_3("\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/Lack explainability")\n    Method --\x3e M1("\u5fae\u8c03VLM\u6846\u67b6/Fine-tuned VLM framework")\n    M1 --\x3e M1_1("\u591a\u6a21\u6001\u63a8\u7406/Multimodal reasoning")\n    M1 --\x3e M1_2("\u5e8f\u5217\u9884\u6d4b/Sequential prediction")\n    M1 --\x3e M1_3("\u4e0a\u4e0b\u6587\u805a\u7126\u7a97\u53e3/Context-focus window")\n    Method --\x3e M2("\u7f6e\u4fe1\u5ea6\u63d0\u53d6/Confidence score extraction")\n    Method --\x3e M3("\u751f\u6210\u89e3\u91ca/Generate rationales")\n    Results --\x3e R1("SOTA\u6027\u80fd/State-of-the-art performance")\n    Results --\x3e R2("\u663e\u8457\u63d0\u5347/Significant improvement on MovieNet")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21789",children:"https://arxiv.org/pdf/2512.21789"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u79d1\u5b66\u56fe\u8868\u8bf4\u660e\u8d28\u91cf\u5dee/Poor quality of scientific figure captions]\n    Problem --\x3e P2[\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6/Lack of large-scale real-world dataset]\n    Method --\x3e M1[\u6784\u5efaarXiv\u56fe\u8868-\u8bf4\u660e\u5bf9\u6570\u636e\u96c6/Construct arXiv figure-caption dataset]\n    Method --\x3e M2[\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e0e\u8bc4\u4f30/Domain-specific training & evaluation]\n    Method --\x3e M3[\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5174\u8d77/Navigate rise of LLMs]\n    Results --\x3e R1[\u603b\u7ed3\u6280\u672f\u65b9\u6cd5\u7ecf\u9a8c/Summarize technical & methodological lessons]\n    Results --\x3e R2[\u63d0\u51fa\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411/Outline future challenges & directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance Inc., Rutgers University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21788",children:"https://arxiv.org/pdf/2512.21788"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/yanq095/InstructMoLE",children:"https://github.com/yanq095/InstructMoLE"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms LoRA & MoLE variants on benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21792",children:"https://arxiv.org/pdf/2512.21792"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI\u7528\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u8db3\u83cc\u80bf\u8bca\u65ad\uff1aMICCAI 2024\u6311\u6218\u8d5b") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u8db3\u83cc\u80bf\u8bca\u65ad\u56f0\u96be\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a<br>Mycetoma diagnosis is challenging, especially in low-resource settings")\n    Method --\x3e M1("\u7ec4\u7ec7mAIcetoma\u6311\u6218\u8d5b\uff0c\u5f00\u53d1AI\u6a21\u578b<br>Organized mAIcetoma challenge to develop AI models")\n    Method --\x3e M2("\u63d0\u4f9bMyData\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u5272\u548c\u5206\u7c7b<br>Provided MyData dataset for segmentation and classification")\n    Results --\x3e R1("\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272<br>Models achieved high segmentation accuracy")\n    Results --\x3e R2("\u9876\u7ea7\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u663e\u8457<br>Top models showed significant classification performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abu Hanif Muhammad Syarubany"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science & Technology (KAIST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21797",children:"https://arxiv.org/pdf/2512.21797"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Diffusion Posterior Sampling for Super-Resolution<br>\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7528\u4e8e\u8d85\u5206\u8fa8\u7387"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Single-image super-resolution under Gaussian noise<br>\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Diffusion Posterior Sampling (DPS)<br>\u6269\u6563\u540e\u9a8c\u91c7\u6837<br>Unconditional prior + likelihood-guided conditioning<br>\u65e0\u6761\u4ef6\u5148\u9a8c + \u4f3c\u7136\u5f15\u5bfc\u7684\u6761\u4ef6\u5316"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Optimal PS scale 0.95, noise \u03c3=0.01<br>\u6700\u4f73PS\u5c3a\u5ea60.95, \u566a\u58f0\u03c3=0.01<br>Balancing prior and data yields sharp details<br>\u5e73\u8861\u5148\u9a8c\u4e0e\u6570\u636e\u5f97\u5230\u6e05\u6670\u7ec6\u8282"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21803",children:"https://arxiv.org/pdf/2512.21803"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CellMamba: Adaptive Mamba for Cell Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Cell detection challenges in pathological images]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: CellMamba with TMAC module & Adaptive Mamba Head]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines, lightweight & efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] S&P 500 Stock's Movement Prediction using CNN"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Gupta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," None (No affiliation or email domain provided in the given content)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21804",children:"https://arxiv.org/pdf/2512.21804"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["S&P 500 Stock\'s Movement Prediction using CNN<br>\u4f7f\u7528CNN\u9884\u6d4b\u6807\u666e500\u80a1\u7968\u8d70\u52bf"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Predicting stock price movement<br>\u9884\u6d4b\u80a1\u7968\u4ef7\u683c\u8d70\u52bf"] --\x3e P1["\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u5de5\u7a0b<br>Traditional methods rely on engineered features"]\n    Problem --\x3e P2["\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u5355\u7ef4\u6570\u636e<br>Existing research often uses single-dimension data"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Use CNN on raw multivariate data<br>\u5bf9\u539f\u59cb\u591a\u53d8\u91cf\u6570\u636e\u4f7f\u7528CNN"] --\x3e M1["\u5c06\u5386\u53f2\u6570\u636e\u77e9\u9635\u89c6\u4e3a\u56fe\u50cf<br>Treat historical data matrices as images"]\n    Method --\x3e M2["\u5305\u542b\u539f\u59cb\u5e02\u573a\u4e8b\u4ef6(\u5982\u62c6\u80a1)<br>Include raw market events (e.g., splits)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Model achieves promising results<br>\u6a21\u578b\u53d6\u5f97\u6709\u5e0c\u671b\u7684\u7ed3\u679c"] --\x3e R1["\u652f\u6301\u80a1\u7968/\u884c\u4e1a/\u7ec4\u5408\u7ea7\u522b\u9884\u6d4b<br>Supports stock/sector/portfolio prediction"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Australian National University, The University of Queensland, GE Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21815",children:"https://arxiv.org/pdf/2512.21815"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[VLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb/VLMs are vulnerable to adversarial attacks]\n    Problem --\x3e P2[\u5148\u9a8c\u653b\u51fb\u5047\u8bbe\u6240\u6709token\u540c\u7b49\u91cd\u8981/Prior attacks assume all tokens are equally important]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u8bc6\u522b\u9ad8\u71b5\u5173\u952e\u51b3\u7b56\u70b9/Identify high-entropy critical decision points]\n    Method --\x3e M2[\u63d0\u51fa\u71b5\u5e93\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9ad8\u6548\u653b\u51fb:\u5c0f\u9884\u7b97\u5b9e\u73b0\u5f3a\u8bed\u4e49\u9000\u5316/Efficient attack: strong degradation with small budget]\n    Results --\x3e R2[\u9ad8\u6709\u5bb3\u8f6c\u5316\u7387:35-49%/High harmful conversion: 35-49%]\n    Results --\x3e R3[\u53ef\u884c\u8fc1\u79fb\u6027:17-26%/Feasible transferability: 17-26%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenwei Yang, Yibo Ai, Weidong Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology Beijing, National Center for Materials Service Safety"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21831",children:"https://arxiv.org/pdf/2512.21831"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u611f\u77e5\u6311\u6218:\u906e\u6321,\u89c6\u89d2\u9650\u5236,\u901a\u4fe1\u5ef6\u8fdf/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]\nC --\x3e C1[\u63d0\u51faXET-V2X\u6846\u67b6/Proposes XET-V2X Framework]\nC1 --\x3e C2[\u53cc\u5c42\u7ea7\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757/Dual-layer Spatial Cross-Attention Module]\nC2 --\x3e C3[\u591a\u89c6\u56fe\u56fe\u50cf\u7279\u5f81\u805a\u5408/Multi-view Image Feature Aggregation]\nC2 --\x3e C4[\u70b9\u4e91\u878d\u5408/Point Cloud Fusion]\nD --\x3e D1[\u5728V2X-Seq-SPD\u7b49\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347/Performance Improvements on V2X-Seq-SPD, etc.]\nD1 --\x3e D2[\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6027\u80fd\u589e\u5f3a/Enhanced Detection & Tracking Performance]\nD2 --\x3e D3[\u9c81\u68d2\u4e14\u65f6\u5e8f\u7a33\u5b9a\u7684\u611f\u77e5/Robust & Temporally Stable Perception]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. Affiliation information is not included."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21845",children:"https://arxiv.org/pdf/2512.21845"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URLETF2"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898 / Problem"] --\x3e P1["\u8fc7\u62df\u5408\u65b0\u6570\u636e / Overfitting to new data"]\n    Problem --\x3e P2["\u707e\u96be\u6027\u9057\u5fd8\u65e7\u6570\u636e / Catastrophic forgetting of old data"]\n    Problem --\x3e P3["\u7279\u5f81\u5dee\u5f02\u4e0e\u7c7b\u522b\u9519\u4f4d / Feature difference & Class misalignment"]\n\n    Method["\u4e3b\u8981\u65b9\u6cd5 / Method"] --\x3e M1["SCL-PNC\u65b9\u6cd5 / SCL-PNC Method"]\n    M1 --\x3e M1_1["\u81ea\u9002\u5e94\u5c42\u6269\u5c55\u4e3b\u5e72 / Adapt-layer for backbone expansion"]\n    M1 --\x3e M1_2["\u52a8\u6001\u53c2\u6570\u5316ETF\u5206\u7c7b\u5668 / Dynamic Parametric ETF Classifier"]\n    M1 --\x3e M1_3["\u5e76\u884c\u6269\u5c55\u4e0e\u77e5\u8bc6\u84b8\u998f / Parallel expansion & Knowledge distillation"]\n\n    Results["\u5173\u952e\u7ed3\u679c / Results"] --\x3e R1["\u9ad8\u6548\u5904\u7406\u7c7b\u522b\u589e\u957f / Efficiently handles increasing categories"]\n    Results --\x3e R2["\u89e3\u51b3\u7c7b\u522b\u9519\u4f4d / Addresses class misalignment"]\n    Results --\x3e R3["\u786e\u4fdd\u7279\u5f81\u4e00\u81f4\u6027 / Ensures feature consistency"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian Minzu University, Shandong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21856",children:"https://arxiv.org/pdf/2512.21856"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/HTUTU2/TPS-SCL",children:"https://github.com/HTUTU2/TPS-SCL"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Breaking Alignment Barriers: TPS-SCL<br>\u7a81\u7834\u5bf9\u9f50\u58c1\u5792: TPS-SCL] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u9f50\u6570\u636e\u96c6<br>Existing methods rely on aligned datasets]\n    B --\x3e B2[\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u672a\u5bf9\u9f50<br>Real-world images are unaligned]\n    C --\x3e C1[\u53cc\u6d41MobileViT\u7f16\u7801\u5668<br>Dual-stream MobileViT encoder]\n    C --\x3e C2[TPS\u5bf9\u9f50\u6a21\u5757<br>TPS Alignment Module]\n    C --\x3e C3[\u8bed\u4e49\u5173\u8054\u7ea6\u675f\u6a21\u5757<br>Semantic Correlation Constraint Module]\n    C --\x3e C4[\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5757<br>Cross-Modal Correlation Module]\n    D --\x3e D1[\u8f7b\u91cf\u7ea7SOTA\u6027\u80fd<br>Lightweight SOTA performance]\n    D --\x3e D2[\u8d85\u8d8a\u4e3b\u6d41RGB-T\u65b9\u6cd5<br>Outperforms mainstream RGB-T methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," CyberAgent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21860",children:"https://arxiv.org/pdf/2512.21860"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings",children:"https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u5168\u5c40\u56fe\u50cf\u5d4c\u5165\u65e0\u6cd5\u805a\u7126\u4e8e\u7279\u5b9a\u6587\u672c\u6761\u4ef6]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: DIOR - \u5229\u7528LVLM\u751f\u6210\u5355\u5b57\u63cf\u8ff0\u5e76\u63d0\u53d6\u6700\u540etoken\u7684\u9690\u85cf\u72b6\u6001]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u6761\u4ef6\u56fe\u50cf\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21857",children:"https://arxiv.org/pdf/2512.21857"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Haodong-Lei-Ray/ADT-Tree",children:"https://github.com/Haodong-Lei-Ray/ADT-Tree"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fast Inference of Visual AR Model with ADT-Tree<br>\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5feb\u901f\u63a8\u7406\u4e0eADT-Tree"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Visual AR models have slow sequential inference.<br>\u89c6\u89c9AR\u6a21\u578b\u63a8\u7406\u6162"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>\u63d0\u51fa\u90bb\u63a5\u81ea\u9002\u5e94\u52a8\u6001\u8349\u7a3f\u6811"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b03.13x/3.05x\u52a0\u901f"]\n    Problem --\x3e P1["Spatially varying token prediction difficulty.<br>\u7a7a\u95f4\u53d8\u5316\u7684token\u9884\u6d4b\u96be\u5ea6"]\n    Method --\x3e M1["Dynamically adjusts tree depth & width.<br>\u52a8\u6001\u8c03\u6574\u6811\u6df1\u5ea6\u4e0e\u5bbd\u5ea6"]\n    Method --\x3e M2["Leverages adjacency & prior acceptance rates.<br>\u5229\u7528\u90bb\u63a5\u5173\u7cfb\u548c\u5148\u9a8c\u63a5\u53d7\u7387"]\n    Results --\x3e R1["Integrates with relaxed sampling.<br>\u53ef\u4e0e\u677e\u5f1b\u91c7\u6837\u65b9\u6cd5\u7ed3\u5408"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," North South University, Korea Institute of Oriental Medicine, American International University\u2013Bangladesh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21861",children:"https://arxiv.org/pdf/2512.21861"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yihan Hu, Xuelin Chen, Xiaodong Cun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Great Bay University, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21865",children:"https://arxiv.org/pdf/2512.21865"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/GVCLab/EasyOmnimatte",children:"https://github.com/GVCLab/EasyOmnimatte"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EasyOmnimatte] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u6162\u4e14\u6b21\u4f18/Existing methods are slow and suboptimal]\n    C --\x3e C1[\u53cc\u4e13\u5bb6\u5fae\u8c03/Dual-Expert Finetuning]\n    C1 --\x3e C2[\u6548\u679c\u4e13\u5bb6/Effect Expert]\n    C1 --\x3e C3[\u8d28\u91cf\u4e13\u5bb6/Quality Expert]\n    D --\x3e D1[\u9ad8\u8d28\u91cf\u5206\u89e3/High-quality decomposition]\n    D --\x3e D2[\u9ad8\u6548\u5feb\u901f/Efficient and fast]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, San Diego, Dolby Laboratories"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21867",children:"https://arxiv.org/pdf/2512.21867"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21883",children:"https://arxiv.org/pdf/2512.21883"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/dtc111111/Reloc-VGGT",children:"https://github.com/dtc111111/Reloc-VGGT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southern University of Science and Technology, University of Warwick, Fudan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21881",children:"https://arxiv.org/pdf/2512.21881"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u74f6\u9888/Bottlenecks of Existing Methods]\n    B1 --\x3e B1_1[\u56fe\u8c31\u65b9\u6cd5: \u4e22\u5931\u7ec6\u8282, \u9700\u5927\u6570\u636e/Atlas-based: lose details, need big data]\n    B1 --\x3e B1_2[\u65e0\u56fe\u8c31\u65b9\u6cd5: \u5185\u5b58\u8ba1\u7b97\u6210\u672c\u9ad8/Atlas-free: high memory & compute cost]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8bbe\u8ba1/Two-stage Adaptive Design]\n    C1 --\x3e C1_1[\u8f7b\u91cf\u65f6\u5e8f\u63d0\u53d6\u5668: \u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u663e\u8457\u6027\u6392\u5e8f/Lightweight Temporal Extractor: global context & saliency ranking]\n    C1 --\x3e C1_2[4D\u5206\u5c42\u7f16\u7801\u5668: \u4eceTop-k\u7a97\u53e3\u5b66\u4e60/4D Hierarchical Encoder: learn from top-k windows]\n    D --\x3e D1[\u6027\u80fd/Performance]\n    D --\x3e D2[\u6548\u7387/Efficiency]\n    D1 --\x3e D1_1[\u5728\u4e03\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA/Achieves SOTA on seven benchmarks]\n    D2 --\x3e D2_1[\u4ec5\u97004\u5343\u6b21\u9884\u8bad\u7ec3\u4f1a\u8bdd/Only 4k pre-training sessions]\n    D2 --\x3e D2_2[GPU\u5185\u5b58\u964d\u81f330%/GPU memory reduced to ~30%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21890",children:"https://arxiv.org/pdf/2512.21890"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CrownGen: Patient-customized Crown Generation<br>\u57fa\u4e8e\u70b9\u6269\u6563\u6a21\u578b\u7684\u4e2a\u6027\u5316\u7259\u51a0\u751f\u6210] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Digital crown design is labor-intensive<br>\u6570\u5b57\u5316\u7259\u51a0\u8bbe\u8ba1\u8d39\u65f6\u8d39\u529b]\n    C --\x3e C1[Uses a point diffusion model<br>\u4f7f\u7528\u70b9\u6269\u6563\u6a21\u578b]\n    C --\x3e C2[Has a boundary prediction module<br>\u5305\u542b\u8fb9\u754c\u9884\u6d4b\u6a21\u5757]\n    D --\x3e D1[Surpasses SOTA in geometric fidelity<br>\u51e0\u4f55\u4fdd\u771f\u5ea6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5]\n    D --\x3e D2[Reduces active design time<br>\u51cf\u5c11\u4e3b\u52a8\u8bbe\u8ba1\u65f6\u95f4]\n    D --\x3e D3[Crowns are clinically non-inferior<br>\u4e34\u5e8a\u8d28\u91cf\u4e0d\u52a3\u4e8e\u4eba\u5de5\u8bbe\u8ba1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Guangzhou Quwan Network Technology, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21905",children:"https://arxiv.org/pdf/2512.21905"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>\u57fa\u4e8e\u6269\u6563Transformer\u7684\u9ad8\u4fdd\u771f\u957f\u65f6\u4eba\u4f53\u56fe\u50cf\u52a8\u753b"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results"]\n    B --\x3e B1["\u957f\u89c6\u9891\u751f\u6210\u6311\u6218<br>Long-duration Video Generation"]\n    B --\x3e B2["\u9762\u90e8\u4e0e\u624b\u90e8\u7ec6\u8282\u5408\u6210\u4e0d\u8db3<br>Lack of Fine-grained Facial/Hand Details"]\n    C --\x3e C1["\u6df7\u5408\u9690\u5f0f\u5f15\u5bfc\u4fe1\u53f7<br>Hybrid Implicit Guidance"]\n    C --\x3e C2["\u4f4d\u7f6e\u504f\u79fb\u81ea\u9002\u5e94\u6a21\u5757<br>Position Shift Adaptive Module"]\n    C --\x3e C3["\u6570\u636e\u589e\u5f3a\u4e0e\u9aa8\u67b6\u5bf9\u9f50<br>Data Augmentation & Skeleton Alignment"]\n    D --\x3e D1["\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5<br>Outperforms SOTA"]\n    D --\x3e D2["\u5b9e\u73b0\u8d851\u5206\u949f\u52a8\u753b<br>Exceeds 1-minute Animation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zeyu Liang, Hailun Xia, Naichuan Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21916",children:"https://arxiv.org/pdf/2512.21916"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[RGB\u4e0e\u9aa8\u67b6\u6a21\u6001\u5f02\u6784\u878d\u5408\u56f0\u96be/RGB-Skeleton Heterogeneous Fusion Difficulty]\n    C --\x3e C1[\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u56fe\u8868\u793a\u5b66\u4e60/Human-Centric Graph Representation Learning]\n    C1 --\x3e C2[\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u540e\u6821\u51c6/Attention-Based Post Calibration]\n    C1 --\x3e C3[\u53cc\u53d8\u4f53: PAN-Ensemble\u4e0ePAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]\n    D --\x3e D1[\u4e09\u4e2a\u6570\u636e\u96c6\u4e0aSOTA\u6027\u80fd/SOTA Performance on Three Datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21924",children:"https://arxiv.org/pdf/2512.21924"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6cdb\u5316\u6027\u5dee\u4e0e\u5f02\u5e38\u6b8b\u7559/Generalizability & Residuals]\n    C --\x3e C1[\u89e3\u8026\u8868\u793a\u6a21\u5757/Disentangled Representation Module]\n    C --\x3e C2[\u8fb9\u7f18\u5230\u56fe\u50cf\u6062\u590d\u6a21\u5757/Edge-to-Image Restoration Module]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a17\u79cdSOTA\u65b9\u6cd5/Outperforms 17 SOTA Methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," JD.COM"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21921",children:"https://arxiv.org/pdf/2512.21921"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JD-GenX/AutoPP",children:"https://github.com/JD-GenX/AutoPP"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AutoPP: Towards Automated Product Poster Generation and Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4eba\u5de5\u5236\u4f5c\u4e0e\u4f18\u5316\u6d77\u62a5\u8017\u65f6\u8017\u529b/Manual poster creation and optimization is laborious]\n    C --\x3e C1[\u81ea\u52a8\u5316\u751f\u6210\u4e0e\u4f18\u5316\u7ba1\u9053/Automated generation and optimization pipeline]\n    C1 --\x3e C1_1[\u751f\u6210\u5668: \u7edf\u4e00\u8bbe\u8ba1\u6a21\u5757\u4e0e\u5143\u7d20\u6e32\u67d3/Generator: Unified design & element rendering]\n    C1 --\x3e C1_2[\u4f18\u5316\u5668: \u5143\u7d20\u66ff\u6362\u4e0eIDPO/Optimizer: Element replacement & IDPO]\n    C --\x3e C2[\u6570\u636e\u96c6: AutoPP1M/Dataset: AutoPP1M]\n    D --\x3e D1[\u79bb\u7ebf\u548c\u5728\u7ebfSOTA\u7ed3\u679c/Offline and online SOTA results]\n    D --\x3e D2[\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u516c\u5f00/Code & dataset released]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiquan Gao, John See"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Heriot-Watt University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21944",children:"https://arxiv.org/pdf/2512.21944"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")\n    Results("\u5173\u952e\u7ed3\u679c/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Saskatchewan, New Mexico State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21948",children:"https://arxiv.org/pdf/2512.21948"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u81ea\u52a8\u5316\u53d1\u73b0\u7528\u4e8e\u690d\u88ab\u5206\u7c7b\u7684\u7d27\u51d1\u5149\u8c31\u6307\u6570/Automated discovery of compact spectral indices for vegetation classification]\n    C --\x3e C1[\u751f\u6210\u5f52\u4e00\u5316\u5dee\u5f02\u591a\u9879\u5f0f\u5019\u9009\u7279\u5f81/Generate candidate features via normalized difference polynomials]\n    C --\x3e C2[\u4f7f\u7528\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u6311\u9009\u6307\u6570/Use feature selection methods to pick indices]\n    D --\x3e D1[\u5355\u4e2a\u4e8c\u9636\u6307\u6570\u8fbe\u523096.26%\u51c6\u786e\u7387/Single degree-2 index achieves 96.26% accuracy]\n    D --\x3e D2[\u6307\u6570\u7b80\u5355\uff0c\u53ef\u76f4\u63a5\u90e8\u7f72\u4e8eGEE/Indices are simple and deployable on GEE]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong (CUHK)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21964",children:"https://arxiv.org/pdf/2512.21964"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: MLLMs are sensitive to input noise, undermining clinical use)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Training-free IMC framework with PDC for vision and SMS for text)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: SOTA performance on new multi-modal noise benchmark)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21985",children:"https://arxiv.org/pdf/2512.21985"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LVLM-Aided Alignment of Task-Specific Vision Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5c0f\u89c4\u6a21\u4efb\u52a1\u4e13\u7528\u89c6\u89c9\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027/Small task-specific vision models rely on spurious correlations]\n    B --\x3e B2[\u5bfc\u81f4\u90e8\u7f72\u65f6\u884c\u4e3a\u8106\u5f31/Leads to brittle behavior when deployed]\n    C --\x3e C1[\u5229\u7528LVLM\u8fdb\u884c\u89c6\u89c9\u5bf9\u9f50/Leverage LVLM for visual alignment]\n    C --\x3e C2[\u53cc\u5411\u63a5\u53e3: \u884c\u4e3a\u8f6c\u8bed\u8a00, \u89c4\u8303\u8f6c\u8bc4\u4f30/Bidirectional interface: behavior to language, specs to critiques]\n    D --\x3e D1[\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u89c4\u8303\u66f4\u597d\u5bf9\u9f50/Better alignment of model behavior with human specifications]\n    D --\x3e D2[\u51cf\u5c11\u5bf9\u865a\u5047\u7279\u5f81\u548c\u504f\u89c1\u7684\u4f9d\u8d56/Reduced dependence on spurious features and biases]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21984",children:"https://arxiv.org/pdf/2512.21984"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/hhwmortal/PELD-Instance-segmentation",children:"https://github.com/hhwmortal/PELD-Instance-segmentation"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u72ed\u7a84\u89c6\u91ce, \u4f2a\u5f71, \u786c\u4ef6\u9650\u5236/Narrow FOV, Artifacts, Hardware Constraints]\n    C --\x3e C1[\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6846\u67b6/Lightweight Multi-scale Attention Framework]\n    C1 --\x3e C2[\u4e3b\u5e72: C2f-Pro (\u91cd\u53c2\u6570\u5316\u5377\u79ef+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]\n    C1 --\x3e C3[\u9888\u90e8: SSFF\u4e0eTFE/Neck: SSFF and TFE]\n    C1 --\x3e C4[\u5934\u90e8: \u8f7b\u91cf\u5171\u4eab\u5934 (LMSH)/Head: Lightweight Shared Head (LMSH)]\n    D --\x3e D1[\u6027\u80fd\u4f18\u8d8a, \u53c2\u6570\u91cf\u5c11 (1.8M)/High Performance, Few Params (1.8M)]\n    D --\x3e D2[\u53d1\u5e03PELD\u6570\u636e\u96c6/Release PELD Dataset]\n    D --\x3e D3[\u826f\u597d\u6cdb\u5316\u6027/Good Generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chongqing University, Xinjiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21999",children:"https://arxiv.org/pdf/2512.21999"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/hujiayu1223/ALEAHallu",children:"https://github.com/hujiayu1223/ALEAHallu"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLM\u5e7b\u89c9\u95ee\u9898/VLM Hallucination Issue]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ALEAHallu\u6846\u67b6/ALEAHallu Framework]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u6709\u6548\u7f13\u89e3\u5e7b\u89c9/Effectively Mitigates Hallucinations]\n    C --\x3e C1[\u6fc0\u6d3b\u6570\u636e\u96c6/Activation Dataset]\n    C --\x3e C2[\u5b9a\u4f4d\u5173\u952e\u53c2\u6570/Locate Critical Parameters]\n    C --\x3e C3[\u5bf9\u6297\u6027\u7f16\u8f91/Adversarial Editing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22009",children:"https://arxiv.org/pdf/2512.22009"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision "slow mode" and an efficient "fast mode" based on task demands. 3. Employs special perception tokens to dynamically guide the model\'s visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Building efficient and precise GUI agents is challenging]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Slow-fast hybrid inference with adaptive perception tokens]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches SOTA performance with compact 2.5B size]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22010",children:"https://arxiv.org/pdf/2512.22010"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Technology and Business University, The University of Sydney"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22016",children:"https://arxiv.org/pdf/2512.22016"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Chinese Academy of Military Science"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22027",children:"https://arxiv.org/pdf/2512.22027"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://gendf.github.io/",children:"https://gendf.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components\u2014deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy\u2014to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Patch-Discontinuity Mining for Generalized Deepfake Detection"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA generalization with only 0.28M parameters."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22046",children:"https://arxiv.org/pdf/2512.22046"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]\n    D[\u5173\u952e\u7ed3\u679c/Results: High ASR, preserves clean performance, defenses ineffective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22065",children:"https://arxiv.org/pdf/2512.22065"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://streamavatar.github.io",children:"https://streamavatar.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22047",children:"https://arxiv.org/pdf/2512.22047"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Tongyi-MAI/MAI-UI",children:"https://github.com/Tongyi-MAI/MAI-UI"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1(\u7f3a\u4e4f\u539f\u751f\u4eba\u673a\u4ea4\u4e92/Lack of Native Agent-User Interaction)\n    B1 --\x3e C2(\u4ec5UI\u64cd\u4f5c\u7684\u9650\u5236/Limits of UI-Only Operation)\n    B1 --\x3e C3(\u7f3a\u4e4f\u5b9e\u7528\u90e8\u7f72\u67b6\u6784/Absence of Practical Deployment Architecture)\n    B1 --\x3e C4(\u52a8\u6001\u73af\u5883\u8106\u5f31\u6027/Brittleness in Dynamic Environments)\n    B2 --\x3e D1(\u81ea\u6f14\u8fdb\u6570\u636e\u7ba1\u9053/Self-Evolving Data Pipeline)\n    B2 --\x3e D2(\u539f\u751f\u8bbe\u5907-\u4e91\u534f\u4f5c\u7cfb\u7edf/Native Device-Cloud Collaboration System)\n    B2 --\x3e D3(\u5728\u7ebfRL\u6846\u67b6/Online RL Framework)\n    D1 --\x3e E1(\u5305\u542b\u7528\u6237\u4ea4\u4e92/Includes User Interaction)\n    D1 --\x3e E2(\u5305\u542bMCP\u5de5\u5177\u8c03\u7528/Includes MCP Tool Calls)\n    D3 --\x3e E3(\u6269\u5c55\u5e76\u884c\u73af\u5883/Scales Parallel Environments)\n    D3 --\x3e E4(\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6/Scales Context Length)\n    B3 --\x3e F1(GUI Grounding SOTA/GUI Grounding SOTA)\n    B3 --\x3e F2(\u79fb\u52a8\u5bfc\u822aSOTA/Mobile Navigation SOTA)\n    B3 --\x3e F3(\u7cfb\u7edf\u6027\u80fd\u63d0\u5347/System Performance Gains)\n    F1 --\x3e G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)\n    F1 --\x3e G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)\n    F2 --\x3e G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)\n    F2 --\x3e G4(MobileWorld: 41.7%/MobileWorld: 41.7%)\n    F3 --\x3e G5(\u8bbe\u5907\u6027\u80fd\u63d0\u534733%/On-Device Perf. +33%)\n    F3 --\x3e G6(\u4e91\u8c03\u7528\u51cf\u5c1140%/Cloud Calls -40%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22096",children:"https://arxiv.org/pdf/2512.22096"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/stdstu12/YUME",children:"https://github.com/stdstu12/YUME"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[\u5927\u6a21\u578b\u53c2\u6570\u4e0e\u6162\u63a8\u7406/Large Model & Slow Inference]\n    Problem --\x3e P2[\u7f3a\u4e4f\u6587\u672c\u63a7\u5236/Lack of Text Control]\n    Method --\x3e M1[\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6/Long-Video Gen Framework]\n    Method --\x3e M2[\u5b9e\u65f6\u6d41\u52a0\u901f\u7b56\u7565/Real-time Streaming]\n    Method --\x3e M3[\u6587\u672c\u63a7\u5236\u4e8b\u4ef6\u751f\u6210/Text-Controlled Events]\n    M1 --\x3e M1_Sub[\u7edf\u4e00\u4e0a\u4e0b\u6587\u538b\u7f29\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b/Unified Context Compression & Linear Attention]\n    M2 --\x3e M2_Sub[\u53cc\u5411\u6ce8\u610f\u529b\u84b8\u998f\u4e0e\u6587\u672c\u5d4c\u5165/Bidirectional Attention Distillation & Text Embedding]\n    Results --\x3e R1[\u751f\u6210\u4ea4\u4e92\u5f0f\u4e16\u754c/Generates Interactive Worlds]\n    Results --\x3e R2[\u652f\u6301\u952e\u76d8\u63a2\u7d22/Supports Keyboard Exploration]\n    Results --\x3e R3[\u516c\u5f00\u4ee3\u7801\u5e93/Public Codebase]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Momir Ad\u017eemovi\u0107"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Belgrade"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22105",children:"https://arxiv.org/pdf/2512.22105"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Robotmurlock/TDLP",children:"https://github.com/Robotmurlock/TDLP"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Microsoft Research, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22120",children:"https://arxiv.org/pdf/2512.22120"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zss02/BiPS",children:"https://github.com/zss02/BiPS"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional "where-to-look" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]\n    D[\u5173\u952e\u7ed3\u679c/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22118",children:"https://arxiv.org/pdf/2512.22118"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://isee-laboratory.github.io/ProEdit/",children:"https://isee-laboratory.github.io/ProEdit/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ProEdit: Inversion-based Editing From Prompts Done Right] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u6ce8\u5165\u6e90\u56fe\u50cf\u4fe1\u606f/Existing methods overly inject source info]\n    B1 --\x3e B2[\u963b\u788d\u5c5e\u6027\u7f16\u8f91\u5982\u59ff\u6001\u3001\u6570\u91cf\u3001\u989c\u8272/Hinders editing attributes like pose, number, color]\n    C --\x3e C1[\u6ce8\u610f\u529b\u5c42\u9762: KV-mix/Attention Aspect: KV-mix]\n    C1 --\x3e C11[\u6df7\u5408\u6e90\u4e0e\u76ee\u6807KV\u7279\u5f81/Mix source & target KV features]\n    C --\x3e C2[\u6f5c\u5728\u5c42\u9762: Latents-Shift/Latent Aspect: Latents-Shift]\n    C2 --\x3e C21[\u6270\u52a8\u7f16\u8f91\u533a\u57df\u7684\u6e90\u6f5c\u5728\u8868\u793a/Perturb source latent in edited region]\n    D --\x3e D1[SOTA\u6027\u80fd/SOTA performance]\n    D --\x3e D2[\u5373\u63d2\u5373\u7528\u8bbe\u8ba1/Plug-and-play design]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Daffodil International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21372",children:"https://arxiv.org/pdf/2512.21372"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem: \u80c3\u80a0\u9053\u75be\u75c5\u56fe\u50cf\u5206\u7c7b\u6311\u6218 / GI Disease Image Classification Challenge)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method: \u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u53cc\u6d41Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results: \u9ad8\u51c6\u786e\u7387\u4e0e\u53ef\u89e3\u91ca\u6027 / High Accuracy & Explainability)\n    B --\x3e B1[\u6570\u636e\u91cf\u5927, \u7c7b\u95f4\u5dee\u5f02\u5c0f / Large Data Volume, Subtle Inter-class Variation]\n    C --\x3e C1[\u6559\u5e08\u6a21\u578b: Swin + ViT / Teacher: Swin + ViT]\n    C --\x3e C2[\u5b66\u751f\u6a21\u578b: Tiny-ViT / Student: Tiny-ViT]\n    C --\x3e C3[\u53ef\u89e3\u91ca\u6027\u5206\u6790: Grad-CAM\u7b49 / XAI: Grad-CAM etc.]\n    D --\x3e D1[\u51c6\u786e\u7387 > 0.99 / Accuracy > 0.99]\n    D --\x3e D2[AUC = 1.0000]\n    D --\x3e D3[\u9002\u7528\u4e8e\u4e34\u5e8a\u73af\u5883 / Suitable for Clinical Settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Takuro Kutsuna"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Toyota Central R&D Labs., Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21593",children:"https://arxiv.org/pdf/2512.21593"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Residual Prior Diffusion (RPD) / \u6b8b\u5dee\u5148\u9a8c\u6269\u6563\u6a21\u578b"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u5355\u4e00\u6269\u6563\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7ec6\u8282 / Single diffusion model struggles with global structure and local details"]\n    Method --\x3e M1["\u4e24\u9636\u6bb5\u6846\u67b6: \u7c97\u7c92\u5ea6\u5148\u9a8c + \u6b8b\u5dee\u6269\u6563\u6a21\u578b / Two-stage framework: coarse prior + residual diffusion model"]\n    Method --\x3e M2["\u6982\u7387\u6a21\u578b\u4e0e\u53ef\u5904\u7406ELBO / Probabilistic model with tractable ELBO"]\n    Results --\x3e R1["\u5728\u5408\u6210\u6570\u636e\u4e0a\u51c6\u786e\u6355\u6349\u7ec6\u8282 / Accurately captures details on synthetic data"]\n    Results --\x3e R2["\u81ea\u7136\u56fe\u50cf\u751f\u6210\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf / Natural image generation matches or exceeds baselines"]\n    Results --\x3e R3["\u5c11\u6b65\u63a8\u7406\u4fdd\u6301\u6027\u80fd / Maintains performance with few inference steps"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21975",children:"https://arxiv.org/pdf/2512.21975"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/ReaganWu/RT-Focuser",children:"https://github.com/ReaganWu/RT-Focuser"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8fd0\u52a8\u6a21\u7cca\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf / Motion blur degrades image quality]\n    B --\x3e B2[\u5b9e\u65f6\u5e94\u7528\u6311\u6218 / Challenges for real-time applications]\n    C --\x3e C1[\u8f7b\u91cf\u7ea7U\u578b\u7f51\u7edc / Lightweight U-shaped network]\n    C --\x3e C2[\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6 / Three key components: LD, MLIA, X-Fuse]\n    D --\x3e D1[30.67 dB PSNR / 30.67 dB PSNR]\n    D --\x3e D2[5.85M\u53c2\u6570, 15.76 GMACs / 5.85M params, 15.76 GMACs]\n    D --\x3e D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sungwoo Kang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21988",children:"https://arxiv.org/pdf/2512.21988"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and defines the "color-clinical decoupling" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker\'s disproportionate sensitivity to noise in the b* color channel.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed "color-clinical decoupling," primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Color-Clinical Decoupling<br>\u989c\u8272-\u4e34\u5e8a\u89e3\u8026] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>Does color calibration ensure clinical reliability?<br>\u8272\u5f69\u6821\u51c6\u80fd\u5426\u786e\u4fdd\u4e34\u5e8a\u53ef\u9760\u6027\uff1f]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Analyze 43,425 images across devices with CCM<br>\u4f7f\u7528CCM\u5206\u679043,425\u5f20\u8de8\u8bbe\u5907\u56fe\u50cf]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Color-clinical decoupling: \u2206E\u2193 but ICC(ITA) poor<br>\u989c\u8272-\u4e34\u5e8a\u89e3\u8026\uff1a\u2206E\u4e0b\u964d\u4f46ITA\u7684ICC\u5dee]\n    B --\x3e D\n    C --\x3e D"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);