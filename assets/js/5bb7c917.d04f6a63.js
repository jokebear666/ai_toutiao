"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[555],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},69043:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CV/20251229-20260104","title":"20251229-20260104 (cs.CV)","description":"2025-12-29","source":"@site/docs/daily/cs_CV/20251229-20260104.md","sourceDirName":"daily/cs_CV","slug":"/daily/cscv/20251229-20260104","permalink":"/ai_toutiao/daily/cscv/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767086937000,"frontMatter":{"slug":"/daily/cscv/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.CV)","permalink":"/ai_toutiao/daily/cscv/20251222-20251228"},"next":{"title":"cs.CY","permalink":"/ai_toutiao/category/cscy"}}');var a=i(74848),r=i(28453);const t={slug:"/daily/cscv/20251229-20260104"},o="20251229-20260104 (cs.CV)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251229-20260104-cscv",children:"20251229-20260104 (cs.CV)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [Vision-Language Models, engagement prediction, multimodal features, YouTube Shorts, regression-based evaluator]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani; GenimeLabs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21402",children:"https://arxiv.org/pdf/2512.21402"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u89c2\u4f17\u53c2\u4e0e\u5ea6/Traditional metrics fail to assess real viewer engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u4f7f\u7528VLM\u63d0\u53d6\u65e0\u76d1\u7763\u89c6\u542c\u7279\u5f81/Use VLM to extract unsupervised audiovisual features]\n    Method --\x3e M2[\u805a\u7c7b\u7279\u5f81\u5e76\u8bad\u7ec3\u56de\u5f52\u8bc4\u4f30\u5668/Cluster features and train regression evaluator]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9884\u6d4b\u4e0e\u771f\u5b9e\u53c2\u4e0e\u5ea6\u5f3a\u76f8\u5173/Strong correlation between predicted and actual engagement]\n    Results --\x3e R2[\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30/Provides interpretable and scalable assessment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," California Institute of Technology, Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21414",children:"https://arxiv.org/pdf/2512.21414"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/christinaliu2020/tool-bottleneck-framework",children:"https://github.com/christinaliu2020/tool-bottleneck-framework"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Tool Bottleneck Framework for Medical Image Understanding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Text-based tool composition fails for medical images with localized features]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches or beats baselines, more interpretable, data-efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scalable Deep Subspace Clustering Network"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Quebec at Montreal"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21434",children:"https://arxiv.org/pdf/2512.21434"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n\xd7n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Deep Subspace Clustering Network"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: O(n^3) \u8ba1\u7b97\u590d\u6742\u5ea6 / O(n^3) Computational Complexity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u5730\u6807\u8fd1\u4f3c\u4e0e\u8054\u5408\u4f18\u5316 / Landmark Approximation & Joint Optimization"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u7ebf\u6027\u590d\u6742\u5ea6\u4e0e\u53ef\u6bd4\u6027\u80fd / Linear Complexity & Comparable Performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [multi-annotator segmentation, skin lesion segmentation, dermoscopy, consensus masks, annotator metadata]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Simon Fraser University, AIP Labs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21472",children:"https://arxiv.org/pdf/2512.21472"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," /githubsfu-mial/IMAplusplus"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Ground Penetrating Radar (GPR), Multi-modal Chain Feature Fusion (MCFF), Global Attention Mechanism (GAM), DCGAN, transfer learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21452",children:"https://arxiv.org/pdf/2512.21452"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Intelligent recognition of GPR road hidden defect images <br/> GPR\u9053\u8def\u9690\u853d\u75c5\u5bb3\u56fe\u50cf\u667a\u80fd\u8bc6\u522b") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n\n    Problem --\x3e P1("Subjective & inefficient GPR interpretation <br/> GPR\u56fe\u50cf\u89e3\u91ca\u4e3b\u89c2\u4e14\u4f4e\u6548")\n    Problem --\x3e P2("Data scarcity <br/> \u6570\u636e\u7a00\u7f3a")\n\n    Method --\x3e M1("DCGAN-based Data Augmentation <br/> \u57fa\u4e8eDCGAN\u7684\u6570\u636e\u589e\u5f3a")\n    Method --\x3e M2("MCGA-Net (MCFF + GAM) <br/> MCGA-Net\u7f51\u7edc")\n    Method --\x3e M3("MS COCO Transfer Learning <br/> MS COCO\u8fc1\u79fb\u5b66\u4e60")\n\n    Results --\x3e R1("High Performance (Precision 92.8%, mAP@50 95.9%) <br/> \u9ad8\u6027\u80fd")\n    Results --\x3e R2("Robust to noise & weak signals <br/> \u5bf9\u566a\u58f0\u548c\u5f31\u4fe1\u53f7\u9c81\u68d2")\n    Results --\x3e R3("New paradigm for automated detection <br/> \u81ea\u52a8\u5316\u68c0\u6d4b\u65b0\u8303\u5f0f")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21459",children:"https://arxiv.org/pdf/2512.21459"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/chloeqxq/CCAD",children:"https://github.com/chloeqxq/CCAD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5f02\u5e38\u68c0\u6d4b\u5728\u6709\u9650\u5f02\u5e38\u6570\u636e\u4e0b\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u6548\u7387\u548c\u7ea6\u675f\u4e0a\u7684\u4e0d\u8db3]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faCCAD\uff0c\u878d\u5408\u91cd\u5efa\u4e0e\u8868\u5f81\u65b9\u6cd5\uff0c\u4f7f\u7528\u538b\u7f29\u7684\u5168\u5c40\u7279\u5f81\u4f5c\u4e3a\u91cd\u5efa\u6a21\u578b\u7684\u6761\u4ef6]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728AUC\u4e0a\u8d85\u8d8aSOTA\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u8d21\u732e\u4e86\u91cd\u65b0\u6807\u6ce8\u7684DAGM 2007\u6570\u636e\u96c6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image retrieval], [polyp re-identification, gated progressive fusion, multimodal feature fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21476",children:"https://arxiv.org/pdf/2512.21476"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JeremyXSC/GPF-Net",children:"https://github.com/JeremyXSC/GPF-Net"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Coarse high-level features lead to inferior results for small polyps]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Generative Multi-Focus Image Fusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image fusion], [multi-focus image fusion, latent diffusion models, generative restoration, StackMFF, IFControlNet]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Donghai Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21495",children:"https://arxiv.org/pdf/2512.21495"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Xinzhe99/StackMFF-Series",children:"https://github.com/Xinzhe99/StackMFF-Series"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Generative Multi-Focus Image Fusion] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u603b\u6709\u5bf9\u7126\u56fe\u50cf/Existing methods assume always an in-focus image]\n    B --\x3e B2[\u8fb9\u7f18\u4f2a\u5f71/Edge artifacts]\n    C --\x3e C1[\u9636\u6bb5\u4e00: \u786e\u5b9a\u6027\u878d\u5408/Stage 1: Deterministic Fusion (StackMFF V4)]\n    C --\x3e C2[\u9636\u6bb5\u4e8c: \u751f\u6210\u5f0f\u6062\u590d/Stage 2: Generative Restoration (IFControlNet)]\n    D --\x3e D1[SOTA\u6027\u80fd/State-of-the-art performance]\n    D --\x3e D2[\u5904\u7406\u590d\u6742\u591a\u7126\u5185\u5bb9/Handles complex multi-focal content]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21510",children:"https://arxiv.org/pdf/2512.21510"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art IMVC performance and superior robustness."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [fixed-threshold evaluation, CNN-ViT hybrid, gated fusion, frequency-domain features, cross-domain detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Ashik Khan, Arafat Alam Jion"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21512",children:"https://arxiv.org/pdf/2512.21512"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion."]\n    Results["\u5173\u952e\u7ed3\u679c/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SVBench: Evaluation of Video Generation Models on Social Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [social reasoning, benchmark, agent-based pipeline, VLM judge, multi-agent interaction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21507",children:"https://arxiv.org/pdf/2512.21507"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Gloria2tt/SVBench-Evaluation",children:"https://github.com/Gloria2tt/SVBench-Evaluation"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SVBench: \u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u793e\u4f1a\u63a8\u7406\u8bc4\u4f30<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u793e\u4f1a\u63a8\u7406\u80fd\u529b<br>Current models lack social reasoning]\n    C --\x3e C1[\u6784\u5efa\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u8303\u5f0f\u7684\u57fa\u51c6<br>Build benchmark based on social psychology paradigms]\n    C --\x3e C2[\u4f7f\u7528\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u514d\u8d39\u6d41\u7a0b\u8fdb\u884c\u8bc4\u4f30<br>Use training-free agent-based pipeline for evaluation]\n    D --\x3e D1[\u6a21\u578b\u5728\u8868\u9762\u5408\u7406\u6027\u4e0a\u8868\u73b0\u826f\u597d<br>Models perform well on surface-level plausibility]\n    D --\x3e D2[\u6a21\u578b\u5728\u793e\u4f1a\u63a8\u7406\u7ef4\u5ea6\u4e0a\u7cfb\u7edf\u6027\u5931\u8d25<br>Models fail systematically on social reasoning dimensions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [parameter-efficient training, frozen encoders, adapters, LoRA, BitFit]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Ashik Khan, Md Nahid Siddique"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Kharagpur, Florida International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21508",children:"https://arxiv.org/pdf/2512.21508"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods\u2014degraded model calibration (ECE: 0.29-0.34)\u2014and suggested post-hoc calibration as a solution for clinical deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u591a\u6a21\u6001\u80f8\u90e8X\u5149\u5206\u6790\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6cdb\u5316\u6027\u5dee / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u51bb\u7ed3\u7f16\u7801\u5668\u548c\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]\n    D[\u5173\u952e\u7ed3\u679c/Results: PET\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u5168\u5fae\u8c03\uff0c\u4f46\u6821\u51c6\u6027\u5dee / PET methods outperform full fine-tuning but have degraded calibration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [GRPO, mode collapse, diversity-aware reward, spectral clustering, structure-aware regularization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21514",children:"https://arxiv.org/pdf/2512.21514"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DiverseGRPO: Mitigating Mode Collapse] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[GRPO\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83/GRPO causes mode collapse]\n    B1 --\x3e B2[\u7f3a\u4e4f\u89c6\u89c9\u591a\u6837\u6027/Lacks visual diversity]\n    C --\x3e C1[\u5956\u52b1\u5c42\u9762: \u5206\u5e03\u521b\u9020\u529b\u5956\u52b1/Reward Level: Distributional Creativity Bonus]\n    C --\x3e C2[\u751f\u6210\u5c42\u9762: \u7ed3\u6784\u611f\u77e5\u6b63\u5219\u5316/Generation Level: Structure-Aware Regularization]\n    C1 --\x3e C3[\u57fa\u4e8e\u8bed\u4e49\u5206\u7ec4\u7684\u8c31\u805a\u7c7b/Spectral Clustering for Semantic Grouping]\n    D --\x3e D1[\u8bed\u4e49\u591a\u6837\u6027\u63d0\u534713%-18%/13%-18% Semantic Diversity Improvement]\n    D --\x3e D2[\u5efa\u7acb\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf/Establishes New Pareto Frontier]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [polarization imaging, computational imaging, underwater imaging, benchmark dataset, multi-scattering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fuzhou University, Research Institute of Highway, Ministry of Transport"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21513",children:"https://arxiv.org/pdf/2512.21513"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/WangPuyun/MuS-Polar3D",children:"https://github.com/WangPuyun/MuS-Polar3D"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u7684\u6563\u5c04\u548c\u89c2\u6d4b\u6761\u4ef6/Existing datasets lack diversity in scattering and observation conditions)\n    C --\x3e C1(\u6784\u5efa\u5305\u542b42\u4e2a\u7269\u4f53\u30017\u79cd\u6563\u5c04\u6761\u4ef6\u30015\u4e2a\u89c6\u89d2\u7684\u504f\u632f\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)\n    C --\x3e C2(\u63d0\u51fa\u89e3\u8026\u7684\u4e24\u9636\u6bb5\u6210\u50cf\u6d41\u7a0b\uff1a\u53bb\u6563\u5c04\u540e3D\u91cd\u5efa/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)\n    D --\x3e D1(\u5b9e\u73b0\u6700\u4f73\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee15.49\u5ea6/Achieve best mean angular error of 15.49 degrees)\n    D --\x3e D2(\u9996\u4e2a\u516c\u5f00\u7684\u5b9a\u91cf\u6d51\u6d4a\u6c34\u4e0b\u504f\u632f3D\u6210\u50cf\u57fa\u51c6\u6570\u636e\u96c6/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21516",children:"https://arxiv.org/pdf/2512.21516"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>\u5168\u5c40-\u5c40\u90e8\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Rare-paired & Mis-paired Samples<br/>\u6837\u672c\u914d\u5bf9\u7a00\u5c11\u4e0e\u9519\u8bef] --\x3e B1[Incomplete & Noise Multi-View Data<br/>\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u591a\u89c6\u56fe\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Unified Contrastive Learning Framework<br/>\u7edf\u4e00\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6] --\x3e C1[Global-Graph Guided CL<br/>\u5168\u5c40\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60]\n    C --\x3e C2[Local-Graph Weighted CL<br/>\u5c40\u90e8\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60]\n    C1 --\x3e C1a[Construct Global Affinity Graph<br/>\u6784\u5efa\u5168\u5c40\u4eb2\u548c\u529b\u56fe]\n    C2 --\x3e C2a[Generate Adaptive Weights<br/>\u751f\u6210\u81ea\u9002\u5e94\u6743\u91cd]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Superior Clustering Performance<br/>\u4f18\u8d8a\u7684\u805a\u7c7b\u6027\u80fd] --\x3e D1[Outperforms SOTA Methods<br/>\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5]\n    D --\x3e D2[Effective on Incomplete & Noise Data<br/>\u5728\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u6570\u636e\u4e0a\u6709\u6548]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Hierarchy-Aware Fine-Tuning of Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal learning], [hierarchical classification, vision-language models, efficient fine-tuning, LoRA, Tree-Path KL Divergence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Washington, Intel"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21529",children:"https://arxiv.org/pdf/2512.21529"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Hierarchy-Aware Fine-Tuning of Vision-Language Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("VLMs\u9002\u5e94\u5c42\u7ea7\u5206\u7c7b\u6548\u7387\u4f4e/VLMs inefficient for hierarchical classification")\n    Problem --\x3e P2("\u6807\u51c6\u65b9\u6cd5\u9884\u6d4b\u4e0d\u4e00\u81f4/Standard methods produce inconsistent predictions")\n    Method --\x3e M1("\u63d0\u51fa\u5c42\u7ea7\u611f\u77e5\u5fae\u8c03\u6846\u67b6/Propose hierarchy-aware fine-tuning framework")\n    Method --\x3e M2("\u7ed3\u5408TP-KL\u4e0eHiSCE\u635f\u5931/Combine TP-KL and HiSCE losses")\n    Method --\x3e M3("\u96c6\u6210\u8f7b\u91cf\u7ea7LoRA\u9002\u914d/Integrate lightweight LoRA adaptation")\n    Results --\x3e R1("\u63d0\u5347\u5168\u8def\u5f84\u7cbe\u5ea6/Improves Full-Path Accuracy")\n    Results --\x3e R2("\u964d\u4f4e\u4e0d\u4e00\u81f4\u6027\u9519\u8bef/Reduces Tree-based Inconsistency Error")\n    Results --\x3e R3("\u53c2\u6570\u5f00\u9500\u6700\u5c0f/Minimal parameter overhead")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Vision Transformers are Circulant Attention Learners"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision transformers], [circulant attention, block circulant matrix with circulant blocks (BCCB), computational complexity, vision transformers, fast Fourier transform (FFT)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21542",children:"https://arxiv.org/pdf/2512.21542"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/LeapLabTHU/Circulant-Attention",children:"https://github.com/LeapLabTHU/Circulant-Attention"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Vision Transformers are Circulant Attention Learners] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Self-attention quadratic complexity O(N\xb2) is computationally heavy for high-resolution vision tasks.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [context-aware object insertion, sponsor-product logo augmentation, vision-language models, diffusion models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21560",children:"https://arxiv.org/pdf/2512.21560"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Toward Intelligent Scene Augmentation<br>\u667a\u80fd\u573a\u666f\u589e\u5f3a] --\x3e B(Problem: Existing image editing lacks contextual appropriateness<br>\u6838\u5fc3\u95ee\u9898: \u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u5408\u7406\u6027)\n    A --\x3e C(Method: Introduce two new tasks & build datasets<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51fa\u4e24\u4e2a\u65b0\u4efb\u52a1\u5e76\u6784\u5efa\u6570\u636e\u96c6)\n    A --\x3e D(Results: Identifies research gap for joint reasoning & generation<br>\u5173\u952e\u7ed3\u679c: \u6307\u51fa\u4e86\u8054\u5408\u63a8\u7406\u4e0e\u751f\u6210\u7684\u7814\u7a76\u7a7a\u767d)\n    B --\x3e E(Task 1: Context-aware object insertion<br>\u4efb\u52a11: \u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u4f53\u63d2\u5165)\n    B --\x3e F(Task 2: Sponsor-product logo augmentation<br>\u4efb\u52a12: \u8d5e\u52a9\u5546\u4ea7\u54c1\u5546\u6807\u589e\u5f3a)\n    C --\x3e G(Utilize VLMs and diffusion models<br>\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b)\n    C --\x3e H(Build annotated datasets<br>\u6784\u5efa\u5e26\u6807\u6ce8\u7684\u6570\u636e\u96c6)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image inpainting], [object removal, dataset-free, test-time adaptation, multimodal large-language model, background-aware reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Seoul National University, OGQ"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21545",children:"https://arxiv.org/pdf/2512.21545"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Exploration of Reproducible Generated Image Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [AIGC detection, reproducibility, generalizability, diffusion models, binary classification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yihang Duan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21562",children:"https://arxiv.org/pdf/2512.21562"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Exploration of Reproducible Generated Image Detection] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Poor Reproducibility & Generalizability]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Literature Review, Dataset Construction, Method Reproduction]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Towards Long-window Anchoring in Vision-Language Model Distillation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [knowledge distillation, long-context, rotary position embeddings (RoPE), attention mechanism, vision-language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Zhongguancun Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21576",children:"https://arxiv.org/pdf/2512.21576"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Long-window Anchoring in Vision-Language Model Distillation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Small distilled VLMs have limited effective context windows]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: LAid - Progressive attention matching & learnable RoPE modulation]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves up to 3.2x longer context, maintains benchmark performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LLM-Free Image Captioning Evaluation in Reference-Flexible Settings"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image captioning evaluation], [LLM-free evaluation, reference-flexible, supervised metric, image-caption similarity, human-annotated dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Keio University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21582",children:"https://arxiv.org/pdf/2512.21582"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("LLM-Free Image Captioning Evaluation in Reference-Flexible Settings") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("LLM-based metrics lack neutrality/LLM\u6307\u6807\u7f3a\u4e4f\u4e2d\u7acb\u6027")\n    Problem --\x3e P2("LLM-free metrics lack performance/\u65e0LLM\u6307\u6807\u6027\u80fd\u4e0d\u8db3")\n    Method --\x3e M1("Propose Pearl metric/\u63d0\u51faPearl\u6307\u6807")\n    Method --\x3e M2("Learn image-caption & caption-caption similarity/\u5b66\u4e60\u56fe\u50cf-\u63cf\u8ff0\u4e0e\u63cf\u8ff0-\u63cf\u8ff0\u76f8\u4f3c\u6027")\n    Method --\x3e M3("Construct large human-annotated dataset/\u6784\u5efa\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6")\n    Results --\x3e R1("Outperforms LLM-free metrics on benchmarks/\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5176\u4ed6\u65e0LLM\u6307\u6807")\n    Results --\x3e R2("Works in reference-based & reference-free settings/\u9002\u7528\u4e8e\u6709\u53c2\u8003\u548c\u65e0\u53c2\u8003\u8bbe\u7f6e")\n    Results --\x3e R3("Fast & aligned with human judgment/\u5feb\u901f\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [Mamba, state-space model, knowledge distillation, lightweight model, U-Net]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21584",children:"https://arxiv.org/pdf/2512.21584"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight Global\u2013Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [harmful meme detection, large multimodal model, agent self-improvement, label-free adaptation, contrastive learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Southwestern University of Finance and Economics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21598",children:"https://arxiv.org/pdf/2512.21598"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [cryo-EM image analysis], [3D Gaussians, conformational heterogeneity, two-encoder-one-decoder, pseudo-atomic model, cryo-EM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21599",children:"https://arxiv.org/pdf/2512.21599"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yusuf Brima, Marcellin Atemkeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Osnabr\xfcck University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21602",children:"https://arxiv.org/pdf/2512.21602"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Imbalanced clinical data in emergency/critical care<br>\u6025\u8bca/\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>\u7cfb\u7edf\u8bc4\u4f30\u6811\u6a21\u578b\u3001TabNet\u53ca\u63d0\u51fa\u7684TabResNet]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost\u6700\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u4e0b\u6027\u80fd\u4e0b\u964d]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [MLLM personalization, long-context, training-free, state-aware, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21616",children:"https://arxiv.org/pdf/2512.21616"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TAMEing Long Contexts in Personalization<br>\u8bba\u6587\u6807\u9898] --\x3e B[Problem: Existing MLLM personalization lacks long-context support<br>\u6838\u5fc3\u95ee\u9898: \u73b0\u6709MLLM\u4e2a\u6027\u5316\u65b9\u6cd5\u7f3a\u4e4f\u957f\u4e0a\u4e0b\u6587\u652f\u6301]\n    A --\x3e C[Method: Proposes TAME framework with double memory & RA2G<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51faTAME\u6846\u67b6, \u5305\u542b\u53cc\u91cd\u8bb0\u5fc6\u548cRA2G\u8303\u5f0f]\n    A --\x3e D[Results: TAME achieves best performance on LCMP benchmark<br>\u5173\u952e\u7ed3\u679c: TAME\u5728LCMP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [novel view synthesis], [diffusion models, 3D Gaussian Splatting, auto-regressive restoration, context-aware inpainting, view synthesis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21618",children:"https://arxiv.org/pdf/2512.21618"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f\u4ea4\u901a\u7f16\u8f91 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u5bf9\u79f0\u81ea\u56de\u5f52\u5728\u7ebf\u4fee\u590d\u8303\u5f0f\u4e0e\u514d\u8bad\u7ec3\u534f\u8c03\u673a\u5236 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u65b0\u89c6\u89d2\u589e\u5f3a\u548c3D\u8f66\u8f86\u63d2\u5165\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [fine-grained visual categorization], [causal intervention, structural causal model, few-shot learning, interventional multi-scale encoder, interventional masked feature reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiwen Yang, Jinglin Xu, Yuxin Pen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, University of Science and Technology Beijing"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21617",children:"https://arxiv.org/pdf/2512.21617"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM",children:"https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]\n    Root --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[Support set as confounder/\u652f\u6301\u96c6\u4f5c\u4e3a\u6df7\u6dc6\u53d8\u91cf]\n    Problem --\x3e P2[Biased data distribution/\u6709\u504f\u6570\u636e\u5206\u5e03]\n    Problem --\x3e P3[Spurious correlations/\u865a\u5047\u5173\u8054]\n    Method --\x3e M1[Causal Intervention/\u56e0\u679c\u5e72\u9884]\n    Method --\x3e M2[IMSE: Sample-level/IMSE: \u6837\u672c\u5c42\u9762]\n    Method --\x3e M3[IMFR: Feature-level/IMFR: \u7279\u5f81\u5c42\u9762]\n    Results --\x3e R1[SOTA Performance/\u6700\u4f18\u6027\u80fd]\n    Results --\x3e R2[Datasets: CUB, Dogs, Cars/\u6570\u636e\u96c6: CUB, Dogs, Cars]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [StyleGAN2, CLIP, L1 regularization, latent space manipulation, attribute disentanglement]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Universitas Brawijaya"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21637",children:"https://arxiv.org/pdf/2512.21637"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Attribute Entanglement in Text-Driven Image Editing)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Sparse Latent Constraints via L1 Regularization)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: More Focused Edits, Reduced Unintended Changes)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal weather modeling], [multimodal foundation model, weather generation, weather understanding, Chain-of-Thought, self-attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21643",children:"https://arxiv.org/pdf/2512.21643"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Zhouzone/OmniWeather",children:"https://github.com/Zhouzone/OmniWeather"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Omni-Weather: \u7edf\u4e00\u591a\u6a21\u6001\u5929\u6c14\u57fa\u7840\u6a21\u578b / Unified Multimodal Foundation Model for Weather] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898 / Problem: \u5929\u6c14\u5efa\u6a21\u4e2d\u751f\u6210\u4e0e\u7406\u89e3\u4efb\u52a1\u5206\u79bb / Separation of generation and understanding in weather modeling]\n    C[\u4e3b\u8981\u65b9\u6cd5 / Method: \u7edf\u4e00\u67b6\u6784\uff0c\u5171\u4eab\u81ea\u6ce8\u610f\u529b\uff0c\u601d\u7ef4\u94fe\u6570\u636e\u96c6 / Unified architecture, shared self-attention, Chain-of-Thought dataset]\n    D[\u5173\u952e\u7ed3\u679c / Results: SOTA\u6027\u80fd\uff0c\u4efb\u52a1\u4e92\u589e\u5f3a / State-of-the-art performance, mutual enhancement of tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object grounding], [temporal multimodal grounding, LiDAR-image fusion, language-conditioned decoding, UniScene representation, NuPrompt benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Fudan University, Huawei Technologies Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21641",children:"https://arxiv.org/pdf/2512.21641"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TrackTeller: Temporal Multimodal 3D Grounding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u52a8\u60013D\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u4f9d\u8d56\u8bed\u8a00\u6307\u4ee3/Dynamic 3D Behavior-Dependent Language Grounding]\n    C --\x3e C1[\u7edf\u4e00\u591a\u6a21\u6001\u65f6\u5e8f\u6846\u67b6/Unified Temporal Multimodal Framework]\n    C1 --\x3e C2[LiDAR-\u56fe\u50cf\u878d\u5408\u4e0e\u8bed\u8a00\u89e3\u7801/LiDAR-Image Fusion & Language Decoding]\n    C1 --\x3e C3[\u6784\u5efaUniScene\u8868\u793a/Build UniScene Representation]\n    C1 --\x3e C4[\u5229\u7528\u8fd0\u52a8\u5386\u53f2\u63a8\u7406/Reason with Motion History]\n    D --\x3e D1[\u5728NuPrompt\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd/Significant Improvement on NuPrompt]\n    D1 --\x3e D2[AMOTA\u63d0\u534770%/70% AMOTA Gain]\n    D1 --\x3e D3[\u8bef\u62a5\u7387\u964d\u4f4e3.15-3.4\u500d/3.15-3.4x FA Reduction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Subramanyam Sahoo, Jared Junkin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley, Johns Hopkins University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21670",children:"https://arxiv.org/pdf/2512.21670"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective",children:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u662f\u9ed1\u76d2\u6a21\u578b/Deepfake detectors are black boxes]\n    C --\x3e C1[\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790/Sparse Autoencoder (SAE) Analysis]\n    C --\x3e C2[\u6cd5\u8bc1\u6d41\u5f62\u5206\u6790/Forensic Manifold Analysis]\n    D --\x3e D1[\u6f5c\u5728\u7279\u5f81\u7a00\u758f\u4f7f\u7528/Latent features are sparsely used]\n    D --\x3e D2[\u6d41\u5f62\u51e0\u4f55\u7279\u6027\u63ed\u793a\u4f2a\u5f71/Manifold geometry reveals artifacts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jalal Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," United Arab Emirates University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21673",children:"https://arxiv.org/pdf/2512.21673"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd/Evaluate DL model performance for AV perception]\nC --\x3e C1[\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6bd4\u8f83YOLO-NAS\u4e0eYOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\nD --\x3e D1[YOLOv8s\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1175%/YOLOv8s saves 75% training time]\nD --\x3e D2[YOLOv8s\u51c6\u786e\u7387\u66f4\u9ad8(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal understanding], [perceptual-level image understanding, multimodal large language models, domain-adaptive pre-training, task-aligned reinforcement learning, unified benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21675",children:"https://arxiv.org/pdf/2512.21675"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/thunderbolt215/UniPercept",children:"https://github.com/thunderbolt215/UniPercept"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["UniPercept: \u7edf\u4e00\u611f\u77e5\u7ea7\u56fe\u50cf\u7406\u89e3 / Unified Perceptual-Level Image Understanding"] --\x3e Problem["MLLMs\u611f\u77e5\u80fd\u529b\u6709\u9650 / MLLMs\' Perceptual Ability is Limited"]\n    Root --\x3e Method["\u63d0\u51fa\u7edf\u4e00\u57fa\u51c6\u4e0e\u6a21\u578b / Proposes Unified Benchmark & Model"]\n    Root --\x3e Results["\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6a21\u578b / Outperforms Existing Models"]\n    Problem --\x3e P1["\u611f\u77e5\u7ea7\u7279\u5f81\u7406\u89e3\u4e0d\u8db3 / Limited Perceptual-Level Feature Understanding"]\n    Method --\x3e M1["UniPercept-Bench\u57fa\u51c6 / UniPercept-Bench Benchmark"]\n    Method --\x3e M2["DAPT\u4e0eTask-Aligned RL\u8bad\u7ec3 / DAPT & Task-Aligned RL Training"]\n    Results --\x3e R1["\u5728VR\u4e0eVQA\u4efb\u52a1\u4e0a\u6cdb\u5316 / Generalizes on VR & VQA Tasks"]\n    Results --\x3e R2["\u53ef\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u5956\u52b1\u6a21\u578b / Serves as Reward Model for Generation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [blockchain provenance, vision-language models, semantic extraction, reproducibility, educational AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Rensselaer Polytechnic Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21684",children:"https://arxiv.org/pdf/2512.21684"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VLM\u8bed\u4e49\u8f93\u51fa\u96be\u4ee5\u9a8c\u8bc1\u3001\u590d\u73b0\u548c\u5ba1\u8ba1\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6eaf\u6e90\u6846\u67b6\uff0c\u4ece\u5e7b\u706f\u7247\u4e2d\u63d0\u53d6\u6982\u5ff5\u548c\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u54c8\u5e0c\u4e0a\u94fe / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63ed\u793a\u6a21\u578b\u95f4\u663e\u8457\u5dee\u5f02\uff0c\u5b9e\u73b0\u5b8c\u7f8e\u7be1\u6539\u68c0\u6d4b\u548c\u786e\u5b9a\u6027\u590d\u73b0\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b8c\u6574\u6027 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [graph neural network, contrastive learning, few-shot learning, cross-domain, structural consistency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology, University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21683",children:"https://arxiv.org/pdf/2512.21683"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/primebo1/C-Graph",children:"https://github.com/primebo1/C-Graph"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u8fc7\u6ee4\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u635f\u5bb3\u6027\u80fd/Existing methods filter domain-specific info, harming performance]\n    C --\x3e C1[\u5229\u7528\u7ed3\u6784\u4e00\u81f4\u6027\u4f5c\u4e3a\u5148\u9a8c/Use structural consistency as prior]\n    C --\x3e C2[\u56fe\u5efa\u6a21: SPG\u5c42, SMD\u89e3\u7801, CNC\u635f\u5931/Graph Modeling: SPG layer, SMD decoding, CNC loss]\n    D --\x3e D1[\u8de8\u57df\u6027\u80fdSOTA/Cross-domain SOTA performance]\n    D --\x3e D2[\u4fdd\u6301\u6e90\u57df\u7cbe\u5ea6/Preserves source-domain accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [attention collapse, degenerate diffusion, token-merging, mean-field PDE, VGGT]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21691",children:"https://arxiv.org/pdf/2512.21691"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[VGGT\u4e2d\u7684\u6ce8\u610f\u529b\u5d29\u6e83\u73b0\u8c61/Attention Collapse in VGGT]\n    C --\x3e C1[\u5c06\u6ce8\u610f\u529b\u8fed\u4ee3\u5efa\u6a21\u4e3a\u9000\u5316\u6269\u6563\u8fc7\u7a0b/Model Attention Iteration as Degenerate Diffusion]\n    D --\x3e D1[\u63a8\u5bfc\u51fa\u9884\u6d4b\u5d29\u6e83\u7684\u5747\u503c\u573aPDE/Derive Mean-Field PDE Predicting Collapse]\n    D --\x3e D2[\u7406\u8bba\u89e3\u91ca\u4ee4\u724c\u5408\u5e76\u7684\u7f13\u89e3\u4f5c\u7528/Theory Explains Token-Merging Remedy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [neural rendering], [Neural Radiance Fields, anisotropic specular reflections, Anisotropic Spherical Gaussian, von Mises-Fisher distribution, material editing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Albert Barreiro, Roger Mar\xed, Rafael Redondo, Gloria Haro, Carles Bosch"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Eurecat, Centre Tecnol\xf2gic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21692",children:"https://arxiv.org/pdf/2512.21692"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21\u5404\u5411\u5f02\u6027\u9ad8\u5149/Existing methods struggle with anisotropic specular reflections]\n    C --\x3e C1[\u63d0\u51faShinyNeRF\u6846\u67b6/Propose ShinyNeRF framework]\n    C1 --\x3e C2[\u4f7f\u7528\u5404\u5411\u540c\u6027vMF\u6df7\u5408\u8fd1\u4f3c\u51fa\u5c04\u8f90\u5c04\u5ea6/Use isotropic vMF mixture to approximate outgoing radiance]\n    C2 --\x3e C3[\u8054\u5408\u4f30\u8ba1\u6cd5\u7ebf\u3001\u5207\u7ebf\u3001\u9ad8\u5149\u53c2\u6570/Jointly estimate normals, tangents, specular parameters]\n    D --\x3e D1[\u5b9e\u73b0SOTA\u6027\u80fd/Achieves SOTA performance]\n    D --\x3e D2[\u63d0\u4f9b\u7269\u7406\u89e3\u91ca\u548c\u6750\u8d28\u7f16\u8f91/Provides physical interpretation and material editing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [anatomical prior, attention mechanism, variational autoencoder, densely connected blocks, spatial pyramid pooling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Li Yang, Yuting Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Wannan Medical College"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21693",children:"https://arxiv.org/pdf/2512.21693"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Prior-AttUNet: \u89c6\u7f51\u819cOCT\u6db2\u4f53\u5206\u5272 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6a21\u7cca\u8fb9\u754c\u4e0e\u8bbe\u5907\u5dee\u5f02 / Ambiguous Boundaries & Device Heterogeneity]\n    C --\x3e C1[\u53cc\u8def\u5f84\u67b6\u6784 / Dual-Path Architecture]\n    C --\x3e C2[\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5148\u9a8c / VAE Priors]\n    C --\x3e C3[\u4e09\u91cd\u6ce8\u610f\u529b\u673a\u5236 / Triple-Attention Mechanism]\n    D --\x3e D1[\u9ad8Dice\u5206\u6570 / High Dice Scores]\n    D --\x3e D2[\u4f4e\u8ba1\u7b97\u6210\u672c / Low Computational Cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [handwritten text generation], [Generative Adversarial Networks, Handwritten Text Generation, Bengali, Dataset, Pre-processing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21694",children:"https://arxiv.org/pdf/2512.21694"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[BeHGAN: Bengali Handwritten Word Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[HTG is challenging & understudied for Bengali<br/>\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u751f\u6210\u7814\u7a76\u4e0d\u8db3\u4e14\u56f0\u96be]\n    C --\x3e C1[Propose GAN-based method<br/>\u63d0\u51fa\u57fa\u4e8eGAN\u7684\u65b9\u6cd5]\n    C --\x3e C2[Use self-collected dataset<br/>\u4f7f\u7528\u81ea\u6536\u96c6\u6570\u636e\u96c6]\n    C --\x3e C3[Pre-process images<br/>\u9884\u5904\u7406\u56fe\u50cf]\n    D --\x3e D1[Generates diverse handwritten words<br/>\u751f\u6210\u591a\u6837\u5316\u624b\u5199\u8bcd]\n    D --\x3e D2[Contributes to Bengali HTG research<br/>\u63a8\u52a8\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u751f\u6210\u7814\u7a76]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [ai-generated image detection], [Fast Fourier Transform, CLIP, hybrid system, spectral features, semantic features]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content. Affiliation/email domain not present."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21695",children:"https://arxiv.org/pdf/2512.21695"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Reliable detection of AI-generated images"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: SOTA on Chameleon, strong generalization across datasets"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion prediction], [Mixture of Experts, Mamba, spatiotemporal dependencies, computational efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology, Nanjing Forestry University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21707",children:"https://arxiv.org/pdf/2512.21707"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/alanyz106/ST-MoE",children:"https://github.com/alanyz106/ST-MoE"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u4e0d\u7075\u6d3b\u7684\u65f6\u7a7a\u8868\u793a/Inflexible spatiotemporal representation")\n    Problem --\x3e P2("\u9ad8\u8ba1\u7b97\u6210\u672c/High computational cost")\n    Method --\x3e M1("\u63d0\u51faST-MoE\u6a21\u578b/Propose ST-MoE model")\n    M1 --\x3e M2("\u56db\u79cd\u65f6\u7a7a\u4e13\u5bb6/Four spatiotemporal experts")\n    M2 --\x3e M3("\u53cc\u5411\u65f6\u7a7aMamba/Bidirectional spatiotemporal Mamba")\n    Results --\x3e R1("\u7cbe\u5ea6\u8d85\u8d8aSOTA/Outperforms SOTA in accuracy")\n    Results --\x3e R2("\u53c2\u6570\u51cf\u5c1141.38%/Reduces parameters by 41.38%")\n    Results --\x3e R3("\u8bad\u7ec3\u52a0\u901f3.6\u500d/3.6x training speedup")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video prediction], [Efficient Video Attention (EVA), spatiotemporal factorization, real-time inference, on-device AI, training curriculum]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21710",children:"https://arxiv.org/pdf/2512.21710"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Thelegendzz/RAPTOR",children:"https://github.com/Thelegendzz/RAPTOR"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[RAPTOR: \u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u89c6\u9891\u9884\u6d4b<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u89c6\u9891\u9884\u6d4b\u7684"\u4e09\u96be\u56f0\u5883": \u901f\u5ea6\u3001\u5206\u8fa8\u7387\u3001\u8d28\u91cf<br>Video Prediction Trilemma: Speed, Resolution, Quality]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u9ad8\u6548\u89c6\u9891\u6ce8\u610f\u529b (EVA) \u6a21\u5757<br>Efficient Video Attention (EVA) Module]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u9996\u4e2a\u5728Jetson\u4e0a512^2\u89c6\u9891>30 FPS<br>First >30 FPS for 512^2 video on Jetson]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AstraNav-World: World Model for Foresight Control and Consistency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [world models], [world model, diffusion model, embodied navigation, foresight control, vision-language policy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Amap Alibaba, Peking University (PKU), Tsinghua University (THU)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21714",children:"https://arxiv.org/pdf/2512.21714"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://astra-amap.github.io/AstraNav-World.github.io/",children:"https://astra-amap.github.io/AstraNav-World.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AstraNav-World] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [autoregressive video generation, KV caching, sliding window attention, temporal knot, chunk-wise generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21734",children:"https://arxiv.org/pdf/2512.21734"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://humanaigc.github.io/knot_forcing_demo_page/",children:"https://humanaigc.github.io/knot_forcing_demo_page/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A "running ahead" mechanism that dynamically updates the reference frame\'s temporal coordinate to maintain long-term coherence.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: \u9a6f\u670d\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>\u5b9e\u65f6\u8096\u50cf\u52a8\u753b\u9700\u8981\u4f4e\u5ef6\u8fdf\u548c\u4e00\u81f4\u6027\uff0c\u4f46\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u8fd0\u52a8\u4e0d\u8fde\u7eed\u95ee\u9898\u3002"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and \'Running Ahead\' mechanism.<br>\u5206\u5757\u751f\u6210\u4e0eKV\u7f13\u5b58\uff0c\u7528\u4e8e\u5e73\u6ed1\u8fc7\u6e21\u7684\u65f6\u5e8f\u7ed3\u6a21\u5757\uff0c\u4ee5\u53ca\'\u8d85\u524d\u8fd0\u884c\'\u673a\u5236\u3002"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u3001\u4ea4\u4e92\u5f0f\u7684\u5b9e\u65f6\u8096\u50cf\u52a8\u753b\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [lip-syncing, diffusion transformer, two-stage learning, inpainting, self-correction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21736",children:"https://arxiv.org/pdf/2512.21736"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://humanaigc.github.io/sync_anyone_demo_page/",children:"https://humanaigc.github.io/sync_anyone_demo_page/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[SyncAnyone] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u73b0\u6709\u65b9\u6cd5\u7834\u574f\u65f6\u7a7a\u4e0a\u4e0b\u6587 / Existing methods disrupt spatiotemporal context"]\n    C --\x3e C1["\u9636\u6bb51: \u57fa\u4e8e\u63a9\u7801\u7684\u6269\u6563\u53d8\u6362\u5668\u8bad\u7ec3 / Stage 1: Mask-based DiT training"]\n    C --\x3e C2["\u9636\u6bb52: \u65e0\u63a9\u7801\u8c03\u4f18\u4e0e\u81ea\u6821\u6b63 / Stage 2: Mask-free tuning & self-correction"]\n    D --\x3e D1["SOTA\u89c6\u89c9\u8d28\u91cf\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027 / SOTA visual quality & temporal coherence"]\n    D --\x3e D2["\u66f4\u597d\u7684\u8eab\u4efd\u4e0e\u80cc\u666f\u4fdd\u6301 / Better identity & background preservation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hengyi Wu, Zhenyi Wang, Heng Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Maryland, College Park, University of Central Florida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21743",children:"https://arxiv.org/pdf/2512.21743"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --\x3e P1[\u9ad8\u71b5\u5c42\u6b20\u62df\u5408/High-entropy layers underfit]\n    Problem --\x3e P2[\u4f4e\u71b5\u5c42\u8fc7\u62df\u5408/Low-entropy layers overfit]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Entropy-aware dynamic feedback for layer-wise control] --\x3e M1[\u51cf\u5c11\u9ad8\u71b5\u5c42\u71b5\u503c/Reduce entropy in high-entropy layers]\n    Method --\x3e M2[\u589e\u52a0\u4f4e\u71b5\u5c42\u71b5\u503c/Increase entropy in low-entropy layers]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Improved generalization and performance] --\x3e R1[\u6536\u655b\u5230\u66f4\u5bbd\u7684\u5c40\u90e8\u6781\u5c0f\u503c/Converge to wider local minima]\n    Results --\x3e R2[\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5/Outperforms state-of-the-art baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Roorkee, OP Jindal University, Lule\xe5 University of Technology, Indian Institute of Technology Dhanbad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21747",children:"https://arxiv.org/pdf/2512.21747"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Driver drowsiness detection for road safety)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved stability on SEED-VIG, SOTA on STEW)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [Quaternion Neural Networks, Cross-Attention, Unpaired Data, Multimodal Learning, Explainable AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Anna University, National Institute of Technology Tiruchirappalli"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21760",children:"https://arxiv.org/pdf/2512.21760"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Scarcity of paired & aligned multimodal medical datasets]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Evgeny Alves Limarenko, Anastasiia Studenikina"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Moscow Institute of Physics and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21769",children:"https://arxiv.org/pdf/2512.21769"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("BertsWin: 3D MAE\u4f18\u5316") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("3D MAE\u62d3\u6251\u7a00\u758f\u6027/Topological Sparsity in 3D MAE")\n    Problem --\x3e P2("\u7834\u574f\u7a7a\u95f4\u5173\u7cfb/Destroys Spatial Context")\n    Method --\x3e M1("BertsWin\u6df7\u5408\u67b6\u6784/BertsWin Hybrid Architecture")\n    Method --\x3e M2("\u5b8c\u65743D\u4ee4\u724c\u7f51\u683c/Full 3D Token Grid")\n    Method --\x3e M3("Swin\u7a97\u53e3 & \u7ed3\u6784\u635f\u5931/Swin Windows & Structural Loss")\n    Results --\x3e R1("5.8x\u8bed\u4e49\u6536\u655b\u52a0\u901f/5.8x Faster Convergence")\n    Results --\x3e R2("15\u500d\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c11/15x Fewer Epochs")\n    Results --\x3e R3("FLOPs\u6301\u5e73\uff0c\u603b\u8d44\u6e90\u51cf\u5c11/FLOP Parity, Net Resource Reduction")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Inference-based GAN Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [VAE-GAN, Markov chain, long video generation, temporal consistency, encoder-decoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jingbo Yang, Adrian G. Bors"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of York"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21776",children:"https://arxiv.org/pdf/2512.21776"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Inference-based GAN Video Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u751f\u6210\u957f\u89c6\u9891/Existing models struggle with long video generation]\n    P1 --\x3e P2[\u89c6\u9891\u957f\u5ea6\u589e\u52a0\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d/Increased length degrades quality]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faVAE-GAN\u6df7\u5408\u89c6\u9891\u751f\u6210\u5668/Propose VAE-GAN hybrid video generator]\n    M1 --\x3e M2[\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u6269\u5c55/Extend with Markov chain framework]\n    M2 --\x3e M3[\u72b6\u6001\u4ee3\u8868\u77ed\u89c6\u9891\u751f\u6210\u5668/Each state is a short video generator]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u80fd\u591f\u751f\u6210\u957f\u89c6\u9891\u5e8f\u5217/Can generate long video sequences]\n    R1 --\x3e R2[\u786e\u4fdd\u65f6\u5e8f\u8fde\u7eed\u6027\u4e0e\u4e00\u81f4\u6027/Ensures temporal continuity and consistency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video scene segmentation], [vision-language model, multimodal reasoning, context-focus window, confidence score extraction, explainable AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Ben-Gurion University, Amazon Prime Video, Tel-Aviv University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21778",children:"https://arxiv.org/pdf/2512.21778"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Scene-VLM: Multimodal Video Scene Segmentation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u73b0\u6709\u65b9\u6cd5\u9650\u5236/Limitations of existing methods")\n    P1 --\x3e P1_1("\u89c6\u89c9\u4e2d\u5fc3\u504f\u89c1/Visual-centric bias")\n    P1 --\x3e P1_2("\u5b64\u7acb\u5206\u7c7b/Isolated shot classification")\n    P1 --\x3e P1_3("\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/Lack explainability")\n    Method --\x3e M1("\u5fae\u8c03VLM\u6846\u67b6/Fine-tuned VLM framework")\n    M1 --\x3e M1_1("\u591a\u6a21\u6001\u63a8\u7406/Multimodal reasoning")\n    M1 --\x3e M1_2("\u5e8f\u5217\u9884\u6d4b/Sequential prediction")\n    M1 --\x3e M1_3("\u4e0a\u4e0b\u6587\u805a\u7126\u7a97\u53e3/Context-focus window")\n    Method --\x3e M2("\u7f6e\u4fe1\u5ea6\u63d0\u53d6/Confidence score extraction")\n    Method --\x3e M3("\u751f\u6210\u89e3\u91ca/Generate rationales")\n    Results --\x3e R1("SOTA\u6027\u80fd/State-of-the-art performance")\n    Results --\x3e R2("\u663e\u8457\u63d0\u5347/Significant improvement on MovieNet")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21789",children:"https://arxiv.org/pdf/2512.21789"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u79d1\u5b66\u56fe\u8868\u8bf4\u660e\u8d28\u91cf\u5dee/Poor quality of scientific figure captions]\n    Problem --\x3e P2[\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6/Lack of large-scale real-world dataset]\n    Method --\x3e M1[\u6784\u5efaarXiv\u56fe\u8868-\u8bf4\u660e\u5bf9\u6570\u636e\u96c6/Construct arXiv figure-caption dataset]\n    Method --\x3e M2[\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e0e\u8bc4\u4f30/Domain-specific training & evaluation]\n    Method --\x3e M3[\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5174\u8d77/Navigate rise of LLMs]\n    Results --\x3e R1[\u603b\u7ed3\u6280\u672f\u65b9\u6cd5\u7ecf\u9a8c/Summarize technical & methodological lessons]\n    Results --\x3e R2[\u63d0\u51fa\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411/Outline future challenges & directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [Parameter-Efficient Fine-Tuning, Mixture of Low-rank Experts, Instruction-Guided Routing, Multi-Conditional Generation, Diffusion Transformers]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance Inc., Rutgers University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21788",children:"https://arxiv.org/pdf/2512.21788"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/yanq095/InstructMoLE",children:"https://github.com/yanq095/InstructMoLE"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms LoRA & MoLE variants on benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [histopathological images, image segmentation, grain detection, deep learning, mycetoma classification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21792",children:"https://arxiv.org/pdf/2512.21792"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI\u7528\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u8db3\u83cc\u80bf\u8bca\u65ad\uff1aMICCAI 2024\u6311\u6218\u8d5b") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u8db3\u83cc\u80bf\u8bca\u65ad\u56f0\u96be\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a<br>Mycetoma diagnosis is challenging, especially in low-resource settings")\n    Method --\x3e M1("\u7ec4\u7ec7mAIcetoma\u6311\u6218\u8d5b\uff0c\u5f00\u53d1AI\u6a21\u578b<br>Organized mAIcetoma challenge to develop AI models")\n    Method --\x3e M2("\u63d0\u4f9bMyData\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u5272\u548c\u5206\u7c7b<br>Provided MyData dataset for segmentation and classification")\n    Results --\x3e R1("\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272<br>Models achieved high segmentation accuracy")\n    Results --\x3e R2("\u9876\u7ea7\u6a21\u578b\u5206\u7c7b\u6027\u80fd\u663e\u8457<br>Top models showed significant classification performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image super-resolution], [diffusion posterior sampling, single-image super-resolution, inverse problems, measurement consistency, unconditional diffusion prior]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abu Hanif Muhammad Syarubany"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science & Technology (KAIST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21797",children:"https://arxiv.org/pdf/2512.21797"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Diffusion Posterior Sampling for Super-Resolution<br>\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7528\u4e8e\u8d85\u5206\u8fa8\u7387"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Single-image super-resolution under Gaussian noise<br>\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Diffusion Posterior Sampling (DPS)<br>\u6269\u6563\u540e\u9a8c\u91c7\u6837<br>Unconditional prior + likelihood-guided conditioning<br>\u65e0\u6761\u4ef6\u5148\u9a8c + \u4f3c\u7136\u5f15\u5bfc\u7684\u6761\u4ef6\u5316"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Optimal PS scale 0.95, noise \u03c3=0.01<br>\u6700\u4f73PS\u5c3a\u5ea60.95, \u566a\u58f0\u03c3=0.01<br>Balancing prior and data yields sharp details<br>\u5e73\u8861\u5148\u9a8c\u4e0e\u6570\u636e\u5f97\u5230\u6e05\u6670\u7ec6\u8282"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Mamba, Triple-Mapping Adaptive Coupling (TMAC), Adaptive Mamba Head, biomedical instance detection, VSSD backbone]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21803",children:"https://arxiv.org/pdf/2512.21803"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CellMamba: Adaptive Mamba for Cell Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Cell detection challenges in pathological images]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: CellMamba with TMAC module & Adaptive Mamba Head]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines, lightweight & efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] S&P 500 Stock's Movement Prediction using CNN"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [financial time series forecasting], [Convolutional Neural Network (CNN), multivariate raw data, stock movement prediction, historical data matrices, S&P 500]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Gupta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," None (No affiliation or email domain provided in the given content)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21804",children:"https://arxiv.org/pdf/2512.21804"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["S&P 500 Stock\'s Movement Prediction using CNN<br>\u4f7f\u7528CNN\u9884\u6d4b\u6807\u666e500\u80a1\u7968\u8d70\u52bf"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Predicting stock price movement<br>\u9884\u6d4b\u80a1\u7968\u4ef7\u683c\u8d70\u52bf"] --\x3e P1["\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u5de5\u7a0b<br>Traditional methods rely on engineered features"]\n    Problem --\x3e P2["\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u5355\u7ef4\u6570\u636e<br>Existing research often uses single-dimension data"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Use CNN on raw multivariate data<br>\u5bf9\u539f\u59cb\u591a\u53d8\u91cf\u6570\u636e\u4f7f\u7528CNN"] --\x3e M1["\u5c06\u5386\u53f2\u6570\u636e\u77e9\u9635\u89c6\u4e3a\u56fe\u50cf<br>Treat historical data matrices as images"]\n    Method --\x3e M2["\u5305\u542b\u539f\u59cb\u5e02\u573a\u4e8b\u4ef6(\u5982\u62c6\u80a1)<br>Include raw market events (e.g., splits)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Model achieves promising results<br>\u6a21\u578b\u53d6\u5f97\u6709\u5e0c\u671b\u7684\u7ed3\u679c"] --\x3e R1["\u652f\u6301\u80a1\u7968/\u884c\u4e1a/\u7ec4\u5408\u7ea7\u522b\u9884\u6d4b<br>Supports stock/sector/portfolio prediction"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Australian National University, The University of Queensland, GE Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21815",children:"https://arxiv.org/pdf/2512.21815"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[VLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb/VLMs are vulnerable to adversarial attacks]\n    Problem --\x3e P2[\u5148\u9a8c\u653b\u51fb\u5047\u8bbe\u6240\u6709token\u540c\u7b49\u91cd\u8981/Prior attacks assume all tokens are equally important]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u8bc6\u522b\u9ad8\u71b5\u5173\u952e\u51b3\u7b56\u70b9/Identify high-entropy critical decision points]\n    Method --\x3e M2[\u63d0\u51fa\u71b5\u5e93\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9ad8\u6548\u653b\u51fb:\u5c0f\u9884\u7b97\u5b9e\u73b0\u5f3a\u8bed\u4e49\u9000\u5316/Efficient attack: strong degradation with small budget]\n    Results --\x3e R2[\u9ad8\u6709\u5bb3\u8f6c\u5316\u7387:35-49%/High harmful conversion: 35-49%]\n    Results --\x3e R3[\u53ef\u884c\u8fc1\u79fb\u6027:17-26%/Feasible transferability: 17-26%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [autonomous driving perception], [multimodal fusion, multi-view cooperative perception, spatiotemporal modeling, V2X communication, deformable attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenwei Yang, Yibo Ai, Weidong Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology Beijing, National Center for Materials Service Safety"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21831",children:"https://arxiv.org/pdf/2512.21831"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u611f\u77e5\u6311\u6218:\u906e\u6321,\u89c6\u89d2\u9650\u5236,\u901a\u4fe1\u5ef6\u8fdf/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]\nC --\x3e C1[\u63d0\u51faXET-V2X\u6846\u67b6/Proposes XET-V2X Framework]\nC1 --\x3e C2[\u53cc\u5c42\u7ea7\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757/Dual-layer Spatial Cross-Attention Module]\nC2 --\x3e C3[\u591a\u89c6\u56fe\u56fe\u50cf\u7279\u5f81\u805a\u5408/Multi-view Image Feature Aggregation]\nC2 --\x3e C4[\u70b9\u4e91\u878d\u5408/Point Cloud Fusion]\nD --\x3e D1[\u5728V2X-Seq-SPD\u7b49\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347/Performance Improvements on V2X-Seq-SPD, etc.]\nD1 --\x3e D2[\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u6027\u80fd\u589e\u5f3a/Enhanced Detection & Tracking Performance]\nD2 --\x3e D3[\u9c81\u68d2\u4e14\u65f6\u5e8f\u7a33\u5b9a\u7684\u611f\u77e5/Robust & Temporally Stable Perception]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. Affiliation information is not included."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21845",children:"https://arxiv.org/pdf/2512.21845"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URLETF2"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898 / Problem"] --\x3e P1["\u8fc7\u62df\u5408\u65b0\u6570\u636e / Overfitting to new data"]\n    Problem --\x3e P2["\u707e\u96be\u6027\u9057\u5fd8\u65e7\u6570\u636e / Catastrophic forgetting of old data"]\n    Problem --\x3e P3["\u7279\u5f81\u5dee\u5f02\u4e0e\u7c7b\u522b\u9519\u4f4d / Feature difference & Class misalignment"]\n\n    Method["\u4e3b\u8981\u65b9\u6cd5 / Method"] --\x3e M1["SCL-PNC\u65b9\u6cd5 / SCL-PNC Method"]\n    M1 --\x3e M1_1["\u81ea\u9002\u5e94\u5c42\u6269\u5c55\u4e3b\u5e72 / Adapt-layer for backbone expansion"]\n    M1 --\x3e M1_2["\u52a8\u6001\u53c2\u6570\u5316ETF\u5206\u7c7b\u5668 / Dynamic Parametric ETF Classifier"]\n    M1 --\x3e M1_3["\u5e76\u884c\u6269\u5c55\u4e0e\u77e5\u8bc6\u84b8\u998f / Parallel expansion & Knowledge distillation"]\n\n    Results["\u5173\u952e\u7ed3\u679c / Results"] --\x3e R1["\u9ad8\u6548\u5904\u7406\u7c7b\u522b\u589e\u957f / Efficiently handles increasing categories"]\n    Results --\x3e R2["\u89e3\u51b3\u7c7b\u522b\u9519\u4f4d / Addresses class misalignment"]\n    Results --\x3e R3["\u786e\u4fdd\u7279\u5f81\u4e00\u81f4\u6027 / Ensures feature consistency"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [salient object detection], [RGB-T, unaligned images, Thin-Plate Spline, MobileViT, Mamba]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian Minzu University, Shandong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21856",children:"https://arxiv.org/pdf/2512.21856"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/HTUTU2/TPS-SCL",children:"https://github.com/HTUTU2/TPS-SCL"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Breaking Alignment Barriers: TPS-SCL<br>\u7a81\u7834\u5bf9\u9f50\u58c1\u5792: TPS-SCL] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u9f50\u6570\u636e\u96c6<br>Existing methods rely on aligned datasets]\n    B --\x3e B2[\u771f\u5b9e\u573a\u666f\u56fe\u50cf\u672a\u5bf9\u9f50<br>Real-world images are unaligned]\n    C --\x3e C1[\u53cc\u6d41MobileViT\u7f16\u7801\u5668<br>Dual-stream MobileViT encoder]\n    C --\x3e C2[TPS\u5bf9\u9f50\u6a21\u5757<br>TPS Alignment Module]\n    C --\x3e C3[\u8bed\u4e49\u5173\u8054\u7ea6\u675f\u6a21\u5757<br>Semantic Correlation Constraint Module]\n    C --\x3e C4[\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5757<br>Cross-Modal Correlation Module]\n    D --\x3e D1[\u8f7b\u91cf\u7ea7SOTA\u6027\u80fd<br>Lightweight SOTA performance]\n    D --\x3e D2[\u8d85\u8d8a\u4e3b\u6d41RGB-T\u65b9\u6cd5<br>Outperforms mainstream RGB-T methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image embedding], [conditional image embedding, large vision-language model, training-free, image similarity, hidden state]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," CyberAgent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21860",children:"https://arxiv.org/pdf/2512.21860"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings",children:"https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u5168\u5c40\u56fe\u50cf\u5d4c\u5165\u65e0\u6cd5\u805a\u7126\u4e8e\u7279\u5b9a\u6587\u672c\u6761\u4ef6]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: DIOR - \u5229\u7528LVLM\u751f\u6210\u5355\u5b57\u63cf\u8ff0\u5e76\u63d0\u53d6\u6700\u540etoken\u7684\u9690\u85cf\u72b6\u6001]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u6761\u4ef6\u56fe\u50cf\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [speculative decoding, draft tree, inference acceleration, autoregressive image generation, dynamic tree structure]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21857",children:"https://arxiv.org/pdf/2512.21857"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Haodong-Lei-Ray/ADT-Tree",children:"https://github.com/Haodong-Lei-Ray/ADT-Tree"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fast Inference of Visual AR Model with ADT-Tree<br>\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5feb\u901f\u63a8\u7406\u4e0eADT-Tree"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Visual AR models have slow sequential inference.<br>\u89c6\u89c9AR\u6a21\u578b\u63a8\u7406\u6162"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>\u63d0\u51fa\u90bb\u63a5\u81ea\u9002\u5e94\u52a8\u6001\u8349\u7a3f\u6811"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b03.13x/3.05x\u52a0\u901f"]\n    Problem --\x3e P1["Spatially varying token prediction difficulty.<br>\u7a7a\u95f4\u53d8\u5316\u7684token\u9884\u6d4b\u96be\u5ea6"]\n    Method --\x3e M1["Dynamically adjusts tree depth & width.<br>\u52a8\u6001\u8c03\u6574\u6811\u6df1\u5ea6\u4e0e\u5bbd\u5ea6"]\n    Method --\x3e M2["Leverages adjacency & prior acceptance rates.<br>\u5229\u7528\u90bb\u63a5\u5173\u7cfb\u548c\u5148\u9a8c\u63a5\u53d7\u7387"]\n    Results --\x3e R1["Integrates with relaxed sampling.<br>\u53ef\u4e0e\u677e\u5f1b\u91c7\u6837\u65b9\u6cd5\u7ed3\u5408"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," North South University, Korea Institute of Oriental Medicine, American International University\u2013Bangladesh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21861",children:"https://arxiv.org/pdf/2512.21861"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video matting], [video omnimatte, diffusion models, LoRA, DiT blocks, dual-expert]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yihan Hu, Xuelin Chen, Xiaodong Cun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Great Bay University, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21865",children:"https://arxiv.org/pdf/2512.21865"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/GVCLab/EasyOmnimatte",children:"https://github.com/GVCLab/EasyOmnimatte"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EasyOmnimatte] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u6162\u4e14\u6b21\u4f18/Existing methods are slow and suboptimal]\n    C --\x3e C1[\u53cc\u4e13\u5bb6\u5fae\u8c03/Dual-Expert Finetuning]\n    C1 --\x3e C2[\u6548\u679c\u4e13\u5bb6/Effect Expert]\n    C1 --\x3e C3[\u8d28\u91cf\u4e13\u5bb6/Quality Expert]\n    D --\x3e D1[\u9ad8\u8d28\u91cf\u5206\u89e3/High-quality decomposition]\n    D --\x3e D2[\u9ad8\u6548\u5feb\u901f/Efficient and fast]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [autoregressive image generation, dynamic tokenization, next-token prediction entropy, patch merging, training efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, San Diego, Dolby Laboratories"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21867",children:"https://arxiv.org/pdf/2512.21867"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual localization], [early-fusion, sparse mask attention, pose tokenizer, VGGT backbone, multi-view geometry]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21883",children:"https://arxiv.org/pdf/2512.21883"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/dtc111111/Reloc-VGGT",children:"https://github.com/dtc111111/Reloc-VGGT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [fMRI, foundation model, data-efficient, training-efficient, hierarchical encoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southern University of Science and Technology, University of Warwick, Fudan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21881",children:"https://arxiv.org/pdf/2512.21881"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u74f6\u9888/Bottlenecks of Existing Methods]\n    B1 --\x3e B1_1[\u56fe\u8c31\u65b9\u6cd5: \u4e22\u5931\u7ec6\u8282, \u9700\u5927\u6570\u636e/Atlas-based: lose details, need big data]\n    B1 --\x3e B1_2[\u65e0\u56fe\u8c31\u65b9\u6cd5: \u5185\u5b58\u8ba1\u7b97\u6210\u672c\u9ad8/Atlas-free: high memory & compute cost]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u8bbe\u8ba1/Two-stage Adaptive Design]\n    C1 --\x3e C1_1[\u8f7b\u91cf\u65f6\u5e8f\u63d0\u53d6\u5668: \u5168\u5c40\u4e0a\u4e0b\u6587\u4e0e\u663e\u8457\u6027\u6392\u5e8f/Lightweight Temporal Extractor: global context & saliency ranking]\n    C1 --\x3e C1_2[4D\u5206\u5c42\u7f16\u7801\u5668: \u4eceTop-k\u7a97\u53e3\u5b66\u4e60/4D Hierarchical Encoder: learn from top-k windows]\n    D --\x3e D1[\u6027\u80fd/Performance]\n    D --\x3e D2[\u6548\u7387/Efficiency]\n    D1 --\x3e D1_1[\u5728\u4e03\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA/Achieves SOTA on seven benchmarks]\n    D2 --\x3e D2_1[\u4ec5\u97004\u5343\u6b21\u9884\u8bad\u7ec3\u4f1a\u8bdd/Only 4k pre-training sessions]\n    D2 --\x3e D2_2[GPU\u5185\u5b58\u964d\u81f330%/GPU memory reduced to ~30%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CrownGen: Patient-customized Crown Generation via Point Diffusion Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape generation], [diffusion model, point cloud, dental crown, generative framework, boundary prediction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21890",children:"https://arxiv.org/pdf/2512.21890"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CrownGen: Patient-customized Crown Generation<br>\u57fa\u4e8e\u70b9\u6269\u6563\u6a21\u578b\u7684\u4e2a\u6027\u5316\u7259\u51a0\u751f\u6210] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Digital crown design is labor-intensive<br>\u6570\u5b57\u5316\u7259\u51a0\u8bbe\u8ba1\u8d39\u65f6\u8d39\u529b]\n    C --\x3e C1[Uses a point diffusion model<br>\u4f7f\u7528\u70b9\u6269\u6563\u6a21\u578b]\n    C --\x3e C2[Has a boundary prediction module<br>\u5305\u542b\u8fb9\u754c\u9884\u6d4b\u6a21\u5757]\n    D --\x3e D1[Surpasses SOTA in geometric fidelity<br>\u51e0\u4f55\u4fdd\u771f\u5ea6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5]\n    D --\x3e D2[Reduces active design time<br>\u51cf\u5c11\u4e3b\u52a8\u8bbe\u8ba1\u65f6\u95f4]\n    D --\x3e D3[Crowns are clinically non-inferior<br>\u4e34\u5e8a\u8d28\u91cf\u4e0d\u52a3\u4e8e\u4eba\u5de5\u8bbe\u8ba1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human image animation], [diffusion transformer, hybrid implicit guidance, position shift adaptive module, skeleton alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Guangzhou Quwan Network Technology, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21905",children:"https://arxiv.org/pdf/2512.21905"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>\u57fa\u4e8e\u6269\u6563Transformer\u7684\u9ad8\u4fdd\u771f\u957f\u65f6\u4eba\u4f53\u56fe\u50cf\u52a8\u753b"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results"]\n    B --\x3e B1["\u957f\u89c6\u9891\u751f\u6210\u6311\u6218<br>Long-duration Video Generation"]\n    B --\x3e B2["\u9762\u90e8\u4e0e\u624b\u90e8\u7ec6\u8282\u5408\u6210\u4e0d\u8db3<br>Lack of Fine-grained Facial/Hand Details"]\n    C --\x3e C1["\u6df7\u5408\u9690\u5f0f\u5f15\u5bfc\u4fe1\u53f7<br>Hybrid Implicit Guidance"]\n    C --\x3e C2["\u4f4d\u7f6e\u504f\u79fb\u81ea\u9002\u5e94\u6a21\u5757<br>Position Shift Adaptive Module"]\n    C --\x3e C3["\u6570\u636e\u589e\u5f3a\u4e0e\u9aa8\u67b6\u5bf9\u9f50<br>Data Augmentation & Skeleton Alignment"]\n    D --\x3e D1["\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5<br>Outperforms SOTA"]\n    D --\x3e D2["\u5b9e\u73b0\u8d851\u5206\u949f\u52a8\u753b<br>Exceeds 1-minute Animation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal action recognition], [human-centric graph representation learning, attention-based post calibration, spatiotemporal graph, multimodal fusion, skeleton-guided sampling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zeyu Liang, Hailun Xia, Naichuan Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21916",children:"https://arxiv.org/pdf/2512.21916"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[RGB\u4e0e\u9aa8\u67b6\u6a21\u6001\u5f02\u6784\u878d\u5408\u56f0\u96be/RGB-Skeleton Heterogeneous Fusion Difficulty]\n    C --\x3e C1[\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u56fe\u8868\u793a\u5b66\u4e60/Human-Centric Graph Representation Learning]\n    C1 --\x3e C2[\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u540e\u6821\u51c6/Attention-Based Post Calibration]\n    C1 --\x3e C3[\u53cc\u53d8\u4f53: PAN-Ensemble\u4e0ePAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]\n    D --\x3e D1[\u4e09\u4e2a\u6570\u636e\u96c6\u4e0aSOTA\u6027\u80fd/SOTA Performance on Three Datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [unsupervised anomaly detection, disentangled representation, pseudo-healthy image reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21924",children:"https://arxiv.org/pdf/2512.21924"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6cdb\u5316\u6027\u5dee\u4e0e\u5f02\u5e38\u6b8b\u7559/Generalizability & Residuals]\n    C --\x3e C1[\u89e3\u8026\u8868\u793a\u6a21\u5757/Disentangled Representation Module]\n    C --\x3e C2[\u8fb9\u7f18\u5230\u56fe\u50cf\u6062\u590d\u6a21\u5757/Edge-to-Image Restoration Module]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a17\u79cdSOTA\u65b9\u6cd5/Outperforms 17 SOTA Methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," JD.COM"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21921",children:"https://arxiv.org/pdf/2512.21921"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JD-GenX/AutoPP",children:"https://github.com/JD-GenX/AutoPP"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AutoPP: Towards Automated Product Poster Generation and Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4eba\u5de5\u5236\u4f5c\u4e0e\u4f18\u5316\u6d77\u62a5\u8017\u65f6\u8017\u529b/Manual poster creation and optimization is laborious]\n    C --\x3e C1[\u81ea\u52a8\u5316\u751f\u6210\u4e0e\u4f18\u5316\u7ba1\u9053/Automated generation and optimization pipeline]\n    C1 --\x3e C1_1[\u751f\u6210\u5668: \u7edf\u4e00\u8bbe\u8ba1\u6a21\u5757\u4e0e\u5143\u7d20\u6e32\u67d3/Generator: Unified design & element rendering]\n    C1 --\x3e C1_2[\u4f18\u5316\u5668: \u5143\u7d20\u66ff\u6362\u4e0eIDPO/Optimizer: Element replacement & IDPO]\n    C --\x3e C2[\u6570\u636e\u96c6: AutoPP1M/Dataset: AutoPP1M]\n    D --\x3e D1[\u79bb\u7ebf\u548c\u5728\u7ebfSOTA\u7ed3\u679c/Offline and online SOTA results]\n    D --\x3e D2[\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u516c\u5f00/Code & dataset released]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiquan Gao, John See"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Heriot-Watt University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21944",children:"https://arxiv.org/pdf/2512.21944"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")\n    Results("\u5173\u952e\u7ed3\u679c/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [remote sensing, vegetation classification], [normalized difference polynomials, spectral indices, feature selection, Sentinel-2, illumination invariance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Saskatchewan, New Mexico State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21948",children:"https://arxiv.org/pdf/2512.21948"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u81ea\u52a8\u5316\u53d1\u73b0\u7528\u4e8e\u690d\u88ab\u5206\u7c7b\u7684\u7d27\u51d1\u5149\u8c31\u6307\u6570/Automated discovery of compact spectral indices for vegetation classification]\n    C --\x3e C1[\u751f\u6210\u5f52\u4e00\u5316\u5dee\u5f02\u591a\u9879\u5f0f\u5019\u9009\u7279\u5f81/Generate candidate features via normalized difference polynomials]\n    C --\x3e C2[\u4f7f\u7528\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u6311\u9009\u6307\u6570/Use feature selection methods to pick indices]\n    D --\x3e D1[\u5355\u4e2a\u4e8c\u9636\u6307\u6570\u8fbe\u523096.26%\u51c6\u786e\u7387/Single degree-2 index achieves 96.26% accuracy]\n    D --\x3e D2[\u6307\u6570\u7b80\u5355\uff0c\u53ef\u76f4\u63a5\u90e8\u7f72\u4e8eGEE/Indices are simple and deployable on GEE]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal robustness], [multi-modal large language model, input perturbation, training-free calibration, denoising, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong (CUHK)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21964",children:"https://arxiv.org/pdf/2512.21964"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: MLLMs are sensitive to input noise, undermining clinical use)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Training-free IMC framework with PDC for vision and SMS for text)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: SOTA performance on new multi-modal noise benchmark)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LVLM-Aided Alignment of Task-Specific Vision Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [model alignment and interpretability], [LVLM-VA, spurious correlations, explainable AI (XAI), vision-language model, human-in-the-loop]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21985",children:"https://arxiv.org/pdf/2512.21985"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LVLM-Aided Alignment of Task-Specific Vision Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5c0f\u89c4\u6a21\u4efb\u52a1\u4e13\u7528\u89c6\u89c9\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027/Small task-specific vision models rely on spurious correlations]\n    B --\x3e B2[\u5bfc\u81f4\u90e8\u7f72\u65f6\u884c\u4e3a\u8106\u5f31/Leads to brittle behavior when deployed]\n    C --\x3e C1[\u5229\u7528LVLM\u8fdb\u884c\u89c6\u89c9\u5bf9\u9f50/Leverage LVLM for visual alignment]\n    C --\x3e C2[\u53cc\u5411\u63a5\u53e3: \u884c\u4e3a\u8f6c\u8bed\u8a00, \u89c4\u8303\u8f6c\u8bc4\u4f30/Bidirectional interface: behavior to language, specs to critiques]\n    D --\x3e D1[\u6a21\u578b\u884c\u4e3a\u4e0e\u4eba\u7c7b\u89c4\u8303\u66f4\u597d\u5bf9\u9f50/Better alignment of model behavior with human specifications]\n    D --\x3e D2[\u51cf\u5c11\u5bf9\u865a\u5047\u7279\u5f81\u548c\u504f\u89c1\u7684\u4f9d\u8d56/Reduced dependence on spurious features and biases]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [instance segmentation], [re-parameterized convolution, efficient multi-scale attention, lightweight multi-task head]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21984",children:"https://arxiv.org/pdf/2512.21984"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/hhwmortal/PELD-Instance-segmentation",children:"https://github.com/hhwmortal/PELD-Instance-segmentation"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u72ed\u7a84\u89c6\u91ce, \u4f2a\u5f71, \u786c\u4ef6\u9650\u5236/Narrow FOV, Artifacts, Hardware Constraints]\n    C --\x3e C1[\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u6846\u67b6/Lightweight Multi-scale Attention Framework]\n    C1 --\x3e C2[\u4e3b\u5e72: C2f-Pro (\u91cd\u53c2\u6570\u5316\u5377\u79ef+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]\n    C1 --\x3e C3[\u9888\u90e8: SSFF\u4e0eTFE/Neck: SSFF and TFE]\n    C1 --\x3e C4[\u5934\u90e8: \u8f7b\u91cf\u5171\u4eab\u5934 (LMSH)/Head: Lightweight Shared Head (LMSH)]\n    D --\x3e D1[\u6027\u80fd\u4f18\u8d8a, \u53c2\u6570\u91cf\u5c11 (1.8M)/High Performance, Few Params (1.8M)]\n    D --\x3e D2[\u53d1\u5e03PELD\u6570\u636e\u96c6/Release PELD Dataset]\n    D --\x3e D3[\u826f\u597d\u6cdb\u5316\u6027/Good Generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chongqing University, Xinjiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21999",children:"https://arxiv.org/pdf/2512.21999"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/hujiayu1223/ALEAHallu",children:"https://github.com/hujiayu1223/ALEAHallu"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLM\u5e7b\u89c9\u95ee\u9898/VLM Hallucination Issue]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ALEAHallu\u6846\u67b6/ALEAHallu Framework]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u6709\u6548\u7f13\u89e3\u5e7b\u89c9/Effectively Mitigates Hallucinations]\n    C --\x3e C1[\u6fc0\u6d3b\u6570\u636e\u96c6/Activation Dataset]\n    C --\x3e C2[\u5b9a\u4f4d\u5173\u952e\u53c2\u6570/Locate Critical Parameters]\n    C --\x3e C3[\u5bf9\u6297\u6027\u7f16\u8f91/Adversarial Editing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multimodal large language models (MLLMs), slow-fast inference, adaptive perception, visual grounding, lightweight agent]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22009",children:"https://arxiv.org/pdf/2512.22009"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision "slow mode" and an efficient "fast mode" based on task demands. 3. Employs special perception tokens to dynamically guide the model\'s visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Building efficient and precise GUI agents is challenging]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Slow-fast hybrid inference with adaptive perception tokens]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches SOTA performance with compact 2.5B size]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision-and-language navigation], [spatiotemporal context modeling, slot-based compression, prompt-guided multimodal integration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22010",children:"https://arxiv.org/pdf/2512.22010"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Technology and Business University, The University of Sydney"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22016",children:"https://arxiv.org/pdf/2512.22016"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Patch-Discontinuity Mining for Generalized Deepfake Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [patch-discontinuity, feature space redistribution, classification-invariant feature augmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Chinese Academy of Military Science"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22027",children:"https://arxiv.org/pdf/2512.22027"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://gendf.github.io/",children:"https://gendf.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components\u2014deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy\u2014to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Patch-Discontinuity Mining for Generalized Deepfake Detection"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA generalization with only 0.28M parameters."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [backdoor attacks], [video segmentation foundation models, backdoor attack, two-stage training, gradient analysis, attention shift]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22046",children:"https://arxiv.org/pdf/2512.22046"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]\n    D[\u5173\u952e\u7ed3\u679c/Results: High ASR, preserves clean performance, defenses ineffective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22065",children:"https://arxiv.org/pdf/2512.22065"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://streamavatar.github.io",children:"https://streamavatar.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MAI-UI Technical Report: Real-World Centric Foundation GUI Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [GUI agent, device-cloud collaboration, online reinforcement learning, self-evolving data pipeline, foundation model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22047",children:"https://arxiv.org/pdf/2512.22047"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Tongyi-MAI/MAI-UI",children:"https://github.com/Tongyi-MAI/MAI-UI"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1(\u7f3a\u4e4f\u539f\u751f\u4eba\u673a\u4ea4\u4e92/Lack of Native Agent-User Interaction)\n    B1 --\x3e C2(\u4ec5UI\u64cd\u4f5c\u7684\u9650\u5236/Limits of UI-Only Operation)\n    B1 --\x3e C3(\u7f3a\u4e4f\u5b9e\u7528\u90e8\u7f72\u67b6\u6784/Absence of Practical Deployment Architecture)\n    B1 --\x3e C4(\u52a8\u6001\u73af\u5883\u8106\u5f31\u6027/Brittleness in Dynamic Environments)\n    B2 --\x3e D1(\u81ea\u6f14\u8fdb\u6570\u636e\u7ba1\u9053/Self-Evolving Data Pipeline)\n    B2 --\x3e D2(\u539f\u751f\u8bbe\u5907-\u4e91\u534f\u4f5c\u7cfb\u7edf/Native Device-Cloud Collaboration System)\n    B2 --\x3e D3(\u5728\u7ebfRL\u6846\u67b6/Online RL Framework)\n    D1 --\x3e E1(\u5305\u542b\u7528\u6237\u4ea4\u4e92/Includes User Interaction)\n    D1 --\x3e E2(\u5305\u542bMCP\u5de5\u5177\u8c03\u7528/Includes MCP Tool Calls)\n    D3 --\x3e E3(\u6269\u5c55\u5e76\u884c\u73af\u5883/Scales Parallel Environments)\n    D3 --\x3e E4(\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6/Scales Context Length)\n    B3 --\x3e F1(GUI Grounding SOTA/GUI Grounding SOTA)\n    B3 --\x3e F2(\u79fb\u52a8\u5bfc\u822aSOTA/Mobile Navigation SOTA)\n    B3 --\x3e F3(\u7cfb\u7edf\u6027\u80fd\u63d0\u5347/System Performance Gains)\n    F1 --\x3e G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)\n    F1 --\x3e G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)\n    F2 --\x3e G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)\n    F2 --\x3e G4(MobileWorld: 41.7%/MobileWorld: 41.7%)\n    F3 --\x3e G5(\u8bbe\u5907\u6027\u80fd\u63d0\u534733%/On-Device Perf. +33%)\n    F3 --\x3e G6(\u4e91\u8c03\u7528\u51cf\u5c1140%/Cloud Calls -40%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Yume-1.5: A Text-Controlled Interactive World Generation Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [interactive world generation, long-video generation, attention distillation, context compression, text-controlled generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22096",children:"https://arxiv.org/pdf/2512.22096"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/stdstu12/YUME",children:"https://github.com/stdstu12/YUME"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[\u5927\u6a21\u578b\u53c2\u6570\u4e0e\u6162\u63a8\u7406/Large Model & Slow Inference]\n    Problem --\x3e P2[\u7f3a\u4e4f\u6587\u672c\u63a7\u5236/Lack of Text Control]\n    Method --\x3e M1[\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6/Long-Video Gen Framework]\n    Method --\x3e M2[\u5b9e\u65f6\u6d41\u52a0\u901f\u7b56\u7565/Real-time Streaming]\n    Method --\x3e M3[\u6587\u672c\u63a7\u5236\u4e8b\u4ef6\u751f\u6210/Text-Controlled Events]\n    M1 --\x3e M1_Sub[\u7edf\u4e00\u4e0a\u4e0b\u6587\u538b\u7f29\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b/Unified Context Compression & Linear Attention]\n    M2 --\x3e M2_Sub[\u53cc\u5411\u6ce8\u610f\u529b\u84b8\u998f\u4e0e\u6587\u672c\u5d4c\u5165/Bidirectional Attention Distillation & Text Embedding]\n    Results --\x3e R1[\u751f\u6210\u4ea4\u4e92\u5f0f\u4e16\u754c/Generates Interactive Worlds]\n    Results --\x3e R2[\u652f\u6301\u952e\u76d8\u63a2\u7d22/Supports Keyboard Exploration]\n    Results --\x3e R3[\u516c\u5f00\u4ee3\u7801\u5e93/Public Codebase]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Learning Association via Track-Detection Matching for Multi-Object Tracking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-object tracking], [tracking-by-detection, link prediction, association learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Momir Ad\u017eemovi\u0107"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Belgrade"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22105",children:"https://arxiv.org/pdf/2512.22105"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Robotmurlock/TDLP",children:"https://github.com/Robotmurlock/TDLP"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual question answering], [perceptual shaping, KL-consistency, KL-separation, evidence-preserving view, evidence-ablated view]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Microsoft Research, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22120",children:"https://arxiv.org/pdf/2512.22120"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zss02/BiPS",children:"https://github.com/zss02/BiPS"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional "where-to-look" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]\n    D[\u5173\u952e\u7ed3\u679c/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] ProEdit: Inversion-based Editing From Prompts Done Right"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [inversion-based editing, KV-mix, Latents-Shift, plug-and-play, flow inversion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22118",children:"https://arxiv.org/pdf/2512.22118"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://isee-laboratory.github.io/ProEdit/",children:"https://isee-laboratory.github.io/ProEdit/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ProEdit: Inversion-based Editing From Prompts Done Right] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u6ce8\u5165\u6e90\u56fe\u50cf\u4fe1\u606f/Existing methods overly inject source info]\n    B1 --\x3e B2[\u963b\u788d\u5c5e\u6027\u7f16\u8f91\u5982\u59ff\u6001\u3001\u6570\u91cf\u3001\u989c\u8272/Hinders editing attributes like pose, number, color]\n    C --\x3e C1[\u6ce8\u610f\u529b\u5c42\u9762: KV-mix/Attention Aspect: KV-mix]\n    C1 --\x3e C11[\u6df7\u5408\u6e90\u4e0e\u76ee\u6807KV\u7279\u5f81/Mix source & target KV features]\n    C --\x3e C2[\u6f5c\u5728\u5c42\u9762: Latents-Shift/Latent Aspect: Latents-Shift]\n    C2 --\x3e C21[\u6270\u52a8\u7f16\u8f91\u533a\u57df\u7684\u6e90\u6f5c\u5728\u8868\u793a/Perturb source latent in edited region]\n    D --\x3e D1[SOTA\u6027\u80fd/SOTA performance]\n    D --\x3e D2[\u5373\u63d2\u5373\u7528\u8bbe\u8ba1/Plug-and-play design]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Knowledge Distillation, Vision Transformer, Swin Transformer, Explainable AI, Wireless Capsule Endoscopy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Daffodil International University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21372",children:"https://arxiv.org/pdf/2512.21372"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem: \u80c3\u80a0\u9053\u75be\u75c5\u56fe\u50cf\u5206\u7c7b\u6311\u6218 / GI Disease Image Classification Challenge)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method: \u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u53cc\u6d41Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results: \u9ad8\u51c6\u786e\u7387\u4e0e\u53ef\u89e3\u91ca\u6027 / High Accuracy & Explainability)\n    B --\x3e B1[\u6570\u636e\u91cf\u5927, \u7c7b\u95f4\u5dee\u5f02\u5c0f / Large Data Volume, Subtle Inter-class Variation]\n    C --\x3e C1[\u6559\u5e08\u6a21\u578b: Swin + ViT / Teacher: Swin + ViT]\n    C --\x3e C2[\u5b66\u751f\u6a21\u578b: Tiny-ViT / Student: Tiny-ViT]\n    C --\x3e C3[\u53ef\u89e3\u91ca\u6027\u5206\u6790: Grad-CAM\u7b49 / XAI: Grad-CAM etc.]\n    D --\x3e D1[\u51c6\u786e\u7387 > 0.99 / Accuracy > 0.99]\n    D --\x3e D2[AUC = 1.0000]\n    D --\x3e D3[\u9002\u7528\u4e8e\u4e34\u5e8a\u73af\u5883 / Suitable for Clinical Settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Takuro Kutsuna"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Toyota Central R&D Labs., Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21593",children:"https://arxiv.org/pdf/2512.21593"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Residual Prior Diffusion (RPD) / \u6b8b\u5dee\u5148\u9a8c\u6269\u6563\u6a21\u578b"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u5355\u4e00\u6269\u6563\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7ec6\u8282 / Single diffusion model struggles with global structure and local details"]\n    Method --\x3e M1["\u4e24\u9636\u6bb5\u6846\u67b6: \u7c97\u7c92\u5ea6\u5148\u9a8c + \u6b8b\u5dee\u6269\u6563\u6a21\u578b / Two-stage framework: coarse prior + residual diffusion model"]\n    Method --\x3e M2["\u6982\u7387\u6a21\u578b\u4e0e\u53ef\u5904\u7406ELBO / Probabilistic model with tractable ELBO"]\n    Results --\x3e R1["\u5728\u5408\u6210\u6570\u636e\u4e0a\u51c6\u786e\u6355\u6349\u7ec6\u8282 / Accurately captures details on synthetic data"]\n    Results --\x3e R2["\u81ea\u7136\u56fe\u50cf\u751f\u6210\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf / Natural image generation matches or exceeds baselines"]\n    Results --\x3e R3["\u5c11\u6b65\u63a8\u7406\u4fdd\u6301\u6027\u80fd / Maintains performance with few inference steps"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image deblurring], [lightweight network, real-time inference, edge deployment, U-shaped architecture, motion blur]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21975",children:"https://arxiv.org/pdf/2512.21975"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/ReaganWu/RT-Focuser",children:"https://github.com/ReaganWu/RT-Focuser"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8fd0\u52a8\u6a21\u7cca\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf / Motion blur degrades image quality]\n    B --\x3e B2[\u5b9e\u65f6\u5e94\u7528\u6311\u6218 / Challenges for real-time applications]\n    C --\x3e C1[\u8f7b\u91cf\u7ea7U\u578b\u7f51\u7edc / Lightweight U-shaped network]\n    C --\x3e C2[\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6 / Three key components: LD, MLIA, X-Fuse]\n    D --\x3e D1[30.67 dB PSNR / 30.67 dB PSNR]\n    D --\x3e D2[5.85M\u53c2\u6570, 15.76 GMACs / 5.85M params, 15.76 GMACs]\n    D --\x3e D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging], [colorimetric calibration, clinical biomarkers, Individual Typology Angle (ITA), Melanin Index, intraclass correlation coefficient (ICC)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sungwoo Kang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21988",children:"https://arxiv.org/pdf/2512.21988"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and defines the "color-clinical decoupling" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker\'s disproportionate sensitivity to noise in the b* color channel.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed "color-clinical decoupling," primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Color-Clinical Decoupling<br>\u989c\u8272-\u4e34\u5e8a\u89e3\u8026] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>Does color calibration ensure clinical reliability?<br>\u8272\u5f69\u6821\u51c6\u80fd\u5426\u786e\u4fdd\u4e34\u5e8a\u53ef\u9760\u6027\uff1f]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Analyze 43,425 images across devices with CCM<br>\u4f7f\u7528CCM\u5206\u679043,425\u5f20\u8de8\u8bbe\u5907\u56fe\u50cf]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Color-clinical decoupling: \u2206E\u2193 but ICC(ITA) poor<br>\u989c\u8272-\u4e34\u5e8a\u89e3\u8026\uff1a\u2206E\u4e0b\u964d\u4f46ITA\u7684ICC\u5dee]\n    B --\x3e D\n    C --\x3e D"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [Structured Pruning, Multi-Objective Optimization, Edge Inference, MVCNN, View-Adaptive Compression]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22136",children:"https://arxiv.org/pdf/2512.22136"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a framework for lightweight DNN deployment that integrates structured pruning with multi-objective optimization to meet heterogeneous hardware constraints. 2. Demonstrates the framework on MVCNN by quantifying the contribution of individual views to accuracy for view-adaptive pruning budget allocation. 3. Shows experimentally that the compressed models meet user-specified accuracy and memory bounds while achieving 1.2x to 5.0x inference speedup across diverse hardware."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of deploying large DNNs on resource-constrained edge devices. It proposes SlimEdge, a method that combines structured pruning and multi-objective optimization to compress models like MVCNN while preserving task performance. The results show that this approach successfully meets specified accuracy and memory constraints while significantly reducing inference latency on various edge hardware platforms."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SlimEdge: Lightweight Distributed DNN Deployment] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: DNN\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a/DNN deployment on resource-constrained edge devices)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u7ed3\u6784\u5316\u526a\u679d\u4e0e\u591a\u76ee\u6807\u4f18\u5316/Structured Pruning & Multi-Objective Optimization)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u6ee1\u8db3\u7cbe\u5ea6\u4e0e\u5185\u5b58\u7ea6\u675f\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1.2x-5.0x/Meets accuracy & memory bounds, 1.2x-5.0x latency reduction)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22170",children:"https://arxiv.org/pdf/2512.22170"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6570\u636e\u6807\u6ce8\u566a\u58f0/Annotation Noise]\n    B --\x3e B2[\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb/Reward Hacking]\n    B --\x3e B3[\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u4e0d\u8db3/Under-explored RM Architecture]\n    C --\x3e C1[\u5355\u9879\u76ee\u4e8c\u5143\u6807\u6ce8\u4e0e\u8de8\u63d0\u793a\u914d\u5bf9/Single-item Binary Annotations & Cross-prompt Pairing]\n    C --\x3e C2[\u5206\u5c42\u6e10\u8fdb\u67e5\u8be2\u6ce8\u610f\u529b/Hierarchical Progressive Query Attention]\n    C --\x3e C3[\u6539\u8fdb\u7684BT\u635f\u5931\u51fd\u6570/Modified BT Loss for Win-Tie]\n    D --\x3e D1[\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u63d0\u5347/Improved Direct RM Evaluation Metrics]\n    D --\x3e D2[\u89c6\u9891\u751f\u6210\u540e\u8bad\u7ec3\u6548\u679c\u589e\u5f3a/Enhanced Efficacy of Post-training on Video Generation Models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [sign language recognition], [3D CNN, LSTM, real-time processing, spatial-temporal features, edge deployment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dawnena Key"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Denver"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22177",children:"https://arxiv.org/pdf/2512.22177"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A hybrid 3D CNN-LSTM architecture for capturing spatial and temporal features in ASL signs. 2. A training methodology using multiple complementary datasets (WLASL, ASL-LEX, expert-annotated signs). 3. A deployment architecture supporting both cloud (AWS) and edge (OAK-D camera) inference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a real-time American Sign Language recognition system that uses a hybrid 3D CNN and LSTM architecture to process video streams. The system, trained on multiple datasets, achieves high F1-scores and is deployed for practical use on both cloud and edge devices."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Real-Time ASL Recognition Using 3D CNN and LSTM] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6c9f\u901a\u969c\u788d/Communication Barrier]\n    B --\x3e B2[\u5b9e\u65f6\u7ffb\u8bd1\u6311\u6218/Real-time Translation Challenge]\n    C --\x3e C1[\u6df7\u5408\u67b6\u6784/Hybrid Architecture]\n    C1 --\x3e C1_1[3D CNN: \u65f6\u7a7a\u7279\u5f81/Spatial-temporal Features]\n    C1 --\x3e C1_2[LSTM: \u5e8f\u5217\u4f9d\u8d56/Sequential Dependencies]\n    C --\x3e C2[\u591a\u6570\u636e\u96c6\u8bad\u7ec3/Multi-dataset Training]\n    C --\x3e C3[\u4e91\u8fb9\u90e8\u7f72/Cloud-Edge Deployment]\n    D --\x3e D1[\u9ad8F1\u5206\u6570/High F1-scores (0.71-0.99)]\n    D --\x3e D2[\u5b9e\u65f6\u63a8\u7406/Real-time Inference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Characterizing Motion Encoding in Video Diffusion Timesteps"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [video diffusion models, timestep analysis, motion-appearance disentanglement, motion transfer, one-shot customization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Maryland"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22175",children:"https://arxiv.org/pdf/2512.22175"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Characterizing Motion Encoding in Video Diffusion Timesteps] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u8fd0\u52a8\u7f16\u7801\u673a\u5236\u4e0d\u660e\u786e / Motion encoding in video diffusion is poorly understood]\n    C --\x3e C1[\u901a\u8fc7\u6761\u4ef6\u6ce8\u5165\u91cf\u5316\u8fd0\u52a8-\u5916\u89c2\u6743\u8861 / Quantify motion-appearance trade-off via conditional injection]\n    C --\x3e C2[\u5927\u89c4\u6a21\u5b9a\u91cf\u7814\u7a76 / Large-scale quantitative study]\n    D --\x3e D1[\u8bc6\u522b\u65e9\u671f\u8fd0\u52a8\u4e3b\u5bfc\u4e0e\u540e\u671f\u5916\u89c2\u4e3b\u5bfc\u9636\u6bb5 / Identify early motion-dominant and late appearance-dominant regimes]\n    D --\x3e D2[\u7b80\u5316\u5355\u6837\u672c\u8fd0\u52a8\u5b9a\u5236\u8303\u5f0f / Simplify one-shot motion customization paradigm]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Science and Technology (NUST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22182",children:"https://arxiv.org/pdf/2512.22182"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Enhancing Medical Data Analysis through AI-Enhanced LLE] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Handling complex high-dimensional medical data for billing and transcription)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Integrating AI with Locally Linear Embedding (LLE))\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved data processing accuracy and operational efficiency)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22183",children:"https://arxiv.org/pdf/2512.22183"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unbiased Visual Reasoning with Controlled Visual Inputs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[VLMs exploit spurious correlations/VLMs\u5229\u7528\u865a\u5047\u5173\u8054]\n    C --\x3e C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: \u89e3\u8026\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u6a21\u5757\u5316\u6846\u67b6]\n    C1 --\x3e C2[Frozen VLM sensor + LLM reasoner/\u51bb\u7ed3VLM\u611f\u77e5\u5668 + LLM\u63a8\u7406\u5668]\n    C2 --\x3e C3[Train with RL (GRPO)/\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60(GRPO)\u8bad\u7ec3]\n    D --\x3e D1[Improved robustness on SpuriVerse/\u5728SpuriVerse\u4e0a\u9c81\u68d2\u6027\u63d0\u5347]\n    D --\x3e D2[Competitive on MMVP & SeedBench/\u5728MMVP & SeedBench\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b]\n    D --\x3e D3[Transferable & interpretable/\u53ef\u8fc1\u79fb\u4e14\u53ef\u89e3\u91ca]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [computational pathology], [Multiple Instance Learning, Hook Tokens, Linear Complexity, Multimodal Initialization, Hook Diversity Loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Xiamen University, Westlake University, Wuhan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22188",children:"https://arxiv.org/pdf/2512.22188"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/lingxitong/HookMIL",children:"https://github.com/lingxitong/HookMIL"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: MIL loses context; Transformers are inefficient] --\x3e P1[\u4f20\u7edfMIL\u4e22\u5931\u4e0a\u4e0b\u6587/Traditional MIL loses context]\n    Problem --\x3e P2[\u57fa\u4e8eTransformer\u7684MIL\u8ba1\u7b97\u590d\u6742/Transformer-based MIL has quadratic complexity]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: HookMIL Framework] --\x3e M1[\u4f7f\u7528\u53ef\u5b66\u4e60\u7684Hook Tokens/Use learnable Hook Tokens]\n    Method --\x3e M2[\u591a\u6a21\u6001\u521d\u59cb\u5316/Multimodal Initialization]\n    Method --\x3e M3[Hook\u591a\u6837\u6027\u635f\u5931\u4e0e\u901a\u4fe1\u673a\u5236/Hook Diversity Loss & Communication]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[SOTA\u6027\u80fd/State-of-the-art Performance]\n    Results --\x3e R2[\u8ba1\u7b97\u9ad8\u6548/Computationally Efficient]\n    Results --\x3e R3[\u53ef\u89e3\u91ca\u6027/Interpretability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [dual-encoder, data augmentation, transfer learning, intracranial aneurysm detection, Grad-CAM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Antara Titikhsha, Divyanshu Tak"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Harvard Medical School, Mass General Brigham, Dana-Farber Cancer Institute, Brigham and Women\u2019s Hospital"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22185",children:"https://arxiv.org/pdf/2512.22185"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/antitikhsha/SAMM2D",children:"https://github.com/antitikhsha/SAMM2D"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced SAMM2D, a scale-aware multi-modal 2D dual-encoder framework for high-sensitivity intracranial aneurysm screening. 2. Demonstrated through ablation that data augmentation degrades performance when using a strong pretrained backbone, challenging a common assumption in low-data medical imaging. 3. Showed the model achieves 95% sensitivity (surpassing average radiologist performance) and provides interpretable visualizations (Grad-CAM) with clinically relevant focus."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SAMM2D, a dual-encoder framework for detecting intracranial aneurysms in medical images. The key finding is that, contrary to common practice, data augmentation harms performance when using a strong ImageNet-pretrained backbone, suggesting robust pretraining is more beneficial than complex augmentation in low-data medical settings. The model achieves high sensitivity and demonstrates cost-saving potential in clinical screening."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9885\u5185\u52a8\u8109\u7624\u68c0\u6d4b\u6311\u6218/Intracranial Aneurysm Detection Challenges]\n    B1 --\x3e B2[\u5f62\u6001\u7ec6\u5fae/Subtle Morphology]\n    B1 --\x3e B3[\u7c7b\u522b\u4e0d\u5e73\u8861/Class Imbalance]\n    B1 --\x3e B4[\u6807\u6ce8\u6570\u636e\u7a00\u7f3a/Scarce Annotated Data]\n    C --\x3e C1[\u53cc\u7f16\u7801\u5668\u6846\u67b6/Dual-Encoder Framework]\n    C --\x3e C2[\u4f7f\u7528\u5f3a\u9884\u8bad\u7ec3\u9aa8\u5e72/Strong Pretrained Backbone]\n    C --\x3e C3[\u65e0\u6570\u636e\u589e\u5f3a/No Data Augmentation]\n    D --\x3e D1[AUC 0.686 (\u63d0\u534732%)/AUC 0.686 (32% Improvement)]\n    D --\x3e D2[95% \u7075\u654f\u5ea6 (\u8d85\u8d8a\u653e\u5c04\u79d1\u533b\u751f)/95% Sensitivity (Surpasses Radiologists)]\n    D --\x3e D3[\u6570\u636e\u589e\u5f3a\u964d\u4f4e\u6027\u80fd/Augmentation Degrades Performance]\n    D --\x3e D4[\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316/Interpretable Visualizations (Grad-CAM)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Tiny-YOLOSAM: Fast Hybrid Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image segmentation], [Segment Anything Model (SAM), YOLO, hybrid prompting, inference acceleration, object detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kenneth Xu, Songhan Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22193",children:"https://arxiv.org/pdf/2512.22193"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes Tiny-YOLOSAM, a fast hybrid segmentation pipeline that combines YOLO object detection with TinySAM for efficient prompting. 2. Introduces a targeted sparse prompting strategy that samples points only in regions not covered by detector-guided masks to improve coverage. 3. Demonstrates significant improvements in both segmentation coverage (AR, mIoU) and inference speed (4.7x faster) on COCO val2017 compared to the baseline TinySAM "segment-everything" mode.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the high computational cost of the Segment Anything Model (SAM) for full-scene segmentation. It proposes Tiny-YOLOSAM, a hybrid method that uses a YOLO detector to generate box prompts for salient objects and supplements uncovered areas with sparse point prompts. The approach achieves a 4.7x speedup and significantly better coverage compared to the baseline, offering a practical alternative to dense "segment-everything" prompting.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Tiny-YOLOSAM: Fast Hybrid Image Segmentation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: SAM/TinySAM "segment-everything" mode is computationally expensive and slow for full-scene segmentation.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid pipeline using YOLO for box prompts on foreground objects and sparse point prompts for uncovered regions.]\n    D[\u5173\u952e\u7ed3\u679c/Results: 4.7x speedup (10.39s vs 49.20s/image) and improved coverage (mIoU: 67.8% vs 19.2%) on COCO val2017.]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [Vision-Language Model (VLM), Few-Shot Learning, Grad-CAM, Multimodal Explainability, Diabetic Retinopathy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shivum Telang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Pittsburgh Rangos Research Center, North Allegheny Senior High School"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22197",children:"https://arxiv.org/pdf/2512.22197"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel multimodal explainability model that combines fundus and OCT images for Diabetic Retinopathy severity classification, mimicking an ophthalmologist's reasoning. 2. Introduces a Vision-Language Model (VLM) with few-shot learning adaptation to analyze lesion distributions within retinal quadrants and generate natural language explanations. 3. Develops a method to generate paired Grad-CAM heatmaps across both imaging modalities to visually highlight regions contributing to the classification decision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of explainability in AI models for Diabetic Retinopathy (DR) diagnosis by proposing a multimodal Vision-Language Model (VLM). The model uses few-shot learning to analyze lesion distributions in retinal quadrants from both fundus and OCT images and generates paired Grad-CAM heatmaps and natural language explanations for its severity classifications. This approach aims to provide a more interpretable and clinically practical tool for DR diagnostics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Quadrant Segmentation VLM for Diabetic Retinopathy] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027 / AI Models Lack Explainability]\n    B --\x3e B2[\u4f9d\u8d56\u5355\u4e00\u6210\u50cf\u6a21\u6001 / Reliance on Single Imaging Modality]\n    C --\x3e C1[\u591a\u6a21\u6001VLM\u4e0e\u5c11\u6837\u672c\u5b66\u4e60 / Multimodal VLM with Few-Shot Learning]\n    C --\x3e C2[\u8c61\u9650\u5206\u6790\u4e0eGrad-CAM\u70ed\u56fe / Quadrant Analysis & Grad-CAM Heatmaps]\n    D --\x3e D1[\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca / Provides Natural Language Explanations]\n    D --\x3e D2[\u63d0\u5347\u4e34\u5e8a\u5b9e\u7528\u6027 / Improves Clinical Practicality]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Convolutional Neural Network, SHAP, LIME, Saliency Maps, Malaria Diagnosis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Ismiel Hossen Abir, Awolad Hossain"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Department of Computer Science & Engineering, International Standard University, Dhaka, Bangladesh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22205",children:"https://arxiv.org/pdf/2512.22205"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a custom CNN model for automated malaria diagnosis from blood cell images, achieving high accuracy (96%). 2. Compares the performance of the custom CNN with established deep learning architectures like ResNet50 and VGG16. 3. Enhances model interpretability for clinical trust by applying Explainable AI techniques, including SHAP, LIME, and Saliency Maps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a deep learning-based system using a custom Convolutional Neural Network (CNN) to automatically diagnose malaria from blood cell images, achieving high accuracy. It compares this model against several established architectures and applies Explainable AI (XAI) techniques like SHAP and LIME to make the model's decisions interpretable. The study concludes that this approach can provide a quick, accurate, and understandable diagnostic tool, particularly valuable in resource-limited settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional microscopic diagnosis is slow, subjective, and resource-intensive."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: A custom CNN model for classification, compared with other architectures, enhanced with SHAP, LIME, and Saliency Maps for explainability."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves 96% accuracy, high precision/recall, providing a fast and interpretable diagnostic tool."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [crowd counting], [weakly-supervised learning, vision transformer, density-guided aggregation, parameter efficiency, lightweight model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22203",children:"https://arxiv.org/pdf/2512.22203"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[TCFormer: 5M\u53c2\u6570Transformer\u7528\u4e8e\u5f31\u76d1\u7763\u4eba\u7fa4\u8ba1\u6570] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u6807\u6ce8\u6210\u672c\u9ad8/High Annotation Cost]\n    Problem --\x3e P2[\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8/High Computational Complexity]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u9ad8\u6548\u89c6\u89c9Transformer\u7279\u5f81\u63d0\u53d6\u5668/Efficient ViT Feature Extractor]\n    Method --\x3e M2[\u53ef\u5b66\u4e60\u5bc6\u5ea6\u52a0\u6743\u5e73\u5747\u6a21\u5757/Learnable Density-Weighted Averaging]\n    Method --\x3e M3[\u5bc6\u5ea6\u7b49\u7ea7\u5206\u7c7b\u635f\u5931/Density-Level Classification Loss]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4ec55M\u53c2\u6570/Only 5M Parameters]\n    Results --\x3e R2[\u5f31\u76d1\u7763\u4e0b\u7ade\u4e89\u6027\u6027\u80fd/Competitive Performance under Weak Supervision]\n    Results --\x3e R3[\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907/Suitable for Edge Devices]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22208",children:"https://arxiv.org/pdf/2512.22208"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/moxin-org/Moxin-LLM",children:"https://github.com/moxin-org/Moxin-LLM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Open-Source Multimodal Moxin Models<br/>\u5f00\u6e90\u591a\u6a21\u6001Moxin\u6a21\u578b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Proprietary vs. Open-Source LLMs<br/>\u95ed\u6e90\u4e0e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b]\n    B --\x3e B2[Need for transparent, capable open models<br/>\u9700\u8981\u900f\u660e\u3001\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b]\n    C --\x3e C1[Develop Moxin 7B under Model Openness Framework<br/>\u57fa\u4e8e\u6a21\u578b\u5f00\u653e\u6846\u67b6\u5f00\u53d1Moxin 7B]\n    C --\x3e C2[Create variants: VLM, VLA, Chinese<br/>\u521b\u5efa\u53d8\u4f53: VLM, VLA, \u4e2d\u6587\u6a21\u578b]\n    D --\x3e D1[Superior performance in evaluations<br/>\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02]\n    D --\x3e D2[Full release of models, code, data<br/>\u5b8c\u6574\u53d1\u5e03\u6a21\u578b\u3001\u4ee3\u7801\u3001\u6570\u636e]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [pedestrian attribute recognition], [vision-language model, cross-attention fusion, class imbalance, domain generalization, SigLIP]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22217",children:"https://arxiv.org/pdf/2512.22217"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>\u7c7b\u522b\u4e0d\u5e73\u8861, \u5c5e\u6027\u4f9d\u8d56, \u57df\u504f\u79fb]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\u4e0e\u8de8\u6ce8\u610f\u529b\u878d\u5408]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K\u4e0aSOTA, PETA & Market-1501\u4e0a\u63d0\u5347]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [skeleton-based action recognition], [Spiking Neural Networks, Graph Convolutional Networks, Time-Frequency Learning, Topology-Aware Learning, Energy Efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22214",children:"https://arxiv.org/pdf/2512.22214"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Signal-SGN++<br/>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[GCNs\u80fd\u8017\u9ad8<br/>GCNs High Energy Cost]\n    B --\x3e B2[SNNs\u96be\u4ee5\u6355\u6349\u65f6\u7a7a-\u9891\u7387\u4e0e\u62d3\u6251\u4f9d\u8d56<br/>SNNs Limited in Capturing Time-Freq & Topology]\n    C --\x3e C1[\u4e3b\u5e72\u7f51\u7edc: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]\n    C --\x3e C2[\u62d3\u6251\u8f6c\u79fb\u81ea\u6ce8\u610f\u529b TSSA<br/>Topology-Shift Self-Attention TSSA]\n    C --\x3e C3[\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u878d\u5408 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]\n    D --\x3e D1[\u4f18\u4e8e\u73b0\u6709SNN\u65b9\u6cd5<br/>Outperforms Existing SNN Methods]\n    D --\x3e D2[\u4e0e\u5148\u8fdbGCNs\u7ed3\u679c\u76f8\u5f53<br/>Competitive with SOTA GCNs]\n    D --\x3e D3[\u80fd\u8017\u663e\u8457\u964d\u4f4e<br/>Substantially Reduced Energy Consumption]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] On Extending Semantic Abstraction for Efficient Search of Hidden Objects"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [semantic abstraction, relevancy maps, 3D localization, hidden objects, unstructured search]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tasha Pais, Nikhilesh Belulkar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22220",children:"https://arxiv.org/pdf/2512.22220"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("On Extending Semantic Abstraction for Efficient Search of Hidden Objects") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Localizing hidden/occluded objects")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Use VLM relevancy maps & historical data for efficient 3D search")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Faster and accurate 3D localization vs. random search")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual question answering], [signboard VQA, OCR-integrated VQA, multimodal dataset, Vietnamese, multi-agent framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Information Technology, Vietnam National University, Ho Chi Minh City"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22218",children:"https://arxiv.org/pdf/2512.22218"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: \u7406\u89e3\u81ea\u7136\u573a\u666f\u4e2d\u7684\u62db\u724c\u6587\u672c\u5bf9\u4e8eVQA\u7684\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: 1. \u5f15\u5165\u9996\u4e2a\u5927\u89c4\u6a21\u8d8a\u5357\u8bed\u62db\u724cVQA\u6570\u636e\u96c6ViSignVQA\u3002 2. \u901a\u8fc7\u96c6\u6210\u8d8a\u5357\u8bedOCR\u548c\u8bed\u8a00\u6a21\u578b\u6765\u9002\u914dSOTA VQA\u6a21\u578b\u3002 3. \u63d0\u51fa\u7ed3\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u667a\u80fd\u4f53\u53caGPT-4\u7684\u591a\u667a\u80fd\u4f53VQA\u6846\u67b6\u3002"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: 1. \u6dfb\u52a0OCR\u6587\u672c\u4f7fF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe209%\u3002 2. \u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u591a\u6570\u6295\u7968\u8fbe\u523075.98%\u51c6\u786e\u7387\u3002 3. \u521b\u5efa\u4e86\u9996\u4e2a\u6355\u83b7\u771f\u5b9e\u4e16\u754c\u573a\u666f\u6587\u672c\u7279\u5f81\u7684\u5927\u89c4\u6a21\u8d8a\u5357\u8bed\u591a\u6a21\u6001\u57fa\u51c6\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Ant Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22226",children:"https://arxiv.org/pdf/2512.22226"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zheng980629/VideoScaffold",children:"https://github.com/zheng980629/VideoScaffold"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["[arXiv251230] KAN-FPN-Stem",":A"," KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation"]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [pose estimation], [KAN, Feature Pyramid Network, Vision Transformer, multi-scale fusion, convolutional layer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," HaoNan Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," WuHan University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22228",children:"https://arxiv.org/pdf/2512.22228"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified that the performance bottleneck in ViT front-ends for pose estimation lies in the post-fusion smoothing step, not in feature refinement via attention modules. 2. Proposed a novel KAN-enhanced FPN-Stem architecture that replaces the standard linear 3x3 convolution in the FPN with a KAN-based convolutional layer for superior non-linear smoothing. 3. Demonstrated a significant performance improvement of up to +2.0 AP on COCO dataset over the ViTPose-S baseline, providing a plug-and-play module and a new direction for improving feature fusion quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the performance bottleneck in Vision Transformer (ViT) based pose estimation models, which stems from simplistic front-end designs that cause information loss and poor multi-scale handling. The authors propose KAN-FPN-Stem, an architecture that enhances the Feature Pyramid Network by replacing its final linear smoothing convolution with a KAN-based layer to better rectify fusion artifacts. Experiments show a significant performance boost, revealing that improving feature fusion, not just refinement, is key to advancing ViT-based dense prediction tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["KAN-FPN-Stem: A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: ViT front-end (e.g., ViTPose) has simplistic design causing multi-scale handling issues & information loss"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Replace FPN\'s terminal linear 3x3 conv with a KAN-based convolutional layer for adaptive non-linear smoothing"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves +2.0 AP boost on COCO over ViTPose-S; reveals fusion quality is the key bottleneck"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image reconstruction], [diffusion model, cross-domain, meta-information, sinogram adapter, low-dose PET]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanchang University, Hefei Comprehensive National Science Center"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22237",children:"https://arxiv.org/pdf/2512.22237"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Radboud University Medical Center, Radboud University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22242",children:"https://arxiv.org/pdf/2512.22242"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>\u80ba\u764c\u7b5b\u67e5\u98ce\u9669\u4f30\u8ba1\u6a21\u578b\u7684\u516c\u5e73\u6027\u8bc4\u4f30] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br/>AI\u80ba\u764c\u98ce\u9669\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u4e9a\u7ec4\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u662f\u5426\u516c\u5e73\uff1f<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u4f7f\u7528JustEFAB\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u5728NLST\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br/>\u53d1\u73b0Sybil\u548cVenkadesh21\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u3001\u65e0\u6cd5\u7528\u6df7\u6742\u56e0\u7d20\u89e3\u91ca\u7684\u6027\u80fd\u5dee\u5f02<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22238",children:"https://arxiv.org/pdf/2512.22238"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5927\u578bVLM\u96be\u4ee5\u90e8\u7f72\u5230\u79fb\u52a8/\u8fb9\u7f18\u8bbe\u5907/Large VLMs are impractical for mobile/edge deployment]\n    B --\x3e B2[\u5e08\u751f\u6a21\u578b\u5c3a\u5bf8\u5dee\u8ddd\u5bfc\u81f4\u77e5\u8bc6\u84b8\u998f\u4e0d\u7a33\u5b9a/Large size gap causes unstable distillation]\n    C --\x3e C1[\u63a9\u7801\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u6846\u67b6/Mask-progressive RL distillation framework]\n    C --\x3e C2[\u5148\u63a9\u7801\u6559\u5e08\u975e\u4e3b\u5bfc\u6743\u91cd\uff0c\u518d\u6e10\u8fdb\u6062\u590d/First mask non-dominant teacher weights, then progressively restore]\n    C --\x3e C3[\u79bb\u7ebfRL\u9636\u6bb5\u4f7f\u7528\u51c6\u786e\u6027\u548c\u84b8\u998f\u5956\u52b1/Offline RL stage with accuracy and distillation rewards]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u7d27\u51d1\u578bVLM/Outperforms existing compact VLMs on diverse benchmarks]\n    D --\x3e D2[\u6e10\u8fdb\u589e\u52a0\u6559\u5e08\u5c3a\u5bf8\u5e26\u6765\u66f4\u5e73\u6ed1\u6536\u655b\u548c\u66f4\u5f3a\u6cdb\u5316/Gradually increasing teacher size yields smoother convergence & stronger generalization]\n    D --\x3e D3[\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u90e8\u7f72VLM\u7684\u53ef\u6269\u5c55\u8def\u5f84/Provides a scalable path toward efficient, deployable VLMs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [model compression], [knowledge distillation, lightweight CNN, inverted residual blocks, dense connectivity, multi-objective learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," FPT University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22239",children:"https://arxiv.org/pdf/2512.22239"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>\u9762\u5411\u667a\u6167\u519c\u4e1a\u7684\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u76ee\u6807\u6df7\u5408\u77e5\u8bc6\u84b8\u998f"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Deploying deep models on edge devices in smart agriculture<br>\u5728\u667a\u6167\u519c\u4e1a\u4e2d\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u6df1\u5ea6\u6a21\u578b"] --\x3e P1["\u6311\u6218/Challenge<br>Trade-off between efficiency and accuracy<br>\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Hybrid knowledge distillation framework<br>\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u6846\u67b6"] --\x3e M1["\u5b66\u751f\u6a21\u578b/Student Model<br>Customized CNN with inverted residual & dense blocks<br>\u5b9a\u5236\u5316CNN\uff0c\u542b\u5012\u6b8b\u5dee\u4e0e\u5bc6\u96c6\u8fde\u63a5\u5757"]\n    Method --\x3e M2["\u6559\u5e08\u6a21\u578b/Teacher Model<br>ResNet18 guidance<br>ResNet18\u6559\u5e08\u7f51\u7edc\u6307\u5bfc"]\n    Method --\x3e M3["\u591a\u76ee\u6807\u7b56\u7565/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>\u6574\u5408\u786c\u6807\u7b7e\u3001\u7279\u5f81\u7ea7\u3001\u54cd\u5e94\u7ea7\u4e0e\u81ea\u84b8\u998f"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Experiments on agricultural datasets<br>\u5728\u519c\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c"] --\x3e R1["\u6027\u80fd/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>\u6c34\u7a3b\u79cd\u5b50\u5206\u7c7b\u51c6\u786e\u738798.56%\uff08\u6559\u5e08\u6a21\u578b98.65%\uff09"]\n    Results --\x3e R2["\u6548\u7387/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs\uff0c\u7ea6107\u4e07\u53c2\u6570\uff08\u6bd4\u6559\u5e08\u6a21\u578b\u5c0f10\u500d\uff09"]\n    Results --\x3e R3["\u6cdb\u5316/Generalization<br>Consistent gains on plant leaf disease datasets<br>\u5728\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u6027\u80fd\u63d0\u5347"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zheng Xing, Weibing Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shenzhen University, Shenzhen MSU-BIT University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22249",children:"https://arxiv.org/pdf/2512.22249"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem: \u4f20\u7edf\u65e0\u76d1\u7763\u4eba\u4f53\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\u5ffd\u7565\u4e86\u65f6\u5e8f\u8bed\u4e49\u7684\u4f5c\u7528 / Traditional unsupervised HMS overlooks temporal semantics)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method: \u5229\u7528LLM\u63d0\u53d6\u65f6\u5e8f\u89c6\u89c9\u8bed\u4e49\uff0c\u5e76\u878d\u5165\u5b50\u7a7a\u95f4\u805a\u7c7b\u6846\u67b6 / Use LLM to extract TVS and integrate it into subspace clustering)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results: \u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5 / Outperforms SOTA on four benchmark datasets)\n    C --\x3e C1(LLM\u67e5\u8be2 / LLM Query: \u5224\u65ad\u76f8\u90bb\u5e27\u662f\u5426\u63cf\u8ff0\u76f8\u540c\u8fd0\u52a8 / Determine if consecutive frames depict the same motion)\n    C --\x3e C2(\u65f6\u5e8f\u6b63\u5219\u5316 / Temporal Regularizer: \u8bf1\u5bfc\u76f8\u90bb\u5e27\u5171\u4eab\u76f8\u4f3c\u5b50\u7a7a\u95f4\u5d4c\u5165 / Induces similar subspace embeddings for temporal neighbors)\n    C --\x3e C3(\u53cd\u9988\u4f18\u5316 / Feedback Optimization: \u57fa\u4e8e\u5206\u5272\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u5d4c\u5165 / Iteratively optimizes embedding based on segmentation output)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [generative models], [text-to-image diffusion, geometric control, human perception embedding, latent guidance, teacher model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Antara Titikhsha, Om Kulkarni, Dharun Muthaiah"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22272",children:"https://arxiv.org/pdf/2512.22272"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception",children:"https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposing a Human Perception Embedding (HPE) teacher trained on human similarity data to capture geometry-sensitive features. 2. Introducing a model-agnostic latent guidance framework for steering various generative architectures. 3. Demonstrating improved geometric control and semantic alignment in text-to-image synthesis without specialized training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the problem where text-to-image models prioritize style over geometric constraints. The proposed method uses a lightweight "teacher" model, trained on human perceptual data, to guide the diffusion process towards desired shapes. The results show that this approach significantly improves geometric control across different model architectures without requiring retraining.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Human-Aligned Generative Perception<br>\u4eba\u7c7b\u5bf9\u9f50\u7684\u751f\u6210\u5f0f\u611f\u77e5] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Generative models ignore geometry for style<br>\u751f\u6210\u6a21\u578b\u4e3a\u98ce\u683c\u5ffd\u7565\u51e0\u4f55]\n    C --\x3e C1[Use HPE teacher for guidance<br>\u4f7f\u7528HPE\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u5f15\u5bfc]\n    D --\x3e D1[~80% better alignment<br>\u5bf9\u9f50\u5ea6\u63d0\u5347\u7ea680%]\n    D --\x3e D2[Zero-shot shape transfer<br>\u96f6\u6837\u672c\u5f62\u72b6\u8fc1\u79fb]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] GeCo: A Differentiable Geometric Consistency Metric for Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation evaluation], [geometric consistency, video generation, differentiable metric, depth prior, motion prior]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Leslie Gu, Junhwa Hur, Charles Herrmann, Fangneng Zhan, Todd Zickler, Deqing Sun, Hanspeter Pfister"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harvard University, Google DeepMind, Massachusetts Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22274",children:"https://arxiv.org/pdf/2512.22274"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GeCo, a novel differentiable metric that fuses motion and depth priors to detect geometric deformation and occlusion inconsistency in generated videos. 2. Introduces two synthetic datasets, WarpBench and OccluBench, to validate the metric's performance on isolated artifacts. 3. Demonstrates GeCo's dual utility for benchmarking state-of-the-art video generation models and as a training-free guidance loss to reduce geometric artifacts during generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces GeCo, a differentiable metric for evaluating geometric consistency in generated videos by combining motion and depth cues. It validates the metric on new synthetic datasets and uses it to benchmark models and improve video generation by reducing artifacts as a guidance loss."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GeCo: A Differentiable Geometric Consistency Metric for Video Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u751f\u6210\u89c6\u9891\u8fdd\u53cd3D\u51e0\u4f55/Generated videos violate 3D geometry]\n    C --\x3e C1[\u878d\u5408\u8fd0\u52a8\u548c\u6df1\u5ea6\u5148\u9a8c/Fuse motion & depth priors]\n    C --\x3e C2[\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6/Create synthetic datasets]\n    D --\x3e D1[\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b/Systematically benchmark models]\n    D --\x3e D2[\u7528\u4f5c\u65e0\u8bad\u7ec3\u5f15\u5bfc\u635f\u5931/Used as training-free guidance loss]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22263",children:"https://arxiv.org/pdf/2512.22263"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Evaluating an Adaptive Multispectral Turret System] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[RGB\u5728\u4f4e\u5149\u4e0b\u8868\u73b0\u5dee/RGB struggles in low-light]\n    Problem --\x3e P2[\u70ed\u6210\u50cf\u7f3a\u4e4f\u989c\u8272\u7eb9\u7406/Thermal lacks color & texture]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u81ea\u9002\u5e94RGB-LWIR\u878d\u5408\u6846\u67b6/Adaptive RGB-LWIR fusion framework]\n    Method --\x3e M2[\u8bad\u7ec333\u4e2aYOLO\u6a21\u578b/Trained 33 YOLO models]\n    Method --\x3e M3[11\u79cd\u878d\u5408\u6bd4\u4f8b/11 fusion ratios]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5168\u5149\u6a21\u578b: 92.8%\u7f6e\u4fe1\u5ea6/Full-light model: 92.8% confidence]\n    Results --\x3e R2[\u5fae\u5149\u6a21\u578b: 92.0%\u7f6e\u4fe1\u5ea6/Dim-light model: 92.0% confidence]\n    Results --\x3e R3[\u65e0\u5149\u6a21\u578b: 71.0%\u7f6e\u4fe1\u5ea6/No-light model: 71.0% confidence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging], [vision-language models, fetal ultrasound, visual question answering, benchmark, multimodal learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hussain Alasmawi, Numan Saeed, Mohammad Yaqub"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22278",children:"https://arxiv.org/pdf/2512.22278"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Fetal-Gauge, the first and largest visual question answering benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound, comprising over 42,000 images and 93,000 question-answer pairs. 2. Systematically evaluates state-of-the-art general-purpose and medical-specific VLMs, revealing a substantial performance gap where the best model achieves only 55% accuracy. 3. Identifies critical limitations of current VLMs in this domain and establishes a foundation for advancing domain-adapted architectures and specialized training for prenatal care."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of a standardized benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound by introducing Fetal-Gauge, a large-scale visual question answering dataset. The authors evaluate multiple VLMs and find their performance (max 55% accuracy) is far below clinical requirements, highlighting the urgent need for specialized models in this medical domain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5168\u7403\u8d85\u58f0\u533b\u5e08\u77ed\u7f3a / Global Sonographer Shortage]\n    B --\x3e B2[\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6 / Lack of Standardized Benchmark]\n    C --\x3e C1[\u521b\u5efa\u80ce\u513f\u8d85\u58f0VQA\u57fa\u51c6 / Create Fetal Ultrasound VQA Benchmark]\n    C --\x3e C2[\u7cfb\u7edf\u8bc4\u4f30VLMs / Systematically Evaluate VLMs]\n    D --\x3e D1[\u6700\u4f73\u6a21\u578b\u51c6\u786e\u738755% / Best Model Accuracy 55%]\n    D --\x3e D2[\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4e34\u5e8a\u8981\u6c42 / Performance Far Below Clinical Requirements]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal reasoning], [clinical reasoning benchmark, vision-language models, multimodal integration, medical image interpretation, hallucination]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University Third Hospital, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22275",children:"https://arxiv.org/pdf/2512.22275"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Illusion of Clinical Reasoning<br>\u4e34\u5e8a\u63a8\u7406\u7684\u5047\u8c61] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u7efc\u5408\u3001\u591a\u6a21\u6001\u4e34\u5e8a\u63a8\u7406\u3002]\n    C --\x3e C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>\u5f00\u53d1\u4e86\u5305\u542b1245\u4e2a\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u3001\u6db5\u76d67\u9879\u4efb\u52a1\u7684B&J\u57fa\u51c6\u3002]\n    D --\x3e D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>\u5de8\u5927\u6027\u80fd\u5dee\u8ddd\uff1a\u9009\u62e9\u9898\u8868\u73b0\u597d\uff0c\u5f00\u653e\u5f0f\u591a\u6a21\u6001\u4efb\u52a1\u8868\u73b0\u5dee\u3002]\n    D --\x3e D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM\u5728\u56fe\u50cf\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u5c40\u9650\u5e76\u51fa\u73b0\u5e7b\u89c9\u3002]\n    D --\x3e D3[Medically fine-tuned models show no consistent advantage.<br>\u533b\u5b66\u5fae\u8c03\u6a21\u578b\u672a\u663e\u793a\u4e00\u81f4\u4f18\u52bf\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University (Leap Lab), Anyverse Dynamics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22288",children:"https://arxiv.org/pdf/2512.22288"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://co-grpo.github.io",children:"https://co-grpo.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u5339\u914d/Mismatch between Training & Inference]\n    B1 --\x3e B2[\u8bad\u7ec3: \u5355\u6b65BERT\u5f0f/Training: Single-step BERT-style]\n    B1 --\x3e B3[\u63a8\u7406: \u591a\u6b65\u6709\u8c03\u5ea6/Inference: Multi-step with Schedule]\n    C --\x3e C1[\u7edf\u4e00MDP/Unified MDP]\n    C1 --\x3e C2[\u8054\u5408\u4f18\u5316\u6a21\u578b\u4e0e\u8c03\u5ea6/Jointly Optimize Model & Schedule]\n    C2 --\x3e C3[\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Group Relative Policy Optimization]\n    D --\x3e D1[\u63d0\u5347\u751f\u6210\u8d28\u91cf/Improved Generation Quality]\n    D1 --\x3e D2[\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u9a8c\u8bc1/Validated on Four Benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal retrieval and generation], [3D retrieval, 4D generation, cross-modal alignment, multi-head attention, open-vocabulary]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Philip Xu, David Elizondo, Raouf Hamzaoui"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," De Montfort University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22294",children:"https://arxiv.org/pdf/2512.22294"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5927\u89c4\u6a213D\u68c0\u7d22\u4e0e\u53ef\u63a74D\u751f\u6210\u7684\u6311\u6218/Challenges in large-scale 3D retrieval and controlled 4D generation]\n    C --\x3e C1[\u4e09\u7ea7\u5bf9\u9f50\u6846\u67b6: \u6587\u672c-3D-\u56fe\u50cf/Three-level alignment: text-3D-image]\n    C --\x3e C2[ATMS\u6a21\u578b\u4f18\u5316\u68c0\u7d22/ATMS model optimizes retrieval]\n    D --\x3e D1[\u9ad8\u8d28\u91cf3D\u68c0\u7d22/High-quality 3D retrieval]\n    D --\x3e D2[\u53ef\u63a74D\u751f\u6210/Controlled 4D generation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D scene reconstruction], [SirenPose, sinusoidal representation networks, geometric priors, spatiotemporal consistency, dynamic 3D reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tian Guo, Hui Yuan, Philip Xu, David Elizondo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," De Montfort University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22295",children:"https://arxiv.org/pdf/2512.22295"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel loss function "SirenPose" that combines periodic activation from SIREN networks with geometric priors from keypoint structures. 2. Introduces physics-inspired constraint mechanisms to enforce coherent keypoint predictions across spatial and temporal dimensions. 3. Expands the training dataset to 600,000 annotated instances to support robust learning and demonstrates significant improvements in spatiotemporal consistency metrics.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of maintaining motion accuracy and spatiotemporal consistency in dynamic 3D scene reconstruction from monocular videos. It proposes a novel loss function called SirenPose, which integrates sinusoidal representation networks with geometric keypoint priors and physics-based constraints. Experiments show that models using SirenPose achieve superior performance in handling rapid motion and complex scene changes compared to prior methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u5feb\u901f\u8fd0\u52a8\u591a\u76ee\u6807\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u8fd0\u52a8\u5efa\u6a21\u7cbe\u5ea6\u548c\u65f6\u7a7a\u4e00\u81f4\u6027/Existing methods struggle with motion accuracy and spatiotemporal consistency in fast-moving multi-target scenes]\n    C --\x3e C1[\u63d0\u51faSirenPose\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408SIREN\u7684\u5468\u671f\u6fc0\u6d3b\u7279\u6027\u548c\u5173\u952e\u70b9\u51e0\u4f55\u5148\u9a8c/Propose SirenPose loss, combining SIREN's periodic activation and keypoint geometric priors]\n    C --\x3e C2[\u5f15\u5165\u7269\u7406\u542f\u53d1\u7684\u7ea6\u675f\u673a\u5236\uff0c\u786e\u4fdd\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u5173\u952e\u70b9\u9884\u6d4b\u7684\u4e00\u81f4\u6027/Introduce physics-inspired constraints for coherent keypoint predictions across space and time]\n    D --\x3e D1[\u6a21\u578b\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347/Models show significant improvement in spatiotemporal consistency metrics vs. prior methods]\n    D --\x3e D2[\u5728\u5904\u7406\u5feb\u901f\u8fd0\u52a8\u548c\u590d\u6742\u573a\u666f\u53d8\u5316\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd/Demonstrates superior performance in handling rapid motion and complex scene changes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vesal Ahsani, Babak Hossein Khalaj"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sharif University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22298",children:"https://arxiv.org/pdf/2512.22298"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b9e\u65f6DMS\u9700\u6c42 / Real-time DMS needs low latency, low cost, low power]\n    C --\x3e C1[\u7d27\u51d1\u5355\u6444\u50cf\u5934\u7cfb\u7edf / Compact single-camera pipeline]\n    C1 --\x3e C2[\u7d27\u51d1\u89c6\u89c9\u6a21\u578b / Compact per-frame vision model]\n    C1 --\x3e C3[\u6297\u6df7\u6dc6\u6807\u7b7e\u8bbe\u8ba1 / Confounder-aware label design]\n    C1 --\x3e C4[\u65f6\u5e8f\u51b3\u7b56\u5934 / Temporal decision head]\n    D --\x3e D1[\u6027\u80fd: 16 FPS (RPi5), 25 FPS (Edge TPU) / Performance: 16 FPS (RPi5), 25 FPS (Edge TPU)]\n    D --\x3e D2[\u9a8c\u8bc1: \u771f\u5b9e\u8f66\u8f86\u6d4b\u8bd5 / Validation: Real in-vehicle tests]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [counter-forensics, red-team training, test-time defense, two-stream architecture, tamper heatmaps]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Noor Fatima, Hasan Faraz Khan, Muzammil Behzad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22303",children:"https://arxiv.org/pdf/2512.22303"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Robust detection under realistic counter-forensic attacks"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Red-team training + Test-time defense in a two-stream architecture"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [food volume and nutrition estimation], [knowledge distillation, cross-modal learning, point cloud, RGB-to-Geometry Adapter, dual-mode training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Darrin Bright, Rakshith Raj, Kanchan Keisham"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Vellore Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22304",children:"https://arxiv.org/pdf/2512.22304"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed PortionNet, a cross-modal knowledge distillation framework that enables RGB models to learn 3D geometric features from point clouds during training, eliminating the need for depth sensors at inference. 2. Introduced a dual-mode training strategy with a lightweight RGB-to-Geometry Adapter that learns to generate pseudo-3D features from standard RGB images. 3. Achieved state-of-the-art performance on MetaFood3D for volume and energy estimation and demonstrated strong generalization on SimpleFood45."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of estimating food nutrition from a single RGB image, which lacks 3D information. It proposes PortionNet, a framework that uses knowledge distillation to teach an RGB model geometric reasoning from point cloud data during training, requiring only RGB images at test time. The method achieves state-of-the-art accuracy on benchmark datasets, demonstrating effective 3D knowledge transfer without specialized hardware."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u4ece\u5355\u5f20RGB\u56fe\u50cf\u8fdb\u884c\u98df\u7269\u8425\u517b\u4f30\u8ba1\u7f3a\u4e4f3D\u4fe1\u606f/Accurate food nutrition estimation from single RGB images lacks 3D information.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u4ece\u70b9\u4e91\u5b66\u4e60\u51e0\u4f55\u7279\u5f81/Cross-modal knowledge distillation learns geometric features from point clouds.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728MetaFood3D\u4e0a\u53d6\u5f97SOTA\uff0c\u5728SimpleFood45\u4e0a\u5c55\u793a\u5f3a\u6cdb\u5316\u6027/Achieves SOTA on MetaFood3D, shows strong generalization on SimpleFood45.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [multi-subject video generation, scale-aware modulation, fourier fusion, permutation invariance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Run Ling, Ke Cao, Jian Lu, Ao Ma, Haowei Liu, Runze He, Changwei Wang, Rongtao Xu, Yihua Shao, Zhanjie Zhang, Peng Wu, Guibing Guo, Wei Feng, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Xingwei Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," JD.com, Inc., Northeastern University, University of Science and Technology of China, Chongqing University of Post and Telecommunications, University of Chinese Academy of Sciences, Northwestern Polytechnical University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22310",children:"https://arxiv.org/pdf/2512.22310"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Scale-Aware Modulation (SMO), an LLM-guided module to extract implicit scale cues from text prompts to address scale inconsistency. 2. Introduces a Fourier Fusion strategy using Fast Fourier Transform to process reference features for permutation-invariant generation. 3. Designs a dedicated benchmark and a Scale-Permutation Stability Loss to evaluate and jointly optimize for scale consistency and permutation invariance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes MoFu, a framework for multi-subject video generation that tackles scale inconsistency and permutation sensitivity. It introduces a Scale-Aware Modulation module and a Fourier Fusion strategy, validated by a new benchmark and a stability loss. Experiments show MoFu outperforms existing methods in preserving natural scale, subject fidelity, and visual quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MoFu: Multi-Subject Video Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5c3a\u5ea6\u4e0d\u4e00\u81f4/Scale Inconsistency]\n    B --\x3e B2[\u6392\u5217\u654f\u611f\u6027/Permutation Sensitivity]\n    C --\x3e C1[\u5c3a\u5ea6\u611f\u77e5\u8c03\u5236/Scale-Aware Modulation (SMO)]\n    C --\x3e C2[\u5085\u91cc\u53f6\u878d\u5408/Fourier Fusion]\n    C --\x3e C3[\u7a33\u5b9a\u6027\u635f\u5931/Scale-Permutation Stability Loss]\n    D --\x3e D1[\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Outperforms Existing Methods]\n    D --\x3e D2[\u4fdd\u6301\u81ea\u7136\u5c3a\u5ea6\u4e0e\u4fdd\u771f\u5ea6/Preserves Natural Scale & Fidelity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22317",children:"https://arxiv.org/pdf/2512.22317"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: \u77ed\u671f\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u6761\u4ef6\uff0c\u672a\u6765\u8fd0\u52a8\u7ea6\u675f\u5f31]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u8bed\u8a00\u611f\u77e5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728Rectified Flow\u8303\u5f0f\u4e0b\u8fdb\u884c\u6f5c\u7a7a\u95f4\u96c6\u6210]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728\u745e\u5178\u548cMRMS\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\uff0c\u572880\u5206\u949f\u9884\u89c1\u671f\uff0c\u5f3a\u964d\u6c34CSI\u63d0\u5347\u8d8560%\u548c19%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [agentic framework, temporal zoom, reinforcement learning, long video reasoning, multimodal large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, The Chinese University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22315",children:"https://arxiv.org/pdf/2512.22315"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zsgvivo/VideoZoomer",children:"https://github.com/zsgvivo/VideoZoomer"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u957f\u89c6\u9891\u7406\u89e3\u53d7\u9650/Limited Long Video Understanding]\n    B1 --\x3e B2[\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236/Context Window Limitation]\n    B1 --\x3e B3[\u5747\u5300\u91c7\u6837\u5ffd\u7565\u5173\u952e\u8bc1\u636e/Uniform Sampling Overlooks Evidence]\n    C --\x3e C1[\u4ee3\u7406\u6846\u67b6/Agentic Framework]\n    C1 --\x3e C2[\u52a8\u6001\u65f6\u95f4\u805a\u7126/Dynamic Temporal Focusing]\n    C2 --\x3e C3[\u4ece\u7c97\u5230\u7ec6\u63a8\u7406/Coarse-to-Fine Reasoning]\n    C --\x3e C4[\u4e24\u9636\u6bb5\u8bad\u7ec3/Two-Stage Training]\n    C4 --\x3e C5[\u76d1\u7763\u5fae\u8c03/Supervised Fine-Tuning]\n    C4 --\x3e C6[\u5f3a\u5316\u5b66\u4e60/Reinforcement Learning]\n    D --\x3e D1[\u6027\u80fd\u5f3a\u52b2/Strong Performance]\n    D1 --\x3e D2[\u8d85\u8d8a\u5f00\u6e90\u6a21\u578b/Surpasses Open-Source Models]\n    D1 --\x3e D3[\u5ab2\u7f8e\u4e13\u6709\u7cfb\u7edf/Rivals Proprietary Systems]\n    D --\x3e D4[\u9ad8\u6548\u63a8\u7406/Efficient Reasoning]\n    D4 --\x3e D5[\u4f4e\u5e27\u9884\u7b97/Reduced Frame Budget]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SpotEdit: Selective Region Editing in Diffusion Transformers"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Diffusion Transformers, selective region editing, training-free, perceptual similarity, dynamic fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore, Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22323",children:"https://arxiv.org/pdf/2512.22323"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://biangbiang0321.github.io/SpotEdit.github.io/",children:"https://biangbiang0321.github.io/SpotEdit.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SpotEdit: Selective Region Editing in Diffusion Transformers] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5168\u56fe\u53bb\u566a\u5197\u4f59/Full-image denoising is redundant for small edits]\n    C --\x3e C1[SpotSelector: \u8bc6\u522b\u7a33\u5b9a\u533a\u57df/Identifies stable regions via perceptual similarity]\n    C --\x3e C2[SpotFusion: \u52a8\u6001\u7279\u5f81\u878d\u5408/Dynamically fuses features for coherence]\n    D --\x3e D1[\u9ad8\u6548\u7f16\u8f91/Efficient editing]\n    D --\x3e D2[\u4fdd\u6301\u672a\u4fee\u6539\u533a\u57df\u4fdd\u771f\u5ea6/Preserves fidelity in unchanged areas]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Tencent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22322",children:"https://arxiv.org/pdf/2512.22322"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://huggingface.co/collections/yolay/smartsnap",children:"https://huggingface.co/collections/yolay/smartsnap"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n    D[\u5173\u952e\u7ed3\u679c/Results: Performance gains up to 26.08%; competitive with larger models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion generation], [energy-based diffusion model, motion decomposition, compositional training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jianrong Zhang, Hehe Fan, Yi Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, Zhejiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22324",children:"https://arxiv.org/pdf/2512.22324"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://jiro-zhang.github.io/DeMoGen/",children:"https://jiro-zhang.github.io/DeMoGen/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DeMoGen, a compositional training paradigm using an energy-based diffusion model to decompose holistic motions into semantic sub-components without needing ground-truth for individual concepts., 2. Introduces three training variants (DeMoGen-Exp, DeMoGen-OSS, DeMoGen-SC) to encourage decompositional understanding and disentangle reusable motion primitives., 3. Constructs a text-decomposed dataset to support compositional training and demonstrates that decomposed concepts can be recombined to generate novel motions beyond the training distribution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of decomposing holistic human motions into simpler, reusable primitives. It proposes DeMoGen, an energy-based diffusion model training paradigm with three variants to learn this decomposition without individual concept supervision. The method successfully disentangles motion concepts, which can then be flexibly recombined to generate diverse and novel motions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DeMoGen: Decompositional Human Motion Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u5c06\u6574\u4f53\u8fd0\u52a8\u5206\u89e3\u4e3a\u8bed\u4e49\u5b50\u7ec4\u4ef6/How to decompose holistic motion into semantic sub-components?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u80fd\u91cf\u7684\u6269\u6563\u6a21\u578b\u4e0e\u4e09\u79cd\u8bad\u7ec3\u53d8\u4f53/Energy-based diffusion model with three training variants]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u89e3\u8026\u53ef\u91cd\u7528\u8fd0\u52a8\u57fa\u5143\u5e76\u652f\u6301\u91cd\u7ec4\u751f\u6210/Disentangle reusable motion primitives and support recombination generation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [multi-view learning, variational autoencoder, latent representation learning, radiomics, glioblastoma]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mariya Miteva, Maria Nisheva-Pavlova"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22331",children:"https://arxiv.org/pdf/2512.22331"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]\n    D[\u5173\u952e\u7ed3\u679c/Results: Latent embeddings used for MGMT promoter methylation classification]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D scene understanding and manipulation], [Multimodal Large Language Models (MLLMs), 3D object arrangement, tool-augmented agents, MCP-based API, multi-agent framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University, Google, Google DeepMind, New York University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22351",children:"https://arxiv.org/pdf/2512.22351"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," vulcan-3d.github.io"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>MLLMs\u5728\u590d\u67423D\u573a\u666f\u64cd\u63a7\u4e2d\u7684\u5e94\u7528\u672a\u88ab\u5145\u5206\u63a2\u7d22<br>Application of MLLMs to complex 3D scene manipulation is underexplored]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f15\u5165MCP API\u3001\u89c6\u89c9\u5de5\u5177\u5957\u4ef6\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u572825\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf<br>Significantly outperforms baselines on 25 complex tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [vision transformer, whole slide image, HER2 scoring, multi-modality, tumor classification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," North Carolina A&T State University, University of Chichester, Yale University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22335",children:"https://arxiv.org/pdf/2512.22335"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u6311\u6218: \u8054\u5408\u5206\u6790H&E\u548cIHC\u56fe\u50cf\u8fdb\u884cHER2\u8bc4\u5206/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]\nB --\x3e B2[\u96be\u70b9: \u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u50cf\u7d20\u7ea7HER2\u72b6\u6001\u5b9a\u4f4d/Issue: Existing methods lack pixel-level HER2 status localization]\nC --\x3e C1[\u65b9\u6cd5: \u7aef\u5230\u7aef\u591a\u9636\u6bb5\u89c6\u89c9Transformer\u7ba1\u9053/Method: End-to-end multi-stage Vision Transformer pipeline]\nC --\x3e C2[\u521b\u65b0: \u65b0\u9896\u7684\u6620\u5c04\u51fd\u6570\u5173\u8054H&E\u4e0eIHC\u533a\u57df/Innovation: Novel mapping function to correlate H&E and IHC regions]\nC --\x3e C3[\u673a\u5236: \u4e34\u5e8a\u542f\u53d1\u76844\u7ea7HER2\u8bc4\u5206\u673a\u5236/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]\nD --\x3e D1[\u7ed3\u679c: \u80bf\u7624\u5b9a\u4f4d\u5206\u7c7b\u51c6\u786e\u7387\u9ad8/Result: Good classification accuracy for tumor localization]\nD --\x3e D2[\u7ed3\u679c: HER2\u72b6\u6001\u9884\u6d4b\u51c6\u786e\u73870.94/Result: 0.94 accuracy for HER2 status prediction]\nD --\x3e D3[\u7ed3\u8bba: \u7aef\u5230\u7aefViT\u6a21\u578b\u53ef\u7528\u4e8e\u8054\u5408\u8bc4\u4f30H&E\u548cIHC\u56fe\u50cf/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alaa Alahmadi, Mohamed Hasan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Newcastle University, University of Leeds"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22349",children:"https://arxiv.org/pdf/2512.22349"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --\x3e B1\n    A --\x3e B2\n    A --\x3e B3\n    B1[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e C1[\u6570\u636e\u6548\u7387\u4f4e/Lack of data efficiency]\n    B1 --\x3e C2[\u53ef\u89e3\u91ca\u6027\u5dee/Limited explainability]\n    B1 --\x3e C3[\u4e34\u5e8a\u53ef\u9760\u6027\u53d7\u9650/Constrained clinical reliability]\n    B2[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e D1[\u611f\u77e5\u542f\u53d1\u7684\u4f2a\u7740\u8272\u6280\u672f/Perception-informed pseudo-colouring]\n    D1 --\x3e E1[\u7f16\u7801\u4e34\u5e8a\u7279\u5f81/Encode clinical features (e.g., QT-interval)]\n    D1 --\x3e E2[\u7ed3\u6784\u5316\u989c\u8272\u8868\u793a/Structured colour representations]\n    B2 --\x3e D2[\u539f\u578b\u7f51\u7edc\u4e0eResNet-18/Prototypical networks & ResNet-18]\n    B2 --\x3e D3[\u805a\u5408\u591a\u4e2a\u5fc3\u8df3\u5468\u671f/Aggregate multiple cardiac cycles]\n    B3[\u5173\u952e\u7ed3\u679c/Results] --\x3e F1[\u5b9e\u73b0\u5c11\u6837\u672c\u4e0e\u5355\u6837\u672c\u5b66\u4e60/Achieve few-shot & one-shot learning]\n    B3 --\x3e F2[\u63d0\u5347\u53ef\u89e3\u91ca\u6027/Improve explainability (guide attention)]\n    B3 --\x3e F3[\u6865\u63a5\u6570\u636e\u6548\u7387\u4e0e\u56e0\u679c\u63a8\u7406/Bridge data efficiency & causal reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22374",children:"https://arxiv.org/pdf/2512.22374"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables "any-step" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Traditional models need many steps or a teacher model] --\x3e Problem_Sub1[\u4f20\u7edf\u6a21\u578b\u9700\u8981\u591a\u6b65\u6216\u6559\u5e08\u6a21\u578b/Traditional models need many steps or a teacher]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Self-Evaluating Model (Self-E)] --\x3e Method_Sub1[\u7ed3\u5408\u6d41\u5339\u914d\u4e0e\u81ea\u8bc4\u4f30/Combines Flow Matching & Self-Evaluation]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Unified any-step model] --\x3e Results_Sub1[\u5c11\u6b65\u4e0e\u591a\u6b65\u5747\u8868\u73b0\u4f18\u5f02/Excels at both few-step and many-step]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Elsen Ronando, Sozo Inoue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22385",children:"https://arxiv.org/pdf/2512.22385"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LLM-Guided Exemplar Selection for Few-Shot HAR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u4e0e\u51e0\u4f55\u9009\u62e9 / Reliance on large datasets & geometric selection]\n    B --\x3e B2[\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u6d3b\u52a8 / Hard to distinguish similar activities]\n    C --\x3e C1[LLM\u751f\u6210\u8bed\u4e49\u5148\u9a8c / LLM-generated semantic priors]\n    C --\x3e C2[\u7ed3\u5408\u591a\u7ebf\u7d22\u4f18\u5316 / Combine multiple cues for optimization]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf / Outperforms baselines (88.78% F1)]\n    D --\x3e D2[\u8bed\u4e49\u5148\u9a8c\u6709\u6548 / Semantic priors are effective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [on-device AI, LiDAR depth estimation, fused GPS/IMU, sidewalk mapping, accessibility]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Himanshu Naidu, Yuxiang Zhang, Sachin Mehta, Anat Caspi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Washington"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22392",children:"https://arxiv.org/pdf/2512.22392"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces iOSPointMapper, a mobile app for real-time, privacy-conscious sidewalk mapping using iPhones/iPads. 2. Leverages on-device semantic segmentation, LiDAR depth, and fused GPS/IMU to detect and localize sidewalk features like signs and poles. 3. Incorporates a user-guided annotation interface for validation and integrates collected data with the Transportation Data Exchange Initiative (TDEI)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the costly and fragmented collection of sidewalk data by proposing iOSPointMapper, a mobile application that uses on-device AI, LiDAR, and sensor fusion for real-time detection and mapping of pedestrian infrastructure. The system includes a user validation interface and integrates data into a broader transportation dataset. Evaluations show its potential for scalable and enhanced pedestrian mapping."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root(iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u4eba\u884c\u9053\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u788e\u7247\u5316\u4e14\u96be\u4ee5\u6269\u5c55/Sidewalk data collection is costly, fragmented, and hard to scale)\n    Method --\x3e M1(\u79fb\u52a8\u5e94\u7528\u4f7f\u7528\u8bbe\u5907\u7aefAI\u3001LiDAR\u548c\u4f20\u611f\u5668\u878d\u5408/Mobile app uses on-device AI, LiDAR, and sensor fusion)\n    Method --\x3e M2(\u7528\u6237\u5f15\u5bfc\u7684\u6807\u6ce8\u754c\u9762\u8fdb\u884c\u9a8c\u8bc1/User-guided annotation interface for validation)\n    Results --\x3e R1(\u8bc4\u4f30\u663e\u793a\u4eba\u884c\u9053\u7279\u5f81\u68c0\u6d4b\u548c\u7a7a\u95f4\u6620\u5c04\u7684\u6f5c\u529b/Evaluation shows potential for feature detection and spatial mapping)\n    Results --\x3e R2(\u4e3a\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u65b9\u6cd5/Offers a scalable, user-centered approach to data collection)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Conditional Flow Matching, Generative Object Detection, Rectified Flow, Ordinary Differential Equation, Magnetic Resonance Enterography]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hansang Lee, Chaelin Lee, Nieun Seo, Joon Seok Lim, Helen Hong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Seoul Women's University, Severance Hospital, Yonsei University College of Medicine"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22406",children:"https://arxiv.org/pdf/2512.22406"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DeFloMat, a novel generative object detection framework that replaces the slow multi-step stochastic denoising of diffusion models with a fast, deterministic flow field based on Conditional Flow Matching and Rectified Flow. 2. Achieves state-of-the-art accuracy with only 3 inference steps, significantly outperforming prior diffusion-based detectors in speed and performance on a clinical MRE dataset. 3. Demonstrates superior localization stability and recall in the few-step regime, effectively resolving the trade-off between generative accuracy and inference efficiency for time-sensitive applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high latency of diffusion-based object detectors, which is impractical for clinical use. It proposes DeFloMat, a new framework that uses Conditional Flow Matching to create a deterministic flow for fast, stable object localization via an ODE solver. The method achieves superior accuracy with only 3 inference steps on a medical imaging dataset, setting a new standard for efficient generative detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DeFloMat: Detection with Flow Matching] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Diffusion-based detectors are too slow for clinical applications (e.g., Crohn's Disease detection in MRE)]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Replaces stochastic diffusion with deterministic flow field (Conditional Flow Matching, Rectified Flow) solved via ODE]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA accuracy in only 3 steps, 1.4x performance gain, superior stability and recall]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UC Santa Barbara, Allen Institute for Cell Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22423",children:"https://arxiv.org/pdf/2512.22423"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [transformer, fluence map prediction, physics-informed loss, two-stage regression, Swin UNETR]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Wayne State University, Henry Ford Health"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22425",children:"https://arxiv.org/pdf/2512.22425"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Ill-posed inverse problem: complex anatomy-beam relationship / \u75c5\u6001\u9006\u95ee\u9898: \u89e3\u5256\u7ed3\u6784\u4e0e\u5c04\u675f\u5f3a\u5ea6\u7684\u590d\u6742\u5173\u7cfb]\n    B --\x3e B2[CNN struggles with long-range dependencies / CNN\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u4f9d\u8d56]\n    C --\x3e C1[Two-stage transformer framework / \u4e24\u9636\u6bb5Transformer\u6846\u67b6]\n    C1 --\x3e C1_1[Stage 1: Global dose prior / \u9636\u6bb51: \u5168\u5c40\u5242\u91cf\u5148\u9a8c]\n    C1 --\x3e C1_2[Stage 2: Geometry-conditioned fluence regression / \u9636\u6bb52: \u51e0\u4f55\u6761\u4ef6\u5316\u7684\u6ce8\u91cf\u56fe\u56de\u5f52]\n    C --\x3e C2[Fluence-Aware Regression (FAR) loss / \u6ce8\u91cf\u611f\u77e5\u56de\u5f52\u635f\u5931]\n    D --\x3e D1[Reduced Energy Error to 4.5% / \u80fd\u91cf\u8bef\u5dee\u964d\u4f4e\u81f34.5%]\n    D --\x3e D2[Improved structural fidelity (p<0.05) / \u7ed3\u6784\u4fdd\u771f\u5ea6\u663e\u8457\u63d0\u5347]\n    D --\x3e D3[Outperformed benchmark CNN & single-stage methods / \u8d85\u8d8a\u57fa\u51c6CNN\u4e0e\u5355\u9636\u6bb5\u65b9\u6cd5]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [point cloud processing], [Graph Attention Networks, LiDAR reconstruction, beam dropout, gated residual fusion, sparse point cloud]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22439",children:"https://arxiv.org/pdf/2512.22439"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LiDAR\u5782\u76f4\u5206\u8fa8\u7387\u56fa\u5b9a\u4e0e\u5149\u675f\u4e22\u5931\u5bfc\u81f4\u70b9\u4e91\u7a00\u758f]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u5149\u675f\u611f\u77e5\u56fe\u4e0e\u95e8\u63a7\u6b8b\u5dee\u878d\u5408\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u91cd\u5efa\u8bef\u5dee\u66f4\u4f4e\uff0c\u51e0\u4f55\u4e00\u81f4\u6027\u66f4\u597d\uff0c\u7ed3\u6784\u5b8c\u6574\u6027\u4fdd\u6301]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] EmoCtrl: Controllable Emotional Image Content Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image generation], [controllable generation, emotion-aware generation, diffusion models, affective computing, multi-modal learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jingyuan Yang, Weibin Luo, Hui Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shenzhen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22437",children:"https://arxiv.org/pdf/2512.22437"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the novel task of Controllable Emotional Image Content Generation (C-EICG), which aims to generate images faithful to a content description while expressing a target emotion. 2. Proposes the EmoCtrl model, which incorporates textual and visual emotion enhancement modules to bridge abstract emotions to visual cues using learned emotion tokens. 3. Constructs a supporting dataset annotated with content, emotion, and affective prompts, and demonstrates through experiments and user studies that EmoCtrl outperforms existing methods in achieving both content faithfulness and expressive emotion control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap between content-faithful and emotion-expressive image generation by proposing EmoCtrl, a model for Controllable Emotional Image Content Generation. EmoCtrl uses textual and visual modules to learn emotion tokens that enrich affective expression while maintaining semantic content. Experiments and user studies show it outperforms existing methods in balancing content accuracy and emotional tone."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EmoCtrl: Controllable Emotional Image Content Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u60c5\u611f\u63a7\u5236\u6216\u5185\u5bb9\u5931\u771f<br>Existing models lack emotion control or distort content]\n    C --\x3e C1[\u63d0\u51faEmoCtrl\u6a21\u578b\u4e0e\u6570\u636e\u96c6<br>Propose EmoCtrl model and dataset]\n    C1 --\x3e C2[\u6587\u672c\u4e0e\u89c6\u89c9\u60c5\u611f\u589e\u5f3a\u6a21\u5757<br>Textual and visual emotion enhancement modules]\n    C2 --\x3e C3[\u5b66\u4e60\u60c5\u611f\u4ee4\u724c<br>Learn emotion tokens]\n    D --\x3e D1[\u5b9a\u91cf\u5b9a\u6027\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02<br>Quantitative and qualitative experiments show superiority]\n    D --\x3e D2[\u7528\u6237\u7814\u7a76\u7b26\u5408\u4eba\u7c7b\u504f\u597d<br>User studies align with human preference]\n    D --\x3e D3[\u6cdb\u5316\u81f3\u521b\u610f\u5e94\u7528<br>Generalizes to creative applications]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LECalib: Line-Based Event Camera Calibration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [camera calibration], [event camera, line detection, geometric calibration, non-linear optimization, stereo calibration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zibin Liu, Banglei Guana, Yang Shanga, Zhenbao Yu, Yifei Bian, Qifeng Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology, Wuhan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22441",children:"https://arxiv.org/pdf/2512.22441"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Zibin6/line_based_event_camera_calib",children:"https://github.com/Zibin6/line_based_event_camera_calib"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a line-based calibration framework for event cameras that uses geometric lines from common man-made objects, eliminating the need for dedicated flashing patterns or calibration boards. 2. Introduces a method to detect lines directly from raw event streams and leverages an event-line calibration model to generate an initial parameter guess suitable for both planar and non-planar lines. 3. Validates the method's feasibility and accuracy through both simulation and real-world experiments on monocular and stereo event camera setups."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of time-consuming and manually intensive calibration for event cameras. It proposes LECalib, a framework that calibrates event cameras by detecting geometric lines directly from event streams and using them in a linear initialization and non-linear refinement process. The method is validated as feasible and accurate for both monocular and stereo setups."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LECalib: Line-Based Event Camera Calibration] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u4eba\u5de5\u6807\u5b9a\u7269 / Existing methods are time-consuming and require manual calibration objects]\n    C --\x3e C1[\u4ece\u4e8b\u4ef6\u6d41\u76f4\u63a5\u68c0\u6d4b\u7ebf\u7279\u5f81 / Detect lines directly from event streams]\n    C --\x3e C2[\u4f7f\u7528\u4e8b\u4ef6-\u7ebf\u6807\u5b9a\u6a21\u578b\u521d\u59cb\u5316 / Use event-line model for initial guess]\n    C --\x3e C3[\u975e\u7ebf\u6027\u4f18\u5316\u7cbe\u4fee\u53c2\u6570 / Refine parameters with non-linear optimization]\n    D --\x3e D1[\u4eff\u771f\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1 / Validated in simulation and real-world]\n    D --\x3e D2[\u652f\u6301\u5355\u76ee\u4e0e\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a / Works for monocular and stereo event cameras]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [optical-SAR fusion, missing modality, quality-aware fusion, dynamic fusion, orthogonal constraint]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Anhui University, China Electronics Technology Group Corporation (38th Research Institute)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22447",children:"https://arxiv.org/pdf/2512.22447"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [EfficientDet-D2, sound cues, on-device inference, assistive technology, Flutter]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Abu Obaida Zishan, Annajiat Alim Rasel"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," BRAC University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22449",children:"https://arxiv.org/pdf/2512.22449"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/MohammedZ666/SonoVision",children:"https://github.com/MohammedZ666/SonoVision"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed SonoVision, a smartphone application that uses real-time object detection and spatialized sound cues to help visually impaired individuals locate objects independently. 2. Implemented the system using the Flutter framework and the EfficientDet-D2 model, enabling it to function completely offline for safety and user-friendliness. 3. Designed an intuitive auditory interface where object location (left, center, right) is indicated by playing sinusoidal sounds in the corresponding ear(s) of the user's headphones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents SonoVision, a smartphone app that helps visually impaired people locate objects by using the EfficientDet-D2 model for real-time detection and providing directional sound cues through headphones. The application is built with Flutter and works offline, aiming to increase user independence and safety. The authors conclude that this approach can significantly assist users in a user-friendly manner."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Visually impaired individuals struggle to locate objects, hindering independence and safety.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Smartphone app uses EfficientDet-D2 for object detection and provides directional sound cues (sinusoidal tones) via headphones.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: An offline, user-friendly application that can help visually impaired users locate objects more independently.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SAM 3D for 3D Object Reconstruction from Remote Sensing Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [SAM 3D, monocular reconstruction, urban modeling, remote sensing, foundation model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junsheng Yao, Lichao Mou, Qingyu Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen; MedAI Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22452",children:"https://arxiv.org/pdf/2512.22452"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Presents the first systematic evaluation of the general-purpose foundation model SAM 3D for monocular remote sensing building reconstruction. 2. Benchmarks SAM 3D against TRELLIS on the NYC urban dataset using FID and CMMD metrics, showing superior roof geometry and boundary sharpness. 3. Extends SAM 3D to urban scene reconstruction via a novel segment-reconstruct-compose pipeline, demonstrating its potential for broader urban modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the SAM 3D foundation model for reconstructing 3D buildings from single remote sensing images. It benchmarks SAM 3D against TRELLIS, finding it produces more coherent geometry and sharper boundaries. The work also proposes a pipeline to extend the model for urban scene reconstruction, highlighting its potential and practical limitations for scalable urban modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("SAM 3D for 3D Object Reconstruction from Remote Sensing Images<br>\u8bba\u6587\u6807\u9898") --\x3e Problem("Monocular 3D building reconstruction is essential but challenging<br>\u6838\u5fc3\u95ee\u9898\uff1a\u5355\u76ee3D\u5efa\u7b51\u91cd\u5efa\u91cd\u8981\u4f46\u56f0\u96be")\n    Root --\x3e Method("Systematic evaluation & benchmark of SAM 3D; Segment-Reconstruct-Compose pipeline<br>\u4e3b\u8981\u65b9\u6cd5\uff1a\u7cfb\u7edf\u8bc4\u4f30SAM 3D\uff1b\u5206\u5272-\u91cd\u5efa-\u7ec4\u5408\u6d41\u7a0b")\n    Root --\x3e Results("SAM 3D outperforms TRELLIS; Potential for urban scene modeling demonstrated<br>\u5173\u952e\u7ed3\u679c\uff1aSAM 3D\u4f18\u4e8eTRELLIS\uff1b\u5c55\u793a\u4e86\u57ce\u5e02\u573a\u666f\u5efa\u6a21\u6f5c\u529b")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Comparing Object Detection Models for Electrical Substation Component Mapping"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLOv8, YOLOv11, RF-DETR, substation mapping, computer vision]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haley Mody, Namish Bansal, Dennies Kiprono Bor, Edward J. Oughton"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," None (No affiliations or email domains provided in the given content)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22454",children:"https://arxiv.org/pdf/2512.22454"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Training and comparative evaluation of three state-of-the-art object detection models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US electrical substation images. 2. Analysis of model performance based on detection accuracy, precision, and efficiency to identify strengths and limitations for the specific application. 3. Demonstration of a practical use case by utilizing the best-performing model(s) to effectively map substation components across the United States, showcasing autonomous infrastructure assessment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the labor-intensive problem of manually mapping electrical substation components by training and comparing three computer vision models (YOLOv8, YOLOv11, RF-DETR) on a labeled dataset of US substation images. The models are evaluated on accuracy, precision, and efficiency to determine the most reliable solution for large-scale, autonomous substation component mapping. The research concludes by identifying the best model and demonstrating its application for mapping substation infrastructure across the United States."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Comparing Object Detection Models for Electrical Substation Component Mapping] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Manual substation mapping is time-consuming and labor-intensive] --\x3e B1[\u5173\u952e\u5f71\u54cd/Impact<br>Grid failure has economic & safety implications]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Train & compare 3 CV models (YOLOv8, YOLOv11, RF-DETR)] --\x3e C1[\u8bc4\u4f30\u6307\u6807/Evaluation<br>Accuracy, Precision, Efficiency]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Identify best model for reliable, large-scale mapping] --\x3e D1[\u5e94\u7528\u6848\u4f8b/Use Case<br>Map US substation components]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [motion generation and editing], [residual vector quantization (RVQ), pose code, transformer, text-to-motion, motion editing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sukhyun Jeong, Yong-Hoon Choi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kwangwoon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22464",children:"https://arxiv.org/pdf/2512.22464"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/jayze3736/PGR2M",children:"https://github.com/jayze3736/PGR2M"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid motion representation (PGR\xb2M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR\xb2M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Pose-code frameworks struggle to capture subtle temporal dynamics and high-frequency details.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid representation (PGR\xb2M) with pose codes and residual codes (RVQ), using a two-stage Transformer.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Improves FID and reconstruction metrics; enables intuitive, structure-preserving edits.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Event-based high temporal resolution measurement of shock wave motion field"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [event-based vision], [event cameras, shock wave measurement, motion field reconstruction, asymmetry estimation, polar coordinate encoding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Taihang Lei, Banglei Guan, Minzu Liang, Pengju Sun, Jing Tao, Yang Shang, Qifeng Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22474",children:"https://arxiv.org/pdf/2512.22474"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework using multiple event cameras to measure shock wave asymmetry with high spatiotemporal resolution. 2. A method involving polar coordinate event encoding and adaptive ROI extraction for revealing propagation patterns. 3. The derivation of a geometric model for event-based shock wave parameter estimation and 3D motion field reconstruction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a method for high-resolution shock wave measurement using multiple event cameras. It establishes a polar coordinate system to encode events, extracts shock fronts via iterative slope analysis, and derives models for parameter estimation and 3D reconstruction. The method achieves high-precision measurements with errors as low as 0.06% compared to pressure sensors, demonstrating significant progress in the field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Event-based high temporal resolution measurement of shock wave motion field] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Fast, uneven shock wave propagation under unstable conditions)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Multi-event-camera framework with polar encoding, adaptive ROI, iterative slope analysis)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: High-precision measurement, max error 5.20%, min error 0.06%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Segment Anything Model, semi-supervised learning, knowledge distillation, Mixture of Experts, infrared small object detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zihan Liu, Xiangning Ren, Dezhang Kong, Yipeng Zhang, Meng Han"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chengdu University, China University of Geosciences (Beijing), Zhejiang University, Zhejiang Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22483",children:"https://arxiv.org/pdf/2512.22483"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Hierarchical Mixture of Experts (MoE) Adapter to adapt the Segment Anything Model (SAM) to the infrared domain and encode physical priors. 2. Introduced a novel two-stage semi-supervised paradigm (Scalpel-SAM) for knowledge distillation and transfer, requiring only 10% labeled data. 3. Demonstrated that the paradigm enables training lightweight downstream models with pseudo-labels, achieving performance comparable to or surpassing fully supervised models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the data scarcity and domain gap challenges in Infrared Small Object Detection (IR-SOT) by proposing Scalpel-SAM, a semi-supervised paradigm. It adapts the Segment Anything Model (SAM) using a Hierarchical MoE Adapter and a two-stage knowledge distillation/transfer process, enabling efficient downstream models to be trained with minimal annotations. Experiments show the method achieves performance on par with or better than fully supervised counterparts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9ad8\u6807\u6ce8\u6210\u672c\u4e0e\u9886\u57df\u9e3f\u6c9f/High annotation cost & domain gap]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u534a\u76d1\u7763\u8303\u5f0f/Two-stage semi-supervised paradigm]\n    C1 --\x3e C1_1[\u5148\u9a8c\u5f15\u5bfc\u77e5\u8bc6\u84b8\u998f/Prior-Guided Knowledge Distillation]\n    C1 --\x3e C1_2[\u90e8\u7f72\u5bfc\u5411\u77e5\u8bc6\u8fc1\u79fb/Deployment-Oriented Knowledge Transfer]\n    C1_1 --\x3e C1_1a[\u4f7f\u7528MoE\u9002\u914d\u5668\u84b8\u998fSAM/Use MoE Adapter to distill SAM]\n    D --\x3e D1[\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e\u5168\u76d1\u7763/Downstream models match or surpass fully supervised performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Tracking by Predicting 3-D Gaussians Over Time"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video representation learning], [self-supervised learning, Gaussian splatting, masked autoencoder, point tracking, video understanding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22489",children:"https://arxiv.org/pdf/2512.22489"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/tekotan/video-gmae",children:"https://github.com/tekotan/video-gmae"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Video-GMAE, a novel self-supervised method that learns video representations by encoding a sequence into a set of moving 3D Gaussians, enforcing temporal correspondence as an inductive bias. 2. Discovers that tracking emerges naturally from this pretraining, enabling zero-shot point tracking performance comparable to state-of-the-art methods. 3. Demonstrates superior performance after fine-tuning, achieving significant improvements on Kinetics and Kubric datasets over existing self-supervised video approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Video-GMAE, a self-supervised learning method that represents a video as a set of 3D Gaussian primitives moving over time. This representation enforces temporal consistency, allowing the model to learn strong correspondences and enabling zero-shot point tracking. The method outperforms existing self-supervised approaches on video understanding and tracking benchmarks after fine-tuning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Tracking by Predicting 3-D Gaussians Over Time") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Self-supervised video representations lack strong temporal correspondence for tracking.")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Video-GMAE: Masked Autoencoder predicts moving 3D Gaussian splats from video frames.")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Emergent zero-shot tracking; SOTA performance after fine-tuning on Kinetics & Kubric.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hassan Wasswa, Timothy Lynar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of New South Wales"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22488",children:"https://arxiv.org/pdf/2512.22488"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework's effectiveness in dynamic IoT environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection / \u9762\u5411\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u5b89\u5168\uff1a\u6982\u5ff5\u6f02\u79fb\u9c81\u68d2\u7684\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u68c0\u6d4b"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["AI\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6 / AI models rely on stationary datasets"]\n    Problem --\x3e P2["\u771f\u5b9e\u6d41\u91cf\u5b58\u5728\u6982\u5ff5\u6f02\u79fb / Real-world traffic suffers concept drift"]\n    Problem --\x3e P3["\u73b0\u6709\u65b9\u6848\u91cd\u8bad\u7ec3\u5f00\u9500\u5927 / Existing solutions have high retraining cost"]\n    Method --\x3e M1["\u5b66\u4e60\u5386\u53f2\u6d41\u91cf\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a / Learn latent space of historical traffic"]\n    Method --\x3e M2["\u5bf9\u9f50\u6a21\u578b\u6620\u5c04\u65b0\u6d41\u91cf / Alignment model maps new traffic"]\n    Method --\x3e M3["\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b / Graph Neural Network for classification"]\n    Results --\x3e R1["\u4fdd\u6301\u9c81\u68d2\u68c0\u6d4b\u6027\u80fd / Maintains robust detection performance"]\n    Results --\x3e R2["\u9002\u7528\u4e8e\u52a8\u6001\u5927\u89c4\u6a21\u73af\u5883 / Suitable for dynamic, large-scale environments"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [multimodal fusion, BEV perception, coordinate attention, feature alignment, small object detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22503",children:"https://arxiv.org/pdf/2512.22503"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A Section-aware Coordinate Attention (SCA) module to enhance feature discrimination for small, irregular objects. 2. A parameter-efficient Cognitive Adapter for efficient camera backbone tuning. 3. A Contrastive Alignment Module (CAM) to enforce consistency between camera and LiDAR features."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SCAFusion, a multimodal 3D object detection framework built upon BEVFusion to address the challenge of detecting small objects like rocks in lunar exploration. It introduces several modules including a Section-aware Coordinate Attention mechanism to improve small object detection, achieving significant performance gains over the baseline on both standard and simulated lunar datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root(SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u73b0\u6709\u65b9\u6cd5\u5728\u6708\u7403\u73af\u5883\u8868\u73b0\u4e0d\u4f73/Existing methods underperform in lunar environments)\n    Problem --\x3e P2(\u7279\u5f81\u672a\u5bf9\u9f50\uff0c\u5c0f\u7269\u4f53\u68c0\u6d4b\u5f31/Poor feature alignment, weak small-object detection)\n    Method --\x3e M1(\u57fa\u4e8eBEVFusion\u6784\u5efa/Built upon BEVFusion)\n    Method --\x3e M2(\u96c6\u6210\u8ba4\u77e5\u9002\u914d\u5668\u3001\u5bf9\u6bd4\u5bf9\u9f50\u6a21\u5757\u3001\u76f8\u673a\u8f85\u52a9\u8bad\u7ec3\u5206\u652f/Integrates Cognitive Adapter, Contrastive Alignment Module, Camera Auxiliary Training Branch)\n    Method --\x3e M3(\u5f15\u5165\u5206\u6bb5\u5750\u6807\u6ce8\u610f\u529b\u673a\u5236/Introduces Section-aware Coordinate Attention mechanism)\n    Results --\x3e R1(\u5728nuScenes\u4e0amAP 69.7%, NDS 72.1%/69.7% mAP, 72.1% NDS on nuScenes)\n    Results --\x3e R2(\u5728\u6a21\u62df\u6708\u7403\u73af\u5883\u4e0amAP 90.93%/90.93% mAP on simulated lunar environment)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] DreamOmni3: Scribble-based Editing and Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image editing and generation], [scribble-based editing, unified model, joint input scheme, data synthesis, multimodal instruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bin Xia, Bohao Peng, Jiyang Liu, Sitong Wu, Jingyao Li, Junjia Huang, Xu Zhao, Yitong Wang, Ruihang Chu, Bei Yu, Jiaya Jia"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong (CUHK), ByteDance Inc, The Hong Kong University of Science and Technology (HKUST)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22525",children:"https://arxiv.org/pdf/2512.22525"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/dvlab-research/DreamOmni3",children:"https://github.com/dvlab-research/DreamOmni3"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes two new tasks\u2014scribble-based editing and generation\u2014that combine text, images, and freehand sketches for more flexible and precise user control. 2. Introduces a comprehensive data synthesis pipeline to create training data for these tasks, including multiple sub-tasks like doodle editing and image fusion. 3. Designs a novel joint input scheme that feeds both original and scribbled images to the model, using color and shared encodings to precisely localize edit regions without relying on binary masks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces DreamOmni3, a unified model for scribble-based image editing and generation. It addresses the limitations of text-only instructions by allowing users to specify edits with freehand sketches, proposes a data creation pipeline and a joint input scheme for precise localization, and demonstrates outstanding performance on new benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DreamOmni3: Scribble-based Editing and Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Text prompts fail to capture precise edit locations and fine-grained details.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes scribble-based tasks, a data synthesis pipeline, and a joint input scheme using colored scribbles.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves outstanding performance; establishes benchmarks; releases models and code.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [closed-loop framework, entity-level memory, vision-language verification, pacing-aware editing, multi-agent collaboration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22536",children:"https://arxiv.org/pdf/2512.22536"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CoAgent: Coherent Video Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Open-domain video generation lacks coherence and consistency/\u5f00\u653e\u57df\u89c6\u9891\u751f\u6210\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027]\n    C --\x3e C1[Plan-Synthesize-Verify-Edit Pipeline/\u8ba1\u5212-\u5408\u6210-\u9a8c\u8bc1-\u7f16\u8f91\u6d41\u7a0b]\n    C1 --\x3e C2[Storyboard Planner/\u6545\u4e8b\u677f\u89c4\u5212\u5668]\n    C1 --\x3e C3[Global Context Manager/\u5168\u5c40\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668]\n    C1 --\x3e C4[Verifier Agent/\u9a8c\u8bc1\u667a\u80fd\u4f53]\n    C1 --\x3e C5[Pacing-aware Editor/\u8282\u594f\u611f\u77e5\u7f16\u8f91\u5668]\n    D --\x3e D1[Improves coherence, consistency, narrative quality/\u63d0\u5347\u8fde\u8d2f\u6027\u3001\u4e00\u81f4\u6027\u3001\u53d9\u4e8b\u8d28\u91cf]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied ai / robot learning], [vision-language-action models, benchmark, generalization, robustness, structured task design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22539",children:"https://arxiv.org/pdf/2512.22539"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL (from abstract)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Difficult to quantitatively understand the limits and failure modes of VLAs]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Structured benchmark with orthogonal difficulty axes (Task Structure, Language, Visual) and systematic perturbations]\n    D[\u5173\u952e\u7ed3\u679c/Results: Revealed critical VLA limitations (memorization, asymmetric robustness); Provided open-source framework for research]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal reasoning], [self-rewarded learning, process alignment, multimodal large language models, reasoning coherence, visual grounding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22545",children:"https://arxiv.org/pdf/2512.22545"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Self-Rewarded Multimodal Coherent Reasoning<br>\u81ea\u6211\u5956\u52b1\u591a\u6a21\u6001\u8fde\u8d2f\u63a8\u7406] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u7f3a\u9677/Existing Method Flaws<br>Supervises only final answer]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>SR-MCR Framework] --\x3e M1[\u81ea\u6211\u5956\u52b1/Self-Reward<br>Five intrinsic process cues]\n    Method --\x3e M2[\u4f18\u5316\u76ee\u6807/Optimization<br>GRPO with cooling mechanism]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>Evaluation & Ablation] --\x3e R1[\u6027\u80fd\u63d0\u5347/Performance Gain<br>SOTA accuracy (81.4%)]\n    Results --\x3e R2[\u6d88\u878d\u7814\u7a76/Ablation Study<br>Confirms contributions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [3D U-Net, Multi-scale Feature Fusion, Radiomics, Hybrid Upsampling, Residual Skip Mechanism]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Arefin Ittesafun Abian, Yan Zhang, Mirjam Jonkman, Sami Azam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," United International University, Monash University, Charles Darwin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22570",children:"https://arxiv.org/pdf/2512.22570"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ReFRM3D, a novel radiomics-enhanced fused residual multiparametric 3D network for brain tumor characterization, based on a 3D U-Net with multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. 2. Introduces a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented tumor regions. 3. Demonstrates state-of-the-art segmentation performance on multiple BraTS datasets (2019, 2020, 2021), achieving high Dice Similarity Coefficients for whole tumor, enhancing tumor, and tumor core."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses challenges in glioma segmentation and classification from multi-parametric MRI data by proposing ReFRM3D, a novel 3D network architecture enhanced with radiomics and multi-scale feature fusion. The method achieves superior segmentation performance on standard BraTS benchmarks, demonstrating its effectiveness for accurate brain tumor characterization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ReFRM3D: Glioma Characterization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Glioma diagnosis challenges: data variability, inefficient segmentation & classification]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: ReFRM3D network: 3D U-Net, multi-scale fusion, hybrid upsampling, residual skip, radiomics classifier]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High DSC scores on BraTS2019/2020/2021 datasets for WT, ET, TC segmentation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] KV-Tracker: Real-Time Pose Tracking with Transformers"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [pose tracking and reconstruction], [key-value caching, real-time tracking, multi-view geometry, transformer, online reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Imperial College London"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22581",children:"https://arxiv.org/pdf/2512.22581"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://marwan99.github.io/kv_tracker/",children:"https://marwan99.github.io/kv_tracker/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel method for adapting slow, multi-view 3D geometry networks for real-time online use by caching key-value pairs from the transformer's self-attention block. 2. A model-agnostic caching strategy that can be applied to off-the-shelf multi-view networks without retraining, enabling significant inference speedup. 3. Demonstration of the system on challenging tasks like on-the-fly object tracking and reconstruction from monocular RGB video without depth or object priors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that powerful multi-view 3D geometry networks are too slow for real-time applications. It proposes KV-Tracker, which adapts these networks for online use by selecting keyframes, running a full multi-view model on them, and then caching the transformer's key-value pairs to serve as a compact scene representation for fast, real-time pose tracking. This approach achieves up to 15x speedup and high frame rates (e.g., ~27 FPS) on standard datasets while preventing drift and catastrophic forgetting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("KV-Tracker: Real-Time Pose Tracking with Transformers") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u591a\u89c6\u89d23D\u7f51\u7edc\u901f\u5ea6\u6162/Multi-view 3D networks are slow")\n    Problem --\x3e P2("\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528/Difficult for real-time use")\n    Method --\x3e M1("\u9009\u62e9\u5e76\u7ba1\u7406\u5173\u952e\u5e27/Select & manage keyframes")\n    Method --\x3e M2("\u7f13\u5b58KV\u5bf9\u4f5c\u4e3a\u573a\u666f\u8868\u793a/Cache KV pairs as scene representation")\n    Method --\x3e M3("\u6a21\u578b\u65e0\u5173\u7684\u5728\u7ebf\u8ddf\u8e2a/Model-agnostic online tracking")\n    Results --\x3e R1("15\u500d\u63a8\u7406\u52a0\u901f/15x inference speedup")\n    Results --\x3e R2("\u9ad8\u8fbe27 FPS/Up to ~27 FPS")\n    Results --\x3e R3("\u9632\u6b62\u6f02\u79fb\u548c\u9057\u5fd8/Prevents drift & forgetting")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D facial animation], [style disentanglement, modality alignment, Graph Attention Networks, cross-attention, contrastive learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bin Wang, Yang Xu, Huan Zhao, Hao Zhang, Zixing Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hunan University, Central South University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22602",children:"https://arxiv.org/pdf/2512.22602"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework for personalized 3D talking head animation that preserves individual speaking styles through style disentanglement from audio and motion sequences. 2. A three-level modality alignment mechanism (spatial, temporal, feature) to enhance lip-synchronization accuracy between speech and 3D mesh. 3. Extensive experiments demonstrating superior performance in generating realistic, stylized animations compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PTalker, a framework for personalized speech-driven 3D talking head animation. It addresses the lack of individual speaking style in existing methods by disentangling style and content from audio/motion and improves lip-sync via a three-level audio-mesh alignment mechanism. Experiments show PTalker outperforms state-of-the-art methods in generating realistic and stylized animations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PTalker: Personalized Speech-Driven 3D Talking Head Animation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e2a\u6027\u5316\u8bf4\u8bdd\u98ce\u683c / Existing methods overlook individual speaking styles]\n    C --\x3e C1[\u98ce\u683c\u89e3\u8026 / Style Disentanglement]\n    C --\x3e C2[\u6a21\u6001\u5bf9\u9f50 / Modality Alignment]\n    C1 --\x3e C1a[\u4ece\u97f3\u9891\u548c\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5206\u79bb\u98ce\u683c\u4e0e\u5185\u5bb9 / Separate style & content from audio & motion]\n    C2 --\x3e C2a[\u7a7a\u95f4\u5bf9\u9f50 / Spatial Alignment: Graph Attention Networks]\n    C2 --\x3e C2b[\u65f6\u95f4\u5bf9\u9f50 / Temporal Alignment: Cross-Attention]\n    C2 --\x3e C2c[\u7279\u5f81\u5bf9\u9f50 / Feature Alignment: Contrastive Loss & KL Divergence]\n    D --\x3e D1[\u751f\u6210\u903c\u771f\u3001\u4e2a\u6027\u5316\u76843D\u8bf4\u8bdd\u5934 / Generates realistic, personalized 3D talking heads]\n    D --\x3e D2[\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5 / Outperforms state-of-the-art methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [location-based recommendation], [multi-modal learning, spatial-temporal knowledge graph, cross-modal alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Hangzhou City University, Nanjing University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22605",children:"https://arxiv.org/pdf/2512.22605"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://anonymous.4open.science/r/M3ob-62EF",children:"https://anonymous.4open.science/r/M3ob-62EF"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited generalization of next location recommendation methods by proposing M\xb3ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\nA["Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap."]\nA --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment."]\nA --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [diffusion language model, vision-language-action, parallel generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Huawei Technologies"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22615",children:"https://arxiv.org/pdf/2512.22615"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Dream-VL, a state-of-the-art open diffusion-based Vision-Language Model (dVLM) that matches top AR-based VLMs on benchmarks and excels at visual planning. 2. Introduces Dream-VLA, a diffusion-based Vision-Language-Action model built upon Dream-VL, leveraging the bidirectional nature of diffusion for superior action chunking and faster fine-tuning convergence. 3. Demonstrates that diffusion-based VLMs/VLAs outperform autoregressive baselines on downstream tasks, achieving top-tier performance on robotic benchmarks like LIBERO, SimplerEnv-Bridge, and SimplerEnv-Fractal."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes building Vision-Language and Vision-Language-Action models on diffusion-based language models to overcome the limitations of autoregressive models in complex planning and control. The introduced models, Dream-VL and Dream-VLA, leverage the bidirectional, parallel generation nature of diffusion for superior performance in visual planning and robotic tasks, achieving state-of-the-art results on key benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Dream-VL & Dream-VLA<br>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[AR\u6a21\u578b\u5728\u89c6\u89c9\u89c4\u5212\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u5c40\u9650<br>\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6784\u5efaVLM\u548cVLA\u6a21\u578b<br>\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd<br>\u5173\u952e\u7ed3\u679c/Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [face clustering], [Sparse Differential Transformer, Top-K Jaccard similarity, noise resilience]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dafeng Zhang, Yongqi Song, Shizhuo Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Samsung R&D Institute China-Beijing (SRC-B)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22612",children:"https://arxiv.org/pdf/2512.22612"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a prediction-driven Top-K Jaccard similarity coefficient to enhance neighbor purity and similarity measurement reliability. 2. Developed a Transformer-based model to predict relationships near the Top-K boundary for more accurate similarity estimation. 3. Introduced a Sparse Differential Transformer (SDT) to eliminate noise from irrelevant feature relationships and improve the model's anti-noise capability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of noise in face clustering caused by irrelevant nodes in Jaccard similarity measurements. The authors propose a Sparse Differential Transformer (SDT) to predict and refine Top-K Jaccard similarity, enhancing noise resilience. Experiments on datasets like MS-Celeb-1M show the method achieves state-of-the-art performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5f15\u5165\u8fc7\u591a\u65e0\u5173\u8282\u70b9\uff0cJaccard\u7cfb\u6570\u5224\u522b\u529b\u6709\u9650/Existing methods introduce too many irrelevant nodes, Jaccard coefficient has limited discriminative power]\n    C --\x3e C1[\u63d0\u51fa\u9884\u6d4b\u9a71\u52a8\u7684Top-K Jaccard\u76f8\u4f3c\u5ea6\u7cfb\u6570/Propose prediction-driven Top-K Jaccard similarity coefficient]\n    C --\x3e C2[\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u9884\u6d4b\u6a21\u578b/Develop a Transformer-based prediction model]\n    C --\x3e C3[\u63d0\u51fa\u7a00\u758f\u5dee\u5206Transformer (SDT) \u6d88\u9664\u566a\u58f0/Propose Sparse Differential Transformer (SDT) to eliminate noise]\n    D --\x3e D1[\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd/Achieves SOTA performance on multiple datasets]\n    D --\x3e D2[\u4e3a\u9762\u90e8\u805a\u7c7b\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848/Provides a more robust solution for face clustering]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Rethinking Memory Design in SAM-Based Visual Object Tracking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual object tracking], [Segment Anything Model, memory mechanism, hybrid memory, distractor-resolving, occlusion robustness]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Khalifa University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22624",children:"https://arxiv.org/pdf/2512.22624"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/HamadYA/SAM3_Tracking_Zoo",children:"https://github.com/HamadYA/SAM3_Tracking_Zoo"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a systematic analysis of memory design in SAM-based trackers, revealing that methods primarily differ in short-term memory frame selection. 2. Reimplemented and evaluated existing memory mechanisms within the SAM3 framework across ten benchmarks for a controlled analysis. 3. Proposed a unified hybrid memory framework that decomposes memory into short-term appearance and long-term distractor-resolving components, improving robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper systematically studies memory design in SAM-based visual object tracking. It analyzes existing methods, reimplements their memory mechanisms in SAM3, and proposes a unified hybrid memory framework. The framework improves tracking robustness in challenging scenarios like occlusion and distractors for both SAM2 and SAM3 backbones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Rethinking Memory Design in SAM-Based Visual Object Tracking] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u5bf9\u5185\u5b58\u7684\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3/Lack of systematic understanding of memory design in SAM-based tracking]\n    Problem --\x3e P2[\u5185\u5b58\u673a\u5236\u5728\u66f4\u5f3a\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u8fc1\u79fb\u6548\u679c\u672a\u77e5/Unclear how memory mechanisms transfer to next-gen models like SAM3]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5206\u6790\u4ee3\u8868\u6027SAM2\u8ddf\u8e2a\u5668/Analyze representative SAM2-based trackers]\n    Method --\x3e M2[\u5728SAM3\u4e2d\u5fe0\u5b9e\u590d\u73b0\u5185\u5b58\u673a\u5236/Faithfully reimplement memory mechanisms in SAM3]\n    Method --\x3e M3[\u63d0\u51fa\u7edf\u4e00\u7684\u6df7\u5408\u5185\u5b58\u6846\u67b6/Propose a unified hybrid memory framework]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5b9e\u73b0\u5927\u89c4\u6a21\u57fa\u51c6\u8bc4\u4f30/Conduct large-scale benchmark evaluations]\n    Results --\x3e R2[\u5728\u906e\u6321\u3001\u590d\u6742\u8fd0\u52a8\u7b49\u573a\u666f\u63d0\u5347\u9c81\u68d2\u6027/Improve robustness under occlusion, complex motion, distractors]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [embodied visual planning], [video diffusion, goal-conditioned generation, embodied agents, visual imagination, first-and-last-frame-conditioned model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuming Gu, Yizhi Wang, Yining Hong, Yipeng Gao, Hao Jiang, Angtian Wang, Bo Liu, Nathaniel S. Dennler, Zhengfei Kuang, Hao Li, Gordon Wetzstein, Chongyang Ma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California, ByteDance, Stanford University, Massachusetts Institute of Technology, MBZUAI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22626",children:"https://arxiv.org/pdf/2512.22626"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://envision-paper.github.io/",children:"https://envision-paper.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Envision, a two-stage diffusion framework for embodied visual planning that explicitly uses a goal image to constrain trajectory generation. 2. Introduces a Goal Imagery Model that synthesizes a coherent goal image by identifying task-relevant regions and performing region-aware cross attention. 3. Develops an Env-Goal Video Model based on a first-and-last-frame-conditioned video diffusion model (FL2V) to interpolate smooth, physically plausible video trajectories between start and goal states."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of spatial drift and goal misalignment in embodied visual planning by proposing Envision, a two-stage diffusion framework that first generates a goal image and then creates a video trajectory connecting the initial scene to that goal. This method enforces goal consistency and physical plausibility. Experiments show it outperforms baselines in goal alignment and spatial consistency, providing reliable visual plans for robotic control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root["Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion"] --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u73b0\u6709\u65b9\u6cd5\u4e3a\u524d\u5411\u9884\u6d4b/Existing approaches are forward predictive"]\n    Problem --\x3e P2["\u5bfc\u81f4\u7a7a\u95f4\u6f02\u79fb\u548c\u76ee\u6807\u9519\u4f4d/Leading to spatial drift & goal misalignment"]\n    Method --\x3e M1["\u4e24\u9636\u6bb5\u6846\u67b6/Two-stage framework"]\n    M1 --\x3e M1_1["\u76ee\u6807\u56fe\u50cf\u6a21\u578b/Goal Imagery Model"]\n    M1_1 --\x3e M1_1a["\u5408\u6210\u76ee\u6807\u56fe\u50cf/Synthesizes goal image"]\n    M1 --\x3e M1_2["\u73af\u5883-\u76ee\u6807\u89c6\u9891\u6a21\u578b/Env-Goal Video Model"]\n    M1_2 --\x3e M1_2a["\u57fa\u4e8eFL2V\u7684\u63d2\u503c/FL2V-based interpolation"]\n    Results --\x3e R1["\u76ee\u6807\u5bf9\u9f50\u66f4\u4f18/Superior goal alignment"]\n    Results --\x3e R2["\u7a7a\u95f4\u4e00\u81f4\u6027\u66f4\u597d/Better spatial consistency"]\n    Results --\x3e R3["\u652f\u6301\u673a\u5668\u4eba\u89c4\u5212/Supports robotic planning"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image super-resolution], [reinforcement learning from human feedback (RLHF), reward hacking, perceptual quality, curriculum learning, fine-grained assessment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yidi Liu, Zihao Fan, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Xueyang Fu, Zheng-Jun Zha"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Shanghai AI Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22647",children:"https://arxiv.org/pdf/2512.22647"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Fine-grained Perceptual Reward Model (FinPercep-RM) with an encoder-decoder architecture that outputs both a global quality score and a Perceptual Degradation Map to localize defects. 2. Introduces the FGR-30k dataset containing diverse and subtle distortions from real-world super-resolution models for training the reward model. 3. Designs a Co-evolutionary Curriculum Learning (CCL) mechanism that synchronizes the progressive training of the reward model and the ISR model to ensure stable training and suppress reward hacking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of reward hacking in RLHF-based Image Super-Resolution, where traditional Image Quality Assessment models are insensitive to local distortions. The authors propose a fine-grained reward model (FinPercep-RM) and a co-evolutionary curriculum learning strategy to provide localized feedback and stabilize training. Experiments show the method improves both global quality and local realism in generated images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u4f20\u7edfIQA\u6a21\u578b\u5bf9\u5c40\u90e8\u5931\u771f\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u5956\u52b1\u6b3a\u9a97/Reward Hacking]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>1. \u7ec6\u7c92\u5ea6\u611f\u77e5\u5956\u52b1\u6a21\u578b (FinPercep-RM)<br>2. \u534f\u540c\u8fdb\u5316\u8bfe\u7a0b\u5b66\u4e60 (CCL)]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u63d0\u5347\u5168\u5c40\u8d28\u91cf\u4e0e\u5c40\u90e8\u771f\u5b9e\u611f\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3/Improves global quality & local realism, enables stable training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Visual Autoregressive Modelling for Monocular Depth Estimation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [monocular depth estimation], [visual autoregressive modelling, classifier-free guidance, scale-wise conditional upsampling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amir El-Ghoussani, Andr\xe9 Kaup, Nassir Navab, Gustavo Carneiro, Vasileios Belagiannis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Friedrich-Alexander University Erlangen-Nuremberg, Technical University of Munich, University of Surrey"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22653",children:"https://arxiv.org/pdf/2512.22653"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AmirMaEl/VAR-Depth",children:"https://github.com/AmirMaEl/VAR-Depth"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a monocular depth estimation method based on visual autoregressive (VAR) priors as an alternative to diffusion models. 2. Introduces a scale-wise conditional upsampling mechanism with classifier-free guidance for the task. 3. Demonstrates the method's efficiency (10-stage inference, 74K fine-tuning samples) and strong performance, achieving state-of-the-art results on indoor benchmarks under constrained training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new method for monocular depth estimation that uses visual autoregressive (VAR) priors instead of diffusion models. The approach adapts a large text-to-image VAR model with a novel scale-wise upsampling mechanism and achieves competitive results with efficient fine-tuning. The work establishes autoregressive models as a complementary, data-scalable family of generative models for 3D vision tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Visual Autoregressive Modelling for Monocular Depth Estimation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Monocular depth estimation as an ill-posed task)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Adapt VAR priors with scale-wise upsampling & classifier-free guidance)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: SOTA indoor performance, strong outdoor results, efficient fine-tuning)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela S\xfcsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University Hospital Bonn, Technical University of Munich"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22666",children:"https://arxiv.org/pdf/2512.22666"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["INTERACT-CMIL: CMIL\u5206\u7ea7<br>INTERACT-CMIL: CMIL Grading"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>CMIL\u5206\u7ea7\u56f0\u96be\uff0c\u4e3b\u89c2\u6027\u5f3a<br>CMIL grading is difficult and subjective"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u591a\u4efb\u52a1\u5171\u4eab\u5b66\u4e60\u4e0e\u4efb\u52a1\u95f4\u4e00\u81f4\u6027<br>Multi-task Shared Learning & Inter-Task Consistency"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u9884\u6d4b<br>Significant performance gains, interpretable predictions"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [transfer learning / domain adaptation], [Cluster Attention Adapter, adapter tuning, data-limited domains, vision foundation models, adaptive transfer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22664",children:"https://arxiv.org/pdf/2512.22664"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/qklee-lz/CLAdapter",children:"https://github.com/qklee-lz/CLAdapter"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e0b\u6e38\u4efb\u52a1\u6570\u636e\u7a00\u7f3a/Data-limited downstream tasks]\n    C --\x3e C1[\u63d0\u51faCLAdapter/Propose CLAdapter]\n    C1 --\x3e C2[\u6ce8\u610f\u529b\u4e0e\u805a\u7c7b\u4e2d\u5fc3/Attention & Cluster Centers]\n    C2 --\x3e C3[\u4e2a\u6027\u5316\u7279\u5f81\u589e\u5f3a/Personalized Feature Enhancement]\n    D --\x3e D1[10\u4e2a\u6570\u636e\u96c6\u5b9e\u9a8c/Experiments on 10 datasets]\n    D1 --\x3e D2[\u5b9e\u73b0SOTA\u6027\u80fd/Achieves SOTA performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shravan Saranyan, Pramit Saha"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Branham High School, University of Oxford"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22657",children:"https://arxiv.org/pdf/2512.22657"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u624b\u52a8\u8bc4\u4f30LVEF\u8017\u65f6\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02/Manual LVEF assessment is time-consuming and has inter-observer variability]\n    C --\x3e C1[\u7cfb\u7edf\u8bc4\u4f303D Inception\u3001\u53cc\u6d41\u548cCNN-RNN\u67b6\u6784/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]\n    C --\x3e C2[\u5728EchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30/Train and evaluate on the EchoNet-Dynamic dataset]\n    D --\x3e D1[\u6539\u8fdb\u76843D Inception\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0cRMSE\u4e3a6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]\n    D --\x3e D2[\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u66f4\u597d/Smaller, simpler models generalize better]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [text-to-image generation], [inference-time refinement, semantic critique, spectral fusion, diffusion models, vision-language model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," ZhenQi Chen, TsaiChing Ni, YuanFu Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22681",children:"https://arxiv.org/pdf/2512.22681"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces CritiCore, a multimodal semantic critique module using VLM and LLMs to provide high-level feedback for better prompt alignment. 2. Proposes SpecFusion, a frequency-domain method to merge intermediate generation states, preserving high-frequency details while injecting structure. 3. Presents CritiFusion as a plug-in, training-free framework compatible with existing diffusion backbones, improving both semantic correspondence and visual quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of text-to-image diffusion models struggling with semantic alignment to complex prompts. It proposes CritiFusion, an inference-time framework that uses a semantic critique module and spectral fusion to refine generated images, improving faithfulness and detail without requiring additional training. The method achieves results competitive with state-of-the-art reward optimization approaches on standard benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Diffusion models struggle with semantic alignment to complex prompts.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Inference-time framework with CritiCore for semantic critique and SpecFusion for spectral refinement.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improves text-image correspondence and visual quality; competitive with SOTA.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Autoregressive Flow Matching for Motion Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [motion prediction], [autoregressive flow matching, point tracks, probabilistic modeling, video conditioning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22688",children:"https://arxiv.org/pdf/2512.22688"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Johnathan-Xie/arfm-motion-prediction",children:"https://github.com/Johnathan-Xie/arfm-motion-prediction"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data. 2. Develops benchmarks for evaluating motion prediction models on human and robot motion. 3. Demonstrates that conditioning downstream tasks (robot action prediction, human motion prediction) on predicted future tracks improves performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of predicting future motion from videos, which requires understanding agent behavior and physics. It proposes Autoregressive Flow Matching (ARFM), a method trained on diverse video datasets to probabilistically generate future point track locations. The model shows the ability to predict complex motions and its predictions can significantly improve downstream task performance in robotics and human motion analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Autoregressive Flow Matching for Motion Prediction") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u73b0\u6709\u6a21\u578b\u6cdb\u5316\u6027\u5dee\u6216\u8fd0\u52a8\u5efa\u6a21\u4e0d\u51c6 / Existing models lack generality or accurate motion modeling")\n    Method --\x3e M1("\u63d0\u51fa\u81ea\u56de\u5f52\u6d41\u5339\u914d / Propose Autoregressive Flow Matching (ARFM)")\n    Method --\x3e M2("\u5728\u5927\u89c4\u6a21\u591a\u6837\u5316\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3 / Train on large-scale diverse video datasets")\n    Method --\x3e M3("\u9884\u6d4b\u672a\u6765\u70b9\u8f68\u8ff9\u4f4d\u7f6e / Predict future point track locations")\n    Results --\x3e R1("\u80fd\u9884\u6d4b\u590d\u6742\u8fd0\u52a8 / Capable of predicting complex motions")\n    Results --\x3e R2("\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd / Improves downstream task performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Salvador Rodriguez-Sanz, Monica Hernandez"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Zaragoza, Aragon Institute for Engineering Research (I3A)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22689",children:"https://arxiv.org/pdf/2512.22689"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u7b97\u6cd5\u5047\u8bbe\u5f3a\u5ea6\u76f8\u5173\uff0c\u9650\u4e8e\u5355\u6a21\u6001/Traditional methods assume intensity correlation, limited to monomodal]\n    B --\x3e B2[\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6cdb\u5316\u6027\u5dee/Learning-based models need large datasets, poor generalization]\n    C --\x3e C1[\u57fa\u4e8e\u5b9e\u4f8b\u7684\u6846\u67b6/Instance-specific framework]\n    C --\x3e C2[\u4f7f\u7528\u795e\u7ecfODE\u4e0e\u7ed3\u6784\u63cf\u8ff0\u7b26/Using Neural ODEs & Structural Descriptors]\n    C --\x3e C3[\u6574\u5408\u5c40\u90e8\u4e92\u4fe1\u606f/Integrating Local Mutual Information]\n    D --\x3e D1[\u8d85\u8d8aSOTA\u7ed3\u679c/Surpassing SOTA results]\n    D --\x3e D2[\u5bf9\u6b63\u5219\u5316\u9c81\u68d2/Robust to varying regularization]\n    D --\x3e D3[\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u5c3a\u5ea6/Efficient & suitable for varying scales]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [motion capture, wearable computing, human-computer interaction], [IMU, WebXR, SLAM, IoT, edge computing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidan\xe9, Tejaswi Gowda"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22690",children:"https://arxiv.org/pdf/2512.22690"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5\xb0 joint-angle error) at approximately 5% of the cost, with low latency and high reliability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Motion capture is costly and complex, limiting accessibility] --\x3e P1[\u6602\u8d35\u4e14\u590d\u6742/Expensive & Complex]\n    Problem --\x3e P2[\u5c40\u9650\u4e8e\u4e13\u4e1a\u5b9e\u9a8c\u5ba4/Limited to Specialized Labs]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Open-source, low-cost system using IoT sensors and web tech] --\x3e M1[\u8eab\u4f53\u4f69\u6234IMU\u4f20\u611f\u5668\u7f51\u7edc/Body-worn IMU Sensor Network]\n    Method --\x3e M2[\u667a\u80fd\u624b\u673aWebXR SLAM\u5b9a\u4f4d/Smartphone WebXR SLAM for Positioning]\n    Method --\x3e M3[\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u5e94\u7528/Web-Browser-Based Application]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Affordable, accurate, and real-time performance] --\x3e R1[\u6210\u672c\u7ea6\u4e3a\u5546\u4e1a\u7cfb\u7edf\u76845%/~5% Cost of Commercial System]\n    Results --\x3e R2[\u5e73\u5747\u5173\u8282\u89d2\u5ea6\u8bef\u5dee2-5\u5ea6/Mean Joint-Angle Error 2-5\xb0]\n    Results --\x3e R3[\u5b9e\u65f6\u4f4e\u5ef6\u8fdf/Real-Time with Low Latency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [novel view synthesis], [3D Gaussian Splatting, diffusion models, autonomous driving simulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Calgary"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22706",children:"https://arxiv.org/pdf/2512.22706"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified framework (SCPainter) that jointly handles realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. 2. Integration of 3D Gaussian Splatting representations for assets with 3D scene point clouds and a diffusion model for high-quality image generation. 3. Demonstration of the framework's capability to create diverse and realistic driving data on the Waymo Open Dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SCPainter, a unified framework that combines 3D Gaussian Splatting for asset representation with diffusion models to jointly perform realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. The method projects 3D assets and scene point clouds into novel views and uses them to condition a diffusion model to generate high-quality images. Evaluation shows the framework can create diverse and realistic driving scenarios for training data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SCPainter: \u7edf\u4e00\u6846\u67b6 / Unified Framework] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem: \u73b0\u6709\u65b9\u6cd5\u5b64\u7acb\u5904\u7406\u8d44\u4ea7\u63d2\u5165\u4e0eNVS / Existing methods treat asset insertion and NVS in isolation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method: \u96c6\u62103D\u9ad8\u65af\u6e85\u5c04\u4e0e\u6269\u6563\u6a21\u578b / Integrates 3D Gaussian Splatting and diffusion models]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results: \u5728Waymo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u771f\u5b9e\u611f\u751f\u6210 / Enables realistic generation on Waymo dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Memento-II: Learning by Stateful Reflective Memory"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jun Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University College London (UCL)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22716",children:"https://arxiv.org/pdf/2512.22716"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Memento-II: Learning by Stateful Reflective Memory] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca/Lack of theoretical explanation for memory-based continual learning in LLM agents]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u72b6\u6001\u5316\u53cd\u601d\u51b3\u7b56\u8fc7\u7a0b/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u4e0e\u6536\u655b\u4fdd\u8bc1/Provides theoretical framework and convergence guarantees for optimal policy]\n    C --\x3e E[\u5199\u5165\u5bf9\u5e94\u7b56\u7565\u8bc4\u4f30/Writing corresponds to policy evaluation]\n    C --\x3e F[\u8bfb\u53d6\u5bf9\u5e94\u7b56\u7565\u6539\u8fdb/Reading corresponds to policy improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [self-supervised learning, masked autoencoding, Score-CAM, prenatal ultrasound, cystic hygroma]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Carleton University, Ottawa Hospital Research Institute, University of Ottawa, Children's Hospital of Eastern Ontario Research Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22730",children:"https://arxiv.org/pdf/2512.22730"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Applied and fine-tuned a large-scale, ultrasound-specific self-supervised model (USF-MAE) for the task of cystic hygroma detection. 2. Demonstrated that this self-supervised pre-training approach significantly outperforms a supervised baseline (DenseNet-169) trained from scratch on a small labeled dataset. 3. Provided qualitative interpretability analysis using Score-CAM visualizations to show the model's focus on clinically relevant anatomical regions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of automated cystic hygroma detection in prenatal ultrasound, where labeled data is scarce. The authors propose fine-tuning a self-supervised model (USF-MAE) pre-trained on a large corpus of unlabeled ultrasound images. Their method significantly outperforms a supervised baseline, demonstrating that ultrasound-specific self-supervised learning enables accurate, robust, and data-efficient detection for early prenatal screening."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Improved cystic hygroma detection using ultrasound-specific self-supervised learning<br>\u57fa\u4e8e\u8d85\u58f0\u7279\u5f02\u6027\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u56ca\u6027\u6c34\u7624\u68c0\u6d4b\u6539\u8fdb"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Supervised deep learning for cystic hygroma detection is limited by small labeled datasets.<br>\u76d1\u7763\u5f0f\u6df1\u5ea6\u5b66\u4e60\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u3002"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Fine-tune USF-MAE, a self-supervised model pre-trained on 370k+ unlabeled ultrasound images.<br>\u5fae\u8c03\u572837\u4e07+\u65e0\u6807\u7b7e\u8d85\u58f0\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684USF-MAE\u81ea\u76d1\u7763\u6a21\u578b\u3002"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Outperformed DenseNet-169 baseline (Accuracy: 0.96 vs 0.93). Performance gains are statistically significant.<br>\u6027\u80fd\u8d85\u8d8aDenseNet-169\u57fa\u7ebf\uff0c\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [4D scene reconstruction], [Freetime FeatureGS, contrastive loss, streaming feature learning, Gaussian primitives, decomposed reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yongzhen Hu, Yihui Yang, Haotong Lin, Yifan Wang, Junting Dong, Yifu Deng, Xinyu Zhu, Fan Jia, Hujun Bao, Xiaowei Zhou, Sida Peng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Ant Group, Shanghai AI Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22745",children:"https://arxiv.org/pdf/2512.22745"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel decomposed 4D scene reconstruction method that eliminates the need for unstable video segmentation by relying only on per-image segmentation maps. 2. Introduces Freetime FeatureGS, a representation using Gaussian primitives with learnable features and linear motion, and a contrastive loss to enforce instance-level feature consistency. 3. Designs a temporally ordered streaming training strategy to propagate features over time, avoiding local minima and achieving accurate 4D segmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses decomposed 4D scene reconstruction from multi-view videos by proposing a method that avoids reliance on video segmentation. The core idea is to represent the scene with a novel Freetime FeatureGS model and train it with a contrastive loss and a streaming strategy using only per-frame segmentation. Experiments show the method outperforms recent state-of-the-art approaches in reconstruction quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u4f9d\u8d56\u89c6\u9891\u5206\u5272\u76844D\u91cd\u5efa\u65b9\u6cd5\u4e0d\u7a33\u5b9a/Reliance on video segmentation leads to unstable 4D reconstruction]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faFreetime FeatureGS\u4e0e\u6d41\u5f0f\u7279\u5f81\u5b66\u4e60/Propose Freetime FeatureGS and streaming feature learning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Outperforms recent methods on several datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [visual token pruning, long context, adaptive budget allocation, intra-image diversity, inter-image variation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hao Zhang, Mengsi Lyu, Bo Huang, Yulong Ao, Yonghua Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Academy of Artificial Intelligence (BAAI)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22748",children:"https://arxiv.org/pdf/2512.22748"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the specific challenges of visual token pruning in long-context, multi-image scenarios. 2. Proposes a novel two-stage adaptive pruning method that decomposes redundancy into intra-image and inter-image components, guided by diversity and variation metrics. 3. Introduces a Pareto selection procedure in the inter-image stage to balance global token diversity with text alignment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high inference cost of Large Multimodal Models (LMMs) in long-context, multi-image settings by proposing TrimTokenator-LC, an adaptive visual token pruning method. The method uses a two-stage process to dynamically allocate token budgets based on intra-image diversity and inter-image variation, then selects tokens by balancing diversity and text relevance. Experiments show the approach significantly reduces the number of visual tokens while maintaining strong performance in long-context tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u591a\u56fe\u50cf\u63a8\u7406\u6210\u672c\u9ad8/High inference cost from redundant visual tokens in long-context multi-image settings]\n    C --\x3e C1[\u5206\u89e3\u5197\u4f59\u4e3a\u56fe\u50cf\u5185\u548c\u56fe\u50cf\u95f4/Decompose redundancy into intra-image and inter-image]\n    C --\x3e C2[\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u526a\u679d/Two-stage adaptive pruning]\n    C2 --\x3e C2_1[\u56fe\u50cf\u5185\u9636\u6bb5\uff1a\u5185\u5bb9\u611f\u77e5\u9884\u7b97\u4e0e\u8d2a\u5a6a\u9009\u62e9/Intra-image: content-aware budget & greedy selection]\n    C2 --\x3e C2_2[\u56fe\u50cf\u95f4\u9636\u6bb5\uff1a\u5168\u5c40\u591a\u6837\u6027\u8fc7\u6ee4\u4e0e\u5e15\u7d2f\u6258\u9009\u62e9/Inter-image: global diversity filtering & Pareto selection]\n    D --\x3e D1[\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u5f3a\u6027\u80fd/Maintains strong performance in long context]\n    D --\x3e D2[\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u6570\u91cf/Significantly cuts down visual tokens]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision transformer efficiency], [token reduction, Hilbert curve, neighbor-aware pruning, token merging, spatial continuity]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yunge Li, Lanyu Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Oakland University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22760",children:"https://arxiv.org/pdf/2512.22760"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Yunge6666/NAP-MAT",children:"https://github.com/Yunge6666/NAP-MAT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel neighbor-aware token reduction framework for Vision Transformers that explicitly preserves spatial continuity and local context. 2. Introduces Neighbor-Aware Pruning (NAP), which incorporates the influence of neighboring tokens into importance scoring for selective retention. 3. Introduces Merging by Adjacent Token similarity (MAT), a localized token aggregation strategy that computes similarity and merges tokens only within adjacent regions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational inefficiency of Vision Transformers caused by redundant token representations. It proposes a novel token reduction method based on Hilbert curve reordering, which includes Neighbor-Aware Pruning (NAP) and Merging by Adjacent Token similarity (MAT) to preserve spatial neighbor structure. The approach achieves state-of-the-art accuracy-efficiency trade-offs, highlighting the importance of spatial continuity for ViT optimization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("ViT\u8ba1\u7b97\u6548\u7387\u4f4e/ViT Computational Inefficiency")\n    Problem --\x3e P2("\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7a7a\u95f4\u8fde\u7eed\u6027/Existing Methods Overlook Spatial Continuity")\n    Method --\x3e M1("Hilbert\u66f2\u7ebf\u91cd\u6392\u5e8f/Hilbert Curve Reordering")\n    Method --\x3e M2("\u90bb\u5c45\u611f\u77e5\u526a\u679d(NAP)/Neighbor-Aware Pruning (NAP)")\n    Method --\x3e M3("\u76f8\u90bb\u4ee4\u724c\u5408\u5e76(MAT)/Merging by Adjacent Token similarity (MAT)")\n    Results --\x3e R1("SOTA\u7cbe\u5ea6-\u6548\u7387\u6743\u8861/SOTA Accuracy-Efficiency Trade-off")\n    Results --\x3e R2("\u5f3a\u8c03\u7a7a\u95f4\u8fde\u7eed\u6027/Highlights Spatial Continuity")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3d reconstruction], [3D Gaussian Splatting, Next Best View, Active Learning, Fisher Information, Dynamic Scene Modeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiqian Li, Wen Jiang, Kostas Daniilidis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Pennsylvania"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22771",children:"https://arxiv.org/pdf/2512.22771"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Truong Son Nguyen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22774",children:"https://arxiv.org/pdf/2512.22774"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Schr\xf6dinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Schr\xf6dinger AI: A Unified Spectral-Dynamical Framework<br>\u859b\u5b9a\u8c14AI: \u7edf\u4e00\u8c31-\u52a8\u529b\u5b66\u6846\u67b6"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Limitations of conventional ML<br>\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u5c40\u9650"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Quantum-inspired framework<br>\u91cf\u5b50\u542f\u53d1\u7684\u6846\u67b6"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Empirical demonstrations<br>\u5b9e\u8bc1\u6f14\u793a"]\n    Problem --\x3e P1["Struggle with uncertainty & adaptation<br>\u96be\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4e0e\u9002\u5e94\u6027"]\n    Problem --\x3e P2["Brittle symbolic reasoning<br>\u8106\u5f31\u7684\u7b26\u53f7\u63a8\u7406"]\n    Method --\x3e M1["Time-independent wave-energy solver<br>\u65f6\u95f4\u65e0\u5173\u6ce2\u80fd\u6c42\u89e3\u5668"]\n    Method --\x3e M2["Time-dependent dynamical solver<br>\u65f6\u95f4\u76f8\u5173\u52a8\u529b\u5b66\u6c42\u89e3\u5668"]\n    Method --\x3e M3["Low-rank operator calculus<br>\u4f4e\u79e9\u7b97\u5b50\u6f14\u7b97"]\n    Results --\x3e R1["Emergent semantic manifolds<br>\u6d8c\u73b0\u7684\u8bed\u4e49\u6d41\u5f62"]\n    Results --\x3e R2["Dynamic reasoning adaptation<br>\u52a8\u6001\u63a8\u7406\u9002\u5e94"]\n    Results --\x3e R3["Exact operator generalization<br>\u7cbe\u786e\u7b97\u5b50\u6cdb\u5316"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Plug In, Grade Right: Psychology-Inspired AGIQA"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image quality assessment], [AGIQA, Graded Response Model, semantic drift, quality grading, plug-and-play]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhicheng Liao, Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Weisi Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," South China Normal University, City University of Hong Kong, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22780",children:"https://arxiv.org/pdf/2512.22780"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and defines the "semantic drift" problem in existing AGIQA models where image embeddings show inconsistent similarity to multi-grade text descriptions. 2. Proposes a psychology-inspired, improved Graded Response Model (GRM) for AGIQA, framing quality as an image\'s "ability" to meet "difficulty" levels. 3. Designs a novel Arithmetic GRM based Quality Grading (AGQG) module that enforces a unimodal, interpretable quality distribution and demonstrates plug-and-play performance gains across various frameworks.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the "semantic drift" issue in AI-generated image quality assessment (AGIQA), where inconsistent similarities between image and text embeddings degrade reliability. The authors propose a novel Arithmetic GRM based Quality Grading (AGQG) module, inspired by psychometrics, which models image quality as an ability to overcome graded difficulty levels. The plug-and-play module consistently improves state-of-the-art AGIQA models and generalizes to both natural and screen content images.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Plug In, Grade Right: Psychology-Inspired AGIQA"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u8bed\u4e49\u6f02\u79fb/Semantic Drift"]\n    Problem --\x3e P2["\u591a\u6a21\u6001\u76f8\u4f3c\u5ea6\u5206\u5e03/Multimodal Similarity"]\n    Method --\x3e M1["\u5206\u7ea7\u54cd\u5e94\u6a21\u578b/Graded Response Model (GRM)"]\n    Method --\x3e M2["\u53cc\u5206\u652f\u6a21\u5757/Two-branch Module"]\n    Method --\x3e M3["\u7b97\u672f\u96be\u5ea6\u751f\u6210/Arithmetic Difficulty"]\n    Results --\x3e R1["\u5373\u63d2\u5373\u7528\u63d0\u5347/Plug-and-Play Improvement"]\n    Results --\x3e R2["\u6cdb\u5316\u80fd\u529b/Generalization"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object tracking], [vision-language tracking, multimodal large language model, visual prompt, global search, location-aware]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," East China Normal University, Westlake University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22799",children:"https://arxiv.org/pdf/2512.22799"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/jcwang0602/VPTracker",children:"https://github.com/jcwang0602/VPTracker"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first global vision-language tracking framework based on Multimodal Large Language Models (MLLMs)., 2. Proposes a location-aware visual prompting mechanism to incorporate spatial priors and suppress distractions., 3. Demonstrates enhanced tracking stability and target disambiguation in challenging scenarios, opening a new avenue for MLLM integration in visual tracking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes VPTracker, a novel global vision-language tracking framework that leverages Multimodal Large Language Models (MLLMs) for robust target localization across the entire image. To address distractions from global search, it introduces a location-aware visual prompting mechanism that uses the target's previous location as a spatial prior. Experiments show the method significantly improves tracking stability and disambiguation under challenging conditions like occlusions and rapid motion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VPTracker: Global Vision-Language Tracking] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u5728\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u3001\u5feb\u901f\u8fd0\u52a8\u4e0b\u6613\u5931\u6548/Existing local search methods prone to failure under viewpoint changes, occlusions, rapid motion]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8eMLLM\u7684\u5168\u5c40\u8ddf\u8e2a\u6846\u67b6 + \u4f4d\u7f6e\u611f\u77e5\u89c6\u89c9\u63d0\u793a/Global tracking via MLLM + Location-aware visual prompt]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u76ee\u6807\u8fa8\u522b\u80fd\u529b/Significantly enhances tracking stability and target disambiguation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [3D Gaussian Representation, tri-plane representation, sparse reconstruction, semantic segmentation, medical image analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bin Liu, Wenyan Tian, Huangxin Fu, Zizheng Li, Zhifen He, Bo Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanchang Hangkong University, Guilin University Of Electronic Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22800",children:"https://arxiv.org/pdf/2512.22800"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an efficient 3D reconstruction method for medical images by combining 3D Gaussian and tri-plane representations. 2. Enhances structural continuity and semantic consistency under sparse slice conditions, addressing a key limitation of traditional methods. 3. Demonstrates high-quality, anatomically coherent reconstruction on multimodal medical datasets (US, MRI) with improved efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of computationally expensive and detail-losing 3D reconstruction from sparse medical image slices. It proposes a novel method that integrates 3D Gaussian and tri-plane representations to efficiently generate high-quality, anatomically coherent 3D visualizations. Experiments on US and MRI data confirm the method's effectiveness in improving reconstruction quality and efficiency under sparse data conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u7a00\u758f\u5207\u7247\u4e0b\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u3001\u7ed3\u6784\u4e0d\u8fde\u7eed/Traditional 3D reconstruction is computationally expensive and structurally discontinuous with sparse slices)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u4e0e\u4e09\u5e73\u9762\u8868\u793a\u7684\u9ad8\u6548\u91cd\u5efa\u65b9\u6cd5/Efficient reconstruction method based on 3D Gaussian and tri-plane representations)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728\u7a00\u758f\u6570\u636e\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u89e3\u5256\u4e00\u81f4\u4e14\u9ad8\u6548\u76843D\u533b\u5b66\u56fe\u50cf/Generates high-quality, anatomically coherent, and efficient 3D medical images under sparse data)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sharif University of Technology, Alan Turing Institute, London School of Economics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22802",children:"https://arxiv.org/pdf/2512.22802"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ReDiF: Reinforced Distillation for Few Step Diffusion] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Diffusion\u6a21\u578b\u91c7\u6837\u6162/Slow sampling in diffusion models)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u84b8\u998f\u6846\u67b6/Reinforcement learning based distillation framework)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u66f4\u5c11\u6b65\u9aa4\uff0c\u6027\u80fd\u66f4\u4f18/Fewer steps, superior performance)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [open-vocabulary object detection, low-quality image, image degradation, benchmark dataset, OWLv2]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Po-Chih Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22801",children:"https://arxiv.org/pdf/2512.22801"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/gohakushi1118/Low-quality-image-dataset",children:"https://github.com/gohakushi1118/Low-quality-image-dataset"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed a new benchmark dataset simulating real-world low-quality images with four types of degradation (lossy compression, image intensity, noise, blur). 2. Evaluated six state-of-the-art open-vocabulary object detection models on this benchmark to assess their robustness. 3. Provided analysis showing varying model sensitivity to degradation types and levels, with OWLv2 demonstrating more consistent performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the robustness of open-vocabulary object detection models on low-quality images by creating a new benchmark dataset with various image degradations. The experiments show that while models are resilient to mild degradation, severe degradation causes significant performance drops, with OWLv2 models being the most robust."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Evaluating Open-Vocabulary Object Detection in Low-quality Image] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u771f\u5b9e\u4e16\u754c\u4f4e\u8d28\u91cf\u56fe\u50cf\u5bf9\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u5f71\u54cd/Impact of real-world low-quality images on open-vocabulary object detection]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6784\u5efa\u5305\u542b\u56db\u79cd\u9000\u5316\u7c7b\u578b\u7684\u4f4e\u8d28\u91cf\u56fe\u50cf\u6570\u636e\u96c6/Build a low-quality image dataset with four degradation types]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6a21\u578b\u5bf9\u4e25\u91cd\u9000\u5316\u654f\u611f\uff0cOWLv2\u8868\u73b0\u6700\u7a33\u5065/Models are sensitive to severe degradation, OWLv2 is the most robust]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Parallel Diffusion Solver via Residual Dirichlet Policy Optimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [ODE solver, parallel gradient evaluation, reinforcement learning fine-tuning, low-latency sampling, Dirichlet policy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruoyu Wang, Ziyu Li, Beier Zhu, Liangyu Yuan, Hanwang Zhang, Xun Yang, Xiaojun Chang, Chi Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Westlake University, University of Illinois Urbana-Champaign, Nanyang Technological University, Shanghai Jiao Tong University, University of Science and Technology of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22796",children:"https://arxiv.org/pdf/2512.22796"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes EPD-Solver, a novel ODE solver that uses multiple parallel gradient evaluations per step to reduce truncation errors while maintaining low latency. 2. Introduces a two-stage optimization framework, including a parameter-efficient RL fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy to avoid reward hacking. 3. Demonstrates the method's flexibility as a plugin (EPD-Plugin) to enhance existing ODE samplers and shows state-of-the-art performance in both unconditional and text-to-image generation benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high sampling latency of diffusion models by proposing EPD-Solver, a novel ODE solver that incorporates parallel gradient evaluations to reduce errors without increasing latency. The method uses a two-stage optimization, including RL fine-tuning with a Dirichlet policy, and can be used as a plugin. Experiments show it achieves superior image quality at low step counts and improves human preference scores in text-to-image generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Parallel Diffusion Solver via Residual Dirichlet Policy Optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6269\u6563\u6a21\u578b\u91c7\u6837\u5ef6\u8fdf\u9ad8 / High sampling latency of DMs]\n    B --\x3e B2[\u73b0\u6709\u6c42\u89e3\u5668\u5728\u4f4e\u6b65\u6570\u4e0b\u8d28\u91cf\u4e0b\u964d / Existing solvers degrade quality at low NFEs]\n    C --\x3e C1[EPD-Solver: \u96c6\u6210\u5e76\u884c\u65b9\u5411\u6c42\u89e3\u5668 / Ensemble Parallel Direction solver]\n    C --\x3e C2[\u4e24\u9636\u6bb5\u4f18\u5316: \u84b8\u998f + RL\u5fae\u8c03 / Two-stage optimization: Distillation + RL fine-tuning]\n    C --\x3e C3[\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u73b0\u6709\u6c42\u89e3\u5668 / Plugin (EPD-Plugin) for existing samplers]\n    D --\x3e D1[\u4f4e\u5ef6\u8fdf\u4e0bSOTA FID\u5206\u6570 / SOTA FID scores at low latency]\n    D --\x3e D2[\u5728T2I\u4efb\u52a1\u4e2d\u63d0\u5347\u4eba\u7c7b\u504f\u597d\u5206\u6570 / Improved human preference scores in T2I]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [human motion generation], [egocentric video, 3D human reaction, autoregressive generation, VQ-VAE, GPT]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22808",children:"https://arxiv.org/pdf/2512.22808"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u6570\u636e\u7a7a\u95f4\u4e0d\u4e00\u81f4/Existing data spatial misalignment]\n    B --\x3e B2[\u56e0\u679c\u751f\u6210\u4e0e3D\u5bf9\u9f50\u7684\u6311\u6218/Causal generation & 3D alignment challenge]\n    C --\x3e C1[\u6784\u5efaHRD\u6570\u636e\u96c6/Build HRD dataset]\n    C --\x3e C2[VQ-VAE\u538b\u7f29\u8fd0\u52a8/VQ-VAE compresses motion]\n    C --\x3e C3[GPT\u81ea\u56de\u5f52\u751f\u6210/GPT autoregressive generation]\n    C --\x3e C4[\u878d\u51653D\u52a8\u6001\u7279\u5f81/Incorporate 3D dynamic features]\n    D --\x3e D1[\u66f4\u9ad8\u7684\u771f\u5b9e\u611f\u4e0e\u7a7a\u95f4\u4e00\u81f4\u6027/Higher realism & spatial consistency]\n    D --\x3e D2[\u5b9e\u65f6\u751f\u6210\u6548\u7387/Real-time generation efficiency]\n    D --\x3e D3[\u4fdd\u6301\u4e25\u683c\u56e0\u679c\u6027/Maintains strict causality]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] 3D Scene Change Modeling With Consistent Multi-View Aggregation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D scene understanding / change detection], [3D Gaussian Splatting, signed-distance field, multi-view aggregation, continual reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zirui Zhou, Junfeng Ni, Shujie Zhang, Yixin Chen, Siyuan Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, State Key Laboratory of General Artificial Intelligence (BIGAI)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22830",children:"https://arxiv.org/pdf/2512.22830"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://zr-zhou0o0.github.io/SCaR3D/",children:"https://zr-zhou0o0.github.io/SCaR3D/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SCaR-3D, a novel 3D scene change detection framework that uses a signed-distance-based 2D differencing module and multi-view aggregation with voting/pruning to robustly separate pre- and post-change states. 2. Develops a continual scene reconstruction strategy that selectively updates dynamic regions while preserving unchanged areas. 3. Contributes CCS3D, a challenging synthetic dataset for flexible and controlled evaluation of 3D change types."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of detecting object-level changes in 3D scenes from multi-view images, where existing methods suffer from spatial inconsistency and cannot separate pre- and post-change states. The proposed SCaR-3D framework leverages 3D Gaussian Splatting for consistent multi-view aggregation and includes a strategy for continual scene reconstruction. Experiments show the method outperforms existing approaches in both accuracy and efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[3D Scene Change Modeling With Consistent Multi-View Aggregation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u67093D\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u4e14\u65e0\u6cd5\u5206\u79bb\u53d8\u5316\u524d\u540e\u72b6\u6001/Existing 3D change detection methods exhibit spatial inconsistency and fail to separate pre- and post-change states]\n    C --\x3e C1[\u63d0\u51faSCaR-3D\u6846\u67b6: \u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u76842D\u5dee\u5206\u4e0e\u591a\u89c6\u56fe\u805a\u5408\u6295\u7968\u526a\u679d/Propose SCaR-3D: Signed-distance-based 2D differencing and multi-view aggregation with voting & pruning]\n    C --\x3e C2[\u5f00\u53d1\u6301\u7eed\u573a\u666f\u91cd\u5efa\u7b56\u7565: \u9009\u62e9\u6027\u66f4\u65b0\u52a8\u6001\u533a\u57df/Develop continual reconstruction: Selectively update dynamic regions]\n    C --\x3e C3[\u8d21\u732eCCS3D\u5408\u6210\u6570\u636e\u96c6/Contribute CCS3D synthetic dataset]\n    D --\x3e D1[\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u7387/High accuracy and efficiency]\n    D --\x3e D2[\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Outperforms existing methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image super-resolution], [Kolmogorov-Arnold Neural Operator, B-spline functions, interpretability, spectral fitting, degradation modeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chenyu Li, Danfeng Hong, Bing Zhang, Zhaojie Pan, Jocelyn Chanussot"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southeast University, Aerospace Information Research Institute (Chinese Academy of Sciences), Univ. Grenoble Alpes"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22822",children:"https://arxiv.org/pdf/2512.22822"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the novel Kolmogorov-Arnold Neural Operator (KANO) for interpretable image super-resolution, inspired by the Kolmogorov-Arnold theorem. 2. Employs an additive structure of B-spline functions to model the degradation process transparently, capturing key spectral characteristics like local trends and peak-valley structures. 3. Provides a systematic comparative study between MLPs and KANs for complex sequence fitting, offering insights into interpretable SR model design."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of interpretability in single-image super-resolution by proposing a new model called KANO, which uses B-spline functions based on the Kolmogorov-Arnold theorem to transparently model the image degradation process. The method provides physically interpretable results by learning spectral characteristics. The authors demonstrate its effectiveness and compare it with other models across different image types."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Interpretable Super-Resolution] --\x3e P1[SR\u662f\u75c5\u6001\u9006\u95ee\u9898/SR is Ill-posed]\n    Problem --\x3e P2[\u73b0\u6709\u65b9\u6cd5\u9ed1\u76d2/Existing Methods are Black-box]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: KANO Model] --\x3e M1[\u57fa\u4e8eKAT\u5b9a\u7406/Based on KAT]\n    Method --\x3e M2[B\u6837\u6761\u51fd\u6570\u62df\u5408/B-spline Fitting]\n    Method --\x3e M3[\u5b66\u4e60\u5f62\u72b6\u53c2\u6570/Learn Shape Parameters]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u900f\u660e\u964d\u89e3\u8868\u793a/Transparent Degradation Representation]\n    Results --\x3e R2[\u7269\u7406\u53ef\u89e3\u91ca\u6027/Physical Interpretability]\n    Results --\x3e R3[\u7cfb\u7edf\u6a21\u578b\u6bd4\u8f83/Systematic Model Comparison]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["[arXiv251230] Depth Anything in ",(0,a.jsx)(n.span,{className:"katex-error",title:"ParseError: KaTeX parse error: Expected group after '^' at position 4: 360^\u0332",style:{color:"#cc0000"},children:"360^"}),": Towards Scale Invariance in the Wild"]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [depth estimation], [panoramic depth estimation, scale invariance, zero-shot generalization, circular padding, ViT backbone]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Insta360 Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22819",children:"https://arxiv.org/pdf/2512.22819"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://insta360-research-team.github.io/DA360",children:"https://insta360-research-team.github.io/DA360"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DA360, a panoramic-adapted version of Depth Anything V2 that learns a shift parameter to transform scale- and shift-invariant output into scale-invariant disparity for direct 3D point cloud generation. 2. Integrates circular padding into the DPT decoder to eliminate seam artifacts and ensure spatially coherent depth maps respecting spherical continuity. 3. Introduces a new outdoor panoramic depth dataset, Metropolis, for evaluation and demonstrates state-of-the-art zero-shot performance on indoor and outdoor benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap in zero-shot generalization for panoramic depth estimation by proposing DA360, an adaptation of Depth Anything V2. The method learns a shift parameter for scale-invariant output and uses circular padding to prevent seam artifacts, enabling direct conversion to 3D point clouds. The results show significant error reduction compared to the base model and other panoramic methods, establishing new state-of-the-art performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Depth Anything in 360\xb0: Towards Scale Invariance in the Wild"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Panoramic depth estimation lags in zero-shot generalization compared to perspective images."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Adapt Depth Anything V2 with learned shift parameter for scale invariance and circular padding for seam removal."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves >50% and >10% error reduction on indoor/outdoor benchmarks, outperforms PanDA by ~30%."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [relative pose estimation], [affine correspondences, polynomial eigenvalue method, minimal solver, inertial measurement unit, focal length estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenbao Yu, Shirong Ye, Ronghe Jin, Shunkun Liang, Zibin Liu, Huiyun Zhang, Banglei Guan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology, Wuhan University, Henan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22833",children:"https://arxiv.org/pdf/2512.22833"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new minimal solver for estimating 3DOF relative pose and focal length from only two affine correspondences when the vertical direction is known from an IMU. 2. Derives a system of four constraint equations that reduce the problem to solving for only two parameters: focal length and relative rotation angle. 3. Utilizes the polynomial eigenvalue method to efficiently solve the derived equations, demonstrating superior performance over state-of-the-art solvers on synthetic and real-world datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a new minimal solver for estimating the relative pose and focal length between two camera views. The method uses only two affine correspondences and known vertical direction (from an IMU) to formulate constraint equations, which are then solved using a polynomial eigenvalue approach. Experimental results show the proposed solver outperforms existing state-of-the-art methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences<br>\u57fa\u4e8e\u4e24\u4e2a\u4eff\u5c04\u5bf9\u5e94\u7684\u672a\u77e5\u7126\u8ddd\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u6700\u5c0f\u6c42\u89e3\u5668"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Estimate relative pose & focal length from minimal data<br>\u4ece\u6700\u5c11\u6570\u636e\u4f30\u8ba1\u76f8\u5bf9\u4f4d\u59ff\u548c\u7126\u8ddd"] --\x3e P1["\u5df2\u77e5\u5782\u76f4\u65b9\u5411/Known vertical direction (from IMU)<br>\u81ea\u7531\u5ea6\u4ece5DOF\u964d\u81f33DOF"]\n    Problem --\x3e P2["\u8f93\u5165/Input<br>\u4e24\u4e2a\u4eff\u5c04\u5bf9\u5e94/Two affine correspondences"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5efa\u7acb\u7ea6\u675f\u65b9\u7a0b\u5e76\u6c42\u89e3/Establish constraints and solve"] --\x3e M1["\u5efa\u7acb\u65b9\u7a0b/Establish equations from two ACs"]\n    Method --\x3e M2["\u63a8\u5bfc\u7cfb\u7edf/Derive 4 equations for 2 parameters<br>\u7126\u8ddd\u548c\u65cb\u8f6c\u89d2/focal length & rotation angle"]\n    Method --\x3e M3["\u6c42\u89e3\u65b9\u6cd5/Solution Method<br>\u591a\u9879\u5f0f\u7279\u5f81\u503c\u65b9\u6cd5/Polynomial eigenvalue method"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u8bc4\u4f30\u4e0e\u6bd4\u8f83/Evaluation & Comparison"] --\x3e R1["\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e/Synthetic & real-world datasets"]\n    Results --\x3e R2["\u6027\u80fd\u66f4\u597d/Performs better than state-of-the-art solvers"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22854",children:"https://arxiv.org/pdf/2512.22854"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://neutrinoliu.github.io/byteloom/",children:"https://neutrinoliu.github.io/byteloom/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u591a\u89c6\u56fe\u4fe1\u606f\u6ce8\u5165\u673a\u5236/Existing methods lack multi-view injection]\n    B --\x3e B2[\u4e25\u91cd\u4f9d\u8d56\u624b\u90e8\u7f51\u683c\u6807\u6ce8/Heavy reliance on hand mesh annotations]\n    C --\x3e C1[\u63d0\u51faRCM-cache\u673a\u5236/Propose RCM-cache mechanism]\n    C --\x3e C2[\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u5b66\u4e60/Design progressive curriculum learning]\n    D --\x3e D1[\u4fdd\u6301\u7269\u4f53\u51e0\u4f55\u4e00\u81f4\u6027/Preserves object geometry consistency]\n    D --\x3e D2[\u751f\u6210\u5e73\u6ed1\u8fd0\u52a8\u89c6\u9891/Generates smooth motion videos]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [embodied navigation], [socially compliant navigation, multimodal dataset, chain-of-thought, vision language models, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hokkaido University, George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22867",children:"https://arxiv.org/pdf/2512.22867"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[MUSON: \u9762\u5411\u63a8\u7406\u7684\u591a\u6a21\u6001\u57ce\u5e02\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u6570\u636e\u96c6] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u76d1\u7763/Lack explicit reasoning supervision]\n    Problem --\x3e P2[\u52a8\u4f5c\u5206\u5e03\u9ad8\u5ea6\u957f\u5c3e/Highly long-tailed action distribution]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f15\u5165MUSON\u6570\u636e\u96c6/Introduce MUSON dataset]\n    M1 --\x3e M1_Sub1[\u4e94\u6b65\u601d\u7ef4\u94fe\u6807\u6ce8/Five-step Chain-of-Thought annotation]\n    M1 --\x3e M1_Sub2[\u5e73\u8861\u7684\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4/Balanced discrete action space]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Qwen2.5-VL-3B\u53d6\u5f97\u6700\u9ad8\u7cbe\u5ea6/Qwen2.5-VL-3B achieves highest accuracy]\n    Results --\x3e R2[\u6570\u636e\u96c6\u4f5c\u4e3a\u6709\u6548\u57fa\u51c6/Dataset serves as effective benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [self-supervised learning], [self-supervised learning, foundation model, chest radiographs, anatomical consistency, multi-perspective learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Arizona State University, University of Bern, Mayo Clinic"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22872",children:"https://arxiv.org/pdf/2512.22872"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," GitHub.com/JLiangLab/Lamps"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel self-supervised learning framework (Lamps) that explicitly leverages the consistency, coherence, and hierarchy of human anatomy as supervision signals. 2. Demonstrates superior performance and robustness across 10 chest X-ray datasets compared to 10 baseline models. 3. Releases code and pre-trained models to facilitate research in anatomy-aware foundation models for medical imaging."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of existing self-supervised learning methods in medical imaging, which often overlook anatomical structures. It proposes Lamps, a method that learns from multiple anatomical perspectives (consistency, coherence, hierarchy) using self-supervision on chest radiographs. The results show that Lamps achieves superior robustness and transferability, offering a promising foundation model aligned with human anatomy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Lamps: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Existing SSL methods overlook anatomical consistency, coherence, and hierarchy in medical images."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Lamps uses anatomical consistency, coherence, and hierarchy as self-supervision signals for pre-training on chest X-rays."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Superior robustness and transferability across 10 datasets vs. 10 baselines; code and models released."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [debiasing], [spurious correlation, feature transformation, worst group accuracy, bias-invariant representation, empirical risk minimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Weiwei Li, Junzhuo Liu, Yuanyuan Ren, Yuchen Zheng, Yahao Liu, Wen Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Shihezi University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22874",children:"https://arxiv.org/pdf/2512.22874"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/davelee-uestc/nsf_debiasing",children:"https://github.com/davelee-uestc/nsf_debiasing"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a data-oriented pipeline for mitigating spurious correlation without requiring prior annotation of bias attributes. 2. Introduces a method to identify spurious features by observing the dispersed distribution of biased samples in the feature space. 3. Develops a feature transformation learning process to align with a bias-invariant representation, leading to an unbiased classifier."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of deep learning models learning spurious correlations from biased training data. It proposes a method that identifies, neutralizes, and eliminates spurious features by exploiting the clusteredness of samples and aligning feature transformations with a bias-invariant representation. The approach significantly improves worst-group accuracy by over 20% compared to standard training on image and NLP benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Let Samples Speak: Mitigating Spurious Correlation<br>\u8ba9\u6837\u672c\u8bf4\u8bdd\uff1a\u5229\u7528\u6837\u672c\u805a\u7c7b\u6027\u7f13\u89e3\u4f2a\u76f8\u5173] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Deep models learn spurious features<br>\u6df1\u5ea6\u6a21\u578b\u5b66\u4e60\u4f2a\u76f8\u5173\u7279\u5f81]\n    C --\x3e C1[Identify spurious features via sample distribution<br>\u901a\u8fc7\u6837\u672c\u5206\u5e03\u8bc6\u522b\u4f2a\u7279\u5f81]\n    C --\x3e C2[Neutralize & eliminate via feature transformation<br>\u901a\u8fc7\u7279\u5f81\u53d8\u6362\u4e2d\u548c\u4e0e\u6d88\u9664]\n    C --\x3e C3[Update classifier<br>\u66f4\u65b0\u5206\u7c7b\u5668]\n    D --\x3e D1[>20% worst-group accuracy improvement vs ERM<br>\u76f8\u6bd4ERM\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u63d0\u5347>20%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Hash Grid Feature Pruning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [hash grid, Gaussian splatting, feature pruning, rate-distortion, implicit neural field]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yangzhi Ma, Bojun Liu, Jie Li, Li Li, Dong Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22882",children:"https://arxiv.org/pdf/2512.22882"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies the problem of invalid features in hash grids due to the non-uniform distribution of Gaussian splats, leading to storage and transmission redundancy. 2. Proposes a hash grid feature pruning method that identifies and removes invalid features based on input Gaussian splat coordinates before encoding. 3. Demonstrates improved rate-distortion performance with an average 8% bitrate reduction in standardized tests, without compromising model reconstruction quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of redundant storage in hash grids used for Gaussian splatting compression, caused by invalid features in sparse 3D regions. It proposes a pruning method that removes these invalid features before encoding, reducing bitrate. The method achieves an average 8% bitrate reduction without affecting model performance, improving rate-distortion efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Hash Grid Feature Pruning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Hash grid\u4e2d\u5b58\u5728\u5927\u91cf\u65e0\u6548\u7279\u5f81\uff0c\u5bfc\u81f4\u5b58\u50a8\u548c\u4f20\u8f93\u5197\u4f59]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u5750\u6807\u8bc6\u522b\u5e76\u526a\u679d\u65e0\u6548\u7279\u5f81\uff0c\u4ec5\u7f16\u7801\u6709\u6548\u7279\u5f81]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5e73\u5747\u6bd4\u7279\u7387\u964d\u4f4e8%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Guided Path Sampling, Classifier-Free Guidance, iterative refinement, off-manifold, path stability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haosen Li, Wenshuo Chen, Shaofeng Liang, Lei Wang, Haozhe Jia, Yutao Yue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou), Griffith University, Data61/CSIRO"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22881",children:"https://arxiv.org/pdf/2512.22881"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies a fundamental limitation where Classifier-Free Guidance (CFG) causes iterative refinement methods to diverge by pushing the sampling path off the data manifold. 2. Proposes Guided Path Sampling (GPS), a new paradigm that replaces CFG's extrapolation with a principled, manifold-constrained interpolation to ensure path stability. 3. Devises an optimal scheduling strategy to dynamically adjust guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that standard Classifier-Free Guidance (CFG) causes instability in iterative refinement for diffusion models by pushing the sampling path off the data manifold. To solve this, the authors propose Guided Path Sampling (GPS), a method that uses manifold-constrained interpolation to keep the path stable and includes an optimal guidance schedule. Experiments show GPS improves image quality and prompt adherence, establishing path stability as key for effective refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Guided Path Sampling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: CFG\u5bfc\u81f4\u91c7\u6837\u8def\u5f84\u504f\u79bb\u6570\u636e\u6d41\u5f62/CFG pushes sampling path off data manifold]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faGPS\uff0c\u4f7f\u7528\u6d41\u5f62\u7ea6\u675f\u63d2\u503c/Propose GPS with manifold-constrained interpolation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u4e0e\u63d0\u793a\u5bf9\u9f50/Improves image quality & prompt adherence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [concept erasure, multimodal evaluation, inference-time robustness, cross-attention, latent perturbation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Taiwan University, Academia Sinica (Research Center for Information Technology Innovation)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22877",children:"https://arxiv.org/pdf/2512.22877"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces M-ErasureBench, the first comprehensive multimodal benchmark for evaluating concept erasure methods across text prompts, learned embeddings, and inverted latents under white-box and black-box settings. 2. Identifies a critical vulnerability where existing erasure methods fail against non-textual input modalities, with concept reproduction rates exceeding 90%. 3. Proposes IRECE, a plug-and-play inference-time module that enhances robustness by localizing concepts via cross-attention and perturbing associated latents, reducing CRR by up to 40% while preserving image quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that existing concept erasure methods for diffusion models are vulnerable to attacks using non-textual inputs like learned embeddings or inverted latents. To address this, the authors propose a new multimodal benchmark (M-ErasureBench) and a plug-and-play defense module (IRECE) that perturbs latents during inference. Experiments show IRECE significantly reduces concept reproduction rates under challenging attacks while maintaining visual quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u4ec5\u9488\u5bf9\u6587\u672c\u63d0\u793a/Existing methods only target text prompts]\n    Problem --\x3e P2[\u5176\u4ed6\u6a21\u6001\u6210\u4e3a\u653b\u51fb\u9762/Other modalities become attack surfaces]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51fa\u591a\u6a21\u6001\u57fa\u51c6M-ErasureBench/Propose multimodal benchmark M-ErasureBench]\n    Method --\x3e M2[\u63d0\u51fa\u63a8\u7406\u65f6\u589e\u5f3a\u6a21\u5757IRECE/Propose inference-time module IRECE]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u6587\u672c\u6a21\u6001\u4e0b\u5931\u8d25/Existing methods fail under non-text modalities]\n    Results --\x3e R2[IRECE\u5c06CRR\u964d\u4f4e\u8fbe40%/IRECE reduces CRR by up to 40%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [multimodal fusion, text-guided segmentation, transformer-based architecture, lightweight model, 3D segmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hasan Faraz Khan, Noor Fatima, Muzammil Behzad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22878",children:"https://arxiv.org/pdf/2512.22878"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --\x3e Problem_Sub[\u95ee\u9898\u7ec6\u8282/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --\x3e Method_Sub[\u65b9\u6cd5\u7ec6\u8282/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --\x3e Results_Sub[\u7ed3\u679c\u7ec6\u8282/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [benchmark evaluation], [scientific intelligence, hierarchical benchmark, multi-disciplinary evaluation, multimodal inputs, dependency-aware framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22899",children:"https://arxiv.org/pdf/2512.22899"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal learning], [audio-video fusion, instruction tuning, diffusion transformer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei, Tat-Seng Chua"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore (NUS), Zhejiang University (ZJU), Renmin University of China (RUC), University of Rochester (UR), Hong Kong University of Science and Technology (Guangzhou) (HKUST(GZ))"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22905",children:"https://arxiv.org/pdf/2512.22905"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://JavisVerse.github.io/JavisGPT-page",children:"https://JavisVerse.github.io/JavisGPT-page"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes JavisGPT, the first unified multimodal LLM for joint audio-video comprehension and generation. 2. Introduces a novel architecture with a SyncFusion module and synchrony-aware queries to bridge a pretrained JAV-DiT generator for coherent output. 3. Constructs a large-scale, high-quality instruction dataset (JavisInst-Omni) with over 200K GPT-4o-curated dialogues for training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces JavisGPT, a unified multimodal LLM designed for joint audio-video understanding and generation. It features a novel encoder-LLM-decoder architecture with a SyncFusion module and is trained using a three-stage pipeline on a newly created large-scale instruction dataset. Experiments show it outperforms existing models, especially in complex, temporally synchronized tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b/Lack of unified audio-video comprehension and generation model]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u7b80\u6d01\u7684\u7f16\u7801\u5668-LLM-\u89e3\u7801\u5668\u67b6\u6784/Concise encoder-LLM-decoder architecture]\n    M1 --\x3e M1_1[SyncFusion\u6a21\u5757\u7528\u4e8e\u65f6\u7a7a\u878d\u5408/SyncFusion module for spatio-temporal fusion]\n    M1 --\x3e M1_2[\u540c\u6b65\u611f\u77e5\u53ef\u5b66\u4e60\u67e5\u8be2/Synchrony-aware learnable queries]\n    Method --\x3e M2[\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b/Three-stage training pipeline]\n    M2 --\x3e M2_1[\u591a\u6a21\u6001\u9884\u8bad\u7ec3/Multimodal pretraining]\n    M2 --\x3e M2_2[\u97f3\u9891-\u89c6\u9891\u5fae\u8c03/Audio-video fine-tuning]\n    M2 --\x3e M2_3[\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u4f18/Large-scale instruction-tuning]\n    Method --\x3e M3[\u6784\u5efaJavisInst-Omni\u6307\u4ee4\u6570\u636e\u96c6/Construct JavisInst-Omni instruction dataset]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5728\u97f3\u9891-\u89c6\u9891\u7406\u89e3\u4e0e\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02/Outperforms existing MLLMs on benchmarks]\n    R1 --\x3e R1_1[\u5728\u590d\u6742\u548c\u65f6\u95f4\u540c\u6b65\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa/Particularly strong in complex and temporally synchronized settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [autonomous driving], [vision-language-action, latent reasoning, hierarchical parallel planning, trajectory generation, cognitive reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, CUHK MMLab, Voyager Research (Didi Chuxing)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22939",children:"https://arxiv.org/pdf/2512.22939"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified vision-language-action framework that transfers reasoning from discrete text to a continuous latent space, bridging the gap between VLM reasoning and control. 2. A Cognitive Latent Reasoner that compresses scene understanding into decision-oriented meta-action embeddings efficiently using only two VLM forward passes. 3. A Hierarchical Parallel Planner that generates multi-scale, causality-consistent trajectories in a single forward pass, enabling real-time performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes ColaVLA, a framework for autonomous driving that uses cognitive latent reasoning to efficiently translate vision-language understanding into continuous control actions, coupled with a hierarchical parallel planner for trajectory generation. It addresses key challenges in VLM-based planning, such as latency and the text-control mismatch. Experiments on nuScenes show state-of-the-art performance in both open-loop and closed-loop settings with improved efficiency and robustness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ColaVLA] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[VLM\u89c4\u5212\u5668\u6311\u6218: \u6587\u672c\u4e0e\u8fde\u7eed\u63a7\u5236\u4e0d\u5339\u914d, \u9ad8\u5ef6\u8fdf, \u975e\u56e0\u679c/VLM Planner Challenges: Text-Control Mismatch, High Latency, Non-causal]\n    C --\x3e C1[\u8ba4\u77e5\u6f5c\u5728\u63a8\u7406\u5668: \u5c06\u573a\u666f\u7406\u89e3\u538b\u7f29\u4e3a\u5143\u52a8\u4f5c\u5d4c\u5165/Cognitive Latent Reasoner: Compresses scene to meta-action embeds]\n    C --\x3e C2[\u5206\u5c42\u5e76\u884c\u89c4\u5212\u5668: \u5355\u6b21\u524d\u5411\u751f\u6210\u591a\u5c3a\u5ea6\u8f68\u8ff9/Hierarchical Parallel Planner: Single-pass multi-scale trajectory gen]\n    D --\x3e D1[SOTA\u6027\u80fd: \u5728nuScenes\u5f00\u73af\u548c\u95ed\u73af\u6d4b\u8bd5\u4e2d/SOTA Performance: On nuScenes open & closed-loop]\n    D --\x3e D2[\u9ad8\u6548\u4e0e\u9c81\u68d2: \u6709\u5229\u7684\u6548\u7387\u4e0e\u9c81\u68d2\u6027/Favorable Efficiency & Robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [density map, dense tiny objects, remote sensing, cross-attention, feature fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhicheng Zhao, Xuanang Fan, Lingma Sun, Chenglong Li, Jin Tang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Anhui University, Hefei University, 38th Research Institute of China Electronics Technology Group Corporation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22949",children:"https://arxiv.org/pdf/2512.22949"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Density Generation Branch (DGB) to model object distribution and provide spatial priors for focusing on dense regions. 2. Designed a Dense Area Focusing Module (DAFM) that uses density maps to efficiently focus computational resources on dense areas for local-global feature interaction. 3. Introduced a Dual Filter Fusion Module (DFFM) that uses discrete cosine transform and density-guided cross-attention to disentangle and fuse multi-scale features, enhancing complementarity and suppressing background."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting dense, tiny objects in remote sensing imagery by proposing DRMNet, a network that uses density maps to guide adaptive feature learning. The method focuses computational resources on dense regions and fuses multi-scale features effectively. Experiments on AI-TOD and DTOD datasets show it outperforms existing methods, especially in high-density and occluded scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects<br>\u8bba\u6587\u6807\u9898"] --\x3e Problem["\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u4e2d\u5bc6\u96c6\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u7684\u6311\u6218<br>Problem: Detecting dense tiny objects in high-resolution remote sensing imagery"]\n    Root --\x3e Method["\u63d0\u51faDRMNet\uff0c\u5229\u7528\u5bc6\u5ea6\u56fe\u6307\u5bfc\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60<br>Method: Propose DRMNet using density maps to guide adaptive feature learning"]\n    Root --\x3e Results["\u5728AI-TOD\u548cDTOD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA<br>Results: Outperforms SOTA on AI-TOD and DTOD datasets"]\n    Problem --\x3e P1["\u76f8\u4e92\u906e\u6321\u4e25\u91cd\uff0c\u50cf\u7d20\u8db3\u8ff9\u6709\u9650<br>Severe mutual occlusion, limited pixel footprints"]\n    Problem --\x3e P2["\u73b0\u6709\u65b9\u6cd5\u8d44\u6e90\u5206\u914d\u5747\u5300\uff0c\u65e0\u6cd5\u805a\u7126\u5bc6\u96c6\u533a\u57df<br>Existing methods allocate resources uniformly, failing to focus on dense regions"]\n    Method --\x3e M1["\u5bc6\u5ea6\u751f\u6210\u5206\u652f (DGB)<br>Density Generation Branch (DGB)"]\n    Method --\x3e M2["\u5bc6\u96c6\u533a\u57df\u805a\u7126\u6a21\u5757 (DAFM)<br>Dense Area Focusing Module (DAFM)"]\n    Method --\x3e M3["\u53cc\u6ee4\u6ce2\u5668\u878d\u5408\u6a21\u5757 (DFFM)<br>Dual Filter Fusion Module (DFFM)"]\n    Results --\x3e R1["\u5728\u590d\u6742\u9ad8\u5bc6\u5ea6\u548c\u906e\u6321\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02<br>Excels in complex high-density and occlusion scenarios"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object detection], [4D radar-camera fusion, wavelet attention, geometry-guided progressive fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology (Guangzhou)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22972",children:"https://arxiv.org/pdf/2512.22972"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes WRCFormer, a novel framework for fusing raw 4D radar tensors with camera data using multi-view representations. 2. Designs a Wavelet Attention Module within a wavelet-based Feature Pyramid Network to enhance sparse signal representation. 3. Introduces a two-stage query-based, modality-agnostic Geometry-guided Progressive Fusion mechanism for efficient multi-view feature integration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of sparse data and high computational cost in 4D radar-camera fusion for 3D object detection by proposing WRCFormer. The method fuses raw radar tensors and camera inputs using a wavelet-based feature enhancement module and a geometry-guided progressive fusion mechanism. Experiments on the K-Radar benchmark show state-of-the-art performance, particularly highlighting robustness in adverse weather conditions like sleet."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Sparsity & computational cost of 4D radar"] --\x3e P1["\u4fe1\u606f\u635f\u5931/Information loss in point-cloud processing"]\n    Problem --\x3e P2["\u8ba1\u7b97\u6210\u672c\u9ad8/High computational cost of raw data"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>WRCFormer framework"] --\x3e M1["\u5c0f\u6ce2\u6ce8\u610f\u529b\u6a21\u5757/Wavelet Attention Module"]\n    Method --\x3e M2["\u51e0\u4f55\u5f15\u5bfc\u6e10\u8fdb\u878d\u5408/Geometry-guided Progressive Fusion"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>SOTA on K-Radar"] --\x3e R1["\u6574\u4f53\u6027\u80fd\u63d0\u5347 +2.4%/Overall +2.4%"]\n    Results --\x3e R2["\u6076\u52a3\u5929\u6c14\u9c81\u68d2\u6027/Robust in sleet +1.6%"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [camouflaged object detection], [camouflaged image generation, out-painting, layout control, textual-visual guidance, distribution divergence metric]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chunyuan Chen, Yunuo Cai, Shujuan Li, Weiyun Liang, Bin Wang, Jing Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nankai University, Fudan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22974",children:"https://arxiv.org/pdf/2512.22974"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified out-painting framework with explicit layout controls to improve semantic coherence between foreground objects and generated backgrounds. 2. Constructs a multi-modal textual-visual condition combining fine-grained textual descriptions with texture-oriented background retrieval to enhance visual fidelity. 3. Introduces a background-foreground distribution divergence metric for the quantitative assessment of camouflage quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap between generated and real camouflaged images by proposing RealCamo, a framework that uses layout controls and multi-modal textual-visual guidance for realistic out-painting. The method improves both visual similarity and semantic consistency in generated backgrounds. Experiments demonstrate its effectiveness in producing high-quality camouflaged images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RealCamo: Boosting Real Camouflage Synthesis<br>RealCamo: \u63d0\u5347\u771f\u5b9e\u4f2a\u88c5\u5408\u6210] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Existing CIG methods have a gap to real imagery.<br>\u73b0\u6709CIG\u65b9\u6cd5\u4e0e\u771f\u5b9e\u56fe\u50cf\u5b58\u5728\u5dee\u8ddd\u3002]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Proposes RealCamo with layout controls & textual-visual guidance.<br>\u63d0\u51fa\u5e26\u5e03\u5c40\u63a7\u5236\u548c\u6587\u672c-\u89c6\u89c9\u5f15\u5bfc\u7684RealCamo\u3002]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Improves camouflage quality & realism.<br>\u63d0\u5347\u4e86\u4f2a\u88c5\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [CLIP, contrastive learning, joint training, learnable text embeddings, InfoNCE loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sharif University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22969",children:"https://arxiv.org/pdf/2512.22969"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CLIP-Joint-Detect, a detector-agnostic framework for end-to-end joint training of object detectors with CLIP-style contrastive vision-language supervision. 2. Introduces a lightweight parallel head that aligns visual features with learnable class-specific text embeddings using a combination of InfoNCE contrastive loss and an auxiliary cross-entropy term. 3. Demonstrates consistent performance improvements on standard benchmarks (Pascal VOC, MS COCO) with both two-stage (Faster R-CNN) and one-stage (YOLO) architectures while preserving real-time inference speed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the vulnerability of conventional object detectors to class imbalance and label noise by proposing CLIP-Joint-Detect. This framework integrates contrastive vision-language supervision into object detectors via a lightweight parallel head and joint training, aligning visual features with learnable text embeddings. The method improves detection performance across different architectures and datasets without sacrificing inference speed."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("CLIP-Joint-Detect") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u4f20\u7edf\u68c0\u6d4b\u5668\u4f9d\u8d56\u4ea4\u53c9\u71b5\u5206\u7c7b/Traditional detectors rely on cross-entropy classification")\n    Problem --\x3e P2("\u6613\u53d7\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u566a\u58f0\u5f71\u54cd/Vulnerable to class imbalance & label noise")\n    Method --\x3e M1("\u8f7b\u91cf\u7ea7\u5e76\u884c\u5934/Lightweight parallel head")\n    Method --\x3e M2("\u5c06\u7279\u5f81\u6295\u5f71\u5230CLIP\u7a7a\u95f4/Projects features to CLIP space")\n    Method --\x3e M3("\u4e0e\u53ef\u5b66\u4e60\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u9f50/Aligns with learnable text embeddings")\n    Method --\x3e M4("\u4f7f\u7528InfoNCE\u548c\u4ea4\u53c9\u71b5\u635f\u5931/Uses InfoNCE & cross-entropy loss")\n    Method --\x3e M5("\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3/End-to-end joint training")\n    Results --\x3e R1("\u5728Pascal VOC\u548cMS COCO\u4e0a\u6027\u80fd\u63d0\u5347/Performance gains on Pascal VOC & MS COCO")\n    Results --\x3e R2("\u9002\u7528\u4e8e\u4e24\u9636\u6bb5\u548c\u4e00\u9636\u6bb5\u68c0\u6d4b\u5668/Works for two-stage & one-stage detectors")\n    Results --\x3e R3("\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6/Preserves real-time inference speed")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [symmetric optimal transport, spatial-aware guidance, multimodal alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Linglin Liao, Qichuan Geng, Yu Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Capital Normal University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22981",children:"https://arxiv.org/pdf/2512.22981"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Spatial-aware Symmetric Alignment (SSA) framework for handling hybrid medical texts containing locational, descriptive, and diagnostic information. 2. Introduces a Dual-granularity Symmetric Optimal Transport (DSOT) alignment algorithm to establish bi-directional fine-grained multimodal correspondences. 3. Devises a composite directional guidance strategy that explicitly introduces spatial constraints by constructing region-level guidance masks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing text-guided medical image segmentation methods, which struggle with hybrid texts and spatial constraints. It proposes the Spatial-aware Symmetric Alignment (SSA) framework, which uses a symmetric optimal transport mechanism and spatial guidance strategy to improve alignment. Experiments show SSA achieves state-of-the-art performance, especially for lesions with spatial relational constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u74f6\u9888 / Bottlenecks of Existing Methods]\n    B1 --\x3e B2[\u96be\u4ee5\u5904\u7406\u6df7\u5408\u6587\u672c / Struggles with Hybrid Texts]\n    B1 --\x3e B3[\u5ffd\u7565\u7a7a\u95f4\u7ea6\u675f / Ignores Spatial Constraints]\n    C --\x3e C1[SSA\u6846\u67b6 / SSA Framework]\n    C1 --\x3e C2[\u5bf9\u79f0\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50 / Symmetric Optimal Transport Alignment]\n    C1 --\x3e C3[\u590d\u5408\u65b9\u5411\u6027\u5f15\u5bfc\u7b56\u7565 / Composite Directional Guidance Strategy]\n    D --\x3e D1[SOTA\u6027\u80fd / SOTA Performance]\n    D --\x3e D2[\u51c6\u786e\u5206\u5272\u7a7a\u95f4\u7ea6\u675f\u75c5\u7076 / Accurate Segmentation of Spatially Constrained Lesions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [pose estimation], [6DoF pose estimation, multi-modal fusion, event camera, high-speed tracking, unseen objects]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Renmin University of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22979",children:"https://arxiv.org/pdf/2512.22979"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PoseStreamer, a multi-modal framework integrating an Adaptive Pose Memory Queue, Object-centric 2D Tracker, and Ray Pose Filter for robust 6DoF pose estimation in high-speed scenarios. 2. Introduces a novel multi-modal dataset, MoCapCube6D, for benchmarking performance under rapid motion. 3. Demonstrates superior accuracy and generalizability as a template-free framework for unseen moving objects."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of 6DoF pose estimation for unseen objects in high-speed, low-light scenarios where RGB cameras suffer from motion blur. It proposes PoseStreamer, a multi-modal framework that fuses RGB and event camera data with three novel components for temporal consistency, 2D tracking priors, and geometric refinement. Experiments show the framework achieves superior accuracy in high-speed scenarios and strong generalizability for unseen objects."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[PoseStreamer: \u591a\u6a21\u60016DoF\u59ff\u6001\u4f30\u8ba1\u6846\u67b6] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[RGB\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca/Motion Blur in RGB]\n    Problem --\x3e P2[\u9ad8\u901f\u573a\u666f\u6027\u80fd\u4e0d\u4f73/Poor High-speed Performance]\n    Method --\x3e M1[\u81ea\u9002\u5e94\u59ff\u6001\u8bb0\u5fc6\u961f\u5217/Adaptive Pose Memory Queue]\n    Method --\x3e M2[\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u76842D\u8ddf\u8e2a\u5668/Object-centric 2D Tracker]\n    Method --\x3e M3[\u5c04\u7ebf\u59ff\u6001\u6ee4\u6ce2\u5668/Ray Pose Filter]\n    Method --\x3e M4[\u591a\u6a21\u6001\u6570\u636e\u96c6/Multi-modal Dataset (MoCapCube6D)]\n    Results --\x3e R1[\u9ad8\u901f\u573a\u666f\u7cbe\u5ea6\u66f4\u9ad8/Superior High-speed Accuracy]\n    Results --\x3e R2[\u5bf9\u672a\u89c1\u7269\u4f53\u6cdb\u5316\u6027\u5f3a/Strong Generalizability for Unseen Objects]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] YOLO-IOD: Towards Real Time Incremental Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [incremental learning, catastrophic forgetting, knowledge distillation, YOLO, pseudo-labeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shizhou Zhang, Xueqiang Lv, Yinghui Xing, Qirui Wu, Di Xu, Chen Zhao, Yanning Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northwestern Polytechnical University, Huawei"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22973",children:"https://arxiv.org/pdf/2512.22973"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1) Identifies three key knowledge conflicts (foreground-background confusion, parameter interference, misaligned knowledge distillation) causing forgetting in YOLO-based incremental detectors. 2) Proposes the YOLO-IOD framework with three novel components (CPR, IKS, CAKD) to address these conflicts. 3) Introduces a new benchmark, LoCo COCO, designed to prevent data leakage for more realistic incremental learning evaluation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of catastrophic forgetting in real-time incremental object detection (IOD) for YOLO detectors. It proposes YOLO-IOD, a framework built on YOLO-World that uses pseudo-label refinement, kernel selection, and asymmetric knowledge distillation to mitigate forgetting. The method shows superior performance with minimal forgetting on both conventional and a newly introduced, more realistic benchmark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[YOLO-IOD: Towards Real Time Incremental Object Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709IOD\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6YOLO\u6846\u67b6 / Existing IOD methods do not accommodate real-time YOLO]\n    B --\x3e B2[YOLO\u589e\u91cf\u68c0\u6d4b\u5b58\u5728\u77e5\u8bc6\u51b2\u7a81\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8 / Knowledge conflicts cause catastrophic forgetting in YOLO-based IOD]\n    C --\x3e C1[\u57fa\u4e8eYOLO-World\u7684\u9636\u6bb5\u5f0f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 / Stage-wise parameter-efficient fine-tuning on YOLO-World]\n    C --\x3e C2[\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6 / Three Core Components]\n    C2 --\x3e C2_1[CPR: \u51b2\u7a81\u611f\u77e5\u4f2a\u6807\u7b7e\u7ec6\u5316 / Conflict-Aware Pseudo-Label Refinement]\n    C2 --\x3e C2_2[IKS: \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u6838\u9009\u62e9 / Importance-based Kernel Selection]\n    C2 --\x3e C2_3[CAKD: \u8de8\u9636\u6bb5\u975e\u5bf9\u79f0\u77e5\u8bc6\u84b8\u998f / Cross-Stage Asymmetric Knowledge Distillation]\n    C --\x3e C3[\u5f15\u5165\u65b0\u57fa\u51c6LoCo COCO / Introduce new benchmark LoCo COCO]\n    D --\x3e D1[\u5728\u4f20\u7edf\u548cLoCo COCO\u57fa\u51c6\u4e0a\u6027\u80fd\u4f18\u8d8a / Superior performance on conventional and LoCo COCO benchmarks]\n    D --\x3e D2[\u5b9e\u73b0\u6700\u5c0f\u7684\u9057\u5fd8 / Achieves minimal forgetting]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Reverse Personalization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [face anonymization], [diffusion inversion, identity-guided conditioning, attribute-controllable anonymization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Han-Wei Kung, Tuomas Varanka, Nicu Sebe"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Trento, University of Oulu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22984",children:"https://arxiv.org/pdf/2512.22984"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/hanweikung/reverse-personalization",children:"https://github.com/hanweikung/reverse-personalization"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a reverse personalization framework for face anonymization using conditional diffusion inversion, eliminating the need for text prompts or subject-specific fine-tuning. 2. Incorporates an identity-guided conditioning branch to generalize anonymization to subjects not present in the model's pre-training data. 3. Enables attribute-controllable anonymization, allowing users to preserve or modify specific facial attributes while removing identity, a capability lacking in prior methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of removing identity-specific features from facial images while preserving other attributes and scene context. It proposes a reverse personalization method based on conditional diffusion inversion and an identity-guided conditioning branch, which allows for direct image manipulation without text prompts. The method achieves state-of-the-art performance in balancing identity removal, attribute preservation, and image quality, while also offering user control over retained attributes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Reverse Personalization] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem: Removing identity from faces while preserving attributes]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method: Conditional diffusion inversion with identity-guided conditioning]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results: State-of-the-art balance of identity removal, attribute preservation, and quality]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [UAV, YOLOv8, ResNet50, VGG16, Edge Inference]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Soham Dutta, Soham Banerjee, Sneha Mahata, Anindya Sen, Sayantani Datta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Heritage Institute of Technology, Kolkata"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22990",children:"https://arxiv.org/pdf/2512.22990"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified UAV-based pipeline integrating leaf disease detection, apple freshness classification, and fruit localization. 2. A cost-effective hardware architecture using ESP32-CAM and Raspberry Pi for offline, on-site inference. 3. High-performance results using only RGB sensors as a low-cost alternative to multispectral solutions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a low-cost, integrated UAV pipeline using RGB cameras and deep learning models (ResNet50, VGG16, YOLOv8) to perform apple leaf disease diagnosis, fruit freshness assessment, and detection in orchards. The system runs offline on edge devices like Raspberry Pi and ESP32-CAM. Experiments show high accuracy, providing a practical and affordable alternative to expensive multispectral systems for precision agriculture."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["A Low-Cost UAV Deep Learning Pipeline<br>\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem<br>Orchard tasks isolated & costly<br>\u679c\u56ed\u4efb\u52a1\u5b64\u7acb\u4e14\u6210\u672c\u9ad8"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method<br>Unified RGB-only UAV pipeline<br>\u7edf\u4e00\u7684\u4ec5RGB\u65e0\u4eba\u673a\u6d41\u7a0b"]\n    C --\x3e D["Models: ResNet50, VGG16, YOLOv8<br>\u6a21\u578b"]\n    C --\x3e E["Hardware: ESP32-CAM, Raspberry Pi<br>\u786c\u4ef6"]\n    A --\x3e F["\u5173\u952e\u7ed3\u679c/Results<br>High accuracy & offline inference<br>\u9ad8\u7cbe\u5ea6\u4e0e\u79bb\u7ebf\u63a8\u7406"]\n    F --\x3e G["Disease: 98.9%<br>\u75be\u75c5\u68c0\u6d4b"]\n    F --\x3e H["Freshness: 97.4%<br>\u65b0\u9c9c\u5ea6\u8bc4\u4f30"]\n    F --\x3e I["Detection F1: 0.857<br>\u679c\u5b9e\u68c0\u6d4b"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object recognition], [Geo-Semantic Contextual Graph, graph-based classifier, contextual reasoning, panoptic segmentation, metric depth]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ciprian Constantinescu, Marius Leordeanu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Science and Technology POLITEHNICA Bucharest"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23024",children:"https://arxiv.org/pdf/2512.23024"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel method for constructing a Geo-Semantic Contextual Graph (GSCG) from a monocular image by fusing metric depth with panoptic and material segmentation. 2. A specialized graph-based classifier that aggregates features from a target object, its neighbors, and the global scene context for classification. 3. Demonstrating that the explicit, structured, and interpretable context of the GSCG significantly outperforms context-agnostic and strong baseline models on object recognition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel framework for contextual object classification by first constructing a Geo-Semantic Contextual Graph (GSCG) from a single image to represent objects and their relationships, and then using a graph-based classifier to leverage this context. The method achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic models and strong baselines like fine-tuned ResNets and a multimodal LLM, highlighting the power of structured, interpretable scene context."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u4f20\u7edf\u7269\u4f53\u8bc6\u522b\u7cfb\u7edf\u5ffd\u7565\u5173\u952e\u4e0a\u4e0b\u6587\u4fe1\u606f/Traditional object recognition systems ignore vital contextual information.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6784\u5efa\u5730\u7406\u8bed\u4e49\u4e0a\u4e0b\u6587\u56fe\u5e76\u4f7f\u7528\u56fe\u5206\u7c7b\u5668/Build Geo-Semantic Contextual Graph and use a graph-based classifier.]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u51c6\u786e\u738773.4%\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf/Context-aware model achieves 73.4% accuracy, significantly surpassing baselines.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D visual grounding], [open-world, zero-shot, active cognition-based reasoning, object lookup table, visual language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University, China Mobile Zijin Innovation Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23020",children:"https://arxiv.org/pdf/2512.23020"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u5bf9\u8c61\u8868\uff0c\u65e0\u6cd5\u5904\u7406\u672a\u5b9a\u4e49\u76ee\u6807/Existing methods rely on pre-defined OLT, limiting open-world application]\n    C --\x3e C1[\u63d0\u51faOpenGround\u6846\u67b6\u4e0e\u4e3b\u52a8\u8ba4\u77e5\u63a8\u7406\u6a21\u5757/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]\n    C1 --\x3e C2[\u901a\u8fc7\u8ba4\u77e5\u4efb\u52a1\u94fe\u548c\u52a8\u6001\u66f4\u65b0\u7684OLT\u589e\u5f3aVLM\u8ba4\u77e5/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]\n    D --\x3e D1[Nr3D\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff0cScanRefer\u4e0a\u8fbe\u5230SOTA/Competitive on Nr3D, SOTA on ScanRefer]\n    D --\x3e D2[\u5728OpenTarget\u6570\u636e\u96c6\u4e0a\u63d0\u534717.6%/17.6% improvement on OpenTarget dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [MobResTaNet, Explainable AI (XAI), lightweight model, ultrasound diagnosis, web-mobile platform]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fuyad Hasan Bhoyan, Prashanta Sarker, Parsia Noor Ethila, Md. Emon Hossain, Md Kaviul Hossain, Md Humaion Kabir Mehedi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Liberal Arts Bangladesh, BRAC University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23033",children:"https://arxiv.org/pdf/2512.23033"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid deep learning model (MobResTaNet) for classifying ten gallbladder conditions from ultrasound images with high accuracy (99.85%) and low parameter count (2.24M). 2. Developed an interpretable diagnostic system with real-time Explainable AI (XAI) visualizations to support transparent clinical decision-making. 3. Deployed the system as an efficient and accessible web-mobile software platform using technologies like HTML, CSS, JavaScript, Bootstrap, and Flutter for point-of-care use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of interpreting gallbladder ultrasound images by developing an AI-driven diagnostic software. The core method is a lightweight hybrid deep learning model called MobResTaNet, which classifies diseases and provides real-time, interpretable predictions via XAI. The main conclusion is that the system achieves high accuracy with a small model size and is successfully deployed as accessible web and mobile applications for clinical support."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Interpretable Gallbladder Ultrasound Diagnosis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Challenging ultrasound interpretation for gallbladder diseases]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: AI software with lightweight MobResTaNet model & real-time XAI]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 99.85% accuracy, 2.24M parameters, deployed web-mobile platform]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] An Architecture-Led Hybrid Report on Body Language Detection Project"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [vision-language models, structured generation, bounding boxes, mixture-of-experts, video analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Thomson Tong, Diba Darooneh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," None"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23028",children:"https://arxiv.org/pdf/2512.23028"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," BodyLanguageDetection repository [1]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[An Architecture-Led Hybrid Report on Body Language Detection Project] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5982\u4f55\u57fa\u4e8eVLM\u67b6\u6784\u6784\u5efa\u53ef\u9760\u7684\u5e94\u7528\u7cfb\u7edf/How to build reliable application systems based on VLM architecture]\n    C --\x3e C1[\u5206\u6790\u4e24\u79cdVLM\u67b6\u6784\u5e76\u6620\u5c04\u5230\u89c6\u9891\u5904\u7406\u6d41\u7a0b/Analyze two VLM architectures and map to a video processing pipeline]\n    C --\x3e C2[\u7cfb\u7edf\u91c7\u6837\u89c6\u9891\u5e27\uff0c\u63d0\u793aVLM\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa/System samples video frames, prompts VLM for structured output]\n    C --\x3e C3[\u4f7f\u7528\u9884\u5b9a\u4e49\u6a21\u5f0f\u9a8c\u8bc1\u8f93\u51fa\u7ed3\u6784/Validate output structure with predefined schema]\n    D --\x3e D1[\u7ed3\u6784\u5316\u8f93\u51fa\u53ef\u80fd\u8bed\u6cd5\u6b63\u786e\u4f46\u8bed\u4e49\u9519\u8bef/Structured outputs can be syntactically valid but semantically incorrect]\n    D --\x3e D2[\u6a21\u5f0f\u9a8c\u8bc1\u662f\u7ed3\u6784\u6027\u7684\uff0c\u975e\u51e0\u4f55\u6b63\u786e\u6027/Schema validation is structural, not geometric]\n    D --\x3e D3[\u7406\u89e3\u67b6\u6784\u5bf9\u8bbe\u8ba1\u7a33\u5065\u63a5\u53e3\u548c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981/Understanding architecture is critical for robust interface design and evaluation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [semi-supervised learning, pseudo-label drift, vision-language model, self-supervised model, dual-student architecture]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Qinghai University, Beijing Jiaotong University, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23035",children:"https://arxiv.org/pdf/2512.23035"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://xavierjiezou.github.io/Co2S/",children:"https://xavierjiezou.github.io/Co2S/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Co2S, a stable semi-supervised RS segmentation framework that fuses priors from vision-language and self-supervised models. 2. Introduced a heterogeneous dual-student architecture with ViT models initialized by CLIP and DINOv3 to mitigate error accumulation. 3. Developed an explicit-implicit semantic co-guidance mechanism and a global-local feature collaborative fusion strategy to enhance semantic consistency and segmentation precision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of pseudo-label drift in semi-supervised remote sensing image segmentation. It proposes the Co2S framework, which uses a dual-student architecture combining CLIP and DINOv3 models, along with co-guidance and co-fusion mechanisms, to improve stability and accuracy. Experiments on six datasets show the method achieves leading performance across various scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u4f2a\u6807\u7b7e\u6f02\u79fb/Pseudo-label Drift")\n    Problem --\x3e P2("\u6807\u6ce8\u8d1f\u62c5/Annotation Burden")\n    Method --\x3e M1("\u5f02\u6784\u53cc\u5b66\u751f\u67b6\u6784/Heterogeneous Dual-Student")\n    Method --\x3e M2("\u663e\u5f0f-\u9690\u5f0f\u8bed\u4e49\u534f\u540c\u5f15\u5bfc/Explicit-Implicit Co-Guidance")\n    Method --\x3e M3("\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408/Global-Local Feature Fusion")\n    M1 --\x3e M1_1("CLIP ViT")\n    M1 --\x3e M1_2("DINOv3 ViT")\n    Results --\x3e R1("\u516d\u4e2a\u6570\u636e\u96c6\u9886\u5148\u6027\u80fd/Leading Performance on Six Datasets")\n    Results --\x3e R2("\u591a\u79cd\u5212\u5206\u534f\u8bae\u9c81\u68d2/Robust Across Partition Protocols")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] 3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D scene understanding], [self-supervised learning, point clouds, video reconstruction, geometric regularization, indoor segmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," AIST, University of Technology Nuremberg, INRIA (Valeo.ai)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23042",children:"https://arxiv.org/pdf/2512.23042"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LAM3C, a self-supervised framework for learning 3D representations from video-generated point clouds without real 3D scans. 2. Introduces RoomTours, a large-scale dataset of 49,219 video-generated point cloud scenes from web videos. 3. Designs a noise-regularized loss to enforce local geometric smoothness and stabilize feature learning on noisy point clouds."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high cost of collecting 3D scans by proposing a method to learn 3D representations from unlabeled videos. The core method, LAM3C, learns from video-generated point clouds using a novel noise-regularized loss and a new dataset called RoomTours. The results show that this approach outperforms previous self-supervised methods on indoor segmentation tasks, demonstrating that videos are a viable and abundant source for 3D pre-training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u5927\u89c4\u6a213D\u626b\u63cf\u6602\u8d35\u4e14\u8017\u65f6/Large-scale 3D scan collection is expensive and labor-intensive")\n    Method --\x3e M1("\u63d0\u51faLAM3C\u6846\u67b6/Propose LAM3C framework")\n    Method --\x3e M2("\u5f15\u5165RoomTours\u6570\u636e\u96c6/Introduce RoomTours dataset")\n    Method --\x3e M3("\u4f7f\u7528\u566a\u58f0\u6b63\u5219\u5316\u635f\u5931/Use noise-regularized loss")\n    Results --\x3e R1("\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5/Outperforms previous self-supervised methods")\n    Results --\x3e R2("\u8bc1\u660e\u89c6\u9891\u662f\u4e30\u5bcc\u76843D\u6570\u636e\u6e90/Demonstrates videos as an abundant 3D data source")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Video-BrowseComp: Benchmarking Agentic Video Research on Open Web"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [agentic video research, open-web benchmark, temporal visual evidence, video browsing, multimodal reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Singapore Management University, University of Trento, Beijing Academy of Artificial Intelligence, Beijing University of Posts and Telecommunications, Hong Kong Polytechnic University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23044",children:"https://arxiv.org/pdf/2512.23044"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://liang-zhengyang.github.io/video-browsecomp/",children:"https://liang-zhengyang.github.io/video-browsecomp/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Video-BrowseComp, the first benchmark for open-web agentic video reasoning, comprising 210 questions that require active navigation of video timelines. 2. Enforces a mandatory dependency on temporal visual evidence, ensuring answers cannot be derived from text search alone, thus evaluating true video grounding. 3. Reveals a critical performance bottleneck in state-of-the-art models, showing they rely heavily on textual proxies and fail in metadata-sparse, dynamic video domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a gap in evaluating proactive, agentic video research on the open web. To address this, it introduces the Video-BrowseComp benchmark, which requires models to actively navigate and reason over video timelines to answer questions. The evaluation shows that even advanced models perform poorly, highlighting a critical reliance on text and a failure in visually-grounded video understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Video-BrowseComp: Benchmarking Agentic Video Research on Open Web] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8bc4\u4f30\u88ab\u52a8\u89c6\u9891\u611f\u77e5/Existing benchmarks only evaluate passive video perception]\n    B --\x3e B2[\u7f3a\u4e4f\u5bf9\u4e3b\u52a8\u3001\u5f00\u653e\u5f0f\u7f51\u7edc\u89c6\u9891\u7814\u7a76\u7684\u8bc4\u4f30/Lack of evaluation for agentic, open-web video research]\n    C --\x3e C1[\u63d0\u51faVideo-BrowseComp\u57fa\u51c6/Propose Video-BrowseComp benchmark]\n    C --\x3e C2[\u5f3a\u5236\u4f9d\u8d56\u65f6\u5e8f\u89c6\u89c9\u8bc1\u636e/Enforce mandatory dependency on temporal visual evidence]\n    C --\x3e C3[\u5305\u542b210\u4e2a\u6311\u6218\u6027\u95ee\u9898/Comprises 210 challenging questions]\n    D --\x3e D1[\u6700\u5148\u8fdb\u6a21\u578b\u51c6\u786e\u7387\u4f4e/State-of-the-art models achieve low accuracy (e.g., 15.24%)]\n    D --\x3e D2[\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4ee3\u7406/Models heavily rely on textual proxies]\n    D --\x3e D3[\u5728\u5143\u6570\u636e\u7a00\u758f\u7684\u52a8\u6001\u73af\u5883\u4e2d\u5931\u8d25/Models fail in metadata-sparse, dynamic environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northeastern University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23073",children:"https://arxiv.org/pdf/2512.23073"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Ming-K9/MFT-VLM",children:"https://github.com/Ming-K9/MFT-VLM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model's existing knowledge, not just from updating weights."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models<br>\u91cd\u65b0\u601d\u8003\u5fae\u8c03\uff1a\u89e3\u9501\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u80fd\u529b"]\n    Root --\x3e Problem["Problem: Traditional fine-tuning overlooks underutilized structures in pre-trained VLMs.<br>\u6838\u5fc3\u95ee\u9898\uff1a\u4f20\u7edf\u5fae\u8c03\u5ffd\u7565\u4e86\u9884\u8bad\u7ec3VLM\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u7ed3\u6784\u3002"]\n    Root --\x3e Method["Method: Apply Mask Fine-Tuning (MFT) to VLMs for structural reparameterization.<br>\u4e3b\u8981\u65b9\u6cd5\uff1a\u5c06\u63a9\u7801\u5fae\u8c03\uff08MFT\uff09\u5e94\u7528\u4e8eVLM\u8fdb\u884c\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u3002"]\n    Root --\x3e Results["Results: MFT surpasses LoRA and full fine-tuning without altering the backbone.<br>\u5173\u952e\u7ed3\u679c\uff1aMFT\u8d85\u8d8a\u4e86LoRA\u548c\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u4e14\u4e0d\u6539\u53d8\u4e3b\u5e72\u7f51\u7edc\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MedSAM-based lung masking for multi-label chest X-ray classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [MedSAM, lung segmentation, multi-label classification, chest X-ray, spatial prior]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Missouri State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23089",children:"https://arxiv.org/pdf/2512.23089"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MedSAM-based lung masking for multi-label chest X-ray classification] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Automated CXR interpretation is challenging<br>\u81ea\u52a8CXR\u89e3\u8bfb\u5177\u6709\u6311\u6218\u6027]\n    C --\x3e C1[Fine-tune MedSAM for lung segmentation<br>\u5fae\u8c03MedSAM\u8fdb\u884c\u80ba\u90e8\u5206\u5272]\n    C --\x3e C2[Use masks to guide multi-label classification<br>\u4f7f\u7528\u63a9\u7801\u6307\u5bfc\u591a\u6807\u7b7e\u5206\u7c7b]\n    D --\x3e D1[Masking effect is task/architecture dependent<br>\u63a9\u7801\u6548\u679c\u4f9d\u8d56\u4e8e\u4efb\u52a1\u548c\u67b6\u6784]\n    D --\x3e D2[Trade-off: abnormality vs. normal screening<br>\u6743\u8861\uff1a\u5f02\u5e38\u68c0\u6d4b\u4e0e\u6b63\u5e38\u7b5b\u67e5]\n    D --\x3e D3[Masking is a controllable spatial prior<br>\u63a9\u7801\u662f\u4e00\u79cd\u53ef\u63a7\u7684\u7a7a\u95f4\u5148\u9a8c]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image synthesis], [diffusion model, disentangled representation, pathological residual, anatomical manifold, seam-aware fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23130",children:"https://arxiv.org/pdf/2512.23130"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: \u6210\u50cf-\u75c5\u7406MRI\u5408\u6210"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u5bfc\u81f4\u89e3\u5256\u7ed3\u6784\u635f\u574f"] --\x3e P1["\u73b0\u6709\u8303\u5f0f/Existing Paradigms<br>Global pixel domain or binary masks<br>\u5168\u5c40\u50cf\u7d20\u57df\u6216\u4e8c\u8fdb\u5236\u63a9\u7801"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Disentangled Deviation Diffusion<br>\u89e3\u8026\u504f\u5dee\u6269\u6563"] --\x3e M1["\u5206\u89e3\u4efb\u52a1/Decompose Task<br>1. Deterministic anatomical reconstruction<br>\u786e\u5b9a\u6027\u89e3\u5256\u91cd\u5efa<br>2. Stochastic deviation modeling<br>\u968f\u673a\u504f\u5dee\u5efa\u6a21"]\n    Method --\x3e M2["\u6838\u5fc3\u6a21\u578b/Core Model<br>Deviation-Space Diffusion Model<br>\u504f\u5dee\u7a7a\u95f4\u6269\u6563\u6a21\u578b<br>Learns pathological residuals<br>\u5b66\u4e60\u75c5\u7406\u6b8b\u5dee"]\n    Method --\x3e M3["\u878d\u5408\u4e0e\u7a33\u5b9a/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>\u63a5\u7f1d\u611f\u77e5\u878d\u5408\u4e0e\u63a8\u7406\u65f6\u7a33\u5b9a\u6a21\u5757"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Outperforms baselines<br>\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b"] --\x3e R1["\u8bc4\u4f30/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>\u80bf\u7624\u57fa\u51c6\u4e0a\u7684\u5b9a\u91cf\u4e0e\u5b9a\u6027\u8bc4\u4f30"]\n    Results --\x3e R2["\u4f18\u52bf/Advantages<br>Higher perceptual realism & anatomical fidelity<br>\u66f4\u9ad8\u7684\u611f\u77e5\u771f\u5b9e\u6027\u4e0e\u89e3\u5256\u4fdd\u771f\u5ea6"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image registration], [deformable registration, domain shift, local features, UniReg, cross-modal]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mingzhen Shao, Sarang Joshi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Utah"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23142",children:"https://arxiv.org/pdf/2512.23142"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that domain-shift immunity is an inherent property of deep deformable registration models, stemming from their reliance on local feature representations. 2. Introduces UniReg, a universal registration framework that decouples feature extraction from deformation estimation to validate this mechanism. 3. Reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the robustness of deep learning models for deformable image registration under domain shift. It proposes UniReg, a framework using fixed feature extractors and a UNet, to show that reliance on local features provides inherent domain-shift immunity. The findings indicate that local feature consistency is key to robustness, and early CNN layers are the source of failure in conventional models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\nA["Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Are deep deformable registration models robust to domain shift?"]\nA --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Propose UniReg, a framework decoupling feature extraction and deformation estimation."]\nA --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Models have inherent domain-shift immunity due to local features; UniReg shows robust cross-domain performance."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object detection], [semi-supervised learning, teacher-student framework, geometric relation supervision, voxel-wise augmentation, pseudo-labeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fudan University, Shanghai Innovation Institute, Tongji University, The University of Hong Kong, Huazhong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23147",children:"https://arxiv.org/pdf/2512.23147"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/SII-Whaleice/GeoTeacher",children:"https://github.com/SII-Whaleice/GeoTeacher"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student model. 2. A voxel-wise data augmentation strategy with a distance-decay mechanism to increase the diversity of object geometries while preserving distant object integrity. 3. A flexible framework that can be combined with different semi-supervised 3D object detection methods to further improve their performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes GeoTeacher, a geometry-guided semi-supervised 3D object detection method. It introduces a geometric relation supervision module and a novel voxel-wise augmentation strategy to enhance the student model's ability to capture object geometries when labeled data is limited. Experiments on ONCE and Waymo datasets show the method achieves state-of-the-art performance and demonstrates good generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u51e0\u4f55\u4fe1\u606f/Existing methods overlook geometric information]\n    C --\x3e C1[\u5173\u952e\u70b9\u51e0\u4f55\u5173\u7cfb\u76d1\u7763/Keypoint-based geometric relation supervision]\n    C --\x3e C2[\u4f53\u7d20\u6570\u636e\u589e\u5f3a\u4e0e\u8ddd\u79bb\u8870\u51cf/Voxel-wise augmentation with distance-decay]\n    D --\x3e D1[\u5728ONCE\u548cWaymo\u4e0aSOTA/SOTA on ONCE and Waymo]\n    D --\x3e D2[\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027/Method shows good generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotic imitation learning], [world model, vision-language-action (VLA) model, inverse dynamics model, surgical robotics, synthetic data generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23162",children:"https://arxiv.org/pdf/2512.23162"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Data scarcity for paired video-action data in surgical robotics]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Build SurgWorld world model to generate synthetic videos; Use inverse dynamics to infer pseudo-kinematics]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Surgical VLA policy trained with augmented data outperforms policy trained only on real data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [text-image alignment evaluation], [reinforcement learning, multimodal large language models, visual reasoning, element-level grounding, group relative policy optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fulin Shi, Wenyi Xiao, Bin Chen, Liang Din, Leilei Gan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23169",children:"https://arxiv.org/pdf/2512.23169"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes REVEALER, a unified framework for element-level text-image alignment evaluation based on a structured "grounding-reasoning-conclusion" visual reasoning paradigm. 2. Introduces a reinforcement learning optimization method using Group Relative Policy Optimization (GRPO) with a composite reward function to enhance judgment quality. 3. Demonstrates state-of-the-art performance and superior inference efficiency across multiple benchmarks compared to existing methods and proprietary models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of coarse-grained and non-interpretable evaluation of text-to-image model outputs. It proposes REVEALER, a framework that uses reinforcement-guided visual reasoning with MLLMs to perform fine-grained, element-level alignment assessment. Experiments show REVEALER achieves state-of-the-art performance and is more efficient than existing iterative reasoning methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027/Existing evaluation lacks fine-grained interpretability]\n    C --\x3e C1[\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u63a8\u7406/Reinforcement-guided structured visual reasoning]\n    C --\x3e C2["\u8303\u5f0f: \u5b9a\u4f4d-\u63a8\u7406-\u7ed3\u8bba/Paradigm: grounding-reasoning-conclusion"]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA/Achieves SOTA on multiple benchmarks]\n    D --\x3e D2[\u4f18\u4e8e\u4e13\u6709\u6a21\u578b\u548c\u57fa\u7ebf/Superior to proprietary models & baselines]\n    D --\x3e D3[\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387/Higher inference efficiency]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Milwaukee School of Engineering, Emory University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23177",children:"https://arxiv.org/pdf/2512.23177"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Project VIPR: Machine Learning-Assisted Vocal Cord Ultrasound Examination] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VCUS accuracy is operator-dependent]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use ML models for vocal cord segmentation and VCP classification]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Segmentation accuracy 96%, VIPRnet classification accuracy 99%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object detection], [Gaussian Splatting, Voxel Representation, Multi-View Learning, Synergistic Learning, Geometry Extraction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong Polytechnic University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23176",children:"https://arxiv.org/pdf/2512.23176"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel synergistic framework (GVSynergy-Det) that integrates continuous Gaussian and discrete voxel representations for complementary geometric feature learning in 3D object detection. 2. Adapts generalizable Gaussian Splatting to extract geometric features for detection and develops a cross-representation enhancement mechanism to enrich voxel features with Gaussian-derived details. 3. Achieves state-of-the-art performance on major indoor benchmarks (ScanNetV2, ARKitScenes) without requiring dense 3D supervision (e.g., depth or point clouds)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GVSynergy-Det, a novel image-based 3D object detection framework that synergistically combines Gaussian and voxel representations to capture complementary geometric information without dense 3D supervision. The method integrates features from both representations through a learnable mechanism, enabling more accurate object localization. Experiments show it achieves state-of-the-art results on indoor benchmarks, outperforming existing methods while maintaining a compact model size."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u57fa\u4e8e\u56fe\u50cf\u76843D\u68c0\u6d4b\u6311\u6218/Image-based 3D Detection Challenges]\n    P1 --\x3e P2[\u9ad8\u7cbe\u5ea6\u9700\u5bc6\u96c63D\u76d1\u7763/High accuracy requires dense 3D supervision]\n    P1 --\x3e P3[\u65e0\u76d1\u7763\u96be\u4ee5\u63d0\u53d6\u51c6\u786e\u51e0\u4f55/Unsupervised struggles with geometry extraction]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u53cc\u8868\u5f81\u534f\u540c\u5b66\u4e60/Dual-Representation Synergistic Learning]\n    M1 --\x3e M2[\u9ad8\u65af\u6e85\u5c04\u63d0\u53d6\u51e0\u4f55\u7279\u5f81/Gaussian Splatting for geometric features]\n    M1 --\x3e M3[\u4f53\u7d20\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587/Voxels provide spatial context]\n    M1 --\x3e M4[\u8de8\u8868\u5f81\u589e\u5f3a\u673a\u5236/Cross-representation enhancement mechanism]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[SOTA\u6027\u80fd/State-of-the-art performance]\n    R1 --\x3e R2[ScanNetV2 & ARKitScenes\u6570\u636e\u96c6/ScanNetV2 & ARKitScenes datasets]\n    Results --\x3e R3[\u65e0\u9700\u5bc6\u96c63D\u76d1\u7763/No dense 3D supervision required]\n    Results --\x3e R4[\u7d27\u51d1\u6a21\u578b\u5c3a\u5bf8/Compact model size]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [autonomous driving, 3D scene understanding], [3D Gaussian Splatting, Driving World Model, Multi-Modal Generation, Vision-Language Model, Scene Representation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Tsinghua University, MEGVII Technology, Mach Drive"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23180",children:"https://arxiv.org/pdf/2512.23180"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified Driving World Model framework based on 3D Gaussian scene representation for joint 3D scene understanding and multi-modal generation. 2. Introduces early modality alignment by embedding linguistic features into 3D Gaussian primitives and a task-aware language-guided sampling strategy to inject compact 3D tokens into an LLM. 3. Designs a dual-condition multi-modal generation model that uses high-level language conditions and low-level image conditions to guide the generation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes GaussianDWM, a novel Driving World Model framework that uses 3D Gaussians as a scene representation to unify 3D scene understanding and multi-modal generation for autonomous driving. It aligns text with the 3D scene via Gaussian primitives and uses a dual-condition model for generation. The method achieves state-of-the-art performance on nuScenes and NuInteract datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GaussianDWM: 3D Gaussian Driving World Model] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem: Existing DWMs lack 3D understanding and precise text-scene alignment)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method: Unified framework using 3D Gaussians for representation, with early modality alignment and dual-condition generation)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA performance on driving datasets)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [Object-Based Image Analysis (OBIA), Deep Learning, Sentinel-2, Forest Cover Mapping, UNet]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," BRAC University, Charles Sturt University, University of Technology Sydney"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23196",children:"https://arxiv.org/pdf/2512.23196"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes "ForCM", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ForCM: Forest Cover Mapping] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Accurate forest cover mapping for environmental monitoring]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Exploring Syn-to-Real Domain Adaptation for Military Target Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [domain adaptation, synthetic-to-real, Unreal Engine, military target detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23208",children:"https://arxiv.org/pdf/2512.23208"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u519b\u4e8b\u76ee\u6807\u6570\u636e\u96c6/Lack of military target datasets]\n    B --\x3e B2[SAR\u6570\u636e\u6210\u672c\u9ad8/High cost of SAR data]\n    B --\x3e B3[\u8de8\u57df\u9002\u5e94\u6311\u6218/Cross-domain adaptation challenge]\n    C --\x3e C1[\u4f7f\u7528Unreal Engine\u751f\u6210\u5408\u6210RGB\u6570\u636e/Generate synthetic RGB data using Unreal Engine]\n    C --\x3e C2[\u5408\u6210\u5230\u771f\u5b9e\u57df\u9002\u5e94\u5b9e\u9a8c/Synthetic-to-real domain adaptation experiments]\n    C --\x3e C3[\u57fa\u51c6\u6d4b\u8bd5SOTA\u65b9\u6cd5/Benchmark SOTA DA methods]\n    D --\x3e D1[\u6700\u5c0f\u76d1\u7763\u65b9\u6cd5\u8868\u73b0\u6700\u4f73/Minimal supervision methods perform best]\n    D --\x3e D2[\u8bc6\u522b\u5f53\u524d\u6311\u6218/Identify current challenges]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [diffusion timestep selection, few-shot learning, dense prediction, Taskonomy, parameter-efficient fine-tuning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST (Korea Advanced Institute of Science and Technology)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23210",children:"https://arxiv.org/pdf/2512.23210"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Task-aware Timestep Selection (TTS) module to adaptively select ideal diffusion timesteps for a given task based on losses and similarity scores. 2. Introduces a Timestep Feature Consolidation (TFC) module to consolidate the selected timestep features to improve dense prediction performance. 3. Presents a framework that, with a parameter-efficient fine-tuning adapter, achieves superior few-shot dense prediction performance on unseen tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the sub-optimal performance of diffusion models in few-shot dense prediction tasks due to heuristic timestep feature selection. It proposes a framework with learnable modules (TTS and TFC) to adaptively select and consolidate diffusion timestep features, which is validated on the Taskonomy dataset, showing superior performance in universal few-shot learning scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks<br>\u9762\u5411\u901a\u7528\u5c11\u6837\u672c\u5bc6\u96c6\u4efb\u52a1\u7684\u9762\u5411\u4efb\u52a1\u53ef\u5b66\u4e60\u6269\u6563\u65f6\u95f4\u6b65"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["Heuristic diffusion timestep selection leads to sub-optimal, task-biased performance.<br>\u542f\u53d1\u5f0f\u6269\u6563\u65f6\u95f4\u6b65\u9009\u62e9\u5bfc\u81f4\u6b21\u4f18\u7684\u3001\u504f\u5411\u7279\u5b9a\u4efb\u52a1\u7684\u6027\u80fd\u3002"]\n    Method["Proposes TTS for adaptive timestep selection and TFC for feature consolidation.<br>\u63d0\u51faTTS\u7528\u4e8e\u81ea\u9002\u5e94\u65f6\u95f4\u6b65\u9009\u62e9\u548cTFC\u7528\u4e8e\u7279\u5f81\u6574\u5408\u3002"]\n    Results["Achieves superior few-shot dense prediction on the Taskonomy dataset.<br>\u5728Taskonomy\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5c11\u6837\u672c\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [autonomous driving, adverse conditions, semantic segmentation, depth estimation, multi-task learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST (Korea Advanced Institute of Science and Technology)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23215",children:"https://arxiv.org/pdf/2512.23215"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the AVOID dataset, a new simulated driving dataset focused on unexpected small road obstacles captured under diverse adverse weather and time conditions. 2. Provides rich multi-modal annotations per image, including semantic and depth maps, LiDAR data (raw and semantic), and waypoints to support various perception tasks. 3. Benchmarks real-time obstacle detection networks and proposes ablation studies using a comprehensive multi-task network for semantic segmentation, depth, and waypoint prediction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the AVOID dataset to address the lack of driving datasets containing small, unexpected road obstacles under varied adverse conditions. The dataset, collected in simulation, provides rich multi-modal annotations and is used to benchmark real-time obstacle detection and multi-task perception models. The work aims to improve the robustness of visual perception for autonomous driving in challenging scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[AVOID Dataset] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e0d\u5229\u6761\u4ef6\u4e0b\u7684\u610f\u5916\u5c0f\u969c\u788d\u7269/Existing datasets lack small unexpected obstacles under adverse conditions]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faAVOID\u6a21\u62df\u9a7e\u9a76\u6570\u636e\u96c6/Propose AVOID simulated driving dataset]\n    M1 --\x3e M2[\u5305\u542b\u591a\u6a21\u6001\u6807\u6ce8: \u8bed\u4e49/\u6df1\u5ea6/LiDAR/\u8def\u5f84\u70b9/Contains multi-modal annotations: semantic/depth/LiDAR/waypoints]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4e3a\u5b9e\u65f6\u969c\u788d\u7269\u68c0\u6d4b\u63d0\u4f9b\u57fa\u51c6/Benchmark for real-time obstacle detection]\n    Results --\x3e R2[\u63d0\u51fa\u5e76\u8bc4\u4f30\u591a\u4efb\u52a1\u7f51\u7edc/Propose and evaluate multi-task network]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal evaluation], [multimodal large language models, unmanned aerial vehicles, benchmark, low-altitude scenarios, spatial bias]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shiqi Dai, Zizhi Ma, Zhicong Luo, Xuesong Yang, Yibin Huang, Wanyue Zhang, Chi Chen, Zonghao Guo, Wang Xu, Yufei Sun, Maosong Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Nankai University, Northwest Polytechnical University, Chinese Academy of Sciences, Harbin Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23219",children:"https://arxiv.org/pdf/2512.23219"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/MM-UAV/MM-UAVBench",children:"https://github.com/MM-UAV/MM-UAVBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MM-UAVBench, a comprehensive benchmark for evaluating MLLMs in low-altitude UAV scenarios across perception, cognition, and planning dimensions. 2. Constructs a dataset of over 5.7K manually annotated questions derived from real-world UAV data, covering 19 sub-tasks. 3. Through extensive experiments on 16 MLLMs, identifies critical bottlenecks like spatial bias and multi-view understanding that hinder model performance in these scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MM-UAVBench, a new benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in low-altitude Unmanned Aerial Vehicle (UAV) scenarios. The benchmark systematically tests models on perception, cognition, and planning using over 5.7K annotated questions from real UAV data. The evaluation reveals that current MLLMs struggle with the complex demands of low-altitude environments, highlighting key bottlenecks like spatial bias."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MM-UAVBench: How Well Do MLLMs See, Think, and Plan in Low-Altitude UAV Scenarios?] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u672a\u8986\u76d6\u4f4e\u7a7a\u65e0\u4eba\u673a\u573a\u666f\u7684\u72ec\u7279\u6311\u6218 / Existing benchmarks lack coverage of unique low-altitude UAV challenges]\n    C --\x3e C1[\u63d0\u51fa\u591a\u7ef4\u5ea6\u8bc4\u4f30\u57fa\u51c6MM-UAVBench / Propose multi-dimensional benchmark MM-UAVBench]\n    C --\x3e C2[\u5305\u542b19\u4e2a\u5b50\u4efb\u52a1\u4e0e5.7K+\u4eba\u5de5\u6807\u6ce8\u95ee\u9898 / Contains 19 sub-tasks and 5.7K+ manually annotated questions]\n    D --\x3e D1[\u5f53\u524dMLLMs\u96be\u4ee5\u9002\u5e94\u4f4e\u7a7a\u590d\u6742\u9700\u6c42 / Current MLLMs struggle to adapt to complex low-altitude demands]\n    D --\x3e D2[\u53d1\u73b0\u7a7a\u95f4\u504f\u89c1\u7b49\u5173\u952e\u74f6\u9888 / Uncover critical bottlenecks like spatial bias]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Detection Transformer (DETR), Contextual Information, Holistic Detection, Fashion Item Detection, Co-occurrence Relationship]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Youngchae Kwon, Jinyoung Choi, Injung Kim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Handong Global University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23221",children:"https://arxiv.org/pdf/2512.23221"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information\u2014co-occurrence, spatial arrangements, and body keypoints\u2014to improve detection accuracy. The method shows improved performance over baseline DETR models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: \u6574\u4f53\u65f6\u5c1a\u7269\u54c1\u68c0\u6d4b] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>\u65f6\u5c1a\u7269\u54c1\u68c0\u6d4b\u56e0\u5916\u89c2\u591a\u6837\u548c\u5b50\u7c7b\u522b\u76f8\u4f3c\u800c\u5177\u6709\u6311\u6218\u6027\u3002]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>\u63d0\u51faHoli-DETR\uff0c\u5229\u7528\u5171\u73b0\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u8eab\u4f53\u5173\u952e\u70b9\u4e09\u79cd\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u6574\u4f53\u68c0\u6d4b\u5668\u3002]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>\u6027\u80fd\u8d85\u8d8a\u539f\u59cbDETR (+3.6pp AP) \u548c Co-DETR (+1.1pp AP)\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Anomaly Detection by Effectively Leveraging Synthetic Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [synthetic data, image-to-image translation, image retrieval, two-stage training, MVTec AD]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sungkyunkwan University, The University of Texas MD Anderson Cancer Center"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23227",children:"https://arxiv.org/pdf/2512.23227"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Anomaly Detection by Effectively Leveraging Synthetic Images] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u771f\u5b9e\u7f3a\u9677\u56fe\u50cf\u7a00\u7f3a\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5728\u6210\u672c\u4e0e\u8d28\u91cf\u95f4\u96be\u4ee5\u6743\u8861/Scarcity of real defect images, trade-off between cost and quality in synthesis]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: 1. \u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u68c0\u7d22\u6a21\u578b\u9ad8\u6548\u751f\u6210\u7f3a\u9677\u56fe/Use pre-trained image-to-image & retrieval models for generation. 2. \u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u5148\u9884\u8bad\u7ec3\u518d\u5fae\u8c03/Two-stage training: pre-train then fine-tune]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728MVTec AD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd/Validated on MVTec AD, reduces cost and improves performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [diffusion-based inverse problems, Stein's Unbiased Risk Estimate (SURE), posterior sampling, trajectory correction, PCA-based noise estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Minwoo Kim, Hongki Lim"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Inha University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23232",children:"https://arxiv.org/pdf/2512.23232"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations in diffusion-based inverse problem solvers. 2. Introduces the use of Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA-based noise estimation to mitigate noise-induced errors during early and middle sampling stages. 3. Demonstrates that SGPS maintains high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs), outperforming existing methods at low NFE counts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of error accumulation in diffusion-based inverse problem solvers, which typically require many steps for high-quality reconstruction. The authors propose SURE Guided Posterior Sampling (SGPS), a method that corrects the sampling trajectory using SURE gradient updates and PCA-based noise estimation. The method reduces error accumulation, enabling high-quality reconstructions in fewer than 100 steps and outperforming existing approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["SURE Guided Posterior Sampling<br>SURE\u5f15\u5bfc\u540e\u9a8c\u91c7\u6837"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3<br>Diffusion-Based Inverse Problems"]\n    Problem --\x3e P2["\u8bef\u5dee\u7d2f\u79ef\u9700\u5927\u91cf\u6b65\u6570<br>Error Accumulation Requires Many Steps"]\n    Method --\x3e M1["SURE\u5f15\u5bfc\u8f68\u8ff9\u6821\u6b63<br>SURE Guided Trajectory Correction"]\n    Method --\x3e M2["PCA\u566a\u58f0\u4f30\u8ba1<br>PCA-Based Noise Estimation"]\n    Results --\x3e R1["\u5c11\u4e8e100\u6b21NFE<br>Fewer than 100 NFEs"]\n    Results --\x3e R2["\u9ad8\u8d28\u91cf\u91cd\u5efa<br>High-Quality Reconstruction"]\n    Results --\x3e R3["\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5<br>Outperforms Existing Methods"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Bridging Your Imagination with Audio-Video Generation via a Unified Director"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [unified director model, Mixture-of-Transformers, interleaved concept learning, disentangled expert learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance Intelligent Creation, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23222",children:"https://arxiv.org/pdf/2512.23222"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://kebii.github.io/UniMAGE",children:"https://kebii.github.io/UniMAGE"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a "first interleaving, then disentangling" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel "first interleaving, then disentangling" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u7cfb\u7edf\u5c06\u811a\u672c\u8d77\u8349\u4e0e\u5173\u952e\u955c\u5934\u8bbe\u8ba1\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1/Existing systems treat script drafting and key-shot design as disjoint tasks]\n    C --\x3e C1[\u63d0\u51fa\u7edf\u4e00\u5bfc\u6f14\u6a21\u578bUniMAGE/Propose unified director model UniMAGE]\n    C --\x3e C2[\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784/Employ Mixture-of-Transformers architecture]\n    C --\x3e C3[\u5f15\u5165"\u5148\u4ea4\u9519\uff0c\u540e\u89e3\u8026"\u8bad\u7ec3\u8303\u5f0f/Introduce "first interleaving, then disentangling" training paradigm]\n    D --\x3e D1[\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd/Achieves SOTA performance among open-source models]\n    D --\x3e D2[\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u7684\u811a\u672c\u548c\u89c6\u89c9\u4e00\u81f4\u7684\u56fe\u50cf/Generates logically coherent scripts and visually consistent keyframes]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [physics-inspired modeling, edge detection, content-adaptive routing, multi-scale feature fusion, infrared gas leak detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," (Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23234",children:"https://arxiv.org/pdf/2512.23234"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u7ea2\u5916\u6c14\u4f53\u6cc4\u6f0f\u68c0\u6d4b\u56f0\u96be/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faPEG-DRNet/Propose PEG-DRNet)\n    C --\x3e C1(\u6c14\u4f53\u5757\u5efa\u6a21\u6c14\u4f53\u4f20\u8f93/Gas Block models gas transport)\n    C --\x3e C2(\u81ea\u9002\u5e94\u68af\u5ea6\u76f8\u4f4d\u8fb9\u7f18\u7b97\u5b50/Adaptive Gradient and Phase Edge Operator (AGPEO))\n    C --\x3e C3(\u5185\u5bb9\u81ea\u9002\u5e94\u7a00\u758f\u8def\u7531\u805a\u5408\u7f51\u7edc/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728IIG\u548cLangGas\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u8d8a/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [data pruning, remote sensing, diffusion foundation models, entropy-based selection, scene-aware clustering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Sun Yat-sen University, National Supercomputing Center in Shenzhen, Tsinghua Shenzhen International Graduate School, The University of Hong Kong, Peking University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23239",children:"https://arxiv.org/pdf/2512.23239"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel, training-free, two-stage data pruning method for remote sensing diffusion models that jointly considers local information content and global scene-level diversity. 2. A reference-guided, scene-aware clustering strategy that leverages existing classification datasets to efficiently select a high-quality subset from large-scale unlabeled data. 3. Demonstrated effectiveness under high pruning ratios (e.g., 85%), significantly improving model convergence and generation quality, and achieving state-of-the-art performance on downstream tasks like super-resolution."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of training diffusion-based remote sensing foundation models on large, redundant datasets. It proposes RS-Prune, a training-free, two-stage data pruning method that first removes low-information samples and then performs scene-aware clustering for fine-grained selection. The method enables rapid model convergence even after pruning 85% of data and achieves superior performance on downstream generation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RS-Prune: Training-Free Data Pruning for RS Diffusion Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6570\u636e\u5197\u4f59\u4e0e\u566a\u58f0/Data Redundancy & Noise]\n    B --\x3e B2[\u8bad\u7ec3\u6548\u7387\u4f4e/Low Training Efficiency]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u526a\u679d/Two-Stage Pruning]\n    C1 --\x3e C1a[\u57fa\u4e8e\u71b5\u7684\u7b5b\u9009/Entropy-Based Filtering]\n    C1 --\x3e C1b[\u573a\u666f\u611f\u77e5\u805a\u7c7b/Scene-Aware Clustering]\n    D --\x3e D1[\u9ad8\u526a\u679d\u7387\u6709\u6548/Effective at High Ratios (85%)]\n    D --\x3e D2[\u63d0\u5347\u6536\u655b\u4e0e\u8d28\u91cf/Improved Convergence & Quality]\n    D --\x3e D3[\u4e0b\u6e38\u4efb\u52a1SOTA/SOTA on Downstream Tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Contour Information Aware 2D Gaussian Splatting for Image Representation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image representation], [2D Gaussian Splatting, contour awareness, segmentation priors, warm-up scheme]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Masaya Takabe, Hiroshi Watanabe, Sujun Hong, Tomohiro Ikai, Zheming Fan, Ryo Ishimoto, Kakeru Sugimoto, Ruri Imichi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Waseda University, Sharp Corporation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23255",children:"https://arxiv.org/pdf/2512.23255"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors to preserve edge structures. 2. A region-constrained rasterization method that prevents cross-boundary blending of Gaussians. 3. A warm-up training scheme to stabilize optimization and improve convergence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of blurry object boundaries in highly compressed 2D Gaussian Splatting (2DGS) for image representation. The proposed method integrates segmentation priors to constrain Gaussians to specific object regions during rasterization, preventing blending across edges. Experiments show it achieves higher reconstruction quality around edges with very few Gaussians while maintaining fast rendering and low memory usage."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Contour Information Aware 2D Gaussian Splatting<br/>\u8f6e\u5ed3\u4fe1\u606f\u611f\u77e5\u76842D\u9ad8\u65af\u6cfc\u6e85] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[Problem: Blurry boundaries in compressed 2DGS<br/>\u95ee\u9898\uff1a\u538b\u7f292DGS\u4e2d\u7684\u6a21\u7cca\u8fb9\u754c]\n    C[Method: Region-constrained rasterization with segmentation priors<br/>\u65b9\u6cd5\uff1a\u7ed3\u5408\u5206\u5272\u5148\u9a8c\u7684\u533a\u57df\u7ea6\u675f\u6805\u683c\u5316]\n    D[Results: Better edge quality with few Gaussians<br/>\u7ed3\u679c\uff1a\u7528\u5c11\u91cf\u9ad8\u65af\u5b9e\u73b0\u66f4\u597d\u7684\u8fb9\u7f18\u8d28\u91cf]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal fusion], [Dynamic Resolution Input Strategy (DRIS), Multi-scale Vision-language Alignment Mechanism (MS-VLAM), Vision-language Model (VLM), remote sensing image captioning, cross-modal retrieval]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siyu Zhang, Ying Chen, Lianlei Shan, Runhe Qiu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sanda University, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23243",children:"https://arxiv.org/pdf/2512.23243"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Dynamic Resolution Input Strategy (DRIS) that adaptively allocates computational resources based on image content complexity to balance detail and efficiency. 2. Introduces a Multi-scale Vision-language Alignment Mechanism (MS-VLAM) with object, local-region, and global-level alignment to capture cross-modal semantic consistency. 3. Presents an integrated VLM framework that demonstrates superior performance in remote sensing image captioning and cross-modal retrieval tasks on the RS-GPT4V dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of fixed-resolution inputs and the lack of semantic hierarchy in single-scale alignment for multimodal remote sensing interpretation. It proposes a novel Vision-language Model framework featuring a Dynamic Resolution Input Strategy and a Multi-scale Vision-language Alignment Mechanism. The framework significantly improves both semantic understanding accuracy and computational efficiency in tasks like image captioning and cross-modal retrieval, as validated on the RS-GPT4V dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multimodal Interpretation of Remote Sensing Images<br>\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u89e3\u8bd1] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1[Fixed resolution fails to balance efficiency and detail<br>\u56fa\u5b9a\u5206\u8fa8\u7387\u65e0\u6cd5\u5e73\u8861\u6548\u7387\u4e0e\u7ec6\u8282]\n    B1 --\x3e C2[Single-scale alignment lacks semantic hierarchy<br>\u5355\u5c3a\u5ea6\u5bf9\u9f50\u7f3a\u4e4f\u8bed\u4e49\u5c42\u6b21]\n    B2 --\x3e D1[Dynamic Resolution Input Strategy (DRIS)<br>\u52a8\u6001\u5206\u8fa8\u7387\u8f93\u5165\u7b56\u7565]\n    B2 --\x3e D2[Multi-scale Vision-language Alignment Mechanism (MS-VLAM)<br>\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u673a\u5236]\n    B3 --\x3e E1[Improves accuracy in captioning & retrieval<br>\u63d0\u5347\u63cf\u8ff0\u4e0e\u68c0\u7d22\u7cbe\u5ea6]\n    B3 --\x3e E2[Enhances computational efficiency<br>\u63d0\u5347\u8ba1\u7b97\u6548\u7387]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [text-to-image generation], [identity-consistent generation, text embedding modification, padding embeddings, adaptive feature-sharing, Consistency Quality Score (CQS)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shin seong Kim, Minjung Shin, Hyunin Cho, Youngjung Uh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yonsei University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23245",children:"https://arxiv.org/pdf/2512.23245"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://minjung-s.github.io/asemconsist",children:"https://minjung-s.github.io/asemconsist"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework (ASemConsist) for identity-consistent generation using selective text embedding modification and a semantic control strategy that repurposes padding embeddings as semantic containers. 2. An adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to ambiguous identity prompts. 3. A unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and prompt alignment into a single metric to capture performance imbalances."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating a sequence of images with consistent character identity while maintaining alignment with diverse per-image prompts. The proposed ASemConsist framework modifies text embeddings to control identity semantics, uses padding embeddings as semantic containers, and adaptively applies constraints based on prompt ambiguity. It achieves state-of-the-art performance by overcoming the trade-off between identity consistency and prompt alignment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ASemConsist: Adaptive Semantic Feature Control] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8eab\u4efd\u4e00\u81f4\u6027\u751f\u6210\u4e2d\u7684\u6743\u8861/Trade-off in identity-consistent generation]\n    B1 --\x3e B2[\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u4e0e\u786e\u4fdd\u6bcf\u5f20\u56fe\u50cf\u63d0\u793a\u5bf9\u9f50\u4e4b\u95f4\u7684\u51b2\u7a81/Conflict between identity preservation and per-image prompt alignment]\n    C --\x3e C1[\u9009\u62e9\u6027\u6587\u672c\u5d4c\u5165\u4fee\u6539/Selective text embedding modification]\n    C --\x3e C2[\u5c06\u586b\u5145\u5d4c\u5165\u91cd\u65b0\u7528\u4f5c\u8bed\u4e49\u5bb9\u5668/Repurposing padding embeddings as semantic containers]\n    C --\x3e C3[\u81ea\u9002\u5e94\u7279\u5f81\u5171\u4eab\u7b56\u7565/Adaptive feature-sharing strategy]\n    D --\x3e D1[\u63d0\u51fa\u7edf\u4e00\u8bc4\u4f30\u534f\u8baeCQS/Proposed unified evaluation protocol CQS]\n    D --\x3e D2[\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd/Achieved state-of-the-art performance]\n    D --\x3e D3[\u514b\u670d\u4e86\u5148\u524d\u7684\u6743\u8861/Overcame prior trade-offs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [diffusion transformer, inference acceleration, caching, error minimization, dynamic programming]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tong Shao, Yusen Fu, Guoying Sun, Jingde Kong, Zhuotao Tian, Jingyong Su"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Shenzhen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23258",children:"https://arxiv.org/pdf/2512.23258"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CEM, a novel plugin for optimizing caching strategies in DiT acceleration via cumulative error minimization. 2. Introduces a dynamic programming algorithm guided by a predefined error prior to adaptively minimize caching error. 3. Demonstrates the method's model-agnostic nature, seamless integration into existing frameworks, and significant fidelity improvements across multiple models and tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the slow inference of Diffusion Transformers (DiTs) by proposing CEM, a plug-and-play fidelity optimization plugin. CEM minimizes cumulative caching error via a dynamic programming algorithm, adapting to error variations during denoising. The method is training-free, model-agnostic, and significantly improves generation fidelity when integrated with existing acceleration techniques."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: DiT\u63a8\u7406\u6162\uff0c\u73b0\u6709\u7f13\u5b58\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u56fa\u5b9a\u7b56\u7565\u5bfc\u81f4\u7684\u7d2f\u79ef\u8bef\u5dee/DiT inference is slow; existing caching-based acceleration suffers from fixed-strategy cumulative error"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faCEM\u63d2\u4ef6\uff0c\u901a\u8fc7\u7d2f\u79ef\u8bef\u5dee\u6700\u5c0f\u5316\u7684\u52a8\u6001\u89c4\u5212\u4f18\u5316\u7f13\u5b58\u7b56\u7565/Propose CEM plugin, optimizing caching strategy via cumulative error minimization with dynamic programming"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u663e\u8457\u63d0\u5347\u751f\u6210\u4fdd\u771f\u5ea6\uff0c\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210/Significantly improves generation fidelity, model-agnostic, seamlessly integrable"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [change detection], [vision-language model, remote sensing, semantic change detection, supervised fine-tuning, reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fudan University, Shanghai Artificial Intelligence Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23244",children:"https://arxiv.org/pdf/2512.23244"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ViLaCD-R1: \u9065\u611f\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u7684\u89c6\u89c9\u8bed\u8a00\u6846\u67b6] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1[\u4f20\u7edf\u65b9\u6cd5\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3/Traditional methods lack semantic understanding]\n    B1 --\x3e C2[\u73b0\u6709VLM\u65b9\u6cd5\u5b9a\u4f4d\u4e0d\u51c6\u786e/Existing VLM methods have inaccurate localization]\n    B2 --\x3e D1[\u4e24\u9636\u6bb5\u6846\u67b6/Two-stage framework]\n    D1 --\x3e E1[\u591a\u56fe\u50cf\u63a8\u7406\u5668/Multi-Image Reasoner]\n    E1 --\x3e F1[SFT\u4e0eRL\u8bad\u7ec3/SFT and RL training]\n    E1 --\x3e F2[\u751f\u6210\u7c97\u53d8\u5316\u63a9\u7801/Generate coarse change mask]\n    D1 --\x3e E2[\u63a9\u7801\u5f15\u5bfc\u89e3\u7801\u5668/Mask-Guided Decoder]\n    E2 --\x3e F3[\u878d\u5408\u7279\u5f81\u4e0e\u63a9\u7801/Fuse features and mask]\n    E2 --\x3e F4[\u9884\u6d4b\u7cbe\u7ec6\u53d8\u5316\u56fe/Predict precise change map]\n    B3 --\x3e G1[\u63d0\u5347\u8bed\u4e49\u53d8\u5316\u8bc6\u522b/Improves semantic change recognition]\n    B3 --\x3e G2[\u6291\u5236\u975e\u8bed\u4e49\u53d8\u5316/Suppresses non-semantic variations]\n    B3 --\x3e G3[\u8fbe\u5230SOTA\u6027\u80fd/Achieves SOTA performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Mixture-of-Experts, adaptive computation, real-time detection, dynamic routing, YOLO]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xu Lin, Jinlong Peng, Zhenye Gan, Jiawen Zhu, Jun Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent Youtu Lab, Singapore Management University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23273",children:"https://arxiv.org/pdf/2512.23273"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/isLinXu/YOLO-Master",children:"https://github.com/isLinXu/YOLO-Master"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for real-time object detection. 2. Designs an Efficient Sparse Mixture-of-Experts (ES-MoE) block to dynamically allocate computational resources based on scene complexity. 3. Introduces a lightweight dynamic routing network with a diversity-enhancing objective to encourage complementary expert specialization and efficient inference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that existing YOLO models use static computation, leading to inefficiency on simple scenes and poor performance on complex ones. To solve this, it proposes YOLO-Master, which uses a Mixture-of-Experts block and a dynamic router to adaptively allocate computation per input. Experiments show it achieves higher accuracy and faster speed than baselines, especially on dense scenes, while maintaining real-time performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[YOLO-Master] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9759\u6001\u8ba1\u7b97\u5bfc\u81f4\u8d44\u6e90\u9519\u914d/Static computation misallocates resources]\n    C --\x3e C1[\u9ad8\u6548\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u5757/ES-MoE Block]\n    C --\x3e C2[\u52a8\u6001\u8def\u7531\u7f51\u7edc/Dynamic Routing Network]\n    D --\x3e D1[\u66f4\u9ad8\u7cbe\u5ea6\u4e0e\u901f\u5ea6/Higher Accuracy & Speed]\n    D --\x3e D2[\u5728\u5bc6\u96c6\u573a\u666f\u63d0\u5347\u663e\u8457/Better on Dense Scenes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal emotion recognition], [micro-gesture recognition, behavior-based emotion prediction, multimodal fusion, Cross-Modal Token Fusion, InterFusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arman Martirosyan, Shahane Tigranyan, Maria Razzhivina, Artak Aslanyan, Nazgul Salikhova, Ilya Makarov, Andrey Savchenko, Aram Avetisyan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Russian-Armenian University, ISP RAS, HSE University, Innopolis University, AIRI, Sber AI Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23291",children:"https://arxiv.org/pdf/2512.23291"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a multimodal framework for micro-gesture classification that fuses RGB (MViTv2-S) and 3D pose (2s-AGCN) embeddings via a Cross-Modal Token Fusion module. 2. Developed a separate multimodal framework for behavior-based emotion prediction that fuses facial (SwinFace) and contextual (MViTv2-S) embeddings via an InterFusion module. 3. Demonstrated robust performance on the iMiGUE dataset, achieving 2nd place in the behavior-based emotion prediction task of the MiGA 2025 Challenge."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents two multimodal frameworks to tackle micro-gesture recognition and behavior-based emotion prediction on the iMiGUE dataset. The first framework fuses video and skeletal pose data, while the second fuses facial and contextual embeddings for emotion classification. The method demonstrated strong performance, securing 2nd place in the emotion prediction task of the MiGA 2025 Challenge."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-Track Multimodal Learning on iMiGUE] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1(\u5fae\u624b\u52bf\u8bc6\u522b/Micro-gesture Recognition)\n    B1 --\x3e C2(\u884c\u4e3a\u60c5\u7eea\u9884\u6d4b/Behavior-based Emotion Prediction)\n    B2 --\x3e D1(\u591a\u6a21\u6001\u6846\u67b6/Multimodal Framework)\n    D1 --\x3e E1(\u5fae\u624b\u52bf: RGB+\u59ff\u6001\u878d\u5408/Micro-gesture: RGB+Pose Fusion)\n    D1 --\x3e E2(\u60c5\u7eea: \u9762\u90e8+\u4e0a\u4e0b\u6587\u878d\u5408/Emotion: Facial+Context Fusion)\n    E1 --\x3e F1(\u4f7f\u7528MViTv2-S\u548c2s-AGCN/Use MViTv2-S & 2s-AGCN)\n    E1 --\x3e F2(\u8de8\u6a21\u6001\u4ee4\u724c\u878d\u5408/Cross-Modal Token Fusion)\n    E2 --\x3e F3(\u4f7f\u7528SwinFace\u548cMViTv2-S/Use SwinFace & MViTv2-S)\n    E2 --\x3e F4(\u5185\u90e8\u878d\u5408\u6a21\u5757/InterFusion Module)\n    B3 --\x3e G1(\u5728iMiGUE\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30/Evaluated on iMiGUE Dataset)\n    B3 --\x3e G2(MiGA 2025\u6311\u6218\u8d5b\u7b2c\u4e8c\u540d/2nd Place in MiGA 2025 Challenge)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical image classification], [MedGemma, GPT-4, LoRA, zero-shot classification, multimodal LLM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Sazzadul Islam Prottasha, Nabil Walid Rafi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bangladesh University of Professionals"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23304",children:"https://arxiv.org/pdf/2512.23304"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MedGemma vs GPT-4: \u533b\u5b66\u56fe\u50cf\u96f6\u6837\u672c\u75be\u75c5\u5206\u7c7b] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u6bd4\u8f83\u5f00\u6e90\u4e0e\u95ed\u6e90\u591a\u6a21\u6001LLM\u5728\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u6027\u80fd]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528LoRA\u5fae\u8c03\u7684MedGemma\u4e0e\u672a\u8c03\u4f18\u7684GPT-4\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u5bf9\u6bd4]\n    D[\u5173\u952e\u7ed3\u679c/Results: MedGemma\u51c6\u786e\u7387(80.37%)\u548c\u654f\u611f\u6027\u66f4\u9ad8\uff0c\u9886\u57df\u5fae\u8c03\u5bf9\u51cf\u5c11\u5e7b\u89c9\u81f3\u5173\u91cd\u8981]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [visual SLAM], [ORB-SLAM3, YOLOv8, dynamic object filtering, point cloud refinement, CUDA acceleration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yuan Ze University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23318",children:"https://arxiv.org/pdf/2512.23318"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3's robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: vSLAM accuracy compromised by dynamic objects]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: ORB-SLAM3 + YOLOv8 segmentation + multi-stage point cloud filtering]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Mixed performance, notable improvement in specific sequences (e.g., Seq04)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23328",children:"https://arxiv.org/pdf/2512.23328"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLM\u667a\u80fd\u4f53\u7f3a\u4e4f\u7269\u7406\u4e16\u754c\u90e8\u7f72\u6240\u9700\u7684\u7a33\u5065\u7a7a\u95f4\u5fc3\u667a\u6a21\u578b/LLM agents lack robust spatial mental models for physical-world deployment]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u9b54\u65b9\u7684\u4e09\u5c42\u8bca\u65ad\u57fa\u51c6/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]\n    C --\x3e C2[\u4ece\u5b8c\u6574\u7b26\u53f7\u72b6\u6001\u5230\u90e8\u5206\u89c6\u89c9\u72b6\u6001\u9010\u6b65\u8bc4\u4f30/Progressive evaluation from full symbolic to partial visual state]\n    D --\x3e D1[\u9886\u5148LLM\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u901a\u8fc7\u7387\u4e3a0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]\n    D --\x3e D2[\u63ed\u793a\u4e86\u957f\u671f\u89c4\u5212\u548c\u4e3b\u52a8\u63a2\u7d22\u7684\u6839\u672c\u6027\u5931\u8d25/Exposes fundamental failure in long-term planning and active exploration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [CAD code generation, multi-expert reinforcement learning, Chain-of-Thought, CADExpert benchmark, CADQuery]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ke Niu, Haiyang Yu, Zhuofan Chen, Zhengtao Yao, Weitao Jia, Xiaodong Ge, Jingqun Tang, Benlei Cui, Bin Li, Xiangyang Xue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fudan University, ByteDance Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23333",children:"https://arxiv.org/pdf/2512.23333"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm for generating precise and editable CAD models., 2. Introduces a two-stage training process: Multi-Expert Fine-Tuning (MEFT) and Multi-Expert Reinforcement Learning (MERL)., 3. Presents CADExpert, an open-source benchmark with 17,299 instances including orthographic projections, CoT processes, CADQuery code, and 3D models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of automating the generation of high-precision, editable CAD models from sketches, which existing methods struggle with. It proposes a new training paradigm called CME-CAD, which uses a two-stage process of Multi-Expert Fine-Tuning and Reinforcement Learning to collaboratively improve model performance. The approach aims to generate accurate, constraint-compatible CAD code and is supported by a new open-source benchmark called CADExpert."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CME-CAD: CAD\u4ee3\u7801\u751f\u6210] --\x3e B1\n    A --\x3e B2\n    A --\x3e B3\n    B1[\u6838\u5fc3\u95ee\u9898/Problem<br>\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u6a21\u578b\u4e0d\u53ef\u7f16\u8f91\u3001\u4e0d\u7cbe\u786e<br>\u4f9d\u8d56\u6587\u672c/\u56fe\u50cf\u8f93\u5165\uff0c\u6807\u6ce8\u6210\u672c\u9ad8]\n    B2[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f02\u6784\u534f\u4f5c\u591a\u4e13\u5bb6\u5f3a\u5316\u5b66\u4e60<br>\u4e24\u9636\u6bb5\u8bad\u7ec3: MEFT + MERL]\n    B3[\u5173\u952e\u7ed3\u679c/Results<br>\u751f\u6210\u7cbe\u786e\u3001\u53ef\u7f16\u8f91\u7684CAD\u6a21\u578b<br>\u53d1\u5e03CADExpert\u57fa\u51c6\u6570\u636e\u96c6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Visual Language Hypothesis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiu Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bytedance"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23335",children:"https://arxiv.org/pdf/2512.23335"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the "Visual Language Hypothesis," framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an "expand-and-snap" process to achieve the necessary topology change for semantic abstraction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes the "Visual Language Hypothesis," which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific "expand-and-snap" process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Visual Language Hypothesis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: What structural properties enable semantic abstraction in vision?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose hypothesis of discrete semantic language, derive geometric (fiber bundle) structure]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Semantic invariance needs discriminative target; Model needs "expand-and-snap" for topology change]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23343",children:"https://arxiv.org/pdf/2512.23343"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AgentMemory/Huaman-Agent-Memory",children:"https://github.com/AgentMemory/Huaman-Agent-Memory"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[AI Meets Brain: Memory Systems / AI\u4e0e\u5927\u8111\uff1a\u8bb0\u5fc6\u7cfb\u7edf] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[Interdisciplinary Gap / \u8de8\u5b66\u79d1\u9e3f\u6c9f]\n    P1 --\x3e P2[Existing works struggle to assimilate human memory essence / \u73b0\u6709\u5de5\u4f5c\u96be\u4ee5\u5438\u6536\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u7cbe\u9ad3]\n\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Systematic Synthesis / \u7cfb\u7edf\u7efc\u8ff0]\n    M1 --\x3e M2[Comparative Analysis / \u5bf9\u6bd4\u5206\u6790]\n    M2 --\x3e M3[Review Benchmarks & Security / \u56de\u987e\u57fa\u51c6\u4e0e\u5b89\u5168]\n\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Unified Memory Framework / \u7edf\u4e00\u7684\u8bb0\u5fc6\u6846\u67b6]\n    R1 --\x3e R2[Future Directions / \u672a\u6765\u65b9\u5411]\n    R2 --\x3e R3[Multimodal Memory & Skill Acquisition / \u591a\u6a21\u6001\u8bb0\u5fc6\u4e0e\u6280\u80fd\u83b7\u53d6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CountGD++: Generalized Prompting for Open-World Counting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object counting], [open-world counting, negative prompting, pseudo-exemplars]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Niki Amini-Naieni, Andrew Zisserman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Oxford (Visual Geometry Group)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23351",children:"https://arxiv.org/pdf/2512.23351"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/niki-amini-naieni/CountGDPlusPlus",children:"https://github.com/niki-amini-naieni/CountGDPlusPlus"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extended the counting prompt to include negative specifications (what not to count) via text and/or visual examples. 2. Introduced 'pseudo-exemplars' to automate the annotation of visual examples at inference time. 3. Enabled counting models to accept visual examples from both natural and synthetic external images, and integrated the model as a vision expert agent for an LLM."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses limitations in open-world object counting by introducing CountGD++, a model that significantly expands prompt flexibility. It allows specifying what not to count, automates visual example annotation via pseudo-exemplars, and accepts external visual examples, leading to improved accuracy and generalization across datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CountGD++: Generalized Prompting for Open-World Counting] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Limited prompt flexibility in object counting models]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Negative prompts, pseudo-exemplars, external visual examples]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved accuracy, efficiency, and generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SpatialMosaic: A Multiview VLM Dataset for Partial Visibility"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-view vision-language reasoning], [spatial reasoning, multi-view images, partial visibility, instruction-tuning, 3D reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Seoul National University, University College London, POSTECH"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23365",children:"https://arxiv.org/pdf/2512.23365"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A scalable multi-view data generation and annotation pipeline for realistic spatial reasoning QA pairs. 2. SpatialMosaic, a comprehensive 2M QA pair instruction-tuning dataset, and SpatialMosaic-Bench, a 1M QA pair benchmark for evaluating multi-view spatial reasoning under challenging conditions. 3. SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of enabling Vision-Language Models (VLMs) to perform 3D spatial reasoning directly from multi-view images under realistic conditions like partial visibility and occlusion. It proposes a data generation pipeline to create the SpatialMosaic dataset and benchmark, and introduces a hybrid VLM framework that incorporates 3D reconstruction models. Experiments show the approach effectively enhances spatial reasoning in challenging multi-view scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SpatialMosaic: A Multiview VLM Dataset for Partial Visibility] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d563D\u91cd\u5efa\uff0c\u9650\u5236\u53ef\u6269\u5c55\u6027/Existing methods rely on 3D reconstruction, limiting scalability]\n    B --\x3e B2[\u771f\u5b9e\u573a\u666f\u7684\u788e\u7247\u5316\u89c6\u89c9\u7ebf\u7d22\u672a\u88ab\u5145\u5206\u63a2\u7d22/Real-world fragmented visual cues are under-explored]\n    C --\x3e C1[\u53ef\u6269\u5c55\u7684\u591a\u89c6\u56fe\u6570\u636e\u751f\u6210\u4e0e\u6807\u6ce8\u6d41\u6c34\u7ebf/Scalable multi-view data generation & annotation pipeline]\n    C --\x3e C2[\u6784\u5efaSpatialMosaic\u6570\u636e\u96c6\u4e0e\u57fa\u51c6/Build SpatialMosaic dataset & benchmark]\n    C --\x3e C3[\u63d0\u51fa\u96c6\u62103D\u91cd\u5efa\u6a21\u578b\u7684\u6df7\u5408VLM\u6846\u67b6/Propose hybrid VLM framework with 3D reconstruction models]\n    D --\x3e D1[\u751f\u62102M QA\u5bf9\u7528\u4e8e\u6307\u4ee4\u5fae\u8c03/Generated 2M QA pairs for instruction-tuning]\n    D --\x3e D2[\u6784\u5efa1M QA\u5bf9\u7684\u8bc4\u4f30\u57fa\u51c6/Built 1M QA pair evaluation benchmark]\n    D --\x3e D3[\u6709\u6548\u63d0\u5347\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b/Effectively enhances spatial reasoning under challenging conditions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [two-view correspondence / outlier rejection], [geometric attention, graph neural network, cross-stage consensus, outlier rejection, camera pose estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuyuan Lin, Mengtin Lo, Haosheng Chen, Yanjie Liang, Qiangqiang Wu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Jinan University, Chongqing University of Posts and Telecommunications, Peng Cheng Laboratory, City University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23369",children:"https://arxiv.org/pdf/2512.23369"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"http://www.linshuyuan.com",children:"http://www.linshuyuan.com"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Contextual Geometric Attention (CGA) module that dynamically integrates spatial and feature information to capture local and global geometric relationships. 2. Introduced a Cross-Stage Multi-Graph Consensus (CSMGC) module to ensure geometric consistency across different network stages via a sparse graph network. 3. Demonstrated state-of-the-art performance on the YFCC100M and SUN3D datasets for outlier rejection and camera pose estimation tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of robust two-view correspondence learning for tasks like camera pose estimation. It proposes MGCA-Net, a novel network featuring a Contextual Geometric Attention module and a Cross-Stage Multi-Graph Consensus module to better model geometric constraints and ensure information consistency. Experiments show it outperforms existing methods on standard benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MGCA-Net: \u591a\u56fe\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u7f51\u7edc / Multi-Graph Contextual Attention Network] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u51e0\u4f55\u5efa\u6a21\u548c\u8de8\u9636\u6bb5\u4fe1\u606f\u4f18\u5316\u4e0a\u5b58\u5728\u5c40\u9650 / Limitations in local geometric modeling and cross-stage information optimization]\n    B --\x3e B2[\u96be\u4ee5\u51c6\u786e\u6355\u83b7\u5339\u914d\u5bf9\u7684\u51e0\u4f55\u7ea6\u675f\uff0c\u964d\u4f4e\u6a21\u578b\u9c81\u68d2\u6027 / Hard to capture geometric constraints, reducing robustness]\n    C --\x3e C1[\u4e0a\u4e0b\u6587\u51e0\u4f55\u6ce8\u610f\u529b\u6a21\u5757 / Contextual Geometric Attention (CGA) Module]\n    C --\x3e C2[\u8de8\u9636\u6bb5\u591a\u56fe\u5171\u8bc6\u6a21\u5757 / Cross-Stage Multi-Graph Consensus (CSMGC) Module]\n    C1 --\x3e C1a[\u52a8\u6001\u6574\u5408\u7a7a\u95f4\u4f4d\u7f6e\u548c\u7279\u5f81\u4fe1\u606f / Dynamically integrates spatial position and feature information]\n    C1 --\x3e C1b[\u589e\u5f3a\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u5173\u7cfb\u7684\u80fd\u529b / Enhances capability to capture local and global geometric relationships]\n    C2 --\x3e C2a[\u901a\u8fc7\u8de8\u9636\u6bb5\u7a00\u758f\u56fe\u7f51\u7edc\u5efa\u7acb\u51e0\u4f55\u5171\u8bc6 / Establishes geometric consensus via cross-stage sparse graph network]\n    C2 --\x3e C2b[\u786e\u4fdd\u4e0d\u540c\u9636\u6bb5\u95f4\u51e0\u4f55\u4fe1\u606f\u7684\u4e00\u81f4\u6027 / Ensures consistency of geometric information across stages]\n    D --\x3e D1[\u5728YFCC100M\u548cSUN3D\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5 / Significantly outperforms SOTA on YFCC100M and SUN3D datasets]\n    D --\x3e D2[\u5728\u79bb\u7fa4\u70b9\u5254\u9664\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02 / Excels in outlier rejection and camera pose estimation tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [image manipulation detection, generalization benchmark, cross-dimension evaluation, AIGC-based manipulation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yifei Li, Haoyuan He, Yu Zheng, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jie Zhou, Jiwen Lu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23374",children:"https://arxiv.org/pdf/2512.23374"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JoeLeelyf/NeXT-IMDL",children:"https://github.com/JoeLeelyf/NeXT-IMDL"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes NeXT-IMDL, a large-scale diagnostic benchmark for Image Manipulation Detection and Localization (IMDL) designed to systematically probe the generalization boundaries of detectors. 2. Categorizes AI-generated content (AIGC) manipulations along four fundamental axes (editing models, manipulation types, content semantics, forgery granularity) and implements five rigorous cross-dimension evaluation protocols. 3. Through extensive experiments on 11 representative models, reveals that current models exhibit systemic failures and significant performance degradation under the proposed protocols, challenging the perceived progress in the field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper identifies a "benchmark illusion" in Image Manipulation Detection and Localization (IMDL), where current cross-dataset evaluations overestimate model robustness. To address this, the authors propose NeXT-IMDL, a diagnostic benchmark that systematically categorizes manipulations and introduces rigorous cross-dimension evaluation protocols. Experiments show that 11 state-of-the-art models suffer significant performance drops under these new protocols, highlighting their fragility and the need for more robust next-generation IMDL models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[NeXT-IMDL: Next-Generation Image Manipulation Detection & Localization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u9ad8\u4f30\u6a21\u578b\u9c81\u68d2\u6027/Current evaluations overestimate model robustness]\n    C --\x3e C1[\u6784\u5efa\u8bca\u65ad\u6027\u57fa\u51c6 NeXT-IMDL/Build diagnostic benchmark NeXT-IMDL]\n    C1 --\x3e C2[\u56db\u7ef4\u64cd\u7eb5\u5206\u7c7b/Four-axis manipulation categorization]\n    C1 --\x3e C3[\u4e94\u7ef4\u4ea4\u53c9\u8bc4\u4f30\u534f\u8bae/Five cross-dimension evaluation protocols]\n    D --\x3e D1[11\u4e2a\u6a21\u578b\u51fa\u73b0\u7cfb\u7edf\u6027\u5931\u8d25/11 models exhibit systemic failures]\n    D --\x3e D2[\u6027\u80fd\u663e\u8457\u4e0b\u964d/Significant performance degradation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SoulX-LiveTalk Technical Report"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Self-correcting Bidirectional Distillation, Multi-step Retrospective Self-Correction, hybrid sequence parallelism, Parallel VAE, kernel-level optimizations]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Soul AI Lab, Donghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23379",children:"https://arxiv.org/pdf/2512.23379"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://soul-ailab.github.io/soulx-livetalk/",children:"https://soul-ailab.github.io/soulx-livetalk/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SoulX-LiveTalk] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5b9e\u65f6\u65e0\u9650\u65f6\u957f\u97f3\u9891\u9a71\u52a8\u5316\u8eab\u751f\u6210\u4e2d\u8ba1\u7b97\u8d1f\u8f7d\u4e0e\u4f4e\u5ef6\u8fdf\u7684\u51b2\u7a81]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u81ea\u6821\u6b63\u53cc\u5411\u84b8\u998f\u4e0e\u591a\u6b65\u56de\u987e\u81ea\u6821\u6b63\u673a\u5236]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 0.87\u79d2\u542f\u52a8\u5ef6\u8fdf\uff0c32 FPS\u5b9e\u65f6\u541e\u5410]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Urmia University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23380",children:"https://arxiv.org/pdf/2512.23380"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/your-repo/CoLog",children:"https://github.com/your-repo/CoLog"}),' (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image aesthetics assessment], [aesthetic description, multimodal learning, large language model, hierarchical learning, entropy minimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Kuaishou Technology, Pengcheng Laboratory, National Cheng Kung University, E Fund Management Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23413",children:"https://arxiv.org/pdf/2512.23413"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Refined Aesthetic Description (RAD) dataset, a large-scale, multi-dimensional structured dataset generated via an iterative pipeline to address data scarcity and imbalance in artistic aesthetics. 2. Proposes ArtQuant, an aesthetics assessment framework that couples isolated aesthetic dimensions through joint description generation and utilizes LLM decoders to better model long-text semantics. 3. Provides a theoretical analysis showing that the semantic adequacy of the RAD dataset and the generation paradigm of ArtQuant collectively minimize prediction entropy, offering mathematical grounding for the framework."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges in artistic image aesthetics assessment by introducing a large-scale dataset (RAD) and a novel framework (ArtQuant) that uses hierarchical description learning with LLMs. The method achieves state-of-the-art performance on several benchmarks while requiring significantly fewer training epochs, effectively narrowing the cognitive gap between images and human aesthetic judgment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6570\u636e\u7a00\u7f3a\u4e0e\u6a21\u578b\u788e\u7247\u5316 / Data Scarcity & Model Fragmentation]\n    C --\x3e C1[\u63d0\u51faRAD\u6570\u636e\u96c6 / Propose RAD Dataset]\n    C --\x3e C2[\u63d0\u51faArtQuant\u6846\u67b6 / Propose ArtQuant Framework]\n    C2 --\x3e C2_1[\u8054\u5408\u63cf\u8ff0\u751f\u6210 / Joint Description Generation]\n    C2 --\x3e C2_2[LLM\u89e3\u7801\u5668 / LLM Decoder]\n    D --\x3e D1[SOTA\u6027\u80fd / SOTA Performance]\n    D --\x3e D2[\u51cf\u5c11\u8bad\u7ec3\u6210\u672c / Reduced Training Cost]\n    D --\x3e D3[\u7f29\u5c0f\u8ba4\u77e5\u5dee\u8ddd / Narrow Cognitive Gap]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["[arXiv251230] DriveLaW",":Unifying"," Planning and Video Generation in a Latent Driving World"]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [autonomous driving], [world model, video generation, diffusion planner, latent representation, unified planning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, Haiyang Sun, Bing Wang, Kun Ma, Hangjun Ye, Wenyu Liu, Xinggang Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Xiaomi EV"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23421",children:"https://arxiv.org/pdf/2512.23421"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DriveLaW, a novel paradigm that unifies video generation and motion planning for autonomous driving by directly injecting latent representations from the video generator into the planner. 2. Introduces a three-stage progressive training strategy to jointly optimize the video generation component (DriveLaW-Video) and the diffusion planning component (DriveLaW-Act). 3. Achieves state-of-the-art performance on both video prediction and motion planning benchmarks, significantly surpassing previous methods in metrics like FID, FVD, and NAVSIM."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the decoupling of world prediction and motion planning in current autonomous driving world models. It proposes DriveLaW, a unified framework that connects a video generator and a diffusion planner via latent representations, ensuring consistency between future scene generation and trajectory planning. The method achieves new state-of-the-art results on both video forecasting and planning benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DriveLaW: Unifying Planning and Video Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Decoupled world prediction and planning in autonomous driving]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified paradigm with latent injection from video generator to diffusion planner]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA in video prediction (FID, FVD) and planning (NAVSIM)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [2D-3D fusion, instance segmentation, order-aware matching, semantic feature injection, center-guided refinement]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaolan Li, Wanquan Liu, Pengcheng Li, Pengyu Jie, Chenqiang Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University, Hainan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23411",children:"https://arxiv.org/pdf/2512.23411"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A point-wise residual gating module that injects frozen 2D SAM embeddings into 3D point features to refine boundaries without 2D mask supervision. 2. A center-guided mask refinement mechanism that regularizes consistency between instance masks and geometric centroids to reduce center drift. 3. An order-aware Hungarian matching strategy that integrates anatomical tooth order and center distance for coherent instance labeling, especially for cases with missing or crowded teeth."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses challenges in 3D tooth instance segmentation, such as boundary leakage and inconsistent labeling, by proposing SOFTooth, a framework that fuses 2D semantic features from SAM with 3D geometric data. The method introduces modules for boundary refinement, center stabilization, and order-aware instance assignment. It achieves state-of-the-art performance on a benchmark dataset, demonstrating effective transfer of 2D semantics to 3D segmentation without fine-tuning the 2D model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[SOFTooth: \u8bed\u4e49\u589e\u5f3a\u7684\u987a\u5e8f\u611f\u77e5\u878d\u5408\u7528\u4e8e\u7259\u9f7f\u5b9e\u4f8b\u5206\u5272] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[\u62e5\u6324\u7259\u5f13\u4e0e\u6a21\u7cca\u8fb9\u754c/Crowded arches & ambiguous boundaries]\n    Problem --\x3e P2[\u4e2d\u5fc3\u6f02\u79fb\u4e0e\u8eab\u4efd\u4e0d\u4e00\u81f4/Center drift & inconsistent identities]\n    Problem --\x3e P3[\u7a00\u6709\u7b2c\u4e09\u78e8\u7259/Rare third molars]\n    Method --\x3e M1[\u70b9\u7ea7\u6b8b\u5dee\u95e8\u63a7\u6ce8\u51652D\u8bed\u4e49/Point-wise residual gating for 2D semantics]\n    Method --\x3e M2[\u4e2d\u5fc3\u5f15\u5bfc\u7684\u63a9\u7801\u7ec6\u5316/Center-guided mask refinement]\n    Method --\x3e M3[\u987a\u5e8f\u611f\u77e5\u5308\u7259\u5229\u5339\u914d/Order-aware Hungarian matching]\n    Results --\x3e R1[\u57283DTeethSeg'22\u4e0a\u8fbe\u5230SOTA/SOTA on 3DTeethSeg'22]\n    Results --\x3e R2[\u5728\u6d89\u53ca\u7b2c\u4e09\u78e8\u7259\u7684\u6848\u4f8b\u4e0a\u63d0\u5347\u660e\u663e/Clear gains on third molar cases]\n    Results --\x3e R3[\u65e0\u97002D\u5fae\u8c03\u7684\u6709\u65482D-3D\u8fc1\u79fb/Effective 2D-3D transfer without fine-tuning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [preference optimization, diffusion models, score-space supervision, text-to-image synthesis, denoising trajectory]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dohyun Kim, Seungwoo Lyu, Seung Wook Kim, Paul Hongsuck Seo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Korea University, NVIDIA"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23426",children:"https://arxiv.org/pdf/2512.23426"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," this https URL"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DDSPO, a method for direct score-space preference optimization in diffusion models that provides per-timestep supervision from contrastive policy pairs. 2. Introduces a practical strategy to automatically generate preference signals using a pretrained model and semantically degraded prompts, avoiding costly human-labeled data. 3. Demonstrates improved text-image alignment and visual quality, outperforming or matching existing methods with significantly less supervision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Direct Diffusion Score Preference Optimization (DDSPO), a method that optimizes diffusion models by applying preference supervision directly in the score space at each denoising timestep, using automatically generated signals from a reference model. This approach provides dense, stepwise learning signals and reduces reliance on expensive human-labeled data. Empirical results show DDSPO improves text-image alignment and visual quality, matching or outperforming prior preference-based methods with less supervision."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Direct Diffusion Score Preference Optimization <br/> \u76f4\u63a5\u6269\u6563\u5206\u6570\u504f\u597d\u4f18\u5316] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[Problem: Diffusion models struggle with user intent alignment & aesthetic consistency <br/> \u95ee\u9898: \u6269\u6563\u6a21\u578b\u96be\u4ee5\u5bf9\u9f50\u7528\u6237\u610f\u56fe\u548c\u4fdd\u6301\u7f8e\u5b66\u4e00\u81f4\u6027]\n    C[Method: DDSPO provides stepwise score-space supervision via contrastive policy pairs <br/> \u65b9\u6cd5: DDSPO\u901a\u8fc7\u5bf9\u6bd4\u7b56\u7565\u5bf9\u63d0\u4f9b\u9010\u6b65\u5206\u6570\u7a7a\u95f4\u76d1\u7763]\n    D[Results: Improves alignment & quality, reduces supervision need <br/> \u7ed3\u679c: \u6539\u5584\u5bf9\u9f50\u4e0e\u8d28\u91cf\uff0c\u51cf\u5c11\u76d1\u7763\u9700\u6c42]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jesse Brouwers, Xiaoyan Xing, Alexander Timans"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UvA-Bosch Delta Lab, University of Amsterdam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23427",children:"https://arxiv.org/pdf/2512.23427"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/JesseBrouw/UncertSAM",children:"https://github.com/JesseBrouw/UncertSAM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Integrating Uncertainty for Domain-Agnostic Segmentation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: SAM\u5728\u57df\u504f\u79fb\u6216\u77e5\u8bc6\u6709\u9650\u573a\u666f\u4e0b\u8106\u5f31/SAM vulnerable in shifted or limited-knowledge domains]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6784\u5efaUncertSAM\u57fa\u51c6\uff0c\u8bc4\u4f30\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u5c1d\u8bd5\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f18\u5316/Build UncertSAM benchmark, evaluate post-hoc UQ methods, attempt uncertainty-guided refinement]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0e\u8bef\u5dee\u76f8\u5173\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e0d\u786e\u5b9a\u6027\u6574\u5408\u6f5c\u529b/Laplace approximation yields correlated uncertainty, preliminary potential of integrating UQ]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image classification], [convolutional neural networks, fuzzy logic, road surface classification, intelligent transport systems, data fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mustafa Demetgul, Sanja Lazarova Molnar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Karlsruhe Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23436",children:"https://arxiv.org/pdf/2512.23436"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Classical road monitoring is expensive and unsystematic.]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Over 95% classification accuracy achieved.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [benchmark, multi-view, physical degradation, neural radiance field, Gaussian splatting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Tokyo, NII, Tohoku University, University of W\xfcrzburg, RIKEN"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23437",children:"https://arxiv.org/pdf/2512.23437"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RealX3D: A Physically-Degraded 3D Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u771f\u5b9e\u4e16\u754c\u9000\u5316\u5f71\u54cd3D\u91cd\u5efa/Real-world degradations hinder 3D reconstruction]\n    C --\x3e C1[\u6784\u5efa\u771f\u5b9e\u6355\u83b7\u57fa\u51c6/Build real-capture benchmark]\n    C --\x3e C2[\u56db\u7c7b\u9000\u5316, \u591a\u4e25\u91cd\u7ea7\u522b/Four corruption families, multiple severity levels]\n    C --\x3e C3[\u63d0\u4f9b\u5bf9\u9f50\u7684LQ/GT\u89c6\u56fe, RAW\u6570\u636e, \u6fc0\u5149\u626b\u63cf/Provide aligned LQ/GT views, RAW, laser scans]\n    D --\x3e D1[\u5f53\u524d\u65b9\u6cd5\u8d28\u91cf\u663e\u8457\u4e0b\u964d/Current methods show substantial quality degradation]\n    D --\x3e D2[\u7a81\u51fa\u73b0\u5b9e\u6311\u6218\u6027/Underscores fragility in challenging real environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\u0107"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23441",children:"https://arxiv.org/pdf/2512.23441"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer's Disease."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Stochastic Siamese MAE Pretraining for Longitudinal Medical Images] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: MAE lacks temporal awareness for longitudinal medical data.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: STAMP - Stochastic Siamese MAE using conditional variational inference.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms existing methods on AMD and AD progression prediction.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [hallucination mitigation, coarse-to-fine conditioning, Wasserstein fusion, generative feedback, training-free decoding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lenovo (PCIE), University of Minnesota (UMN)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23453",children:"https://arxiv.org/pdf/2512.23453"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AI-Researcher-Team/CoFi-Dec",children:"https://github.com/AI-Researcher-Team/CoFi-Dec"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CoFi-Dec: Hallucination-Resistant Decoding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LVLMs\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u5185\u5bb9]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u7c97\u5230\u7ec6\u89c6\u89c9\u6761\u4ef6\u7684\u751f\u6210\u5f0f\u81ea\u53cd\u9988\u4e0eWasserstein\u878d\u5408]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u6a21\u578b\u65e0\u5173]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLOv8-Pose, multimodal LLMs, waterline detection, scale gap estimation, geometric calibration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kayathri Vigneswaran, Hugo Retief, Jai Clifford Holmes, Mariangel Garcia Andarcia, Hansaka Tennakoon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," International Water Management Institute (IWMI), Association for Water and Rural Development (AWARD)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23454",children:"https://arxiv.org/pdf/2512.23454"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel hybrid framework combining vision-based waterline detection, YOLOv8-Pose for scale extraction, and multimodal LLMs for automated river gauge reading. 2. Demonstrates that incorporating geometric metadata (scale gap) significantly improves the predictive accuracy of LLMs for water level estimation. 3. Provides a scalable and efficient solution for automated hydrological monitoring, highlighting its sensitivity to image quality and potential for real-time digitization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of automated river water level monitoring by proposing a hybrid framework that integrates computer vision (waterline detection and YOLOv8-Pose) with multimodal large language models (GPT-4o and Gemini 2.0 Flash) to read gauge plates. The method uses geometric calibration from scale gap detection to enhance LLM performance, achieving high accuracy under optimal conditions. The study concludes that combining geometric metadata with multimodal AI offers a robust, scalable solution for real-time hydrological monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Automated River Gauge Plate Reading<br/>\u81ea\u52a8\u5316\u6cb3\u6d41\u6c34\u4f4d\u8ba1\u8bfb\u6570] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Manual gauge reading errors & environmental constraints<br/>\u4eba\u5de5\u8bfb\u6570\u8bef\u5dee\u4e0e\u73af\u5883\u9650\u5236]\n    C --\x3e C1[Hybrid Object Detection & Generative AI Framework<br/>\u6df7\u5408\u76ee\u6807\u68c0\u6d4b\u4e0e\u751f\u6210\u5f0fAI\u6846\u67b6]\n    C1 --\x3e C2[Vision-based waterline detection<br/>\u57fa\u4e8e\u89c6\u89c9\u7684\u6c34\u4f4d\u7ebf\u68c0\u6d4b]\n    C1 --\x3e C3[YOLOv8-Pose scale extraction<br/>YOLOv8-Pose\u5c3a\u5ea6\u63d0\u53d6]\n    C1 --\x3e C4[Multimodal LLMs (GPT-4o, Gemini) for reading<br/>\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bfb\u6570]\n    D --\x3e D1[High precision waterline detection (94.24%)<br/>\u9ad8\u7cbe\u5ea6\u6c34\u4f4d\u7ebf\u68c0\u6d4b]\n    D --\x3e D2[Improved LLM accuracy with scale gap metadata<br/>\u5c3a\u5ea6\u95f4\u9699\u5143\u6570\u636e\u63d0\u5347LLM\u7cbe\u5ea6]\n    D --\x3e D3[Gemini Stage 2: MAE=5.43cm, RMSE=8.58cm<br/>Gemini Stage 2\u6700\u4f73\u6027\u80fd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image-to-image translation], [Brownian bridge, deterministic translation, denoising diffusion, dual approximators, super-resolution]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bohan Xiao, Peiyong Wang, Qisheng He, Ming Dong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Wayne State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23463",children:"https://arxiv.org/pdf/2512.23463"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/bohan95/dual-app-bridge",children:"https://github.com/bohan95/dual-app-bridge"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel denoising Brownian bridge model with dual neural network approximators for deterministic I2I translation., 2. Introduces a method that guarantees consistent, predictable outputs with high fidelity to ground truth, addressing the limitations of stochastic models., 3. Demonstrates superior performance in tasks like super-resolution compared to both stochastic and deterministic baseline models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Dual-approx Bridge, a novel generative model for deterministic image-to-image translation. It uses Brownian bridge dynamics with two neural approximators to produce high-fidelity, low-variance outputs. Experiments show it outperforms existing baselines in image quality and faithfulness to ground truth."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u786e\u5b9a\u6027\u56fe\u50cf\u8f6c\u6362\u9700\u6c42 / Need for deterministic I2I translation (e.g., super-resolution)]\n    C --\x3e C1[\u53cc\u8fd1\u4f3c\u5668\u53bb\u566a\u5e03\u6717\u6865\u6a21\u578b / Denoising Brownian Bridge Model with Dual Approximators]\n    D --\x3e D1[\u8f93\u51fa\u4e00\u81f4\u4e14\u8d28\u91cf\u9ad8 / Consistent, high-quality output with high fidelity to GT]\n    D --\x3e D2[\u4f18\u4e8e\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u57fa\u7ebf / Superior to stochastic & deterministic baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [motion generation], [flow matching, diffusion transformer (DiT), reinforcement learning from human feedback (RLHF)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent Hunyuan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23464",children:"https://arxiv.org/pdf/2512.23464"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Tencent-Hunyuan/HY-Motion-1.0",children:"https://github.com/Tencent-Hunyuan/HY-Motion-1.0"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Generating high-quality, text-aligned 3D human motions"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: SOTA performance, Extensive motion coverage, Open-source release"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [point cloud registration], [graph neural network, multi-domain context, dynamic inlier selection, feature aggregation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Jinan University, Qingdao University of Science and Technology, Shenzhen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23472",children:"https://arxiv.org/pdf/2512.23472"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"http://www.linshuyuan.com",children:"http://www.linshuyuan.com"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A graph neighborhood aggregation module that constructs a global graph to capture overall structural relationships in point clouds. 2. A progressive context interaction module that enhances feature discriminability through intra-domain decoupling and inter-domain interaction. 3. A dynamic inlier selection method that optimizes inlier weights using residual information from iterative pose estimation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes MCI-Net, a novel network for point cloud registration that improves feature learning by integrating contextual cues from multiple domains. The method introduces modules for graph-based neighborhood aggregation, progressive context interaction, and dynamic inlier selection. Experiments show it achieves state-of-the-art performance, including a 96.4% registration recall on the 3DMatch dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[MCI-Net: Point Cloud Registration] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6b27\u6c0f\u90bb\u57df/Existing methods rely on Euclidean neighborhoods)\n    Problem --\x3e P2(\u96be\u4ee5\u6355\u6349\u9690\u5f0f\u8bed\u4e49\u548c\u7ed3\u6784\u4e00\u81f4\u6027/Struggle to capture implicit semantics and structural consistency)\n    Method --\x3e M1(\u56fe\u90bb\u57df\u805a\u5408\u6a21\u5757/Graph Neighborhood Aggregation Module)\n    Method --\x3e M2(\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u4ea4\u4e92\u6a21\u5757/Progressive Context Interaction Module)\n    Method --\x3e M3(\u52a8\u6001\u5185\u70b9\u9009\u62e9\u65b9\u6cd5/Dynamic Inlier Selection Method)\n    Results --\x3e R1(\u5728\u5ba4\u5185\u5916\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u8d8a/Superior performance on indoor and outdoor datasets)\n    Results --\x3e R2(\u57283DMatch\u4e0a\u8fbe\u523096.4%\u7684\u53ec\u56de\u7387/Achieves 96.4% recall on 3DMatch)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [correspondence learning], [adaptive focused regularization, bilateral field adjustment, position-aware recovery]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuyuan Lin, Hailiang Liao, Qiang Qi, Junjie Huang, Taotao Lai, Jian Weng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Jinan University, Qingdao University of Science and Technology, Minjiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23473",children:"https://arxiv.org/pdf/2512.23473"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"http://www.linshuyuan.com",children:"http://www.linshuyuan.com"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Adaptive Focused Regularization (AFR) module to enhance position-awareness and robustness against spurious motion samples. 2. Proposed the Bilateral Field Adjustment (BFA) module to refine motion fields by modeling long-range relationships across spatial and channel dimensions. 3. Proposed a Position-Aware Recovery (PAR) module to ensure consistent and precise recovery of motion vectors from the refined field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of CNN-based two-view correspondence learning failing to aggregate global context and oversmoothing motion fields. It proposes SC-Net, a novel network that integrates spatial and cross-channel context using three key modules (AFR, BFA, PAR). Experiments show SC-Net outperforms state-of-the-art methods on pose estimation and outlier removal tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SC-Net] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[CNN\u65e0\u6cd5\u6709\u6548\u805a\u5408\u5168\u5c40\u4e0a\u4e0b\u6587/CNN fails to aggregate global context]\n    B --\x3e B2[\u5927\u89c6\u5dee\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\u573a\u8fc7\u5e73\u6ed1/Oversmoothing in large disparity scenes]\n    C --\x3e C1[\u81ea\u9002\u5e94\u805a\u7126\u6b63\u5219\u5316\u6a21\u5757/AFR Module]\n    C --\x3e C2[\u53cc\u8fb9\u573a\u8c03\u6574\u6a21\u5757/BFA Module]\n    C --\x3e C3[\u4f4d\u7f6e\u611f\u77e5\u6062\u590d\u6a21\u5757/PAR Module]\n    D --\x3e D1[\u5728YFCC100M\u548cSUN3D\u4e0a\u8d85\u8d8aSOTA/Outperforms SOTA on YFCC100M & SUN3D]\n    D --\x3e D2[\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u548c\u79bb\u7fa4\u70b9\u53bb\u9664/Improves pose estimation & outlier removal]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [temporal-aware retrieval, entropy-weighted sampling, training-free framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zongsheng Cao, Yangfan He, Anran Liu, Feng Chen, Zepeng Wang, Jun Xie"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lenovo (PCIE), University of Minnesota (UMN)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23483",children:"https://arxiv.org/pdf/2512.23483"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AI-Researcher-Team/TV-RAG",children:"https://github.com/AI-Researcher-Team/TV-RAG"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A time-decay retrieval module that injects temporal offsets into similarity computation to rank queries by their true multimedia context. 2. An entropy-weighted key-frame sampler that selects information-dense frames to reduce redundancy while preserving representativeness. 3. A lightweight, training-free architecture that can be grafted onto any Large Video Language Model (LVLM) for improved long-video reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TV-RAG, a training-free framework to enhance long video understanding by Large Video Language Models (LVLMs). It introduces a temporal-aware retrieval module and an entropy-weighted frame sampler to better capture semantic shifts and temporal dependencies in long videos. The system outperforms leading baselines on multiple benchmarks without requiring model retraining."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LVLMs struggle with long videos: narrow temporal windows, miss fine-grained semantic shifts/LVLMs\u5904\u7406\u957f\u89c6\u9891\u56f0\u96be\uff1a\u65f6\u95f4\u7a97\u53e3\u7a84\uff0c\u5ffd\u7565\u7ec6\u7c92\u5ea6\u8bed\u4e49\u53d8\u5316]\n    B --\x3e B2[Text-based retrieval ignores temporal interdependence among multimodal channels/\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u901a\u9053\u95f4\u7684\u65f6\u95f4\u4f9d\u8d56\u6027]\n    C --\x3e C1[Time-decay retrieval module/\u65f6\u95f4\u8870\u51cf\u68c0\u7d22\u6a21\u5757]\n    C --\x3e C2[Entropy-weighted key-frame sampler/\u71b5\u52a0\u6743\u5173\u952e\u5e27\u91c7\u6837\u5668]\n    D --\x3e D1[Surpasses leading baselines on Video-MME, MLVU, LongVideoBench/\u5728Video-MME, MLVU, LongVideoBench\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u57fa\u7ebf]\n    D --\x3e D2[Lightweight, training-free, graftable onto any LVLM/\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u9002\u914d\u4efb\u4f55LVLM]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multi-label Classification with Panoptic Context Aggregation Networks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [multi-label classification], [context modeling, cross-scale aggregation, attention mechanism, geometric relationships, Hilbert space]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhengzhou University, Sorbonne University, Xiamen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23486",children:"https://arxiv.org/pdf/2512.23486"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Deep Panoptic Context Aggregation Network (PanCAN) that hierarchically integrates multi-order geometric contexts. 2. Introduces a method combining random walks with attention to learn multi-order neighborhood relationships in a high-dimensional Hilbert space. 3. Demonstrates effective cross-scale modeling by cascading modules and dynamically fusing salient anchor features, significantly improving complex scene understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of existing multi-label classification methods in modeling cross-scale contextual interactions. It proposes the Panoptic Context Aggregation Network (PanCAN), which hierarchically aggregates multi-order geometric contexts using attention and random walks in a Hilbert space. Experiments on standard benchmarks show PanCAN outperforms state-of-the-art methods, substantially improving classification performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-label Classification with Panoptic Context Aggregation Networks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u8de8\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u4ea4\u4e92/Existing methods neglect cross-scale contextual interactions]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faPanCAN\uff0c\u5728\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u901a\u8fc7\u6ce8\u610f\u529b\u548c\u968f\u673a\u6e38\u8d70\u8fdb\u884c\u8de8\u5c3a\u5ea6\u591a\u9636\u4e0a\u4e0b\u6587\u805a\u5408/Proposes PanCAN, aggregates cross-scale multi-order contexts via attention & random walks in Hilbert space]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eSOTA\uff0c\u63d0\u5347\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd/Outperforms SOTA on multiple benchmarks, improves multi-label classification performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image generation], [identity-preserving generation, iterative identity discovery, re-denoising identity injection, human-centric story generation, character consistency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Donghao Zhou, Jingyu Lin, Guibao Shen, Quande Liu, Jialin Gao, Lihao Liu, Lan Du, Cunjian Chen, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Monash University, The Hong Kong University of Science and Technology (Guangzhou), Kuaishou Technology, Amazon, South China University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23519",children:"https://arxiv.org/pdf/2512.23519"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://correr-zhou.github.io/IdentityStory",children:"https://correr-zhou.github.io/IdentityStory"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IdentityStory, a framework for human-centric story generation that ensures consistent character identity across sequential images. 2. Introduces Iterative Identity Discovery to extract cohesive character identities. 3. Presents Re-denoising Identity Injection to inject identities into images while preserving the desired context."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of generating a series of images with consistent human characters from text prompts, a task known as human-centric story generation. It proposes the IdentityStory framework, which uses Iterative Identity Discovery and Re-denoising Identity Injection to tame identity-preserving generators. Experiments show it outperforms existing methods in maintaining face consistency and supports multi-character combinations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[IdentityStory: \u4eba\u7c7b\u4e2d\u5fc3\u6545\u4e8b\u751f\u6210 / Human-Centric Story Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results]\n    B --\x3e B1[\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u4e00\u81f4\u6027 / Maintaining Character Identity Consistency]\n    B --\x3e B2[\u534f\u8c03\u591a\u89d2\u8272 / Coordinating Multiple Characters]\n    C --\x3e C1[\u8fed\u4ee3\u8eab\u4efd\u53d1\u73b0 / Iterative Identity Discovery]\n    C --\x3e C2[\u91cd\u53bb\u566a\u8eab\u4efd\u6ce8\u5165 / Re-denoising Identity Injection]\n    D --\x3e D1[\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5 / Outperforms Existing Methods]\n    D --\x3e D2[\u652f\u6301\u591a\u89d2\u8272\u7ec4\u5408 / Supports Multi-Character Combinations]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image super-resolution], [diffusion models, inference-time scaling, iterative refinement, frequency steering, training-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hexin Zhang, Dong Li, Jie Huang, Bingzhou Wang, Xueyang Fu, Zhengjun Zha"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23532",children:"https://arxiv.org/pdf/2512.23532"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IAFS, a training-free framework that combines iterative refinement and frequency-aware particle fusion for diffusion-based super-resolution. 2. Introduces adaptive frequency steering to balance high-frequency perceptual quality and low-frequency structural fidelity. 3. Demonstrates through extensive experiments that IAFS effectively resolves the perception-fidelity conflict and outperforms existing inference-time scaling methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of balancing perceptual quality and structural fidelity in diffusion-based image super-resolution. It proposes IAFS, a training-free framework that uses iterative inference-time scaling with adaptive frequency steering to progressively refine images. Experiments show IAFS outperforms existing methods in achieving better detail and accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u611f\u77e5\u8d28\u91cf\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\u51b2\u7a81/Perception-Fidelity Conflict")\n    Method --\x3e M1("\u8fed\u4ee3\u63a8\u7406\u65f6\u7f29\u653e/Iterative Inference-time Scaling")\n    Method --\x3e M2("\u81ea\u9002\u5e94\u9891\u7387\u5f15\u5bfc/Adaptive Frequency Steering")\n    M1 --\x3e M1_1("\u8fed\u4ee3\u7ec6\u5316/Iterative Refinement")\n    M2 --\x3e M2_1("\u9891\u7387\u611f\u77e5\u7c92\u5b50\u878d\u5408/Frequency-aware Particle Fusion")\n    Results --\x3e R1("\u89e3\u51b3\u611f\u77e5-\u4fdd\u771f\u5ea6\u51b2\u7a81/Resolves Perception-Fidelity Conflict")\n    Results --\x3e R2("\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Outperforms Existing Methods")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [multi-subject customization, layout guidance, attention decoupling, training-free, image adapter]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, HKUST (The Hong Kong University of Science and Technology)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23537",children:"https://arxiv.org/pdf/2512.23537"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AnyMS: \u5e03\u5c40\u5f15\u5bfc\u514d\u8bad\u7ec3\u591a\u4e3b\u4f53\u5b9a\u5236] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u591a\u4e3b\u4f53\u5b9a\u5236\u4e2d\u6587\u672c\u5bf9\u9f50\u3001\u8eab\u4efd\u4fdd\u6301\u4e0e\u5e03\u5c40\u63a7\u5236\u7684\u5e73\u8861\u95ee\u9898/Balancing text alignment, identity preservation, and layout control in multi-subject customization]\n    C --\x3e C1[\u63d0\u51fa\u514d\u8bad\u7ec3\u6846\u67b6AnyMS/Proposes training-free framework AnyMS]\n    C1 --\x3e C2[\u5f15\u5165\u81ea\u5e95\u5411\u4e0a\u53cc\u7ea7\u6ce8\u610f\u529b\u89e3\u8026\u673a\u5236/Introduces bottom-up dual-level attention decoupling]\n    C2 --\x3e C3[\u5168\u5c40\u89e3\u8026\u786e\u4fdd\u6587\u672c\u5bf9\u9f50/Global decoupling ensures text alignment]\n    C2 --\x3e C4[\u5c40\u90e8\u89e3\u8026\u9632\u6b62\u4e3b\u4f53\u51b2\u7a81/Local decoupling prevents subject conflicts]\n    C --\x3e C5[\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u9002\u914d\u5668\u63d0\u53d6\u7279\u5f81/Uses pre-trained image adapters for feature extraction]\n    D --\x3e D1[\u5b9e\u73b0SOTA\u6027\u80fd/Achieves SOTA performance]\n    D --\x3e D2[\u652f\u6301\u590d\u6742\u7ec4\u5408\u4e0e\u66f4\u591a\u4e3b\u4f53/Supports complex compositions and scales to more subjects]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [computational pathology], [agentic multimodal model, evidence-seeking inference, reinforcement learning, whole-slide images, vision-language model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23545",children:"https://arxiv.org/pdf/2512.23545"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PathFound: Agentic Multimodal Model] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Static inference vs. clinical workflow]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Agentic model with VFM, VLM, RL]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA accuracy, discovers subtle details]\n    B --\x3e B1[\u9759\u6001\u63a8\u7406\u8303\u5f0f/Static inference paradigm]\n    B --\x3e B2[\u7f3a\u4e4f\u8bc1\u636e\u518d\u83b7\u53d6/Lacks reassessment & evidence acquisition]\n    C --\x3e C1[\u591a\u9636\u6bb5\u8bca\u65ad/Multi-stage diagnosis]\n    C --\x3e C2[\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6/Proactive information acquisition]\n    D --\x3e D1[\u8bca\u65ad\u51c6\u786e\u6027\u63d0\u5347/Improved diagnostic accuracy]\n    D --\x3e D2[\u53d1\u73b0\u7ec6\u5fae\u7279\u5f81/Discover subtle pathological features]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [text-to-image generation, prompt purification, semantic distance, null space projection, training-free safety]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Minnesota, Lenovo PCIE"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23546",children:"https://arxiv.org/pdf/2512.23546"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AI-Researcher-Team/PurifyGen",children:"https://github.com/AI-Researcher-Team/PurifyGen"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel, training-free, dual-stage strategy for safe text-to-image generation that retains the original model weights. 2. Proposes a fine-grained risk discrimination method using complementary semantic distance to classify prompt tokens without keyword matching. 3. Develops a dual-space transformation for semantic purification, projecting risky embeddings into the null space of toxic concepts and the range space of clean concepts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PurifyGen, a training-free method to make text-to-image generation safer. It works by first identifying risky tokens in a prompt using semantic distances and then purifying them by removing harmful semantic components while reinforcing safe ones. Experiments show it effectively reduces unsafe content across multiple datasets and competes with training-dependent approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PurifyGen: Safe Text-to-Image Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Diffusion models risk generating unsafe content]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Dual-stage prompt purification via risk discrimination & semantic transformation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms current methods, competes with training-dependent approaches]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal large language models], [chemical reaction understanding, multimodal benchmark, scientific literature, visual perception, cross-modal integration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23565",children:"https://arxiv.org/pdf/2512.23565"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Instruction-Following Evaluation of Large Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [instruction-following evaluation], [large vision-language models, visual instruction tuning, output format specification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Daiki Shiono, Shumpei Miyawaki, Ryota Tanaka, Jun Suzuki"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University, NTT Corporation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23572",children:"https://arxiv.org/pdf/2512.23572"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Quantitatively demonstrates the decline in instruction-following ability of LVLMs after visual instruction fine-tuning. 2. Constructs new training datasets that highlight whether the output format is specified. 3. Shows that explicitly indicating the output format during fine-tuning helps LVLMs follow instructions more accurately."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies and quantifies a problem where Large Vision-Language Models (LVLMs) lose their instruction-following ability after visual instruction tuning. The authors propose constructing datasets that explicitly specify the output format and find that training with such data mitigates the performance decline. The main conclusion is that including instructions on output format during fine-tuning can help preserve LVLMs' instruction-following capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Instruction-Following Evaluation of Large Vision-Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LVLMs lose instruction-following ability after fine-tuning)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Construct datasets highlighting output format specification)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Explicit output format instructions improve instruction-following)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ThinkGen: Generalized Thinking for Visual Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [text-to-image generation], [Chain-of-Thought (CoT), Multimodal Large Language Model (MLLM), Diffusion Transformer (DiT), reinforcement learning, SepGRPO]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Jiaotong University, Bytedance"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23568",children:"https://arxiv.org/pdf/2512.23568"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/jiaosiyuu/ThinkGen",children:"https://github.com/jiaosiyuu/ThinkGen"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning for various generation tasks. 2. Introduces a decoupled architecture using a pretrained MLLM to generate instructions and a DiT for image synthesis. 3. Proposes a separable GRPO-based training paradigm (SepGRPO) for alternating reinforcement learning between modules, enabling joint training across diverse datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces ThinkGen, a framework that integrates Chain-of-Thought reasoning from Multimodal LLMs with a Diffusion Transformer for visual generation. It uses a decoupled architecture and a novel separable reinforcement learning training method to generalize across diverse generation scenarios. Experiments show it achieves state-of-the-art performance on multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ThinkGen: Generalized Thinking for Visual Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[CoT\u63a8\u7406\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5e94\u7528\u6709\u9650/CoT for generation is nascent and scenario-specific]\n    C --\x3e C1[\u89e3\u8026\u67b6\u6784: MLLM + DiT/Decoupled architecture: MLLM + DiT]\n    C --\x3e C2[\u53ef\u5206\u79bbGRPO\u8bad\u7ec3\u8303\u5f0f/SepGRPO training paradigm]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA/Achieves SOTA across multiple benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ProGuard: Towards Proactive Multimodal Safeguard"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal safety], [proactive guard, out-of-distribution (OOD) detection, reinforcement learning (RL), multimodal safety taxonomy, synonym-bank reward]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Artificial Intelligence Laboratory, Nanjing University, Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23573",children:"https://arxiv.org/pdf/2512.23573"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://yushaohan.github.io/ProGuard",children:"https://yushaohan.github.io/ProGuard"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ProGuard, a vision-language proactive guard model that identifies and describes out-of-distribution safety risks without requiring model adjustments. 2. Constructs a modality-balanced dataset of 87K samples with binary safety labels and hierarchical risk categories to mitigate modality bias. 3. Trains the model purely via reinforcement learning augmented with a synonym-bank-based similarity reward to enhance OOD risk inference and description."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes ProGuard, a proactive multimodal safeguard that uses reinforcement learning on a balanced dataset to detect and describe unseen safety risks. It achieves performance comparable to closed-source models on safety classification and significantly improves OOD risk detection and description by over 50%."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ProGuard: Towards Proactive Multimodal Safeguard] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u751f\u6210\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u6301\u7eed\u7684\u591a\u6a21\u6001\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u3002]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faProGuard\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5f15\u5165OOD\u7c7b\u522b\u63a8\u65ad\u4efb\u52a1\u548c\u540c\u4e49\u8bcd\u5e93\u5956\u52b1\u3002]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4e8c\u5143\u5b89\u5168\u5206\u7c7b\u4e0a\u5ab2\u7f8e\u95ed\u6e90\u5927\u6a21\u578b\uff0cOOD\u98ce\u9669\u68c0\u6d4b\u63d0\u534752.6%\uff0c\u63cf\u8ff0\u63d0\u534764.8%\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Image Denoising Using Global and Local Circulant Representation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [image denoising], [circulant representation, tensor-SVD, Haar transform]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhaoming Kong, Xiaowei Yang, Jiahuan Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," South China University of Technology, Guangdong Provincial People's Hospital, Southern Medical University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23569",children:"https://arxiv.org/pdf/2512.23569"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/ZhaomingKong/Haar-tSVD",children:"https://github.com/ZhaomingKong/Haar-tSVD"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Established a theoretical connection between PCA and the Haar transform under circulant representation. 2. Proposed a computationally simple, one-step plug-and-play denoiser (Haar-tSVD) that balances speed and performance by capturing global and local correlations. 3. Introduced an adaptive noise estimation scheme and integrated deep neural networks to enhance robustness under severe noise conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new image denoising method called Haar-tSVD, which combines tensor singular value decomposition with the Haar transform to efficiently remove noise. It is designed as a fast, parallelizable algorithm that does not require learning and can be integrated with deep networks for better performance. Experiments show the method is effective and efficient for noise removal across various datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Image Denoising Using Global and Local Circulant Representation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: High demand for efficient and effective image denoising] --\x3e P1[\u6311\u6218/Challenge: Noise degrades image quality]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Haar-tSVD] --\x3e M1[\u7406\u8bba/Theoretical: Connect PCA and Haar transform]\n    Method --\x3e M2[\u7b97\u6cd5/Algorithm: Unified t-SVD projection with Haar transform]\n    Method --\x3e M3[\u589e\u5f3a/Enhancement: Adaptive noise estimation & DNN integration]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Efficient and effective noise removal] --\x3e R1[\u9a8c\u8bc1/Validation: Experiments on various datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [on-policy distillation, real-time video diffusion, multimodal conditioning, Anchor-Heavy Identity Sinks, autoregressive generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," SII, SJTU, GAIR"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23576",children:"https://arxiv.org/pdf/2512.23576"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," /githubCode"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an improved on-policy distillation recipe for real-time multimodal video diffusion, addressing issues like flickering and quality degradation in prior methods. 2. Develops LiveTalk, a real-time interactive avatar system integrating the distilled model with audio language models and long-form video inference techniques. 3. Demonstrates 20x reduction in inference cost/latency while matching baseline quality, and outperforms SOTA in multi-turn coherence and content quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of real-time interactive video generation by improving on-policy distillation for multimodal-conditioned diffusion models. The proposed method enhances training stability and output quality, enabling a 20x speedup. The resulting LiveTalk system achieves real-time, coherent multi-turn avatar interactions, significantly outperforming existing models in latency and quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u6269\u6563\u6a21\u578b\u53cc\u5411\u6ce8\u610f\u529b\u8fed\u4ee3\u8fc7\u7a0b\u963b\u788d\u5b9e\u65f6\u4ea4\u4e92 / Diffusion model's iterative bidirectional attention prevents real-time interaction]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6539\u8fdb\u7684\u6309\u7b56\u7565\u84b8\u998f\uff0c\u5f3a\u8c03\u6761\u4ef6\u8f93\u5165\u8d28\u91cf\u4e0e\u4f18\u5316\u8ba1\u5212 / Improved on-policy distillation, emphasizing condition input quality & optimization schedule]\n    D[\u5173\u952e\u7ed3\u679c/Results: 20\u500d\u52a0\u901f\uff0c\u5b9e\u65f6\u5ef6\u8fdf\uff0c\u591a\u8f6e\u4ea4\u4e92\u8d28\u91cf\u8d85\u8d8aSOTA / 20x acceleration, real-time latency, multi-turn quality surpasses SOTA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Same or Not? Enhancing Visual Perception in Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [vision-language models], [fine-grained visual understanding, dataset, benchmark, visual perception, image-pair queries]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," California Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23592",children:"https://arxiv.org/pdf/2512.23592"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced TWIN, a large-scale dataset of 561,000 image-pair queries designed to train VLMs on fine-grained visual perception by determining if two similar images depict the same object. 2. Introduced FGVQA, a benchmark suite of 12,000 queries to evaluate fine-grained VQA capabilities across multiple domains. 3. Demonstrated that fine-tuning VLMs on TWIN significantly improves their fine-grained recognition on FGVQA (up to 19.3%) without harming general VQA performance, and showed the importance of dataset scale."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of Vision-Language Models (VLMs) in fine-grained visual perception. It proposes a new training dataset (TWIN) and a benchmark (FGVQA) to enhance VLMs' ability to notice subtle visual details. The results show that fine-tuning on TWIN significantly improves fine-grained recognition on unseen domains without compromising general performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Same or Not? Enhancing Visual Perception in Vision-Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLMs lack fine-grained perception, miss subtle details \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u611f\u77e5\uff0c\u5ffd\u7565\u7ec6\u5fae\u5dee\u522b]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce TWIN dataset & FGVQA benchmark \u5f15\u5165TWIN\u6570\u636e\u96c6\u548cFGVQA\u57fa\u51c6]\n    D[\u5173\u952e\u7ed3\u679c/Results: Fine-tuning on TWIN improves fine-grained recognition by up to 19.3% \u5728TWIN\u4e0a\u5fae\u8c03\u5c06\u7ec6\u7c92\u5ea6\u8bc6\u522b\u63d0\u5347\u9ad8\u8fbe19.3%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [Scalable Residual Feature Aggregation (SRFA), Hybrid Metaheuristic Optimization (HHO-BA), Vision Transformer (ViT) with EfficientNet-B3]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Janani Annur Thiruvengadam, Kiran Mayee Nabigaru, Anusha Kovi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Amazon.com Services LLC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23597",children:"https://arxiv.org/pdf/2512.23597"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Scalable Residual Feature Aggregation (SRFA) framework integrating MAGRes-UNet for segmentation and DenseNet-121 for hierarchical feature extraction. 2. Introduces a hybrid HHO-BA metaheuristic feature selection strategy to refine the optimal feature subset. 3. Develops a novel hybrid classifier combining Vision Transformer (ViT) and EfficientNet-B3, fine-tuned using a dual SSA-GWO optimization mechanism for robust classification."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of early pancreatic neoplasm detection in multimodal CT imaging by proposing a Scalable Residual Feature Aggregation (SRFA) framework. The method combines advanced segmentation, feature extraction with residual storage, hybrid metaheuristic feature selection, and a novel ViT-EfficientNet-B3 classifier optimized with SSA and GWO. The proposed system achieves high performance (96.23% accuracy), demonstrating significant improvement over traditional and contemporary models for robust early detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u80f0\u817a\u80bf\u7624\u65e9\u671f\u68c0\u6d4b\u56f0\u96be/Early pancreatic neoplasm detection is difficult due to subtle, low-contrast lesions and high patient variability in CT scans.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: SRFA\u6846\u67b6\u6574\u5408MAGRes-UNet\u5206\u5272\u3001DenseNet-121\u7279\u5f81\u63d0\u53d6\u3001HHO-BA\u7279\u5f81\u9009\u62e9\u3001ViT-EfficientNet-B3\u6df7\u5408\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528SSA-GWO\u4f18\u5316/SRFA framework integrates MAGRes-UNet segmentation, DenseNet-121 feature extraction, HHO-BA feature selection, ViT-EfficientNet-B3 hybrid classifier, optimized with SSA-GWO.]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u6a21\u578b\u8fbe\u523096.23%\u51c6\u786e\u7387\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCNN\u548c\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u6a21\u578b/Model achieves 96.23% accuracy, significantly outperforming traditional CNNs and contemporary transformer-based models.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Detection Fire in Camera RGB-NIR"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLOv11, EfficientNetV2, two-stage detection, NIR dataset, Patched-YOLO]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nguyen Truong Khai, Luong Duc Vinh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Viettel (inferred from email domain ",(0,a.jsx)(n.a,{href:"mailto:vinhld@viettel.com",children:"vinhld@viettel.com"}),")"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23594",children:"https://arxiv.org/pdf/2512.23594"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An additional NIR dataset with various data augmentation strategies to address data scarcity. 2. A two-stage detection pipeline combining YOLOv11 and EfficientNetV2-B0 to improve night-time fire detection accuracy and reduce false positives from artificial lights. 3. Patched-YOLO, a patch-based processing method to enhance detection of small and distant fire objects in RGB images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of improving fire detection accuracy, especially at night using NIR cameras and for small objects in RGB images. It proposes a two-stage model (YOLOv11 + EfficientNetV2-B0) for NIR detection and a Patched-YOLO method for RGB, alongside an augmented NIR dataset. The proposed approaches aim to achieve higher accuracy and reduce false positives compared to previous methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Detection Fire in Camera RGB-NIR] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u591c\u95f4\u7ea2\u5916\u706b\u7130\u68c0\u6d4b\u51c6\u786e\u6027\u4f4e / Low accuracy in nighttime NIR fire detection]\nB --\x3e B2[\u8bef\u5c06\u4eae\u5149\u8bc6\u522b\u4e3a\u706b\u7130 / Misclassification of bright lights as fire]\nB --\x3e B3[RGB\u56fe\u50cf\u4e2d\u5c0f\u578b/\u8fdc\u8ddd\u79bb\u706b\u7130\u68c0\u6d4b\u96be / Difficulty detecting small/distant fire in RGB]\nC --\x3e C1[\u65b0\u589eNIR\u6570\u636e\u96c6 / Additional NIR Dataset]\nC --\x3e C2[\u4e24\u9636\u6bb5\u68c0\u6d4b\u6a21\u578b / Two-Stage Detection Model]\nC --\x3e C3[Patched-YOLO / Patched-YOLO]\nD --\x3e D1[\u68c0\u6d4b\u7cbe\u5ea6\u8d85\u8d8a\u5148\u524d\u6a21\u578b / Detection accuracy surpasses previous models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, Harvard University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23628",children:"https://arxiv.org/pdf/2512.23628"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," github.com/zlab-princeton/3d_mem"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Memorization in 3D Shape Generation: An Empirical Study] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(3D\u751f\u6210\u6a21\u578b\u662f\u5426\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e?/Do 3D generative models memorize training data?)\n    C --\x3e C1(\u8bbe\u8ba1\u91cf\u5316\u6846\u67b6/Design evaluation framework)\n    C --\x3e C2(\u4f7f\u7528Vecset\u6269\u6563\u6a21\u578b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c/Use Vecset diffusion model for controlled experiments)\n    D --\x3e D1(\u6570\u636e\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u6761\u4ef6\u589e\u52a0\u8bb0\u5fc6/Data diversity & fine-grained conditioning increase memorization)\n    D --\x3e D2(\u9002\u5ea6\u5f15\u5bfc\u89c4\u6a21\u5cf0\u503c\u8bb0\u5fc6/Moderate guidance scale peaks memorization)\n    D --\x3e D3(\u66f4\u957fVecsets\u548c\u65cb\u8f6c\u589e\u5f3a\u53ef\u7f13\u89e3\u8bb0\u5fc6/Longer Vecsets & rotation augmentation mitigate memorization)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [active perception, audio-guided, tool orchestration, coarse-to-fine perception, multimodal alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Westlake University, Ant Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23646",children:"https://arxiv.org/pdf/2512.23646"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://kd-tao.github.io/OmniAgent",children:"https://kd-tao.github.io/OmniAgent"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces OmniAgent, an audio-guided active perception agent that shifts from passive response to active multimodal inquiry. 2. Proposes a novel coarse-to-fine audio-guided perception paradigm that uses audio cues to localize events and guide reasoning. 3. Demonstrates state-of-the-art performance on audio-video benchmarks, outperforming leading models by 10%-20% accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of fine-grained cross-modal understanding in omnimodal LLMs by proposing OmniAgent, an active perception agent that dynamically orchestrates tools using audio cues to guide video analysis. It achieves superior performance on audio-video understanding benchmarks, significantly outperforming existing models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[OmniAgent: Audio-Guided Active Perception Agent] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Omnimodal LLMs lack fine-grained cross-modal understanding and multimodal alignment]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Audio-guided active perception agent with dynamic tool orchestration and coarse-to-fine perception]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA, outperforms leading models by 10%-20% accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D object detection and tracking], [spatio-temporal alignment, multi-hypothesis decoding, motion modeling, end-to-end perception, autonomous driving]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, IROOTECH"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23635",children:"https://arxiv.org/pdf/2512.23635"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/lixiaoyu2000/HAT",children:"https://github.com/lixiaoyu2000/HAT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HAT, a novel spatio-temporal alignment module that adaptively decodes optimal alignment from multiple motion hypotheses without direct supervision. 2. Integrates both explicit motion models and semantic cues to address suboptimal alignment caused by varying object motion states and features. 3. Demonstrates consistent improvements in 3D perception and tracking performance across diverse baselines and enhances robustness in end-to-end autonomous driving systems, especially under semantic corruption."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that existing spatio-temporal alignment methods in end-to-end 3D perception for autonomous driving are suboptimal due to simplified motion models. It proposes HAT, a module that generates multiple motion-aware feature proposals and adaptively selects the best alignment using semantic and motion cues. HAT improves detection and tracking performance on benchmarks and enhances the robustness of end-to-end autonomous driving systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u56e0\u7b80\u5316\u8fd0\u52a8\u6a21\u578b\u800c\u6b21\u4f18<br>Existing alignment is suboptimal due to simplified motion models] --\x3e P1[\u53d8\u5316\u5bfc\u81f4\u6311\u6218/Variations in motion states and features pose challenges]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51faHAT\u6a21\u5757/Propose HAT module] --\x3e M1[\u591a\u5047\u8bbe\u751f\u6210/Multi-hypothesis generation using explicit models]\n    Method --\x3e M2[\u81ea\u9002\u5e94\u89e3\u7801/Adaptive decoding with semantic & motion cues]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u6027\u80fd\u63d0\u5347\u4e0e\u9c81\u68d2\u6027\u589e\u5f3a/Performance improvement & enhanced robustness] --\x3e R1[SOTA\u8ddf\u8e2a\u7ed3\u679c/SOTA tracking results (46.0% AMOTA)]\n    Results --\x3e R2[\u63d0\u5347\u7aef\u5230\u7aef\u7cfb\u7edf/Improves E2E AD perception and reduces collisions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [video-to-locomotion, visual motion intent, diffusion policy]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23649",children:"https://arxiv.org/pdf/2512.23649"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework's effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RoboMirror: Video to Humanoid Locomotion] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u89c6\u89c9\u7406\u89e3\u4e0e\u63a7\u5236\u5b58\u5728\u9e3f\u6c9f/Gap between visual understanding and control]\n    B --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3/Existing methods lack genuine visual understanding]\n    C --\x3e C1[\u5229\u7528VLM\u63d0\u53d6\u89c6\u89c9\u8fd0\u52a8\u610f\u56fe/Use VLMs to distill visual motion intents]\n    C --\x3e C2[\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u751f\u6210\u52a8\u4f5c/Diffusion-based policy generates actions]\n    C --\x3e C3[\u65e0\u9700\u59ff\u6001\u91cd\u5efa\u6216\u91cd\u5b9a\u5411/No explicit pose reconstruction or retargeting]\n    D --\x3e D1[\u652f\u6301\u7b2c\u4e00\u4e0e\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u63a7\u5236/Supports egocentric & third-person control]\n    D --\x3e D2[\u964d\u4f4e80%\u63a7\u5236\u5ef6\u8fdf/Reduces control latency by 80%]\n    D --\x3e D3[\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53473.7%/3.7% higher task success rate]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [intrinsic image decomposition], [multi-view consistency, transformer, physically grounded model, feed-forward inference, specular shading]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kang Du, Yirui Guan, Zeyu Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Tencent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23667",children:"https://arxiv.org/pdf/2512.23667"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IDT, a feed-forward transformer framework for multi-view intrinsic decomposition that directly outputs view-consistent factors without iterative sampling. 2. Introduces a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. 3. Demonstrates superior performance in producing cleaner decompositions and better multi-view consistency compared to prior methods on synthetic and real-world datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of view inconsistency in multi-view intrinsic image decomposition. It proposes IDT, a feed-forward transformer that jointly processes multiple views using a physically grounded model to decompose images into reflectance and shading components. Experiments show IDT achieves more consistent and interpretable decompositions than previous methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Multi-view intrinsic decomposition suffers from view inconsistency")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Feed-forward transformer with a physically grounded model for joint multi-view reasoning")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Achieves view-consistent, cleaner decompositions (diffuse reflectance, diffuse shading, specular shading)")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Web World Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles, University of Pennsylvania"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23676",children:"https://arxiv.org/pdf/2512.23676"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://princeton-ai2-lab.github.io/Web-World-Models/",children:"https://princeton-ai2-lab.github.io/Web-World-Models/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world "physics" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Web World Models] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models)."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [depth estimation], [video diffusion, transparent objects, LoRA, depth estimation, normal estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Academy of Artificial Intelligence, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23705",children:"https://arxiv.org/pdf/2512.23705"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://daniellli.github.io/projects/DKT/",children:"https://daniellli.github.io/projects/DKT/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces TransPhy3D, a large-scale synthetic video dataset for transparent/reflective scenes with RGB, depth, and normal maps. 2. Proposes DKT, a method that repurposes a pre-trained video diffusion model via lightweight LoRA adapters for temporally consistent depth and normal estimation from videos. 3. Demonstrates state-of-the-art zero-shot performance on real and synthetic benchmarks and shows practical improvement in robotic grasping tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenging problem of depth and normal estimation for transparent and reflective objects in videos. The proposed method, DKT, repurposes a pre-trained video diffusion model using LoRA adapters and a novel synthetic dataset (TransPhy3D) to achieve temporally consistent predictions. The results show state-of-the-art zero-shot performance on benchmarks and improved robotic manipulation, supporting the claim that generative video priors effectively capture the physics of transparency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Transparent/reflective objects break assumptions of traditional depth sensing, causing holes and instability."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Repurpose video diffusion model with LoRA adapters, trained on new synthetic dataset TransPhy3D."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Achieves SOTA zero-shot performance on benchmarks and improves robotic grasping success."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video super-resolution], [diffusion models, online processing, low-latency, auto-regressive, temporal guidance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University, Shanda AI Research Tokyo, MediaTek Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23709",children:"https://arxiv.org/pdf/2512.23709"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://jamichss.github.io/stream-diffvsr-project-page/",children:"https://jamichss.github.io/stream-diffvsr-project-page/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A causally conditioned diffusion framework for online VSR that operates strictly on past frames, enabling low-latency deployment. 2. An Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising to enhance temporal coherence. 3. A lightweight temporal-aware decoder with a Temporal Processor Module (TPM) and a four-step distilled denoiser, achieving fast inference (0.328s per 720p frame) while maintaining high perceptual quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the impracticality of diffusion-based video super-resolution (VSR) for low-latency applications by proposing Stream-DiffVSR, an online framework that uses causal conditioning, a distilled denoiser, and novel temporal modules. The method significantly reduces latency to 0.328 seconds per frame while improving perceptual quality, making it the first diffusion VSR approach suitable for real-time online deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Stream-DiffVSR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6269\u6563VSR\u5ef6\u8fdf\u9ad8/Diffusion VSR has high latency]\n    B --\x3e B2[\u4f9d\u8d56\u672a\u6765\u5e27/Relies on future frames]\n    C --\x3e C1[\u56e0\u679c\u6761\u4ef6\u6269\u6563/Causally Conditioned Diffusion]\n    C --\x3e C2[\u81ea\u56de\u5f52\u65f6\u5e8f\u5f15\u5bfc/Auto-regressive Temporal Guidance (ARTG)]\n    C --\x3e C3[\u56db\u6b65\u84b8\u998f\u53bb\u566a\u5668/4-step Distilled Denoiser]\n    D --\x3e D1[\u5ef6\u8fdf0.328\u79d2/Latency 0.328s per frame]\n    D --\x3e D2[\u611f\u77e5\u8d28\u91cf\u63d0\u5347/Improved perceptual quality (LPIPS)]\n    D --\x3e D3[\u9996\u4e2a\u5728\u7ebf\u6269\u6563VSR/First online diffusion VSR]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [MobileNetV2, radiomics, Grad-CAM, RandomForest, feature fusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Areeb Ehsan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Georgia State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22184",children:"https://arxiv.org/pdf/2512.22184"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a hybrid "virtual biopsy" pipeline combining a lightweight CNN (MobileNetV2) with handcrafted radiomics features for brain tumor classification. 2. Employs a late fusion strategy using a RandomForest classifier on the concatenated CNN embeddings and radiomics features to improve performance. 3. Provides model explainability through Grad-CAM visualizations and radiomics feature importance analysis, and evaluates robustness under low-resolution and noisy imaging conditions relevant to low-resource settings.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of brain tumor diagnosis in low-resource settings by proposing a virtual biopsy pipeline. The method combines a lightweight CNN with interpretable radiomics features and fuses them using a RandomForest classifier. Experiments show the fusion approach improves classification performance and the analysis highlights its sensitivity to image quality issues common in resource-constrained environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Brain tumor diagnosis is difficult in low-resource settings due to limited expert access, hardware, and invasive biopsies."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Uses a hybrid pipeline with a lightweight CNN (MobileNetV2) and radiomics features, fused via RandomForest."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Fusion improves performance; robustness tests reveal sensitivity to low-quality images relevant to low-resource environments."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image super-resolution], [diffusion models, SR3, DDPM, capsule endoscopy, HyperKvasir]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haozhe Jia"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Boston University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22209",children:"https://arxiv.org/pdf/2512.22209"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [nnU-Net, MRI field strength, radiomic analysis, UMAP clustering, model generalizability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22176",children:"https://arxiv.org/pdf/2512.22176"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>\u78c1\u5171\u632f\u6210\u50cf\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\u4e2d\u573a\u5f3a\u4f9d\u8d56\u7684\u6027\u80fd\u53d8\u5f02\u6027") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI\u573a\u5f3a\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>\u57281.5T\u30013.0T\u53ca\u6df7\u5408\u6570\u636e\u4e0a\u8bad\u7ec3/\u8bc4\u4f30nnU-Net\u6a21\u578b\uff1b\u4f7f\u7528UMAP\u548c\u5f71\u50cf\u7ec4\u5b66\u5206\u6790")\n    Results("\u5173\u952e\u7ed3\u679c/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u573a\u5f3a\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u8f6f\u7ec4\u7ec7")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image reconstruction], [Swin Transformer, complex-valued network, k-space undersampling, super-resolution, Parkinson's disease]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Usman, Sung-Min Gho"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University, DeepNoid Inc."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22202",children:"https://arxiv.org/pdf/2512.22202"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel complex-valued Swin Transformer network for super-resolution reconstruction of multi-echo MRI data. 2. Demonstrated high-quality SMWI reconstruction from low-resolution/undersampled k-space data, significantly reducing required scan time. 3. Validated the method's ability to preserve critical diagnostic features for Parkinson's Disease, enhancing clinical applicability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the long scan time problem of Susceptibility Map Weighted Imaging (SMWI) for Parkinson's disease diagnosis. The authors propose a complex-valued Swin Transformer network to reconstruct high-quality SMWI images from reduced k-space data. Experimental results show the method achieves high reconstruction quality (SSIM 0.9116) while preserving diagnostic details, enabling faster clinical scans."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>SMWI full scan time too long] --\x3e P1[\u5173\u952e\u9650\u5236/Key Limitation<br>Long scan time limits clinical use]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Complex Swin Transformer Network] --\x3e M1[\u6280\u672f\u6838\u5fc3/Technical Core<br>Super-resolve multi-echo MRI data]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>High-quality reconstruction from reduced data] --\x3e R1[\u91cf\u5316\u6307\u6807/Quantitative Metrics<br>SSIM: 0.9116, MSE: 0.076]\n    Results --\x3e R2[\u4e34\u5e8a\u4ef7\u503c/Clinical Value<br>Preserves diagnostic features, faster scan]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [point cloud compression], [Mamba, end-to-end learning, joint geometry-attribute compression, entropy model, rate-distortion optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Chung Cheng University, National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22463",children:"https://arxiv.org/pdf/2512.22463"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MEGA-PCC, a fully end-to-end learning-based framework for joint point cloud geometry and attribute compression that eliminates the need for post-hoc recoloring and manual bitrate tuning. 2. Introduces a Mamba-based Entropy Model (MEM) that captures spatial and channel-wise correlations to improve probability estimation for entropy coding. 3. Employs a shared encoder with dual decoders built on the Mamba architecture to model long-range dependencies, enabling data-driven bitrate allocation and superior rate-distortion performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing point cloud compression methods, which rely on complex post-processing and manual bitrate allocation. The authors propose MEGA-PCC, an end-to-end framework using a Mamba-based shared encoder and a specialized entropy model for joint geometry and attribute compression. Experiments show the method outperforms traditional and learning-based baselines in both performance and efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u540e\u5904\u7406\u4e0e\u624b\u52a8\u7801\u7387\u5206\u914d/Existing methods rely on recoloring & manual bit allocation]\n    C --\x3e C1[\u7aef\u5230\u7aef\u8054\u5408\u538b\u7f29\u6846\u67b6/End-to-end joint compression framework]\n    C1 --\x3e C2[\u5171\u4eab\u7f16\u7801\u5668\u4e0e\u53cc\u89e3\u7801\u5668/Shared encoder & dual decoders]\n    C1 --\x3e C3[Mamba\u71b5\u6a21\u578b/Mamba-based Entropy Model (MEM)]\n    D --\x3e D1[\u4f18\u5f02\u7684\u7387\u5931\u771f\u6027\u80fd/Superior rate-distortion performance]\n    D --\x3e D2[\u66f4\u9ad8\u7684\u8fd0\u884c\u65f6\u6548\u7387/Higher runtime efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image reconstruction], [semantic contrastive learning, orthogonal CT, U-Net, GAN, sparse-view reconstruction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22674",children:"https://arxiv.org/pdf/2512.22674"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel semantic feature contrastive learning loss function for CT reconstruction, 2. Introduces a three-stage U-Net-based architecture for coarse reconstruction, detail refinement, and semantic similarity measurement, 3. Demonstrates superior reconstruction quality and faster processing on a chest dataset with orthogonal projections."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of streak artifacts in sparse-view X-ray CT reconstruction by proposing a novel semantic contrastive learning loss and a three-stage U-Net architecture. The method improves image quality and processing speed compared to other algorithms, offering a practical solution for orthogonal CT reconstruction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction<br/>\u57fa\u4e8e\u8bed\u4e49\u5bf9\u6bd4\u5b66\u4e60\u7684\u6b63\u4ea4X\u5c04\u7ebfCT\u91cd\u5efa"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Sparse-view CT leads to streak artifacts<br/>\u7a00\u758f\u89c6\u56feCT\u5bfc\u81f4\u6761\u7eb9\u4f2a\u5f71"] --\x3e P1["Ill-posed reconstruction<br/>\u75c5\u6001\u91cd\u5efa"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Semantic contrastive learning & three-stage U-Net<br/>\u8bed\u4e49\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u4e09\u9636\u6bb5U-Net"] --\x3e M1["Novel loss function<br/>\u65b0\u9896\u635f\u5931\u51fd\u6570"]\n    Method --\x3e M2["Coarse & detail refinement<br/>\u7c97\u91cd\u5efa\u4e0e\u7ec6\u8282\u4f18\u5316"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Superior quality & faster processing<br/>\u66f4\u4f18\u8d28\u91cf\u4e0e\u66f4\u5feb\u5904\u7406"] --\x3e R1["Improved image quality<br/>\u63d0\u5347\u56fe\u50cf\u8d28\u91cf"]\n    Results --\x3e R2["Low computational complexity<br/>\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] JParc: Joint cortical surface parcellation with registration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [cortical parcellation, surface registration, atlas propagation, deep learning, geometric features]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jian Li, Karthik Gopinath, Brian L. Edlow, Adrian V. Dalca, Bruce Fischl"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Athinoula A. Martinos Center for Biomedical Imaging (MGH & HMS), MIT Computer Science and Artificial Intelligence Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22485",children:"https://arxiv.org/pdf/2512.22485"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes JParc, a novel joint framework that integrates cortical surface registration and parcellation into a single learning-based model. 2. Demonstrates that the performance improvement is primarily due to accurate registration and a learned parcellation atlas, providing an explanation for the success of learning-based methods. 3. Achieves state-of-the-art parcellation accuracy (Dice >90%) on the Mindboggle dataset using only basic geometric features like sulcal depth and curvature."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces JParc, a joint framework for cortical surface registration and parcellation. It shows that combining these tasks and learning an atlas leads to superior performance, achieving over 90% Dice score on a standard dataset using simple geometric features. This accuracy can enhance brain mapping studies and clinical applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[JParc: Joint cortical surface parcellation with registration] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u5b66\u4e60\u4e0e\u6ce8\u518c\u5206\u79bb/Learning vs. Registration Gap]\n    Problem --\x3e P2[\u9700\u8981\u81ea\u52a8\u7cbe\u51c6\u5206\u5272/Need for Accurate Automated Parcellation]\n    Method --\x3e M1[\u8054\u5408\u6846\u67b6/Joint Registration & Parcellation Framework]\n    Method --\x3e M2[\u4f7f\u7528\u6d45\u5c42\u5b50\u7f51\u7edc\u5fae\u8c03/Shallow Subnetwork for Fine-tuning]\n    Method --\x3e M3[\u4ec5\u7528\u51e0\u4f55\u7279\u5f81/Using Basic Geometric Features]\n    Results --\x3e R1[Dice\u5206\u6570 >90%/Dice Score >90%]\n    Results --\x3e R2[\u6027\u80fd\u5f52\u56e0\u4e8e\u6ce8\u518c\u4e0e\u56fe\u8c31/Performance Attributed to Registration & Atlas]\n    Results --\x3e R3[\u63d0\u5347\u4e0b\u6e38\u5e94\u7528/Enhances Downstream Applications]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging reconstruction], [Compton camera, Swin Transformer, end-to-end reconstruction, list-mode data, transposed convolution]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Minghao Dong, Xinyang Luo, Xujian Ouyang, Yongshun Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22766",children:"https://arxiv.org/pdf/2512.22766"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SwinCCIR, a novel end-to-end deep learning framework for Compton camera imaging that directly maps list-mode events to source distribution, bypassing traditional back-projection. 2. Introduces the use of Swin Transformer blocks to model the complex relationships in the data, combined with a transposed convolution-based image generation module. 3. Demonstrates the method's effectiveness on both simulated and practical datasets, showing it overcomes artifacts and deformations inherent in conventional reconstruction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SwinCCIR, an end-to-end deep learning model using Swin Transformer blocks and transposed convolutions to directly reconstruct images from Compton camera list-mode data. The method bypasses the problematic back-projection step of traditional approaches. Experiments on simulated and real data show it effectively reduces artifacts and deformations, improving image quality for practical applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SwinCCIR: \u5eb7\u666e\u987f\u76f8\u673a\u6210\u50cf\u91cd\u5efa\u7f51\u7edc] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u53cd\u6295\u5f71\u5bfc\u81f4\u4e25\u91cd\u4f2a\u5f71\u548c\u53d8\u5f62/Back-projection causes severe artifacts & deformation]\n    B --\x3e B2[\u7cfb\u7edf\u8bef\u5dee\u96be\u4ee5\u6821\u51c6/Systematic errors hard to calibrate]\n    C --\x3e C1[\u91c7\u7528Swin Transformer\u5757/Adopts Swin Transformer blocks]\n    C --\x3e C2[\u8f6c\u7f6e\u5377\u79ef\u56fe\u50cf\u751f\u6210\u6a21\u5757/Transposed convolution generation module]\n    C --\x3e C3[\u5efa\u7acb\u5217\u8868\u6a21\u5f0f\u4e8b\u4ef6\u5230\u6e90\u5206\u5e03\u7684\u6620\u5c04/Establishes mapping from list-mode events to source distribution]\n    D --\x3e D1[\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1/Validated on simulated & practical dataset]\n    D --\x3e D2[\u6709\u6548\u514b\u670d\u4f20\u7edf\u6210\u50cf\u95ee\u9898/Effectively overcomes conventional imaging problems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [remote sensing image segmentation], [GeoSAM, glacier delineation, multi-temporal mapping, Sentinel-2, spectral-index]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexandru Hegyi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Oslo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22855",children:"https://arxiv.org/pdf/2512.22855"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a semi-automatic workflow combining GeoSAM with spectral-index pre-processing for rapid glacier mapping. 2. Demonstrates the method's effectiveness for generating temporally consistent glacier outlines in a challenging Arctic environment (Svalbard). 3. Highlights the workflow's flexibility and transferability to other optical datasets due to its reliance on derived RGB imagery."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This report presents a semi-automatic workflow for rapidly delineating glaciers from satellite imagery. The method uses GeoSAM guided by spectral-index prompts on Sentinel-2 data to create annual glacier outlines. The results show the approach is fast and produces consistent maps for major glaciers, though it requires some user inspection for refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Consistent glacier boundary delineation is essential but difficult to scale across time and environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Combines image compositing, spectral-index pre-processing, prompt-guided GeoSAM segmentation, and post-processing.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Produces spatially coherent and temporally consistent outlines; errors are associated with small, complex features; method is fast and transferable.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Deep Learning for Art Market Valuation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23078",children:"https://arxiv.org/pdf/2512.23078"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Deep Learning for Art Market Valuation<br/>\u827a\u672f\u5e02\u573a\u4f30\u503c\u7684\u6df1\u5ea6\u5b66\u4e60] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>How to improve art market valuation?<br/>\u5982\u4f55\u6539\u8fdb\u827a\u672f\u5e02\u573a\u4f30\u503c\uff1f]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>\u878d\u5408\u8868\u683c\u4e0e\u56fe\u50cf\u6570\u636e\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Visual features help most for fresh-to-market works<br/>\u89c6\u89c9\u7279\u5f81\u5bf9\u9996\u6b21\u4e0a\u5e02\u4f5c\u54c1\u6700\u6709\u5e2e\u52a9]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] EIR: Enhanced Image Representations for Medical Report Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image captioning], [cross-modal transformer, metadata fusion, domain-specific pre-training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, PAII Inc., Beijing University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23185",children:"https://arxiv.org/pdf/2512.23185"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[EIR: Enhanced Image Representations for Medical Report Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u62a5\u544a\u751f\u6210\u8017\u65f6\u8017\u529b/Report generation is time-consuming]\n    Problem --\x3e P2[\u4fe1\u606f\u4e0d\u5bf9\u79f0\u4e0e\u9886\u57df\u9e3f\u6c9f/Information asymmetry & domain gap]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u8de8\u6a21\u6001Transformer\u878d\u5408\u5143\u6570\u636e/Cross-modal transformer for metadata fusion]\n    Method --\x3e M2[\u533b\u5b66\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b/Medical domain pre-trained model]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5728MIMIC\u548cOpen-I\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1/Validated on MIMIC & Open-I datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);