"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6526],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(96540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},38808:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_LO/20251215-20251221","title":"20251215-20251221 (cs.LO)","description":"2025-12-19","source":"@site/docs/daily/cs_LO/20251215-20251221.md","sourceDirName":"daily/cs_LO","slug":"/daily/cs_LO/20251215-20251221","permalink":"/ai_toutiao/daily/cs_LO/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767086937000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.LO","permalink":"/ai_toutiao/category/cslo"},"next":{"title":"20251222-20251228 (cs.LO)","permalink":"/ai_toutiao/daily/cslo/20251222-20251228"}}');var r=i(74848),t=i(28453);const o={},a="20251215-20251221 (cs.LO)",l={},c=[{value:"2025-12-19",id:"2025-12-19",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251215-20251221-cslo",children:"20251215-20251221 (cs.LO)"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Hoare logic, weakest precondition reasoning, neurosymbolic AI, OpenJML, counterexample-guided repair]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Daragh King, Vasileios Koutavas, Laura Kovacs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Trinity College Dublin, Lero, TU Wien"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15816",children:"https://arxiv.org/pdf/2512.15816"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents NeuroInv, a neurosymbolic method for generating loop invariants that combines a neural module using LLMs and Hoare logic for backward-chaining weakest precondition reasoning with a symbolic module that iteratively repairs invariants using counterexamples from OpenJML. It achieves a 99.5% success rate on a benchmark of 150 Java programs and demonstrates scalability on complex multi-loop programs, substantially outperforming other approaches."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [computational theory], [G\xf6del incompleteness, Lyapunov exponent, prediction horizon, algorithmic intelligence]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abhisek Ganguly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Jawaharlal Nehru Centre for Advanced Scientific Research"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16707",children:"https://arxiv.org/pdf/2512.16707"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper formalizes two computational limitations\u2014formal incompleteness from logic and dynamical unpredictability from chaos theory\u2014to analyze algorithmic intelligence. It demonstrates that these constraints jointly bound an agent's ability to reason about its own predictive capabilities, concluding that an algorithmic agent generally cannot compute its own maximal prediction horizon."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] Towards Mass Spectrum Analysis with ASP"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [combinatorial search], [Answer Set Programming (ASP), symmetry-breaking, canonical representations, molecular structure, mass spectrometry]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nils K\xfcchenmeister, Alex Ivliev, Markus Kr\xf6tzsch"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," TU Dresden"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16780",children:"https://arxiv.org/pdf/2512.16780"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel application of Answer Set Programming (ASP) to determine molecular structures from mass spectrometry data by encoding chemical knowledge and using canonical representations to break symmetries and constrain the combinatorial search space. The authors evaluate their method on known structures and compare its performance to other ASP symmetry-breaking techniques and a commercial chemistry tool. The approach effectively reduces redundant solutions and demonstrates the utility of ASP for this chemical analysis problem."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [model compression, quantization, pruning, signal temporal logic, bayesian optimization]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Khurram Khalil, Khaza Anuarul Hoque"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Missouri-Columbia"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16855",children:"https://arxiv.org/pdf/2512.16855"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces TOGGLE, a framework that uses Signal Temporal Logic (STL) and Bayesian optimization to guide the layer-wise compression of Large Language Models through quantization and pruning. It formally enforces linguistic properties during compression without requiring retraining. The method achieves significant reductions in model size and computational cost while preserving specified model behaviors, enabling verifiable deployment on edge devices."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);