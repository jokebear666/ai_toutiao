"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9838],{109:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"daily/cs_RO/20251222-20251228","title":"20251222-20251228 (cs.RO)","description":"2025-12-22","source":"@site/docs/daily/cs_RO/20251222-20251228.md","sourceDirName":"daily/cs_RO","slug":"/daily/csro/20251222-20251228","permalink":"/ai_toutiao/daily/csro/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767009989000,"frontMatter":{"slug":"/daily/csro/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.RO)","permalink":"/ai_toutiao/daily/cs_RO/20251215-20251221"},"next":{"title":"20251229-20260104 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20251229-20260104"}}');var r=i(4848),a=i(8453);const t={slug:"/daily/csro/20251222-20251228"},d="20251222-20251228 (cs.RO)",l={},o=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2},{value:"2025-12-24",id:"2025-12-24",level:2},{value:"2025-12-25",id:"2025-12-25",level:2}];function c(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"20251222-20251228-csro",children:"20251222-20251228 (cs.RO)"})}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Toyota Research Institute"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17091",children:"https://arxiv.org/pdf/2512.17091"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [differentiable simulation, graph neural network, SE(3)-equivariance, attention mechanism, 3D Zernike polynomials, shape-matching loss, implicit differentiation, bilevel optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Seong Ho Pahng, Guoye Guan, Benjamin Fefferman, Sahand Hormoz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Harvard University, Harvard Medical School, Dana-Farber Cancer Institute, Broad Institute of MIT and Harvard"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17129",children:"https://arxiv.org/pdf/2512.17129"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces DiffeoMorph, a differentiable framework that uses an attention-based SE(3)-equivariant graph neural network to train agents to collectively morph into target 3D shapes. It employs a novel shape-matching loss based on 3D Zernike polynomials and uses implicit differentiation to handle a bilevel optimization problem for rotation alignment. The method successfully generates complex shapes from simple ellipsoids using minimal spatial cues."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [interactive reinforcement learning, multi-teacher learning, Q-learning, teacher selection, concept drift]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Maher Mesto, Francisco Cruz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of New South Wales, Universidad Central de Chile"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17180",children:"https://arxiv.org/pdf/2512.17180"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [sys], [robotics and navigation], [extended kalman filter, inertial measurement unit, wheel odometer, dead reckoning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yan Gao, Jiliang Wang, Minghan Wang, Xiaohua Chen, Demin Chen, Zhiyong Ren, Tian-Yun Huang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Peking University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17215",children:"https://arxiv.org/pdf/2512.17215"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52e181750dd60029c711d4a23702ec5e96a39d86b1ce8c7f9c34de75f853c369_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a dead reckoning method for a self-propelled pipeline robot, using an IMU for initial attitude estimation, refining it with an Extended Kalman Filter, and combining it with wheel odometer data for localization. The method was tested in a rectangular loop pipeline, and the results verified the effectiveness of the proposed algorithm for navigating complex three-dimensional pipelines."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [imitation learning, dataset aggregation, direct preference optimization, closed-loop evaluation, expert takeover data]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Science and Technology Beijing; Chongqing Chang'an Technology Co., Ltd."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17370",children:"https://arxiv.org/pdf/2512.17370"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b74ccffb4336d971aedcab583ac81f676e7f75bb85a137f4255da4a692fafe_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes TakeAD, a framework that fine-tunes a pre-trained imitation learning policy for autonomous driving using expert takeover data. The method combines iterative Dataset Aggregation (DAgger) for imitation with Direct Preference Optimization (DPO) for preference alignment to improve closed-loop performance. Experiments show it effectively mitigates the open-loop gap and outperforms pure imitation learning methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [sys], [sensor fusion and filtering], [Error-State Extended Kalman Filter, Scaled Unscented Kalman Filter, visual-inertial odometry, quaternion estimation, adaptive covariance, loosely coupled architecture]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ufuk Asil, Efendi Nasibov"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Dokuz Eylul University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17505",children:"https://arxiv.org/pdf/2512.17505"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mahesh Keswani, Raunak Bhattacharyya"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology Delhi"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17586",children:"https://arxiv.org/pdf/2512.17586"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Vidarc: Embodied Video Diffusion Model for Closed-loop Control"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion inference], [video diffusion, autoregressive generation, masked inverse dynamics model, closed-loop control, cross-embodiment pre-training, KV cache]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tsinghua University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17661",children:"https://arxiv.org/pdf/2512.17661"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7977f39cb797f71021c4776c090587d8f5e8e7c33c06e677b445877e8ad4c5d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Vidarc, a method for robotic control that combines an autoregressive video diffusion model with a masked inverse dynamics model to enable fast, closed-loop operation. It is pre-trained on a large dataset of diverse robotic episodes and achieves state-of-the-art performance, including higher success rates and significantly lower latency compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [energy-based models, gradient-based refinement, hindsight goal relabeling, latent-space planning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Carlos V\xe9lez Garc\xeda, Miguel Cazorla, Jorge Pomares"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," INESCOP, University of Alicante"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17846",children:"https://arxiv.org/pdf/2512.17846"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [ViPR, ViPR-Eureka, ViPR-RL, behavior cloning, VLM-in-the-loop Parallel Refinement, LLM-guided contact sampling, sim-to-real transfer, GPU simulation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Robotics and AI Institute"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17853",children:"https://arxiv.org/pdf/2512.17853"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [diffusion model, bird's-eye-view, radar cross section, Doppler, point cloud generation, foundation models]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Technion, MIT, NVIDIA, University of Toronto, Vector Institute"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17897",children:"https://arxiv.org/pdf/2512.17897"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251222] Diffusion Forcing for Multi-Agent Interaction Sequence Modeling"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [multi-agent motion generation], [diffusion forcing, autoregressive diffusion, transformer, multi-agent interaction, denoising]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," UC Berkeley, Sony Group Corporation, Meta"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17900",children:"https://arxiv.org/pdf/2512.17900"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Untethered thin dielectric elastomer actuated soft robot"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xi Wang, Jing Liu, Siqian Li, Hengtai Dai, Jung-Che Chang, Adam Rushworth, Xin Dong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17940",children:"https://arxiv.org/pdf/2512.17940"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/147e184fb3d08f337e1d702b64cecb4a8209fe095f79478259483975e982eb39_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/147e184fb3d08f337e1d702b64cecb4a8209fe095f79478259483975e982eb39_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Untethered thin dielectric elastomer actuated soft robot"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Farida Mohsen, Ali Safa"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17958",children:"https://arxiv.org/pdf/2512.17958"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Robotic VLA Benefits from Joint Learning with Motion Image Diffusion"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18007",children:"https://arxiv.org/pdf/2512.18007"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Robotic VLA Benefits from Joint Learning with Motion Image Diffusion"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18028",children:"https://arxiv.org/pdf/2512.18028"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jacqueline Borgstedt, Jake Bhattacharyya, Matteo Iovino, Frank E. Pollick, Stephen Brewster"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18032",children:"https://arxiv.org/pdf/2512.18032"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab5ccc615f8631e1cb919fee6836987e7793d4803a41430b431092ecd0b25ec_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Unifying Deep Predicate Invention with Pre-trained Foundation Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Qianwei Wang, Bowen Li, Zhanpeng Luo, Yifan Xu, Alexander Gray, Tom Silver, Sebastian Scherer, Katia Sycara, Yaqi Xie"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17992",children:"https://arxiv.org/pdf/2512.17992"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a6ef8216b049bb762fabd2e4dcf908ec9b4a2cc0dfdfed0068704f85aa89e8a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a6ef8216b049bb762fabd2e4dcf908ec9b4a2cc0dfdfed0068704f85aa89e8a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Unifying Deep Predicate Invention with Pre-trained Foundation Models"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Design of a Polymer-based Steerable Cannula for Neurosurgical Applications"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Nidhi Malhotra, Amber K. Rothe, Revanth Konda, Jaydev P. Desai"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18048",children:"https://arxiv.org/pdf/2512.18048"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb51c5c2d914d049ca0f0ea5fe80a670a7185b2b3b169a2816bc271d98f67cde_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb51c5c2d914d049ca0f0ea5fe80a670a7185b2b3b169a2816bc271d98f67cde_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Design of a Polymer-based Steerable Cannula for Neurosurgical Applications"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Juo-Tung Chen, XinHao Chen, Ji Woong Kim, Paul Maria Scheikl, Richard Jaepyeong Cha, Axel Krieger"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18068",children:"https://arxiv.org/pdf/2512.18068"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afad84fb7ac4ae6f6bdc263133a9e2d4acce99d04476e05fcc665011a0948574_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afad84fb7ac4ae6f6bdc263133a9e2d4acce99d04476e05fcc665011a0948574_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Towards Autonomous Navigation in Endovascular Interventions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tudor Jianu, Anh Nguyen, Sebastiano Fichera, Pierre Berthet-Rayne"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18081",children:"https://arxiv.org/pdf/2512.18081"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Towards Autonomous Navigation in Endovascular Interventions"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] On Swarm Leader Identification using Probing Policies"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Stergios E. Bachoumas, Panagiotis Artemiadis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18146",children:"https://arxiv.org/pdf/2512.18146"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," On Swarm Leader Identification using Probing Policies"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Trevor Stepp, Parthan Olikkal, Ramana Vinjamuri, Rajasekhar Anguluri"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18206",children:"https://arxiv.org/pdf/2512.18206"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16675ffdd8bab8dfbdd7e63041cd5c55425ee187db948a8a16692b6b569f0516_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16675ffdd8bab8dfbdd7e63041cd5c55425ee187db948a8a16692b6b569f0516_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Wu-Te Yang, Masayoshi Tomizuka"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18213",children:"https://arxiv.org/pdf/2512.18213"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dc328a115226b3e0e0e21126d10a6edacc4f54483536e9b10d3091f0de254b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dc328a115226b3e0e0e21126d10a6edacc4f54483536e9b10d3091f0de254b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18211",children:"https://arxiv.org/pdf/2512.18211"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shahram Najam Syed, Yitian Hu, Yuchao Yao"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18237",children:"https://arxiv.org/pdf/2512.18237"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Si Wei Feng"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18268",children:"https://arxiv.org/pdf/2512.18268"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d836c87e7156449cac14d8856f6ca606bb4ddeea577e006c464614eac364fe2d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d836c87e7156449cac14d8856f6ca606bb4ddeea577e006c464614eac364fe2d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Youssef Mahran, Zeyad Gamal, Ayman El-Badawy"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18333",children:"https://arxiv.org/pdf/2512.18333"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Youssef Mahran, Zeyad Gamal, Ayman El-Badawy"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18336",children:"https://arxiv.org/pdf/2512.18336"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yihang Zhu, Weiqing Wang, Shijie Wu, Ye Shi, Jingya Wang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18368",children:"https://arxiv.org/pdf/2512.18368"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/004564326e17c1e67b90e1c7ff4c2148031c0e38d68983ac3a5150236f6fa495_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/004564326e17c1e67b90e1c7ff4c2148031c0e38d68983ac3a5150236f6fa495_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yulu Wu, Jiujun Cheng, Haowen Wang, Dengyang Suo, Pei Ren, Qichao Mao, Shangce Gao, Yakun Huang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18396",children:"https://arxiv.org/pdf/2512.18396"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336a4848582e30bb097fa913acf8a86a750fe8f43097ffae12c485a2ff45723d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/336a4848582e30bb097fa913acf8a86a750fe8f43097ffae12c485a2ff45723d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] When Robots Say No: The Empathic Ethical Disobedience Benchmark"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Dmytro Kuzmenko, Nadiya Shvai"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18474",children:"https://arxiv.org/pdf/2512.18474"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," When Robots Say No: The Empathic Ethical Disobedience Benchmark"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] STORM: Search-Guided Generative World Models for Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18477",children:"https://arxiv.org/pdf/2512.18477"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," STORM: Search-Guided Generative World Models for Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Erdao Liang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18537",children:"https://arxiv.org/pdf/2512.18537"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c63a2c59e4cb34d27c44febba963f6d35bc48277e311a14bca062e28fbe5aee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c63a2c59e4cb34d27c44febba963f6d35bc48277e311a14bca062e28fbe5aee_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengcheng Li, Qiang Fang, Tong Zhao, Yixing Lan, Xin Xu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18583",children:"https://arxiv.org/pdf/2512.18583"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fdabaf80e15fc440c53464fb3134095f9121f39b7d6d83850d4cb921dec3fda_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhenhao Zhou, Dan Negrut"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18619",children:"https://arxiv.org/pdf/2512.18619"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Offline Reinforcement Learning for End-to-End Autonomous Driving"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chihiro Noguchi, Takaki Yamamoto"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18662",children:"https://arxiv.org/pdf/2512.18662"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Offline Reinforcement Learning for End-to-End Autonomous Driving"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Geometric-Photometric Event-based 3D Gaussian Ray Tracing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18640",children:"https://arxiv.org/pdf/2512.18640"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Geometric-Photometric Event-based 3D Gaussian Ray Tracing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Cailin Lei, Haiyang Wu, Yuxiong Ji, Xiaoyu Cai, Yuchuan Du"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18703",children:"https://arxiv.org/pdf/2512.18703"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Maozeng Zhang, Ke Shi, Huijun Li, Tongshu Chen, Jiejun Yan, Aiguo Song"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18712",children:"https://arxiv.org/pdf/2512.18712"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/788f7143f635c52e41543f64d13fef2124889bb6d62b5ae4656ad27e5705081b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/788f7143f635c52e41543f64d13fef2124889bb6d62b5ae4656ad27e5705081b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jingjia Teng, Yang Li, Jianqiang Wang, Yingbai Hu, Songyuan Tang, Manjiang Hu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18836",children:"https://arxiv.org/pdf/2512.18836"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a54132191e50063831023e6eae9aa9357ab29b75e7078d8784bd1328c2d0f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a54132191e50063831023e6eae9aa9357ab29b75e7078d8784bd1328c2d0f0_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Feeza Khan Khanzada, Jaerock Kwon"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18850",children:"https://arxiv.org/pdf/2512.18850"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Construction and deformation of P-hedra using control polylines"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Georg Nawratil"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18869",children:"https://arxiv.org/pdf/2512.18869"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c054a6aa06f719232431f4b3e82e7945ca7f3f642dc9e2322fdc9185878ba5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c054a6aa06f719232431f4b3e82e7945ca7f3f642dc9e2322fdc9185878ba5_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Construction and deformation of P-hedra using control polylines"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tianyuan Liu, Richard Dazeley, Benjamin Champion, Akan Cosgun"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18922",children:"https://arxiv.org/pdf/2512.18922"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb36c038274937dd40786c8ac78288222c12acc29b7aee73573c5a6331a6145_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb36c038274937dd40786c8ac78288222c12acc29b7aee73573c5a6331a6145_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Point What You Mean: Visually Grounded Instruction Policy"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18933",children:"https://arxiv.org/pdf/2512.18933"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Point What You Mean: Visually Grounded Instruction Policy"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] A Framework for Deploying Learning-based Quadruped Loco-Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yadong Liu, Jianwei Liu, He Liang, Dimitrios Kanoulas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18938",children:"https://arxiv.org/pdf/2512.18938"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Framework for Deploying Learning-based Quadruped Loco-Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yanding Yang, Weitao Zhou, Jinhai Wang, Xiaomin Guo, Junze Wen, Xiaolong Liu, Lang Ding, Zheng Fu, Jinyu Miao, Kun Jiang, Diange Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18988",children:"https://arxiv.org/pdf/2512.18988"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b25603fac1726ddbcf4804fa919bd28a3ae6d25cea4e62760e5e98ac335e570_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b25603fac1726ddbcf4804fa919bd28a3ae6d25cea4e62760e5e98ac335e570_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18987",children:"https://arxiv.org/pdf/2512.18987"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19021",children:"https://arxiv.org/pdf/2512.19021"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chao Yang, Yingkai Sun, Peng Ye, Xin Chen, Chong Yu, Tao Chen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19043",children:"https://arxiv.org/pdf/2512.19043"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7fe11e8953d7da559e3d75bda7b4d386de464da3c906331d4f0b2c94d096ac4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7fe11e8953d7da559e3d75bda7b4d386de464da3c906331d4f0b2c94d096ac4_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19024",children:"https://arxiv.org/pdf/2512.19024"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19083",children:"https://arxiv.org/pdf/2512.19083"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19133",children:"https://arxiv.org/pdf/2512.19133"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jose Gustavo Buenaventura Carreon, Floris Erich, Roman Mykhailyshyn, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19148",children:"https://arxiv.org/pdf/2512.19148"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956d6e879c4f50e7adb31c9c246b0ef696f20c3d25b5e10073242ec049305b97_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956d6e879c4f50e7adb31c9c246b0ef696f20c3d25b5e10073242ec049305b97_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Vision-Language-Policy Model for Dynamic Robot Task Planning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19178",children:"https://arxiv.org/pdf/2512.19178"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Vision-Language-Policy Model for Dynamic Robot Task Planning"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tarek Bouazza, Alessandro Melis, Soulaimane Berkane, Robert Mahony, Tarek Hamel"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19245",children:"https://arxiv.org/pdf/2512.19245"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhaoyang Liu, Weitao Zhou, Junze Wen, Cheng Jing, Qian Cheng, Kun Jiang, Diange Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19270",children:"https://arxiv.org/pdf/2512.19270"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956fe529e5541a0272de8e8183c92a6a5f010136fae9741d64e0049d6c4ea14c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/956fe529e5541a0272de8e8183c92a6a5f010136fae9741d64e0049d6c4ea14c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Translating Flow to Policy via Hindsight Online Imitation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19269",children:"https://arxiv.org/pdf/2512.19269"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d1e38857114c4e6bdd80a03e175ccdec39b489a4a7abb23d37f7f4402ba68fc_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Translating Flow to Policy via Hindsight Online Imitation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Longxiang Shao, Ulrich Dahmen, Juergen Rossmann"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19289",children:"https://arxiv.org/pdf/2512.19289"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd64634c88e8117da55124a532a6234cc3d8703ceb30084a311c5e28bb6ba516_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd64634c88e8117da55124a532a6234cc3d8703ceb30084a311c5e28bb6ba516_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] OMP: One-step Meanflow Policy with Directional Alignment"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19347",children:"https://arxiv.org/pdf/2512.19347"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bebf7d7dda43e94f094030d3f33a13922da836a13a10a2f445b742b579a24684_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bebf7d7dda43e94f094030d3f33a13922da836a13a10a2f445b742b579a24684_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," OMP: One-step Meanflow Policy with Directional Alignment"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19390",children:"https://arxiv.org/pdf/2512.19390"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Philipp L. Kinon, Simon R. Eugster, Peter Betsch"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19408",children:"https://arxiv.org/pdf/2512.19408"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2011f29ba413592cebb090b3281d18ea6b552fbd91b93ffc64d7c6a5c901ca5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2011f29ba413592cebb090b3281d18ea6b552fbd91b93ffc64d7c6a5c901ca5_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19402",children:"https://arxiv.org/pdf/2512.19402"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Sign Language Recognition using Parallel Bidirectional Reservoir Computing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19451",children:"https://arxiv.org/pdf/2512.19451"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Sign Language Recognition using Parallel Bidirectional Reservoir Computing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhenglong Guo, Yiming Zhao, Feng Jiang, Heng Jin, Zongbao Feng, Jianbin Zhou, Siyuan Xu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19453",children:"https://arxiv.org/pdf/2512.19453"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf7556ca61a6911a607e95a1a25dcd238b160d37af7403e1dcec7e54c013afa6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf7556ca61a6911a607e95a1a25dcd238b160d37af7403e1dcec7e54c013afa6_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19562",children:"https://arxiv.org/pdf/2512.19562"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yanliang Huang, Xia Yan, Peiran Yin, Zhenduo Zhang, Zeyan Shao, Youran Wang, Haoliang Huang, Matthias Althoff"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19564",children:"https://arxiv.org/pdf/2512.19564"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19576",children:"https://arxiv.org/pdf/2512.19576"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Carlos P\xe9rez-Ruiz, Joan Sol\xe0"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19567",children:"https://arxiv.org/pdf/2512.19567"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca49f17edae17bf6ccc3e14c30cbfe1362d605146c63dbbff4a97cc1a226407_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca49f17edae17bf6ccc3e14c30cbfe1362d605146c63dbbff4a97cc1a226407_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19583",children:"https://arxiv.org/pdf/2512.19583"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19629",children:"https://arxiv.org/pdf/2512.19629"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Zero-shot Reconstruction of In-Scene Object Manipulation from Video"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19684",children:"https://arxiv.org/pdf/2512.19684"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Zero-shot Reconstruction of In-Scene Object Manipulation from Video"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sun-Hyun Youn"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18320",children:"https://arxiv.org/pdf/2512.18320"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb9a9476c629b61f7574cea9ece28357bfe67bff74e75f1e094face2582eef9c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb9a9476c629b61f7574cea9ece28357bfe67bff74e75f1e094face2582eef9c_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251223] PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Devi Yuliarti, Ravi Prakash, Hiu Ching Cheung, Amy Strong, Patrick J. Codd, Shan Lin"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19010",children:"https://arxiv.org/pdf/2512.19010"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a46f8c0e00e5fc7cc3fa225c9fc8a67e630549137470ebd50de3e6bd418c8a5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a46f8c0e00e5fc7cc3fa225c9fc8a67e630549137470ebd50de3e6bd418c8a5b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-24",children:"2025-12-24"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Andrew Stirling, Mykola Lukashchuk, Dmitry Bagaev, Wouter Kouw, James R. Forbes"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19855",children:"https://arxiv.org/pdf/2512.19855"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9016f998c96e245ed8aaa845a90618946ec0f9528b50b0585f4ee2e13dca70ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9016f998c96e245ed8aaa845a90618946ec0f9528b50b0585f4ee2e13dca70ac_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] A Class of Axis-Angle Attitude Control Laws for Rotational Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Francisco M. F. R. Gon\xe7alves, Ryan M. Bena, N\xe9stor O. P\xe9rez-Arancibia"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19846",children:"https://arxiv.org/pdf/2512.19846"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/052f19775c0d61e53c6124e8478d4e7ee400cc921f19dbc0a3ec62a387c79f86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/052f19775c0d61e53c6124e8478d4e7ee400cc921f19dbc0a3ec62a387c79f86_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Class of Axis-Angle Attitude Control Laws for Rotational Systems"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sujan Warnakulasooriya, Andreas Willig, Xiaobing Wu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19914",children:"https://arxiv.org/pdf/2512.19914"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sangoh Lee, Sangwoo Mo, Wook-Shin Han"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20014",children:"https://arxiv.org/pdf/2512.20014"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Learning Skills from Action-Free Videos"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20052",children:"https://arxiv.org/pdf/2512.20052"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learning Skills from Action-Free Videos"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Wenzhao Wu, Yahui Tang, Mingfei Cheng, Wenbing Tang, Yuan Zhou, Yang Liu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20083",children:"https://arxiv.org/pdf/2512.20083"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2f6a93b0ca9601c38b73ad3e8c062333303a8f9538415f2546b2a923116462a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2f6a93b0ca9601c38b73ad3e8c062333303a8f9538415f2546b2a923116462a_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20148",children:"https://arxiv.org/pdf/2512.20148"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] LoLA: Long Horizon Latent Action Learning for General Robot Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xiaofan Wang, Xingyu Gao, Jianlong Fu, Zuolei Li, Dean Fortier, Galen Mullins, Andrey Kolobov, Baining Guo"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20166",children:"https://arxiv.org/pdf/2512.20166"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cca5d5cc494ac98e7e5083e1b8aeea89e51f1de0242a845197c5e3b3df56b17_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cca5d5cc494ac98e7e5083e1b8aeea89e51f1de0242a845197c5e3b3df56b17_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LoLA: Long Horizon Latent Action Learning for General Robot Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20188",children:"https://arxiv.org/pdf/2512.20188"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Qijun Qin, Ziqi Zhang, Yihan Zhong, Feng Huang, Xikun Liu, Runzhi Hu, Hang Chen, Wei Hu, Dongzhe Su, Jun Zhang, Hoi-Fung Ng, Weisong Wen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20224",children:"https://arxiv.org/pdf/2512.20224"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eafa66d44e2633f1e1623b3bfe8d87a949ce8da88e1e0430473ab41769747405_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eafa66d44e2633f1e1623b3bfe8d87a949ce8da88e1e0430473ab41769747405_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Imtiaz Ur Rehman, Moussa Labbadi, Amine Abadi, Lew Lew Yan Voon"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20229",children:"https://arxiv.org/pdf/2512.20229"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcb428fb736e259aa4637b3927a792e0cf3fb0c6c2182598360d56c1081362b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fcb428fb736e259aa4637b3927a792e0cf3fb0c6c2182598360d56c1081362b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20276",children:"https://arxiv.org/pdf/2512.20276"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20299",children:"https://arxiv.org/pdf/2512.20299"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Pneumatic bladder links with wide range of motion joints for articulated inflatable robots"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Katsu Uchiyama, Ryuma Niiyama"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20322",children:"https://arxiv.org/pdf/2512.20322"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/358991788a6fb301b2e9eea033e19d4fd9eecadc34cda99479fccc3d85fd1601_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/358991788a6fb301b2e9eea033e19d4fd9eecadc34cda99479fccc3d85fd1601_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Pneumatic bladder links with wide range of motion joints for articulated inflatable robots"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hao Wei, Peiji Wang, Qianhao Wang, Tong Qin, Fei Gao, Yulin Si"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20355",children:"https://arxiv.org/pdf/2512.20355"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b1319141213b2b28f550b4d7cf9685c47d5f6e12e3ee5a934bec5a817f95189_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b1319141213b2b28f550b4d7cf9685c47d5f6e12e3ee5a934bec5a817f95189_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Maulana Bisyir Azhari, Donghun Han, Je In You, Sungjun Park, David Hyunchul Shim"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20475",children:"https://arxiv.org/pdf/2512.20475"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47bd82e4c97fdc34a8d98cf68ba2bc04d4ee3480b229430db92dd47b53b90e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47bd82e4c97fdc34a8d98cf68ba2bc04d4ee3480b229430db92dd47b53b90e1_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20563",children:"https://arxiv.org/pdf/2512.20563"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Changyi Lin, Boda Huo, Mingyang Yu, Emily Ruppel, Bingqing Chen, Jonathan Francis, Ding Zhao"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20591",children:"https://arxiv.org/pdf/2512.20591"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e6d8b8d3a87a06671a80e30688f46eb142134f0545d66682f305f4219edec9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e6d8b8d3a87a06671a80e30688f46eb142134f0545d66682f305f4219edec9_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chanchan Xu, Shuai Dong, Xiaojie Wang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20342",children:"https://arxiv.org/pdf/2512.20342"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373ffac6f0235699aed45ecfc76ef062a0f91e5868ec23a1feee94ffde2e1eed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373ffac6f0235699aed45ecfc76ef062a0f91e5868ec23a1feee94ffde2e1eed_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251224] Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Georg Schildbach"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20391",children:"https://arxiv.org/pdf/2512.20391"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1dffba56daf9a83e73971cbad3df8a558f4f8bd39358567cc9675935db77fa4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1dffba56daf9a83e73971cbad3df8a558f4f8bd39358567cc9675935db77fa4_w640_q70.webp"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-25",children:"2025-12-25"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [robotics], [anytime metaheuristic, minimum latency problem, global route optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jan Mikula, Miroslav Kulich"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Czech Technical University in Prague"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20711",children:"https://arxiv.org/pdf/2512.20711"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Introduces Milaps, a model-based solution framework for Expected-Time Mobile Search (ETS) that integrates novel auxiliary objectives. 2. Adapts a recent anytime metaheuristic for the traveling deliveryman problem to optimize global routes under tight runtime constraints. 3. Presents evaluations on a novel large-scale dataset demonstrating superior trade-offs between solution quality and runtime compared to state-of-the-art baselines."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c34e933b17864fec081613deab6bcbb317d431663dc9f166a2bd0b677d770a42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c34e933b17864fec081613deab6bcbb317d431663dc9f166a2bd0b677d770a42_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the global route optimization problem for Expected-Time Mobile Search (ETS) in continuous environments. It proposes the Milaps framework, which uses minimum latency problems and an adapted anytime metaheuristic to efficiently plan search routes. The method shows better performance and runtime trade-offs than existing approaches on a new large-scale dataset."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Global route optimization for ETS is intractable due to continuous environment and constraints)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Introduces Milaps framework using minimum latency problems and an anytime metaheuristic)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Superior trade-offs between solution quality and runtime on a novel dataset)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Fixed-time control with prescribed performance for path following of underwater gliders"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [control systems], [fixed-time control, prescribed performance, sliding mode disturbance observer, underwater gliders, path following]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hanzhi Yang, Nina Mahmoudian"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Michigan Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20748",children:"https://arxiv.org/pdf/2512.20748"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. A novel fixed-time prescribed performance controller ensuring tracking errors converge within a fixed time independent of initial conditions. 2. A fixed-time sliding mode disturbance observer for exact finite-time estimation of model uncertainties and environmental disturbances. 3. An integrated control scheme with iLOS guidance enabling robust and accurate 3D path following for underwater gliders."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e021d16f533eb36c3285d4b072d1ffbb64723643d1f22973c4807f8c99eff3d2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e021d16f533eb36c3285d4b072d1ffbb64723643d1f22973c4807f8c99eff3d2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a fixed-time prescribed performance control scheme for 3D path following of underwater gliders to handle model uncertainties and environmental disturbances. The method integrates a finite-time performance function with a fixed-time sliding mode disturbance observer and an iLOS guidance law. Simulation results show it outperforms conventional methods in tracking accuracy, convergence speed, and control smoothness."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Fixed-time control with prescribed performance for path following of underwater gliders] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u5f3a\u6d0b\u6d41\u4e0e\u6e4d\u6d41\u4e0b\u7684\u6c34\u4e0b\u6ed1\u7fd4\u673a\u5bfc\u822a\u5b89\u5168/Navigation safety of underwater gliders in strong currents and turbulence]\nC --\x3e C1[\u56fa\u5b9a\u65f6\u95f4\u89c4\u5b9a\u6027\u80fd\u63a7\u5236/Fixed-time prescribed performance control]\nC --\x3e C2[\u56fa\u5b9a\u65f6\u95f4\u6ed1\u6a21\u6270\u52a8\u89c2\u6d4b\u5668/Fixed-time sliding mode disturbance observer]\nC --\x3e C3[\u96c6\u6210iLOS\u5236\u5bfc\u5f8b/Integrated iLOS guidance law]\nD --\x3e D1[\u8ddf\u8e2a\u8bef\u5dee\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u6536\u655b/Tracking errors converge within a fixed time]\nD --\x3e D2[\u4eff\u771f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5/Outperforms conventional methods in simulations]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [multi-agent reinforcement learning], [Dec-POMDP, belief inconsistency, limited communication, action consistency, multi-agent planning]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Moshe Rafaeli Shimron, Vadim Indelman"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Technion - Israel Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20778",children:"https://arxiv.org/pdf/2512.20778"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. A novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies among agents. 2. Provides probabilistic guarantees for both action consistency and performance relative to a fully-communicating baseline. 3. Introduces a mechanism to selectively trigger communication only when necessary and addresses the decision of whether to share data after action selection to improve inference."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4538ed8602d7a249e385e29bfc0423ed3f6682a9ac338c82e10822f10bd96df8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4538ed8602d7a249e385e29bfc0423ed3f6682a9ac338c82e10822f10bd96df8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of multi-agent decision-making under uncertainty when agents have inconsistent beliefs due to limited communication. It proposes a new decentralized framework for Dec-POMDPs that provides performance and action consistency guarantees while minimizing communication. Simulation results demonstrate that the approach outperforms existing state-of-the-art algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Belief Inconsistency & Limited Communication)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Novel Decentralized Framework with Guarantees)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA Algorithms)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [cv], [robotic vision], [Extended Kalman Filter, receding-horizon planner, vision-based interception]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tanmay P. Patel, Erica L. Tevere, Erik H. Kramer, Rudranarayan M. Mukherjee"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Jet Propulsion Laboratory, California Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20769",children:"https://arxiv.org/pdf/2512.20769"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. A streamlined, general-purpose framework for autonomous interception adaptable to robots with varying dynamics (UAV, rover, spacecraft). 2. A comprehensive study of the interception problem under conditions of limited observability (partial FOV, dropouts, occlusions) and no global localization. 3. Integration of a relative pose estimator (EKF), a history-conditioned motion predictor, and a real-time receding-horizon convex planner for robust performance."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28efc9a9f673cd478e9eafde765d4b0dcce3f7e02da154907e1ffc26368fd6d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28efc9a9f673cd478e9eafde765d4b0dcce3f7e02da154907e1ffc26368fd6d0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a general framework for robots to autonomously intercept moving targets using only a monocular camera, without needing global positioning. The method combines relative pose estimation, target motion prediction, and real-time path planning. Experiments on aerial, ground, and spacecraft platforms show the approach is robust, generalizable, and runs efficiently on embedded hardware."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u5728\u6709\u9650\u89c2\u6d4b\u4e0e\u65e0\u5168\u5c40\u5b9a\u4f4d\u4e0b\u62e6\u622a\u975e\u5408\u4f5c\u52a8\u6001\u76ee\u6807/Intercept non-cooperative dynamic targets under limited observability & no global localization)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u89c6\u89c9\u6846\u67b6\u96c6\u6210EKF\u3001\u8fd0\u52a8\u9884\u6d4b\u5668\u4e0e\u6eda\u52a8\u65f6\u57df\u89c4\u5212\u5668/Vision framework integrates EKF, motion predictor & receding-horizon planner)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u3001\u901a\u7528\u6027\u4e0e\u8ba1\u7b97\u6548\u7387/Validated robustness, generalizability & computational efficiency across diverse robotic platforms)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [cv], [human-robot interaction], [motion capture, dataset, handover, weight adaptation, YCB dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Parag Khanna, Karen Jane Dsouza, Chunyu Wang, M\xe5rten Bj\xf6rkman, Christian Smith"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20847",children:"https://arxiv.org/pdf/2512.20847"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Introduces the novel YCB-Handovers dataset, capturing 2771 human-human handover motions with varied object weights., 2. Provides an analysis of the impact of object weight on human reaching motion during handovers., 3. Bridges a gap in human-robot collaboration research by enabling data-driven, human-inspired models for weight-sensitive robotic motion planning."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af98ffe543cfd02847143e303696cbfb3dbc3991e06d43e1e59dda76a0ca8a49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af98ffe543cfd02847143e303696cbfb3dbc3991e06d43e1e59dda76a0ca8a49_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces the YCB-Handovers dataset, a motion capture dataset of human-human handovers with objects of varying weights. The dataset is built upon the YCB object set and aims to provide insights for developing intuitive, weight-adaptive robotic handover motions. The analysis shows that object weight significantly impacts human reaching motion, which can inform more natural and safe robotic handover behaviors."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[YCB-Handovers Dataset] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Lack of data on weight impact in handovers for HRI);\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Capture human-human handover motions with varied weights);\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Dataset & analysis for weight-adaptive robotic motion);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Early warning signals for loss of control"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [control theory, resilience engineering], [critical slowing down, dynamical indicators, loss of control, early warning signals, feedback systems]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jasper J. van Beers, Marten Scheffer, Prashant Solanki, Ingrid A. van de Leemput, Egbert H. van Nes, Coen C. de Visser"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Delft University of Technology, Wageningen University & Research"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20868",children:"https://arxiv.org/pdf/2512.20868"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"}),' 1. Proposes a model-free, holistic safety monitor for feedback systems based on dynamical indicators of resilience. 2. Demonstrates the application of the generic phenomenon of "critical slowing down" to engineered systems for predicting instability. 3. Validates the approach using drones and suggests broad applicability to reactors, aircraft, and self-driving cars.']}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/768d446a196a0959349c5e0870e0edad71ddcadf445696f4881d31eeed1d3098_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/768d446a196a0959349c5e0870e0edad71ddcadf445696f4881d31eeed1d3098_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper addresses the problem of detecting impending instability (loss of control) in feedback systems, which traditional model-based methods may miss when the system is damaged. It proposes a model-free monitoring method based on the generic phenomenon of "critical slowing down," using it as an early warning signal. The core conclusion is that these dynamical indicators can provide real-time warnings and guide resilient design for a wide class of controlled systems.']}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Early warning signals for loss of control] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Model-based control fails under system damage]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Model-free monitor using critical slowing down]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Early warning signals validated, applicable to diverse systems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [robot learning], [vision language model, proprioception, task captioning, subtask segmentation, imitation learning]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kanata Suzuki, Shota Shimizu, Tetsuya Ogata"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Not specified in provided content."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20876",children:"https://arxiv.org/pdf/2512.20876"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"}),' 1. Proposes a method to enhance Vision Language Models (VLMs) by incorporating low-level robot motion data (proprioception) for video understanding. 2. Introduces a two-stage captioning approach that generates individual "scene" captions and then summarizes them into a full task caption. 3. Performs subtask segmentation by comparing text embeddings of image captions, leveraging the enhanced understanding from motion data.']}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0102ecac047d1a1a9b2e14bec98cdce3bc34442aa52215ec414dfed2159bebcd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0102ecac047d1a1a9b2e14bec98cdce3bc34442aa52215ec414dfed2159bebcd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of enabling Vision Language Models (VLMs) to understand robot motion by incorporating proprioceptive data (joint and end-effector states). The proposed method uses this data to generate detailed task captions and perform subtask segmentation from robot task videos. Simulator experiments validate that providing motion information enhances the VLM's performance in these tasks, which can improve robot imitation learning."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Proprioception Enhances VLM in Robot Tasks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: VLMs lack understanding of robot motion from offline data)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Input joint/end-effector states to VLM for caption generation & segmentation)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Enhanced captioning and segmentation performance validated in simulation)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [soft robotics, tactile sensing], [stretchable optical sensor, continuous spectral-filtering, parallel mechanism trajectory tracking, high spatial resolution, force-position decoupling]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yiding Nie, Dongliang Fan, Jiatai Huang, Chunyu Liu, Jian S. Dai"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Southern University of Science and Technology (Shenzhen, China)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20888",children:"https://arxiv.org/pdf/2512.20888"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposed a novel stretchable tactile sensor based on a continuous spectral-filtering principle, enabling super-high spatial and force resolution. 2. Demonstrated the sensor's high linearity and robustness under stretching, bending, piercing, and cutting, with design scalability. 3. Integrated the sensor into a planar parallel mechanism for real-time, high-precision trajectory tracking with a rotational resolution of 0.02\xb0."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24531da8de35dd40b0bb2f80d03204b5835c32af6ae9054191d3d61ee21d03e7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24531da8de35dd40b0bb2f80d03204b5835c32af6ae9054191d3d61ee21d03e7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of achieving high spatial resolution and decoupling in stretchable tactile sensors. It proposes a new sensor based on a continuous spectral-filtering principle, which achieves high linearity and resolution even under deformation. The sensor's performance is validated by its successful integration into a parallel mechanism for precise real-time trajectory tracking."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0e\u81ea\u89e3\u8026\u80fd\u529b/High Spatial Resolution & Self-Decoupling]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u8fde\u7eed\u5149\u8c31\u6ee4\u6ce2\u539f\u7406/Continuous Spectral-Filtering Principle]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u9ad8\u7ebf\u6027\u5ea6\u4e0e\u5206\u8fa8\u7387\uff0c\u5b9e\u65f6\u8f68\u8ff9\u8ddf\u8e2a/High Linearity & Resolution, Real-time Trajectory Tracking]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [sensor fusion and navigation], [GNSS, frame alignment, convex relaxation, Lagrangian duality, certifiable optimality]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Baoshan Song, Matthew Giamou, Penggao Yan, Chunxi Xia, Li-Ta Hsu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"}),' The Hong Kong Polytechnic University (inferred from author "Li-Ta Hsu, Senior member, IEEE" and common affiliation patterns; other authors\' affiliations not explicitly stated in provided text)']}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20931",children:"https://arxiv.org/pdf/2512.20931"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://github.com/Baoshan-Song/Certifiable-Doppler-alignment",children:"https://github.com/Baoshan-Song/Certifiable-Doppler-alignment"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Formulates the GNSS-local frame alignment as a nonconvex QCQP and develops a convex relaxation via Lagrangian duality to achieve global optimality. 2. Performs relaxation tightness and observability analysis to derive criteria for certifiably optimal solutions. 3. Demonstrates robust performance with minimal satellite data (e.g., 2 satellites with Doppler measurements) where prior methods fail."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c8f1d855f3e36da72d29a8f11fc4cb03d3b1deecf237451917efbbef693d9b58_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c8f1d855f3e36da72d29a8f11fc4cb03d3b1deecf237451917efbbef693d9b58_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of aligning a local sensor frame with a global GNSS frame, which is prone to local minima. The authors propose a certifiable, globally optimal solver by transforming raw GNSS measurements into a convexly relaxed Lagrangian dual problem. Experiments show the method reliably finds optimal solutions even in GNSS-degraded environments with few satellites, outperforming existing techniques."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Certifiable Alignment of GNSS and Local Frames<br/>GNSS\u4e0e\u5c40\u90e8\u5750\u6807\u7cfb\u7684\u53ef\u8ba4\u8bc1\u5bf9\u9f50] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Frame alignment suffers from local minima & satellite dependency<br/>\u5750\u6807\u7cfb\u5bf9\u9f50\u5b58\u5728\u5c40\u90e8\u6700\u4f18\u548c\u536b\u661f\u4f9d\u8d56\u95ee\u9898)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Convex relaxation via Lagrangian duality for certifiable global optimum<br/>\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u8fdb\u884c\u51f8\u677e\u5f1b\u4ee5\u83b7\u5f97\u53ef\u8ba4\u8bc1\u7684\u5168\u5c40\u6700\u4f18\u89e3)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Works with only 2 satellites, outperforms VOBA & GVINS<br/>\u4ec5\u97002\u9897\u536b\u661f\u5373\u53ef\u5de5\u4f5c\uff0c\u6027\u80fd\u4f18\u4e8eVOBA\u548cGVINS)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [vision-language navigation], [topological map, reinforcement fine-tuning, GRPO, VLN-CE, Gemini API]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shuhao Ye, Sitong Mao, Yuxiang Cui, Xuan Yu, Shichao Zhai, Wen Chen, Shunbo Zhou, Rong Xiong, Yue Wang"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Huawei Technologies Co., Ltd, Zhejiang Humanoid Robot Innovation Center"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20940",children:"https://arxiv.org/pdf/2512.20940"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://github.com/Cepillar/ETP-R1",children:"https://github.com/Cepillar/ETP-R1"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Constructed a large-scale, high-quality pretraining dataset for topological trajectories using the Gemini API. 2. Introduced a three-stage training paradigm that unifies data from R2R and RxR tasks for joint pretraining. 3. First application of closed-loop, online reinforcement fine-tuning (RFT) to a graph-based VLN-CE model using the GRPO algorithm."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8029b317714b2a8a459b881eeaba1bf8cc335350a443fb795dfc190d0ca8b4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8029b317714b2a8a459b881eeaba1bf8cc335350a443fb795dfc190d0ca8b4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes ETP-R1, a framework that enhances graph-based Vision-Language Navigation in Continuous Environments (VLN-CE) by scaling up data and applying reinforcement fine-tuning. It builds a large pretraining dataset with Gemini, uses a three-stage training process, and applies closed-loop RFT with GRPO. The method achieves state-of-the-art results on R2R-CE and RxR-CE benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[ETP-R1] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Graph-based VLN-CE methods lag behind LVLM methods in data scaling and training paradigms.]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Scale data via Gemini API, joint pretraining, and apply closed-loop RFT with GRPO.]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA on R2R-CE and RxR-CE benchmarks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [human-robot interaction], [racial bias, occupational stereotypes, stereotype priming, skin tone discrimination, anthropomorphism]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jiangen He, Wanqi Zhang, Jessica Barfield"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Tennessee, University of Kentucky"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20951",children:"https://arxiv.org/pdf/2512.20951"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Demonstrated that human occupational biases and skin-tone-based discrimination directly transfer to robot selection decisions. 2. Revealed distinct, context-dependent patterns of robot preference, with lighter-skinned agents favored in healthcare/education and darker-toned agents in construction/athletics. 3. Showed that exposure to human professionals of specific races can prime and systematically shift subsequent robot preferences in stereotype-consistent directions."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e05e8cac9fadcb40ef8a6f45c93ec8405326df2cb55fbb423081a31a9baebd6a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e05e8cac9fadcb40ef8a6f45c93ec8405326df2cb55fbb423081a31a9baebd6a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates how societal biases influence human decisions when selecting robots for professional roles. Through two experiments with over 1000 participants, the study found that preferences for robots with different skin tones vary by occupational context and can be primed by exposure to human racial stereotypes. The main conclusion is that robotic deployment risks perpetuating existing social inequalities by inheriting human biases from human-human evaluation contexts."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[From Human Bias to Robot Choice<br>\u4ece\u4eba\u7c7b\u504f\u89c1\u5230\u673a\u5668\u4eba\u9009\u62e9] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>How societal biases influence robot selection<br>\u793e\u4f1a\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u673a\u5668\u4eba\u9009\u62e9];\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two experiments (N=1038) across four occupational contexts<br>\u4e24\u4e2a\u5b9e\u9a8c\uff0c\u6db5\u76d6\u56db\u79cd\u804c\u4e1a\u573a\u666f];\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Bias transfer & context-dependent preferences<br>\u504f\u89c1\u8f6c\u79fb\u4e0e\u60c5\u5883\u4f9d\u8d56\u7684\u504f\u597d];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [Bayesian Reinforcement Learning, Meta-Reinforcement Learning, Generalised Linear Models, Learnable Basis Functions, Variational Inference]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jingyang You, Hanna Kurniawati"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Australian National University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20974",children:"https://arxiv.org/pdf/2512.20974"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[GLiBRL] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use GLMs with learnable basis for tractable likelihood & inference]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [robotic sensing and manipulation], [multimodal sensing, tactile imaging, force-torque sensor, robotic palpation, subsurface feature detection]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tian-Ao Ren, Jorge Garcia, Seongheon Hong, Jared Grinberg, Hojung Choi, Julia Di, Hao Li, Dmitry Grinberg, Mark R. Cutkosky"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Stanford University, Symbiokinetics Inc"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20992",children:"https://arxiv.org/pdf/2512.20992"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Introduces PhysioVisionFT (PVFT), a compact multimodal sensor integrating a high-resolution vision-based tactile dome with a 6-axis force-torque sensor for robust physiotherapy-scale interactions. 2. Demonstrates through experiments on silicone phantoms that tactile images reveal clear structural details of subsurface features where force signals alone are ambiguous. 3. Shows that combining tactile and force modalities enables robust subsurface feature detection while maintaining safe, controlled robotic palpation."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f076e443ea0d5bb35330eb055b3d2af44e063a02a9d227e1f108a9bf46d7b4a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f076e443ea0d5bb35330eb055b3d2af44e063a02a9d227e1f108a9bf46d7b4a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unreliable subsurface feature detection in robotic palpation using force sensing alone. The authors propose a compact multimodal sensor, PhysioVisionFT, which combines vision-based tactile imaging with a 6-axis force-torque sensor. Preliminary results show this combined approach enables robust detection of subtle subsurface features while maintaining safe force control for physiotherapy applications."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Force sensing is ambiguous for subsurface features in soft tissue)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Integrate vision-based tactile imaging with force-torque sensor)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Tactile images reveal clear structural details; multimodal sensing enables robust detection and control)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [tactile sensing, model-based reinforcement learning, energy abstraction, grasping force control, slip detection]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Cheng-Yu Kuo, Hirofumi Shin, Takamitsu Matsubara"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nara Institute of Science and Technology, Honda R&D Co., Ltd."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21043",children:"https://arxiv.org/pdf/2512.21043"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposed a novel physics-informed energy abstraction that models an object as a virtual energy container, using the inconsistency between applied power and retained energy as a physically grounded signal for slip-aware stability inference. 2. Developed a model-based learning and planning framework that efficiently models energy dynamics from tactile sensing and performs real-time grasping force optimization using probabilistic Model Predictive Control (pMPC). 3. Demonstrated that the method can learn effective grasping force control from scratch within minutes, reducing slippage and extending grasp duration across diverse scenarios without external sensing or prior object knowledge, validated in both simulation and hardware."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e3063d1f417e52ff4adde8d018be97bb4b3da39e8fbdcf6f72fad61691585a2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e3063d1f417e52ff4adde8d018be97bb4b3da39e8fbdcf6f72fad61691585a2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of preventing object slippage during robotic manipulation by proposing a tactile-based, learning-driven force control method. The core idea is to abstract the object as an energy container and use a model-based reinforcement learning framework to learn energy-flow dynamics from touch, enabling real-time force optimization. Experiments show the method can learn quickly from scratch and effectively reduce slippage without relying on external vision or prior object models."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[\u8bba\u6587\u6807\u9898/Paper Title<br>Tracing Energy Flow] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u52a8\u6001\u4ea4\u4e92\u4e2d\u9632\u6ed1\u6293\u53d6/Dynamic Grasping with Slippage]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u89e6\u89c9\u80fd\u91cf\u62bd\u8c61\u4e0e\u6a21\u578b\u5b66\u4e60/Tactile Energy Abstraction & Model Learning]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>\u5feb\u901f\u5b66\u4e60\u4e0e\u6709\u6548\u9632\u6ed1/Fast Learning & Effective Anti-Slip]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [cv], [robotic grasping], [language-guided grasping, cross-modal fusion, coarse-to-fine learning, CLIP embeddings, dynamic convolution]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Technical University of Munich"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21065",children:"https://arxiv.org/pdf/2512.21065"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a hierarchical cross-modal fusion pipeline that progressively injects linguistic cues into visual feature reconstruction for fine-grained visual-semantic alignment., 2. Introduces a language-conditioned dynamic convolution head (LDCH) that adaptively mixes convolution experts based on sentence-level features for instruction-adaptive predictions., 3. Presents a final refinement module to enhance grasp consistency and robustness in complex scenes, validated on real robotic platforms."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes LGGD, a language-guided grasp detection method using a coarse-to-fine learning paradigm with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head. It achieves superior performance on benchmark datasets and demonstrates effective real-world robotic manipulation."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Weak alignment between language instructions and visual grasp reasoning in cluttered scenes.]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Coarse-to-fine learning with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head (LDCH).]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms existing methods, shows strong generalization, and is effective on a real robot.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [aerial manipulation, underactuated system, proximal policy optimization (PPO), incremental nonlinear dynamic inversion (INDI), sim-to-real transfer]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shlok Deshmukh, Javier Alonso-Mora, Sihao Sun"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Delft University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21085",children:"https://arxiv.org/pdf/2512.21085"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a reinforcement learning-based whole-body control strategy for a lightweight, underactuated aerial manipulator (DSAM) to achieve full 6-DoF end-effector pose control. 2. Employs a hybrid control architecture where a PPO agent generates high-level feedforward commands, tracked by an INDI attitude controller and a PID joint controller for robustness. 3. Demonstrates robust real-world performance with centimeter/degree-level accuracy under external disturbances like heavy loads and pushing, bridging the sim-to-real gap."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the control challenge of a lightweight, underactuated aerial manipulator. It uses reinforcement learning (PPO) to train a controller in simulation, which generates commands for a hybrid low-level controller (INDI+PID). Flight experiments show the method achieves precise and robust end-effector pose control, enabling contact-rich tasks with a simple platform."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Lightweight, underactuated aerial manipulator control under disturbances)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: RL (PPO) for feedforward commands + INDI/PID low-level tracking)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Centimeter/degree-level accuracy, robust to loads/pushing)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Model Predictive Control, MuJoCo, Web of Affine Spaces, finite differencing, iLQG]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chen Liang, Daniel Rakita"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Yale University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21109",children:"https://arxiv.org/pdf/2512.21109"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://github.com/chen-dylan-liang/mujoco",children:"https://github.com/chen-dylan-liang/mujoco"})," wasp mpc"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Introduced Web of Affine Spaces (WASP) derivatives as a drop-in replacement for finite differencing in the MuJoCo MPC (MJPC) library to compute derivatives more efficiently and robustly. 2. Demonstrated that WASP integrates seamlessly across diverse robotic tasks, achieves up to 2x speedup over finite differencing, and outperforms stochastic sampling-based planners in MJPC. 3. Released an open-source implementation of MJPC with WASP derivatives fully integrated to support adoption and future research."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ada8447964fa0e64d90a6328db6ae0e8cb32dd7609195104d67ce039de9ad6b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ada8447964fa0e64d90a6328db6ae0e8cb32dd7609195104d67ce039de9ad6b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of using finite differencing for derivative calculations in MuJoCo-based Model Predictive Control (MPC). The authors propose using Web of Affine Spaces (WASP) derivatives within the MJPC library, which reuses prior derivative information to accelerate and stabilize computations. The method is shown to be robust, integrate seamlessly, and achieve up to a 2x speedup compared to finite differencing while also outperforming sampling-based planners."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\n    A[Robust and Efficient MuJoCo-based MPC via WASP Derivatives] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Finite Differencing in MJPC is slow and a bottleneck for real-time MPC]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use Web of Affine Spaces (WASP) derivatives as a drop-in replacement in MJPC]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Up to 2x speedup, robust performance, outperforms sampling planners]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [sparse graph learning, traffic scene representation, multi-agent trajectory generation, graph neural networks, efficient inference]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology, Nanyang Technological University, iFLYTEK Co., Ltd."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21133",children:"https://arxiv.org/pdf/2512.21133"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a topology-guided sparse graph construction method that uses lane graph structure instead of distance thresholds to create efficient and interpretable scene graphs. 2. Introduces a lightweight graph encoder for efficiently aggregating agent-map and agent-agent interactions, leading to compact scene representations. 3. Demonstrates superior scalability and efficiency, capable of generating trajectories for thousands of agents and lanes with low latency and GPU memory usage."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3548cf1020ee1cd5d02c5277e19e837d708b6ef2a3068cbeae46b37bcc824c7e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3548cf1020ee1cd5d02c5277e19e837d708b6ef2a3068cbeae46b37bcc824c7e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SparScene, a sparse graph learning framework for efficient traffic scene representation in multi-agent trajectory generation. It constructs sparse, topology-aware graphs between agents and lanes and uses a lightweight encoder, achieving competitive performance on the Waymo dataset with significantly improved inference speed and scalability for large scenes."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[SparScene] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5bc6\u96c6\u56fe\u6548\u7387\u4f4e\uff0c\u9650\u5236\u5927\u89c4\u6a21\u573a\u666f\u6269\u5c55/Dense graphs are inefficient, limiting scalability to large scenes]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u8f66\u9053\u62d3\u6251\u7684\u7a00\u758f\u56fe\u5b66\u4e60/Sparse graph learning guided by lane topology]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u9ad8\u63a8\u7406\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027/High inference efficiency and scalability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Flocking phase transition and threat responses in bio-inspired autonomous drone swarms"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [swarm robotics], [flocking algorithm, phase transition, collective behavior, drone swarm, bio-inspired]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Matthieu Verdoucq, Dari Trendafilov, Cl\xe9ment Sire, Ram\xf3n Escobedo, Guy Theraulaz, Gautier Hattenberger"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," \xc9cole Nationale de l'Aviation Civile (ENAC), Centre National de la Recherche Scientifique (CNRS), Universit\xe9 de Toulouse"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21196",children:"https://arxiv.org/pdf/2512.21196"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Developed a minimal, bio-inspired 3D flocking algorithm for drones based solely on local alignment and attraction cues. 2. Mapped a phase diagram by tuning interaction gains, identifying a critical transition region that maximizes collective responsiveness and reorganization capacity. 3. Demonstrated through outdoor experiments and simulations that operating near this critical transition enhances the swarm's resilience and ability to perform rapid collective maneuvers in response to threats."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbbec1c0bb405202136a9915b066a8dfdef5d5347b2c64b21f6dedcc0e2f35e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbbbec1c0bb405202136a9915b066a8dfdef5d5347b2c64b21f6dedcc0e2f35e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a bio-inspired flocking algorithm for drone swarms using minimal local interaction rules. By tuning the alignment and attraction gains, the authors identify a critical phase transition region where the swarm's responsiveness peaks. Outdoor experiments with ten drones show that operating near this transition enables rapid, resilient collective responses to external disturbances like intruders."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Flocking phase transition and threat responses in bio-inspired autonomous drone swarms] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How to design resilient and responsive autonomous drone swarms?);\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Bio-inspired 3D flocking algorithm with minimal local interactions);\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Phase diagram with critical transition region enhances swarm responsiveness and resilience);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Schr\xf6dinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [cv], [robot navigation], [zero-shot object navigation, trajectory-conditioned 3D imagination, occlusion-aware planning]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21201",children:"https://arxiv.org/pdf/2512.21201"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://heyu322.github.io/Schrodinger-Navigator.github.io/",children:"https://heyu322.github.io/Schrodinger-Navigator.github.io/"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposed Schr\xf6dinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schr\xf6dinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\n    A[Schr\xf6dinger's Navigator] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: ZSON struggles with occlusions & uncertainty]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Trajectory-conditioned 3D imagination of futures]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines on real robot]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [agent system], [runtime safety guardrail, executable safety logic, hybrid reasoning, temporal safety predicate, context-aware safety predicate]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21220",children:"https://arxiv.org/pdf/2512.21220"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\n    A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [other], [robotics], [Center of Pressure, PID control, load cell, ESP32-C3, wireless feedback]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Muhtadin, Faris Rafi Pramana, Dion Hayu Fandiantoro, Moh Ismarintan Zazuli, Atar Fuady Babgei"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institut Teknologi Sepuluh Nopember, Kumamoto University, Imperial College London"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21219",children:"https://arxiv.org/pdf/2512.21219"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Designed a wireless embedded balance system using ESP32-C3 and custom foot units with load cells to estimate Center of Pressure (CoP) in real-time, reducing wiring complexity. 2. Implemented a PID control strategy that uses CoP feedback to adjust torso, hip, and ankle roll joints for stability on uneven surfaces. 3. Demonstrated high sensor precision (14.8g avg error) and a 100% success rate in balance maintenance during single-leg lifting tasks on a 3-degree incline."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbae0a88bb642f47494a4135950d07297e352dc45ffe7383ebb0a32e065f48fe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbae0a88bb642f47494a4135950d07297e352dc45ffe7383ebb0a32e065f48fe_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a wireless balance control system for a humanoid robot using load cells and an ESP32-C3 to estimate the Center of Pressure (CoP). The CoP data is sent wirelessly to a main controller which uses a PID strategy to adjust joints for stability. The system achieved a 100% success rate in balance tests, validating wireless CoP feedback for enhancing robot stability without restricting movement."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Humanoid robot stability during single-support phase, especially on uneven surfaces]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Wireless CoP estimation using load cells & ESP32-C3, PID control for joint adjustment]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 14.8g sensor error, 100% balance success rate at 3-degree inclination]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [cv], [robot localization], [ArUco marker recognition, optical flow analysis, IMU data fusion, modular robot, relative localization]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shuhan Zhang, Tin Lun Lam"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21226",children:"https://arxiv.org/pdf/2512.21226"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Design and implementation of a relative localization system specifically for the SnailBot modular robot platform. 2. A unified sensor fusion framework integrating ArUco marker recognition, optical flow analysis, and IMU data processing. 3. A rule-based fusion strategy that ensures robust and accurate real-time positioning in dynamic scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a70ca38b5cef5867dcae52114cc8fc7247f94a7a882811060f4c9a4a7e811c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a70ca38b5cef5867dcae52114cc8fc7247f94a7a882811060f4c9a4a7e811c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the relative localization challenge for collaborative modular robots by proposing a system that fuses data from ArUco markers, optical flow, and an IMU. The rule-based fusion method prioritizes reliable sensor inputs to achieve accurate positioning. Experimental results validate the system's effectiveness and potential for scalable deployment in modular robotic systems."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Relative Localization System Design for SnailBot] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u6a21\u5757\u5316\u673a\u5668\u4eba\u76f8\u5bf9\u5b9a\u4f4d/Relative Localization for Modular Robots)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u591a\u4f20\u611f\u5668\u878d\u5408\u6846\u67b6/Multi-sensor Fusion Framework)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5b9e\u65f6\u9c81\u68d2\u5b9a\u4f4d/Real-time Robust Localization)\nC --\x3e E(ArUco\u6807\u8bb0\u8bc6\u522b/ArUco Marker Recognition)\nC --\x3e F(\u5149\u6d41\u5206\u6790/Optical Flow Analysis)\nC --\x3e G(IMU\u6570\u636e\u5904\u7406/IMU Data Processing)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [robotic manipulation], [tactile sensing, representation learning, contrastive learning, skill transfer, dexterous manipulation]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Peking University, BeingBeyond"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21233",children:"https://arxiv.org/pdf/2512.21233"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://beingbeyond.github.io/UniTacHand/",children:"https://beingbeyond.github.io/UniTacHand/"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a unified spatio-tactile representation (UniTacHand) that projects tactile signals from human hands (via gloves) and robotic hands onto a common 2D surface space using the MANO model. 2. Introduces a contrastive learning method to align human and robotic tactile data into a unified latent space, requiring only 10 minutes of paired data. 3. Enables zero-shot tactile-based policy transfer from human demonstrations to a real robot, improving performance and data efficiency compared to using only robotic data."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22dcfd29747b902c665565a7fffc2c6af468e1f6351afb8f649a1b003f734166_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22dcfd29747b902c665565a7fffc2c6af468e1f6351afb8f649a1b003f734166_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of transferring dexterous manipulation skills from humans to robots using tactile sensing. It proposes UniTacHand, a method that unifies human and robotic tactile data onto a common spatial representation and aligns them via contrastive learning. This enables zero-shot policy transfer to robots and improves learning efficiency by leveraging low-cost human data."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\n    A[UniTacHand] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u89e6\u89c9\u6570\u636e\u96be\u4ee5\u5bf9\u9f50/Human-Robot Tactile Data Misalignment]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7edf\u4e00\u8868\u793a\u4e0e\u5bf9\u6bd4\u5b66\u4e60/Unified Representation & Contrastive Learning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u96f6\u6837\u672c\u6280\u80fd\u8fc1\u79fb\u4e0e\u9ad8\u6548\u5b66\u4e60/Zero-shot Transfer & Efficient Learning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] RoboCade: Gamifying Robot Data Collection"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [imitation learning], [gamification, robot teleoperation, demonstration dataset, co-training, user engagement]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Suvir Mirchandani, Mia Tang, Jiafei Duan, Jubayer Ibn Hamid, Michael Cho, Dorsa Sadigh"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Stanford University, University of Washington, FrodoBots"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21235",children:"https://arxiv.org/pdf/2512.21235"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," robocade.github.io"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Developed RoboCade, a gamified remote teleoperation platform for scalable robot demonstration data collection. 2. Proposed design principles for constructing gamified tasks that align with useful downstream robot manipulation tasks. 3. Demonstrated that data collected via the gamified platform improves robot policy success rates and is rated as more enjoyable by novice users."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b1541369e8ca95c4d5be308bf96fdc75adbb1d8b4932ac8966850ffa2c0eeff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b1541369e8ca95c4d5be308bf96fdc75adbb1d8b4932ac8966850ffa2c0eeff_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the scalability challenge of collecting robot demonstration data by introducing RoboCade, a gamified remote teleoperation platform. The platform incorporates game-like elements in its interface and task design to make data collection more accessible and engaging for general users. The results show that policies co-trained with this gamified data achieve higher success rates on target tasks, and users find the platform significantly more enjoyable than a standard non-gamified system."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[RoboCade: Gamifying Robot Data Collection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u6a21\u4eff\u5b66\u4e60\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u8fc7\u7a0b\u4e4f\u5473\u3001\u89c4\u6a21\u53d7\u9650]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5f00\u53d1\u6e38\u620f\u5316\u8fdc\u7a0b\u9065\u64cd\u4f5c\u5e73\u53f0\uff0c\u96c6\u6210\u89c6\u89c9\u53cd\u9988\u3001\u8fdb\u5ea6\u6761\u3001\u6392\u884c\u699c\u7b49\u5143\u7d20]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u4e0b\u6e38\u7b56\u7565\u6210\u529f\u7387(+16-56%)\uff0c\u7528\u6237\u6109\u60a6\u5ea6\u63d0\u5347(+24%)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [embodied ai], [scene graph, vision language model, dynamic planning, memory graph, graph augmentation]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," MIRAI, Cognitive AI Systems Lab"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21243",children:"https://arxiv.org/pdf/2512.21243"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"code:"})," ",(0,r.jsx)(e.a,{href:"https://lookplangraph.github.io/",children:"https://lookplangraph.github.io/"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[LookPlanGraph] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Static scene graphs fail in dynamic environments];\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Dynamic graph update via VLM observation];\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms static methods, new GraSIF dataset];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251225] Quadrupped-Legged Robot Movement Plan Generation using Large Language Model"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [agent system], [Large Language Model, quadruped robot, ROS navigation, sensor fusion, offloaded inference]"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Muhtadin, Vincentius Gusti Putu A. B. M., Ahmad Zaini, Mauridhi Hery Purnomo, I Ketut Eddy Purnama, Chastine Fatichah"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institut Teknologi Sepuluh Nopember (ITS)"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21293",children:"https://arxiv.org/pdf/2512.21293"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"contributions:"})," 1. A novel distributed control architecture that offloads LLM-based high-level planning to an external server to overcome the computational constraints of a lightweight quadruped robot platform. 2. A system that grounds natural language instructions into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, Odometry). 3. Experimental validation in a structured indoor environment demonstrating over 90% aggregate success rate, proving the feasibility of the offloaded LLM planning approach for real-world deployment."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"thumbnail:"})," ",(0,r.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70190e79023c7f7dad391e0e2059aa027ac578d23c266728acc6d3e205234b01_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70190e79023c7f7dad391e0e2059aa027ac578d23c266728acc6d3e205234b01_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a distributed control framework that uses a Large Language Model (LLM) to generate navigation plans for a quadruped robot from natural language commands. The computationally intensive LLM inference is offloaded to an external server, while the robot executes the plans locally using ROS and sensor data. Experiments in indoor environments showed the system is robust, achieving over 90% success rate and validating the approach for intuitive robot control."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(e.mermaid,{value:"graph LR\nA[Quadrupped-Legged Robot Movement Plan Generation using Large Language Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: High barrier to entry for robot control)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Offloaded LLM planning + ROS navigation with sensor fusion)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: >90% success rate in indoor navigation)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>d});var s=i(6540);const r={},a=s.createContext(r);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function d(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);