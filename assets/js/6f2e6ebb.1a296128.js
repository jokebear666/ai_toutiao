"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4423],{496:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_RO/20251215-20251221","title":"20251215-20251221 (cs.RO)","description":"2025-12-18","source":"@site/docs/daily/cs_RO/20251215-20251221.md","sourceDirName":"daily/cs_RO","slug":"/daily/cs_RO/20251215-20251221","permalink":"/ai_toutiao/daily/cs_RO/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766042497000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.RO","permalink":"/ai_toutiao/category/csro"},"next":{"title":"cs.SC","permalink":"/ai_toutiao/category/cssc"}}');var t=n(4848),r=n(8453);const a={},o="20251215-20251221 (cs.RO)",l={},c=[{value:"2025-12-18",id:"2025-12-18",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"20251215-20251221-csro",children:"20251215-20251221 (cs.RO)"})}),"\n",(0,t.jsx)(i.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [mixture-of-experts, reinforcement fine-tuning, semantic similarity reward, vision language model, socially compliant navigation]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Tomohito Kawabata, Xinyu Zhang, Ling Xiao"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," Hokkaido University"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.14757",children:"https://arxiv.org/pdf/2512.14757"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant robot navigation, which is fine-tuned using reinforcement learning with a novel semantic similarity reward. The method balances navigation accuracy and computational efficiency by exploring different small language models and vision encoders. Experiments show it outperforms baseline rewards and achieves a good trade-off for real-time deployment."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15047",children:"https://arxiv.org/pdf/2512.15047"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [particle filter, bird's-eye-view (BEV), feature matching, cross-view geo-localization, absolute trajectory error (ATE)]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Dongmyeong Lee, Jesse Quattrociocchi, Christian Ellis, Rwik Rana, Amanda Adkins, Adam Uccello, Garrett Warnell, Joydeep Biswas"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," The University of Texas at Austin, DEVCOM Army Research Laboratory"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15111",children:"https://arxiv.org/pdf/2512.15111"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces BEV-Patch-PF, a sequential geo-localization system that uses a particle filter to match learned bird's-eye-view features from onboard RGB-D images with patches from aerial feature maps. The method significantly outperforms retrieval-based baselines in off-road environments, achieving much lower trajectory error on both seen and unseen routes while running in real-time."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:'[arXiv251218] I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable'})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [ai], [human-computer interaction], [conversational AI, relational style, transparent style, anthropomorphism, emotional reliance, online experiment, adolescent psychology]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Pilyoung Kim, Yun Xie, Sujin Yang"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," University of Denver, Ewha Womans University"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15117",children:"https://arxiv.org/pdf/2512.15117"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper uses a preregistered online experiment with adolescent-parent dyads to compare how relational versus transparent conversational styles in AI chatbots affect adolescents' perceptions. It finds that a relational style increases anthropomorphism, trust, and emotional closeness, and is especially preferred by socially and emotionally vulnerable adolescents, highlighting a design consideration for youth AI safety."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [criticality metrics, relevance classification, object detection, safety evaluation, bidirectional criticality rating, multi-metric aggregation, DeepAccident dataset]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," J\xf6rg Gamerdinger, Sven Teufel, Stephan Amann, Oliver Bringmann"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15181",children:"https://arxiv.org/pdf/2512.15181"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper analyzes criticality metrics for evaluating the safety of object detection systems in automated driving. It proposes two novel strategies, bidirectional criticality rating and multi-metric aggregation, to improve classification accuracy. The approach demonstrates up to a 100% improvement in criticality classification accuracy, advancing the safety evaluation of automated vehicle perception systems."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [environmental perception safety metric, object detection safety, lane detection safety, joint safety assessment, criticality classification, DeepAccident dataset]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," J\xf6rg Gamerdinger, Sven Teufel, Stephan Amann, Lukas Marc Listl, Oliver Bringmann"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15195",children:"https://arxiv.org/pdf/2512.15195"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel Environmental Perception Safety Metric (EPSM) that jointly evaluates the safety of object and lane detection systems for autonomous driving, integrating a lightweight object safety metric and considering task interdependence. It demonstrates that this approach identifies safety-critical errors missed by conventional metrics like precision and recall. The findings emphasize the need for safety-centric evaluation methods in autonomous vehicle perception."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [3D Gaussian Splatting, progressive three-stage training, geometric safety correction, onboard deployment optimization]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Xin Zhou, Fei Gao"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," Zhejiang University, Differential Robotics"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15258",children:"https://arxiv.org/pdf/2512.15258"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes VLA-AN, an efficient onboard Vision-Language-Action framework for drone navigation. Its core method uses a high-fidelity dataset built with 3D Gaussian Splatting and a three-stage training pipeline, coupled with a lightweight safety-corrected action module. The conclusion is that the framework achieves robust real-time performance and high navigation success rates, providing a practical solution for autonomous aerial robots."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] Remotely Detectable Robot Policy Watermarking"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Michael Amir, Manon Flageat, Amanda Prorok"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," University of Cambridge"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15379",children:"https://arxiv.org/pdf/2512.15379"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot's motions using the policy's inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal training], [human-robot mutual imitation, kinematic rules, left/right hand coordinate systems, vision-language-action model, behavioral priors, cross-embodiment generalization]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, ingkuan Song, Lianli Gao, Heng Tao Shen"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," Tongji University, University of Electronic Science and Technology of China"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15411",children:"https://arxiv.org/pdf/2512.15411"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes MiVLA, a vision-language-action model pre-trained using human-robot mutual imitation, which aligns human and robot action spaces via kinematic rules and coordinate systems to integrate behavioral knowledge from human videos and simulated robot data. This approach enhances generalization across different camera views, appearances, and robot embodiments. Experiments on multiple robots show MiVLA outperforms state-of-the-art models in both simulation and real-world control tasks."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [ai], [computer vision], [monocular depth estimation, 3D hallucination, Laplacian-based evaluation, grounded self-distillation, depth foundation models]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Hoang Nguyen, Xiaohao Xu, Xiaonan Huang"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," University of Michigan, Ann Arbor"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15423",children:"https://arxiv.org/pdf/2512.15423"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"}),' This paper introduces a framework to address the "3D Mirage" problem in monocular depth estimation, where models hallucinate 3D structures from flat but ambiguous inputs like street art. The method includes a new benchmark (3D-Mirage), a Laplacian-based evaluation with metrics (DCS, CCS), and a parameter-efficient correction technique called Grounded Self-Distillation. The work concludes that depth model evaluation must shift from pixel accuracy to structural robustness to mitigate this safety-critical vulnerability.']}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.strong,{children:"[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs"})}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"authors:"})," Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"institution:"})," mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"link:"})," ",(0,t.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15692",children:"https://arxiv.org/pdf/2512.15692"})]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var s=n(6540);const t={},r=s.createContext(t);function a(e){const i=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);