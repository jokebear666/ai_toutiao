"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4423],{496:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/cs_RO/20251215-20251221","title":"20251215-20251221 (cs.RO)","description":"2025-12-18","source":"@site/docs/daily/cs_RO/20251215-20251221.md","sourceDirName":"daily/cs_RO","slug":"/daily/cs_RO/20251215-20251221","permalink":"/ai_toutiao/daily/cs_RO/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766642986000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.RO","permalink":"/ai_toutiao/daily/csro"},"next":{"title":"20251222-20251228 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20251222-20251228"}}');var s=i(4848),a=i(8453);const t={},o="20251215-20251221 (cs.RO)",l={},c=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251215-20251221-csro",children:"20251215-20251221 (cs.RO)"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [mixture-of-experts, reinforcement fine-tuning, semantic similarity reward, vision language model, socially compliant navigation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tomohito Kawabata, Xinyu Zhang, Ling Xiao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hokkaido University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14757",children:"https://arxiv.org/pdf/2512.14757"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant robot navigation, which is fine-tuned using reinforcement learning with a novel semantic similarity reward. The method balances navigation accuracy and computational efficiency by exploring different small language models and vision encoders. Experiments show it outperforms baseline rewards and achieves a good trade-off for real-time deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15047",children:"https://arxiv.org/pdf/2512.15047"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [particle filter, bird's-eye-view (BEV), feature matching, cross-view geo-localization, absolute trajectory error (ATE)]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dongmyeong Lee, Jesse Quattrociocchi, Christian Ellis, Rwik Rana, Amanda Adkins, Adam Uccello, Garrett Warnell, Joydeep Biswas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Texas at Austin, DEVCOM Army Research Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15111",children:"https://arxiv.org/pdf/2512.15111"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces BEV-Patch-PF, a sequential geo-localization system that uses a particle filter to match learned bird's-eye-view features from onboard RGB-D images with patches from aerial feature maps. The method significantly outperforms retrieval-based baselines in off-road environments, achieving much lower trajectory error on both seen and unseen routes while running in real-time."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'[arXiv251218] I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [human-computer interaction], [conversational AI, relational style, transparent style, anthropomorphism, emotional reliance, online experiment, adolescent psychology]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pilyoung Kim, Yun Xie, Sujin Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Denver, Ewha Womans University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15117",children:"https://arxiv.org/pdf/2512.15117"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper uses a preregistered online experiment with adolescent-parent dyads to compare how relational versus transparent conversational styles in AI chatbots affect adolescents' perceptions. It finds that a relational style increases anthropomorphism, trust, and emotional closeness, and is especially preferred by socially and emotionally vulnerable adolescents, highlighting a design consideration for youth AI safety."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [criticality metrics, relevance classification, object detection, safety evaluation, bidirectional criticality rating, multi-metric aggregation, DeepAccident dataset]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," J\xf6rg Gamerdinger, Sven Teufel, Stephan Amann, Oliver Bringmann"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15181",children:"https://arxiv.org/pdf/2512.15181"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes criticality metrics for evaluating the safety of object detection systems in automated driving. It proposes two novel strategies, bidirectional criticality rating and multi-metric aggregation, to improve classification accuracy. The approach demonstrates up to a 100% improvement in criticality classification accuracy, advancing the safety evaluation of automated vehicle perception systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [environmental perception safety metric, object detection safety, lane detection safety, joint safety assessment, criticality classification, DeepAccident dataset]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," J\xf6rg Gamerdinger, Sven Teufel, Stephan Amann, Lukas Marc Listl, Oliver Bringmann"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15195",children:"https://arxiv.org/pdf/2512.15195"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel Environmental Perception Safety Metric (EPSM) that jointly evaluates the safety of object and lane detection systems for autonomous driving, integrating a lightweight object safety metric and considering task interdependence. It demonstrates that this approach identifies safety-critical errors missed by conventional metrics like precision and recall. The findings emphasize the need for safety-centric evaluation methods in autonomous vehicle perception."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [3D Gaussian Splatting, progressive three-stage training, geometric safety correction, onboard deployment optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Xin Zhou, Fei Gao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Differential Robotics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15258",children:"https://arxiv.org/pdf/2512.15258"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes VLA-AN, an efficient onboard Vision-Language-Action framework for drone navigation. Its core method uses a high-fidelity dataset built with 3D Gaussian Splatting and a three-stage training pipeline, coupled with a lightweight safety-corrected action module. The conclusion is that the framework achieves robust real-time performance and high navigation success rates, providing a practical solution for autonomous aerial robots."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] Remotely Detectable Robot Policy Watermarking"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Michael Amir, Manon Flageat, Amanda Prorok"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Cambridge"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15379",children:"https://arxiv.org/pdf/2512.15379"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot's motions using the policy's inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [human-robot mutual imitation, kinematic rules, left/right hand coordinate systems, vision-language-action model, behavioral priors, cross-embodiment generalization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, ingkuan Song, Lianli Gao, Heng Tao Shen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tongji University, University of Electronic Science and Technology of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15411",children:"https://arxiv.org/pdf/2512.15411"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes MiVLA, a vision-language-action model pre-trained using human-robot mutual imitation, which aligns human and robot action spaces via kinematic rules and coordinate systems to integrate behavioral knowledge from human videos and simulated robot data. This approach enhances generalization across different camera views, appearances, and robot embodiments. Experiments on multiple robots show MiVLA outperforms state-of-the-art models in both simulation and real-world control tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [computer vision], [monocular depth estimation, 3D hallucination, Laplacian-based evaluation, grounded self-distillation, depth foundation models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hoang Nguyen, Xiaohao Xu, Xiaonan Huang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Michigan, Ann Arbor"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15423",children:"https://arxiv.org/pdf/2512.15423"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper introduces a framework to address the "3D Mirage" problem in monocular depth estimation, where models hallucinate 3D structures from flat but ambiguous inputs like street art. The method includes a new benchmark (3D-Mirage), a Laplacian-based evaluation with metrics (DCS, CCS), and a parameter-efficient correction technique called Grounded Self-Distillation. The work concludes that depth model evaluation must shift from pixel accuracy to structural robustness to mitigate this safety-critical vulnerability.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15692",children:"https://arxiv.org/pdf/2512.15692"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [embodied ai], [Asymmetric Assistive Reasoning, active querying, Pull-based protocol, Privileged Information Bias, AI2-THOR, Leader-Follower dyad]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shaun Baek, Sam Liu, Joseph Ukpong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Emory University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15776",children:"https://arxiv.org/pdf/2512.15776"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper proposes an Asymmetric Assistive Reasoning framework in AI2-THOR to study how a knowledgeable "Leader" agent can guide a sensor-limited "Follower," finding that standard "Push-based" instruction fails due to Privileged Information Bias. It demonstrates that a "Pull-based" active querying protocol, where the Follower requests clarifications, significantly improves collaborative success by reducing grounding errors.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Large Video Planner Enables Generalizable Robot Control"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [robotics, foundation models], [large-scale video pretraining, generative video planning, vision-language-action (VLA), zero-shot planning, internet-scale video dataset]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," MIT, UC Berkeley, Harvard"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15840",children:"https://arxiv.org/pdf/2512.15840"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an alternative paradigm for building robot foundation models by using large-scale video pretraining, rather than extending multimodal LLMs, to create a generative video planner. The model produces zero-shot video plans from novel instructions, which are then post-processed into executable robot actions. The approach demonstrates robust generalization and real-world feasibility in robot control tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [retrieval-augmented generation, 4D spatio-temporal reasoning, structured memory, vision-language models, embodied AI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15940",children:"https://arxiv.org/pdf/2512.15940"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces R4, a training-free framework that equips vision-language models with a structured, lifelong memory by anchoring object-level semantic descriptions in a 4D spatio-temporal database. This enables retrieval-augmented reasoning over space and time for embodied tasks like question answering and navigation. Experiments show that R4 substantially improves reasoning over spatio-temporal information compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [few-shot learning, in-context learning, ablation study, personalized examples]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qiping Zhang, Nathan Tsoi, Mofeed Nagib, Hao-Tien Lewis Chiang, Marynel V\xe1zquez"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Yale University, Google DeepMind"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16019",children:"https://arxiv.org/pdf/2512.16019"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using Large Language Models (LLMs) with few-shot in-context learning to predict human perceptions of robot performance in social navigation tasks. The method requires significantly less labeled data than traditional supervised learning models and achieves comparable or better performance. The study also finds that using personalized examples from the same user further improves prediction accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [flow matching, adversarial policy, closed-loop evaluation, end-to-end autonomous driving, real-world image generation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiaheng Geng, Jiatong Du, Xinyu Zhang, Ye Li, Panqu Wang, Yanjun Huang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tongji University, ZERON"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16055",children:"https://arxiv.org/pdf/2512.16055"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a closed-loop evaluation platform for end-to-end autonomous driving that uses a flow matching-based real-world image generator and an adversarial traffic policy to create safety-critical corner cases. The platform efficiently generates realistic driving scenes to test models like UniAD and VAD, demonstrating their performance degradation in adversarial scenarios. This method effectively identifies model weaknesses to improve the safety and robustness of autonomous driving systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Towards Closing the Domain Gap with Event Cameras"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [event cameras, domain gap, end-to-end driving, neural networks, illumination invariance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," M. Oltan Sevinc, Liao Wu, Francisco Cruz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of New South Wales, Universidad Central de Chile"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16178",children:"https://arxiv.org/pdf/2512.16178"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using event cameras instead of traditional frame-based cameras for end-to-end autonomous driving to address the domain gap caused by day-night lighting differences. The method involves training neural networks on event camera data and evaluating their performance across different lighting conditions. The main conclusion is that event cameras maintain more consistent performance across lighting domains, showing smaller performance degradation and superior baseline performance in cross-domain scenarios compared to grayscale cameras."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [reinforcement learning, reward engineering, vision-language models, environment perception, terrain sensor analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Enis Yalcin, Joshua O'Hara, Maria Stamatopoulou, Chengxu Zhou, Dimitrios Kanoulas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University College London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16446",children:"https://arxiv.org/pdf/2512.16446"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces E-SDS, a framework that integrates vision-language models with real-time terrain sensor analysis to automatically generate reward functions for training humanoid locomotion policies. The method uniquely enables robust navigation on complex terrains like stairs, significantly reducing velocity tracking error and manual engineering effort. The main conclusion is that this environment-aware approach produces more capable policies than manual or non-perceptive automated methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [4D scene graph, spatio-temporal tokenized patch encoding, HDBSCAN clustering, SAM2 segmentation, SLAM]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16461",children:"https://arxiv.org/pdf/2512.16461"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SNOW, a training-free framework that integrates Vision-Language Model semantics with 3D point cloud geometry and temporal consistency to build a unified 4D Scene Graph for open-world embodied reasoning. It achieves this through object-level proposals, Spatio-Temporal Tokenized Patch Encoding, and a SLAM backend for spatial grounding. Experiments show that SNOW sets new state-of-the-art performance, demonstrating the importance of structured 4D priors for autonomous robotics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Olaf: Bringing an Animated Character to Life in the Physical World"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [reinforcement learning, imitation rewards, thermal actuator modeling, spherical linkages, planar linkages, asymmetric leg design]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," David M\xfcller, Espen Knoop, Dario Mylonopoulos, Agon Serifi, Michael A. Hopkins, Ruben Grandia, Moritz B\xe4cher"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Disney Research Imagineering"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16705",children:"https://arxiv.org/pdf/2512.16705"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper brings the animated character Olaf to life as a physical robot using reinforcement learning guided by animation references for control. Key innovations include a compact mechanical design with hidden asymmetric legs and linkages, and a policy that incorporates actuator temperature inputs to prevent overheating. The approach successfully achieves a high level of believability for the costumed robotic character."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [virtual task-adaptive view, depth-aware module, dynamic coarse-to-fine procedure, 3D point cloud, foundation models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yixiang Chen, Yan Huang, Keji He, Peiyan Li, Liang Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; FiveAges; Shandong University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16724",children:"https://arxiv.org/pdf/2512.16724"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes VERM, a method that uses foundation models to generate a virtual, task-adaptive camera view from a 3D point cloud to reduce visual redundancy in robotic manipulation. This approach, combined with a depth-aware module and a coarse-to-fine procedure, improves efficiency and accuracy. Experiments show it outperforms previous methods while significantly speeding up both training and inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [predictive kinematics, 3D Gaussian geometry, trajectory-level prediction, depth-based rendering, continuous-action policy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jingjing Qian, Boyao Han, Chen Shi, Lei Xiao, Long Yang, Shaoshuai Shi, Li Jiang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen, Hunan University, LiAuto Inc., Voyager Research, Didi Chuxing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16811",children:"https://arxiv.org/pdf/2512.16811"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes GeoPredict, a geometry-aware Vision-Language-Action framework that enhances robotic manipulation by incorporating predictive kinematic priors (for 3D keypoint trajectories) and predictive 3D Gaussian geometry modules. These modules provide training-time supervision via depth-based rendering, while inference remains lightweight. Experiments show GeoPredict outperforms existing VLA baselines in tasks requiring precise 3D reasoning and spatial awareness."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [tactile sensing, egocentric video, cross-modal alignment, grasp understanding, retrieval benchmarks, classification benchmarks]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," MIT, Duke University, Brown University, University of Washington, Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16842",children:"https://arxiv.org/pdf/2512.16842"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces OpenTouch, a multimodal dataset combining synchronized in-the-wild egocentric video, full-hand tactile signals, and hand-pose data. It uses this dataset to create benchmarks for retrieval and classification, demonstrating that tactile information is a powerful cue for understanding grasps and improving cross-modal alignment. The main conclusion is that touch data significantly grounds perception and action, advancing research in multimodal egocentric perception and contact-rich robotics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [imitation learning, reinforcement learning, task decomposition, motion planning, data generation, skill policies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Toronto, Vector Institute, Georgia Institute of Technology, NVIDIA Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16861",children:"https://arxiv.org/pdf/2512.16861"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ReinforceGen, a hybrid system for long-horizon robot manipulation that first decomposes tasks into skills trained via imitation learning on generated data, and then refines them using reinforcement learning. It achieves an 80% success rate on benchmark tasks, with fine-tuning contributing to an 89% average performance increase."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [neural reconstruction, real-to-sim, co-training, simulation environments, policy evaluation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, Karl Pertsch"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Washington, Princeton University, University of California, Berkeley, Stanford University, Toyota Research Institute, University of Southern California, Cornell University, Physical Intelligence"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16881",children:"https://arxiv.org/pdf/2512.16881"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces PolaRiS, a framework that uses neural reconstruction to convert short video scans of real-world scenes into interactive simulation environments for robot policy evaluation. It also employs a simulation data co-training method to bridge the real-to-sim gap. The authors demonstrate that evaluations in PolaRiS correlate more strongly with real-world performance than existing simulated benchmarks, enabling scalable and democratized testing for generalist robot policies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Sceniris: A Fast Procedural Scene Generation Framework"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [procedural scene generation, batch sampling, GPU acceleration, collision checking, robot reachability, cuRobo]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinghuan Shang, Harsh Patel, Ran Gong, Karl Schmeckpeper"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Robotics and AI Institute, University of Waterloo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16896",children:"https://arxiv.org/pdf/2512.16896"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Sceniris is a fast procedural scene generation framework that uses batch sampling and GPU-accelerated collision checking via cuRobo to create large-scale, collision-free 3D scenes. It achieves at least 234x speed-up over prior methods and includes optional robot reachability checks for manipulation tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [reasoning-to-motion framework, trajectory-token interface, stage-aware trajectory prediction, 6DoF trajectory generation, vision-language reasoning, progressive training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Meta, University of Washington"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16907",children:"https://arxiv.org/pdf/2512.16907"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the EgoMAN dataset and model, a reasoning-to-motion framework that links vision-language reasoning with motion generation via a trajectory-token interface to predict 3D hand trajectories. It demonstrates that progressive training aligning reasoning with motion dynamics yields accurate, stage-aware trajectories with generalization across real-world scenes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [behavioral cloning, posterior behavioral cloning, policy pretraining, RL finetuning, generative models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," UC Berkeley, Stanford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16911",children:"https://arxiv.org/pdf/2512.16911"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Posterior Behavioral Cloning (PostBC), a method that pretrains a policy by modeling the posterior distribution of demonstrator behavior from a dataset, rather than exactly matching it. This ensures better coverage of demonstrator actions, which serves as a more effective initialization for subsequent reinforcement learning finetuning. The method is shown to improve RL finetuning performance on robotic control benchmarks and real-world manipulation tasks compared to standard behavioral cloning."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [scene graph, vision-language model, reinforcement learning, task planning, graph-then-plan, state-aware representation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Berkeley, University of Maryland, College Park, University of Toronto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16909",children:"https://arxiv.org/pdf/2512.16909"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces MomaGraph, a unified scene graph representation for embodied agents that integrates spatial, functional, and part-level information, and develops a 7B vision-language model (MomaGraph-R1) trained with reinforcement learning to predict task-oriented graphs for planning. The model, evaluated on a new dataset and benchmark, achieves state-of-the-art performance among open-source models for task planning and generalizes to real-robot experiments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251219] DVGT: Driving Visual Geometry Transformer"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [autonomous driving, 3D reconstruction], [transformer, DINO backbone, cross-view spatial attention, cross-frame temporal attention, multi-view geometry, point map]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Xiaomi EV, University of Macau, Peking University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16919",children:"https://arxiv.org/pdf/2512.16919"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes DVGT, a transformer-based model that reconstructs a global 3D point map from unposed multi-view image sequences using alternating attention mechanisms. It eliminates reliance on precise camera parameters and external sensor alignment, achieving superior performance across diverse driving scenarios."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);