"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3316],{8435:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251208-20251214","title":"20251208-20251214","description":"2025-12-08","source":"@site/docs/daily/20251208-20251214.md","sourceDirName":"daily","slug":"/daily/20251208-20251214","permalink":"/ai_toutiao/daily/20251208-20251214","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766635085000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251201-20251207","permalink":"/ai_toutiao/daily/20251201-20251207"},"next":{"title":"20251215-20251221","permalink":"/ai_toutiao/daily/20251215-20251221"}}');var s=n(4848),t=n(8453);const a={},o="20251208-20251214",l={},c=[{value:"2025-12-08",id:"2025-12-08",level:2},{value:"2025-12-09",id:"2025-12-09",level:2},{value:"2025-12-10",id:"2025-12-10",level:2},{value:"2025-12-11",id:"2025-12-11",level:2},{value:"2025-12-12",id:"2025-12-12",level:2}];function h(i){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251208-20251214",children:"20251208-20251214"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-08",children:"2025-12-08"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 7"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [programming languages], [algebraic data types, Hindley-Milner type inference, typed process identifiers, typed futures, actor model, static typing, Core Erlang]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Miguel de Oliveira Guerreiro"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Lisbon"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05224",children:"https://arxiv.org/pdf/2512.05224"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," NVLang introduces a statically typed functional language for the BEAM virtual machine that uses algebraic data types to encode actor message protocols and extends Hindley-Milner type inference to enforce them at compile time. The main conclusion is that this approach eliminates message-passing errors while preserving the actor model's simplicity and maintains interoperability with the existing Erlang ecosystem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] Metronome: Differentiated Delay Scheduling for Serverless Functions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [delay scheduling, random forest regression, locality-aware scheduling, SLA compliance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhuangbin Chen, Juzheng Zheng, Zibin Zheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05703",children:"https://arxiv.org/pdf/2512.05703"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Metronome, a differentiated delay scheduling framework for serverless functions that uses an online Random Forest Regression model to predict function execution times and identify optimal locality-aware nodes. The implementation on OpenLambda demonstrates that Metronome significantly reduces mean function execution time by 64.88%-95.83% compared to baselines while maintaining SLA compliance under high concurrency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] Model Gateway: Model Management Platform for Model-Driven Drug Discovery"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [MLOps, LLM Agents, Generative AI, model registry, dynamic consensus model, asynchronous execution, cloud computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yan-Shiun Wu, Nathan A. Morin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Eli Lilly and Company"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05462",children:"https://arxiv.org/pdf/2512.05462"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Model Gateway, a cloud-based MLOps platform for managing machine learning and scientific computational models in drug discovery. It integrates LLM Agents and Generative AI tools to handle tasks like model registration and asynchronous execution. The platform demonstrated scalability with a 0% failure rate under high load and is concluded to be a fundamental component for accelerating model-driven drug development."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, model heterogeneity, gradual model restoration, mask-aware aggregation, asynchronous training, bandwidth-constrained clients]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chengjie Ma, Seungeun Oh, Jihong Park, Seong-Lyun Kim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Yonsei University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05372",children:"https://arxiv.org/pdf/2512.05372"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FedGMR, a federated learning method that progressively increases the density of clients' sub-models during training to keep bandwidth-constrained clients effective. It introduces a mask-aware aggregation rule for asynchronous, model-heterogeneous settings and provides convergence guarantees. Experiments show FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [compiler optimization], [AoS-SoA transformation, reduced precision, GPU offloading, compiler annotations, unified memory]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pawel K. Radtke, Tobias Weinzierl"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Durham University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05516",children:"https://arxiv.org/pdf/2512.05516"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces compiler annotations to support AoS-to-SoA data layout transformations and reduced precision for particle simulation codes on heterogeneous GPU platforms. It evaluates strategies for orchestrating these conversions between CPU and GPU, finding that NVIDIA's G200 platform achieved a speedup of around 2.6, while AMD's MI300A showed more robust but less pronounced benefits. The authors conclude that their compiler-based techniques are broadly applicable to Lagrangian codes and other domains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] Are Bus-Mounted Edge Servers Feasible?"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing, vehicular networks], [bus-mounted edge servers, server placement, greedy heuristic algorithm, trace-driven simulation, coverage optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xuezhi Li, Jiancong He, Ming Xie, Xuyang Chen, Le Chang, Li Jiang, Gui Gui"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Guangdong University of Technology, Central South University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05543",children:"https://arxiv.org/pdf/2512.05543"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies the feasibility of deploying edge servers on public buses to serve vehicular networks, using real-world mobility traces and a greedy algorithm to select buses that maximize coverage of demand points under budget constraints. The trace-driven simulations show that bus-mounted servers can effectively handle dynamic user demand, leading to the conclusion that they are a feasible, beneficial, and valuable solution for urban areas."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251208] InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion inference], [invariance caching, deterministic sampling, quantile-based change metrics, re-sampling correction, step-first caching, layer-wise caching]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zihao Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05134",children:"https://arxiv.org/pdf/2512.05134"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces InvarDiff, a training-free acceleration method for diffusion models that exploits temporal invariance across timesteps and layers to cache and reuse intermediate features. It uses a pre-computed binary cache plan and re-sampling correction to reduce redundant computation. Experiments show the method achieves 2\u20133\xd7 speed-ups on models like DiT and FLUX with minimal impact on output quality."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05946",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05962",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05711",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05207",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05291",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05172",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05753",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05591",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05320",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05341",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05122",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Trusted AI Agents in the Cloud ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05887",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05216",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] AI & Human Co-Improvement for Safer Co-Superintelligence ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05356",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05916",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05481",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251208] To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05925",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-09",children:"2025-12-09"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 17"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [A-3PO, proximal policy approximation, decoupled loss, asynchronous RL, trust region]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xiaocan Li, Shiliang Wu, Zheng Shen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Canada"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06547",children:"https://arxiv.org/pdf/2512.06547"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes A-3PO, a method that accelerates asynchronous LLM training by approximating the proximal policy through simple interpolation instead of an extra forward pass. This eliminates a computational bottleneck, reducing training time by 18% while maintaining performance comparable to methods like PPO."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Cloud Revolution: Tracing the Origins and Rise of Cloud Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing], [virtualization, distributed systems, high-speed networking, edge computing, quantum computing services, data privacy, cloud security]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Deepa Gurung, S M Zia Ur Rashid, Zain ul Abdeen, Suman Rath"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Joongbu University, The University of Tulsa, Virginia Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06800",children:"https://arxiv.org/pdf/2512.06800"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper reexamines the historical evolution of cloud computing, analyzing its technological foundations and economic impact. It concludes that cloud computing is a rapidly changing paradigm whose future depends on balancing scalability, openness, and trust, while highlighting challenges like security and vendor lock-in."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [decentralized storage network], [Private Information Retrieval, secure mapping, Byzantine-robust retrieval, file replication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiahao Zhang, Minghui Xu, Hechuan Guo, Xiuzhen Cheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shandong University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07189",children:"https://arxiv.org/pdf/2512.07189"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces PIR-DSN, a decentralized storage network protocol that integrates Private Information Retrieval to protect user privacy during file retrieval. It uses a novel secure mapping method and file replication across miners to enable private, verifiable, and Byzantine-robust operations. The evaluation shows it achieves practical overhead and throughput, making it viable for privacy-sensitive applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Optimizing video analytics inference pipelines: a case study"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [GPU acceleration, parallel processing, vectorized clustering, memory-efficient post-processing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Saeid Ghafouri, Yuming Ding, Katerine Diaz Chito, Jes\xfas Martinez del Rinc\xf3n, Niamh O'Connell, Hans Vandierendonck"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Queen's University Belfast"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07009",children:"https://arxiv.org/pdf/2512.07009"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a case study on optimizing a video analytics pipeline for poultry welfare monitoring through system-level improvements. The core methods include multi-level parallelization, GPU acceleration, vectorized clustering, and memory-efficient post-processing. These optimizations achieved up to a 2x speedup without compromising accuracy, highlighting practical strategies for building high-throughput, low-latency video inference systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [Lyapunov optimization, token routing, mixture of experts, distributed training, edge networks]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Long Shi, Bingyan Ou, Kang Wei, Weihao Zhu, Zhe Wang, Zhiyong Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University of Science and Technology, Southeast University, Shanghai Jiao Tong University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06784",children:"https://arxiv.org/pdf/2512.06784"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Stable-MoE, a Lyapunov-based token routing framework for distributed Mixture-of-Experts training over resource-heterogeneous edge networks. It formulates a stochastic optimization problem to maximize throughput and gating consistency while ensuring queue stability, transforming it into online per-slot decisions. Experiments show it outperforms baselines with significant gains in system throughput and test accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [shared system-level cache, predictive cache management, dead-block prediction, cache bypassing, thrashing mitigation, RTL implementation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhongchun Zhou, Chengtao Lai, Yuhang Gu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology, Southeast University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07312",children:"https://arxiv.org/pdf/2512.07312"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes DCO, a dynamic cache orchestration scheme for multi-core AI accelerators that uses dataflow information to guide predictive cache replacement and bypassing decisions. The method achieves up to 1.80x speedup over conventional caches and is implemented in RTL with a small area footprint, demonstrating the effectiveness of shared cache designs for LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed databases], [chunked-object pattern, multi-region replication, NoSQL, DynamoDB Global Tables, active-active architecture, time-to-consistency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Manideep Reddy Chinthareddy"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06852",children:"https://arxiv.org/pdf/2512.06852"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper proposes a "chunked-object" pattern to store large payloads by splitting them into ordered chunks within a managed NoSQL database itself, avoiding the need for separate object storage. This method eliminates replication lag hazards and reduces cross-region consistency time by keeping data and metadata within a single consistency domain, as demonstrated in a production system handling over 200,000 transactions per hour.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [process mining, edge-cloud continuum, decentralized framework, IoT, IIoT]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hendrik Reiter, Janick Edinger, Martin Kabierski, Agnes Koschmider, Olaf Landsiedel, Arvid Lepsien, Xixi Lu, Andrea Marrella, Estefania Serral, Stefan Schulte, Florian Tschorsch, Matthias Weidlich, Wilhelm Hasselbring"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Kiel University, University of Hamburg, University of Vienna, University of Bayreuth, Hamburg University of Technology, Utrecht University, Sapienza University of Rome, KU Leuven, Dresden University of Technology, Humboldt University of Berlin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07280",children:"https://arxiv.org/pdf/2512.07280"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces ContinuumConductor, a layered decision framework for decentralizing process mining tasks across the edge-cloud continuum. It analyzes trade-offs between centralization and decentralization for steps like preprocessing and discovery to enable privacy-preserving and resource-efficient analysis. The method is demonstrated in a real-world inland port use case, establishing a foundation for computing-aware process mining in IIoT systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [vector lookup table, ultra-low-bit quantization, parallel inference, cache-aware streamed lookup, LUT-based inference]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xiangyu Li, Chengyu Yin, Weijun Wang, Jianyu Wei, Ting Cao, Yunxin Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Beijing Jiaotong University, University of Science and Technology of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06443",children:"https://arxiv.org/pdf/2512.06443"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Vec-LUT, a new vector lookup table paradigm to address the memory bandwidth underutilization of scalar LUT-based inference during parallel LLM inference on edge devices. It introduces a unified LUT across tokens and techniques like a vector LUT-centric tensor layout and cache-aware streamed lookup. Evaluations show Vec-LUT achieves up to 4.2x speedup over state-of-the-art baselines on various edge devices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [bi-level optimization, VM placement, bin-packing, performance analysis, live migration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Roozbeh Bostandoost, Pooria Namyar, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Eli Cortez, Ankur Mallick, Kevin Hsieh, Rodrigo Fonseca, Mohammad Hajiesmaili, Behnaz Arzani"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Amherst, Microsoft Research, Rice University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07750",children:"https://arxiv.org/pdf/2512.07750"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces SANJESH, a tool that uses a novel bi-level optimization approach to analyze the performance of cloud systems that use multiple interacting ML models, such as for VM placement. It finds scenarios where these models can cause significantly worse performance (up to 4x) than what simpler simulation-based methods detect."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Bandwidth-Aware Network Topology Optimization for Decentralized Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [ADMM, conjugate gradient, Mixed-Integer SDP, bandwidth allocation, consensus speed]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yipeng Shen, Zehan Zhu, Yan Huang, Changzhi Yan, Cheng Zhuo, Jinming Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07536",children:"https://arxiv.org/pdf/2512.07536"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a bandwidth-aware network topology optimization framework for decentralized learning, which formulates the problem as a Mixed-Integer SDP and solves it using an ADMM-based method with a conjugate gradient substep. The resulting optimized topologies achieve higher consensus speed and reduce training time for decentralized learning tasks compared to benchmark topologies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [stream processing], [hierarchical schedulers, load balancing, constraint solver, Service Level Objective (SLO), greedy scheduler]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Animesh Dangwal, Yufeng Jiang, Charlie Arnold, Jun Fan, Mohamed Bassem, Aish Rajagopal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Meta Platforms"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07792",children:"https://arxiv.org/pdf/2512.07792"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper designs a hierarchical, multi-objective scheduler system called SPTLB for automated load balancing across tiers in Meta's stream processing infrastructure. It uses a constraint solver to efficiently move applications while respecting properties like CPU/memory utilization and SLOs, and shows it outperforms a baseline greedy scheduler."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, deep learning, anomaly detection, honeynet, dynamic container deployment]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lukas Johannes M\xf6ller"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07827",children:"https://arxiv.org/pdf/2512.07827"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes ADLAH, an adaptive honeynet architecture that uses a reinforcement learning agent to dynamically escalate suspicious sessions from low-interaction to high-interaction honeypots. The core method integrates deep learning for anomaly detection with autonomous infrastructure orchestration to efficiently capture high-value threat intelligence. The main conclusion is that this AI-driven, multi-layered approach provides a practical and cost-effective blueprint for analyzing automated bot attacks and generating actionable cybersecurity insights."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion inference], [latent parallelism, patch-aligned overlapping partition, position-aware latent reconstruction, communication-efficient serving, video diffusion models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhiyuan Wu, Shuai Wang, Li Chen, Kaihui Gao, Dan Li, Yanyu Ren, Qiming Zhang, Yong Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Zhongguancun Laboratory, ZTE Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07350",children:"https://arxiv.org/pdf/2512.07350"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Latent Parallelism (LP), a novel parallelism strategy for serving Video Diffusion Models (VDMs) that reduces communication overhead by dynamically rotating partition dimensions across diffusion timesteps. It introduces a patch-aligned overlapping partition and position-aware latent reconstruction to maintain generation quality. Experiments show LP reduces communication by up to 97% compared to baseline methods while preserving video quality."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [carbon-aware scheduling, job shop scheduling, DAG workloads, makespan, carbon intensity, energy efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Roozbeh Bostandoost, Adam Lechowicz, Walid A. Hanafy, Prashant Shenoy, Mohammad Hajiesmaili"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Amherst"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07799",children:"https://arxiv.org/pdf/2512.07799"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper models the scheduling of batch workloads with task dependencies (DAGs) as a flexible job-shop scheduling problem to quantify the maximum potential carbon reduction. Using an offline solver, the study finds that a dependency-aware approach can reduce carbon emissions by up to 25% on average without increasing the optimal makespan, though it highlights a trade-off between carbon savings, energy use, and completion time."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Otus Supercomputer"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [supercomputer, HPC cluster, CPU nodes, GPU nodes, FPGA nodes, energy efficiency, Top500, Green500]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sadaf Ehtesabi, Manoar Hossain, Tobias Kenter, Andreas Krawinkel, Holger Nitsche, Lukas Ostermann, Christian Plessl, Heinrich Riebler, Stefan Rohde, Robert Schade, Michael Schwarz, Jens Simon, Nils Winnwa, Alex Wiens, Xin Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Paderborn University, Paderborn Center for Parallel Computing (PC2)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07401",children:"https://arxiv.org/pdf/2512.07401"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper provides a comprehensive overview of the Otus supercomputer, detailing its hardware, software, and system integration for energy-efficient operation. It describes Otus as a high-performance computing cluster that complements the Noctua 2 system with increased computing power and specialized node types. The main conclusion is that Otus is a highly energy-efficient system, ranking 5th on the Green500 list with its GPU partition."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251209] Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [edge-cloud disaggregated architecture, scene segmentation and clustering, hierarchical memory, threshold-based progressive sampling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Mu Yuan, Xiaowen Chu, Weijie Hong, Xu Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sun Yat-sen University, The Chinese University of Hong Kong, HKUST (Guangzhou), Shenzhen Smart City Communications Co., Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07344",children:"https://arxiv.org/pdf/2512.07344"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Venus, a system for efficient online video understanding that uses an edge-cloud architecture to process video streams. Its core method involves building a hierarchical memory from keyframes at the edge and using a progressive sampling algorithm for retrieval. The system achieves a significant speedup in response latency while maintaining high reasoning accuracy."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 37'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06920",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06392",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] The Role of Entropy in Visual Grounding: Analysis and Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06726",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07135",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06747",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06218",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06982",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07342",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06912",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] LightSearcher: Efficient DeepSearch via Experiential Memory ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06653",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06471",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Auto-exploration for online reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06244",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06250",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Towards Robust Protective Perturbation against DeepFake Face Swapping ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07228",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07266",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06252",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06835",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06240",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06533",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05969",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06060",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06859",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07200",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06917",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06351",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07417",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07419",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07437",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07497",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Model-Based Reinforcement Learning Under Confounding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07528",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] ReLaX: Reasoning with Latent Exploration for Large Reasoning Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07558",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07611",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07631",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07761",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Learning to Hedge Swaptions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06639",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Statistical analysis of Inverse Entropy-regularized Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06956",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 25'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06676",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] FOAM: Blocked State Folding for Memory-Efficient LLM Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07112",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06392",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Block Sparse Flash Attention ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07011",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06404",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] A Patient-Doctor-NLP-System to contest inequality for less privileged ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06734",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06417",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07173",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Flash Multi-Head Feed-Forward Network ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06989",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06337",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06457",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06530",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06252",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] A self-driving lab for solution-processed electrochromic thin films ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05989",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06038",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Approximate Multiplier Induced Error Propagation in Deep Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.06537",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Leveraging KV Similarity for Online Structured Pruning in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07090",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07371",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07437",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07487",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07490",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07650",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] A scalable and real-time neural decoder for topological quantum codes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07737",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251209] LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07808",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-10",children:"2025-12-10"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 14"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] GPU Memory Prediction for Multimodal Model Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [memory prediction, factorization, layer decomposition, profiling, formulation-based modeling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinwoo Jeong, Minchul Kang, Younghun Go, Changyong Shin, Hyunho Lee, Junho Yoon, Gyeongsik Yang, Chuck Yoo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Korea University, KT Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07853",children:"https://arxiv.org/pdf/2512.07853"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a framework for predicting peak GPU memory usage during multimodal model training by decomposing the model into layers and applying factorization to estimate each layer's memory consumption. It addresses the limitations of prior unimodal-focused methods. The evaluation shows the framework achieves high prediction accuracy with an average MAPE of ~8.7%."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Basic Lock Algorithms in Lightweight Thread Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrency], [TTAS lock, MCS lock, cohort lock, mutex, lightweight threads, coroutines, context switching]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Taras Skazhenik, Nikolai Korobenikov, Andrei Churbanov, Anton Malakhov, Vitaly Aksenov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ITMO University, Huawei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08563",children:"https://arxiv.org/pdf/2512.08563"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper modifies TTAS and MCS lock algorithms for use with lightweight threads, highlighting the importance of yield and sleep context switch mechanisms. It concludes that a cohort lock, which combines multiple MCS queues with a common TTAS, provides a balanced solution for diverse lightweight thread libraries."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [profiling, hardware performance counters, FSDP, DVFS, MFMA utilization, communication/computation overlap, kernel launch overhead]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Marco Kurzynski, Shaizeen Aga, Di Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Central Florida, Advanced Micro Devices, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08242",children:"https://arxiv.org/pdf/2512.08242"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Chopper, a multi-level GPU profiling framework for characterizing distributed LLM training workloads. Using Chopper to analyze Llama 3 8B training on AMD MI300X GPUs, the authors find that frequency overhead (DVFS effects) is the largest contributor to performance inefficiency, surpassing other factors like MFMA utilization loss."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] A scalable high-order multigrid-FFT Poisson solver for unbounded domains on adaptive multiresolution grids"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [computational physics], [multigrid, FFT, high-order compact stencils, adaptive multiresolution grids, Poisson solver, unbounded boundary conditions]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gilles Poncelet, Jonathan Lambrechts, Thomas Gillis, Philippe Chatelain"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe9 catholique de Louvain, Massachusetts Institute of Technology, NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08555",children:"https://arxiv.org/pdf/2512.08555"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a scalable high-order multigrid-FFT Poisson solver designed for unbounded domains on adaptive multiresolution grids. The method combines a Fourier-based direct solver for flexible boundary condition handling with high-order compact stencils to reduce communication and improve accuracy. The solver was validated against analytical solutions and demonstrated scalability up to 16,384 cores on European high-performance computing infrastructures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] CapsuleFS A Multi-credential DataCapsule Filesystem"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing], [DataCapsule, POSIX-compliant filesystem, FUSE, Trusted Execution Environment, Global Data Plane]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qingyang Hu, Yucheng Huang, Manshi Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08067",children:"https://arxiv.org/pdf/2512.08067"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CapsuleFS is a POSIX-compliant filesystem that integrates multi-credential functionality using DataCapsules for edge computing, with a three-part architecture including a DataCapsule server, middleware in a Trusted Execution Environment, and a FUSE-based client. The system demonstrates high functional correctness despite modest performance, making it suitable for real-world software development applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Model-based Testing of Practical Distributed Systems in Actor Model"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems verification], [model-based testing, finite-state automaton, actor model, Viewstamped Replication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ilya Kokorin, Evgeny Chernatskiy, Vitaly Aksenov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ITMO University, VK.com"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08698",children:"https://arxiv.org/pdf/2512.08698"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a model-based testing approach to verify distributed systems implemented in the actor model by generating exhaustive test suites from finite-state automaton models. The method does not require modifying the system code or interfering with its execution environment. As a case study, the authors successfully verified a real-world replication algorithm based on Viewstamped Replication."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [differential energy debugging, energy profiler, operator-level comparison, software energy waste]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi Pan, Wenbo Qian, Dedong Xie, Ruiyan Hu, Yigong Hu, Baris Kasikci"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Washington, Boston University, Shanghai Jiao Tong University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08365",children:"https://arxiv.org/pdf/2512.08365"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Magneton, an energy profiler that uses differential energy debugging to compare energy consumption between similar ML systems at the operator level to pinpoint code and configuration inefficiencies. It successfully detected and diagnosed numerous known and previously unknown cases of software energy waste across popular ML systems. The work highlights software design as a significant, overlooked source of energy inefficiency in ML."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Synergizing Monetization, Orchestration, and Semantics in Computing Continuum"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [computing continuum, edge computing, resource orchestration, semantic interoperability, monetization, distributed marketplace, hyper-distributed applications]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chinmaya Kumar Dehury, Lauri Lov\xe9n, Praveen Kumar Donta, Ilir Murturi, Schahram Dustdar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IISER Berhampur, University of Oulu, Stockholm University, University of Prishtina, TU Wien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08288",children:"https://arxiv.org/pdf/2512.08288"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the HERMES framework, which synergizes resource monetization, orchestration, and semantic interoperability to address challenges in the computing continuum. It aims to create an open, secure environment for managing resources from cloud to edge, enabling efficient and trustworthy hyper-distributed applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Modeling the Potential of Message-Free Communication via CXL.mem"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC communication], [CXL.mem, MPI, performance modeling, Mitos, memory trace sampling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Stepan Vanecek, Matthew Turner, Manisha Gajbe, Matthew Wolf, Martin Schulz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Munich, Hewlett Packard Enterprise"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08005",children:"https://arxiv.org/pdf/2512.08005"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a performance evaluation toolchain and an extended performance model to predict the benefits of using CXL.mem for inter-node communication in MPI applications. The method analyzes data access patterns and MPI traffic to identify which data transfers could be optimized by replacing traditional MPI messages with direct CXL.mem access. The authors validate their model on sample applications, demonstrating its utility for targeted performance optimizations in HPC systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [emulation, mixed-precision computing, Chinese Remainder Theorem, Ozaki-II scheme, INT8 matrix engines]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuki Uchino, Qianxiang Ma, Toshiyuki Imamura, Katsuhisa Ozaki, Patrick Lars Gutsche"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," RIKEN Center for Computational Science, Shibaura Institute of Technology, Ecole Normale Superieure de Lyon"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08321",children:"https://arxiv.org/pdf/2512.08321"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes methods to emulate high-precision complex matrix multiplication on low-precision INT8 hardware using the Ozaki-II scheme and the Chinese Remainder Theorem. On an NVIDIA B200 GPU, the methods achieve significant speedups (4.0x\u20136.5x) over native cuBLAS routines for large problem sizes, while offering a flexible trade-off between speed and accuracy. This suggests the approach has potential as a default algorithm for a wide range of applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing sustainability], [spatial shifting, temporal shifting, workload scheduling, simulation, Land Usage Effectiveness (LUE)]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Giulio Attenni, Youssef Moawad, Novella Bartolini, Lauritz Thamsen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"}),' University of Glasgow, "La Sapienza" University of Rome']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08725",children:"https://arxiv.org/pdf/2512.08725"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper investigates using spatial and temporal shifting of cloud workloads to reduce their environmental footprint. Through simulations with real-world cloud provider data and workload traces, it finds that spatial shifting significantly lowers carbon, water, and land-use impacts, with temporal shifting providing an additional benefit. The study concludes that combined spatio-temporal shifting is a robust strategy for multi-dimensional sustainability optimization in cloud computing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] A Task Parallel Orthonormalization Multigrid Method For Multiphase Elliptic Problems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [multigrid, k-cycle, orthonormalization, task-parallel, asynchronous execution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Teoman Toprak, Florian Kummer"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Darmstadt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08728",children:"https://arxiv.org/pdf/2512.08728"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a task-parallel variant of the K-cycle orthonormalization multigrid method designed for solving multiphase elliptic problems. The core method leverages asynchronous execution to overcome the scalability limitations of traditional bulk-synchronous implementations. The main conclusion is that this approach improves scalability and performance on modern large-scale parallel high-performance computing systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251210] Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [skewness-guided pruning, federated learning, swin transformer, multimodal model, model compression, edge devices]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kuniko Paxton, Koorosh Aslansefat, Dhavalkumar Thakker, Yiannis Papadopoulos"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Hull"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08751",children:"https://arxiv.org/pdf/2512.08751"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a skewness-guided pruning method to compress a multimodal Swin Transformer for skin lesion classification by selectively pruning layers based on the statistical skewness of their outputs. The method was validated in a federated learning setting and achieved approximately 36% model size reduction without accuracy loss, demonstrating efficient compression for privacy-preserving medical AI on edge devices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsxs)(e.strong,{children:["[arXiv251210] Parallel Batch Dynamic Vertex Coloring in ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsxs)(e.mrow,{children:[(0,s.jsx)(e.mi,{children:"O"}),(0,s.jsx)(e.mo,{stretchy:"false",children:"("}),(0,s.jsx)(e.mi,{mathvariant:"normal",children:"\u0394"}),(0,s.jsx)(e.mo,{stretchy:"false",children:")"})]}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"O( \u0394)"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"O"}),(0,s.jsx)(e.span,{className:"mopen",children:"("}),(0,s.jsx)(e.span,{className:"mord",children:"\u0394"}),(0,s.jsx)(e.span,{className:"mclose",children:")"})]})})]})," Amortized Update Time"]})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [batch-dynamic algorithm, vertex coloring, amortized update time, parallel span, randomized algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chase Hutton, Adam Melrod"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Maryland, Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08742",children:"https://arxiv.org/pdf/2512.08742"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a new parallel batch-dynamic algorithm for maintaining a proper (\u0394+1)-vertex coloring in a graph undergoing edge insertions and deletions. The method builds on a sequential dynamic algorithm and achieves O(log \u0394) expected amortized update time with a parallel span of O(polylog b + polylog n) for batches of b updates. This is the first algorithm to provide such parallel efficiency for the dynamic vertex coloring problem."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08230",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Robust Agents in Open-Ended Worlds ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08139",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08341",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08153",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Mind to Hand: Purposeful Robotic Control via Embodied Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08580",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] An Introduction to Deep Reinforcement and Imitation Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08052",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Direct transfer of optimized controllers to similar systems using dimensionless MPC ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08667",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08449",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08012",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Using reinforcement learning to probe the role of feedback in skill acquisition ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08463",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08485",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07843",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08300",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Training LLMs for Honesty via Confessions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08093",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Scalable Offline Model-Based RL with Action Chunks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08108",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08763",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Reinforcement Learning From State and Temporal Differences ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08855",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08601",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 15'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] DeepCode: Open Agentic Coding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07921",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07855",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08256",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08274",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08286",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08077",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08160",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.07847",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08705",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08732",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08769",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08829",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08859",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Open Polymer Challenge: Post-Competition Report ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08896",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251210] Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05791",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-11",children:"2025-12-11"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 11"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Straggler Tolerant and Resilient DL Training on Homogeneous GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [straggler mitigation, synchronous SGD, asynchronous SGD, parameter server, all-reduce, resource reallocation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zeyu Zhang, Haiying Shen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Virginia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09685",children:"https://arxiv.org/pdf/2512.09685"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes STAR, a system that introduces new synchronization modes and resource management to mitigate stragglers in homogeneous GPU-based deep learning training. It concludes that STAR significantly reduces Time-To-Accuracy compared to state-of-the-art methods while maintaining model accuracy, by proactively preventing CPU and bandwidth overloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [distributed vector search, approximate nearest neighbor, disk-based graph index, BatANN, single global graph, scatter-gather baseline]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nam Anh Dang, Ben Landrum, Ken Birman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Cornell University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09331",children:"https://arxiv.org/pdf/2512.09331"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents BatANN, a distributed disk-based vector search system that uses a single global graph index. Its core innovation is to transfer the full query state to another server when accessing remote data, improving locality and enabling near-linear throughput scaling. The system significantly outperforms scatter-gather baselines in throughput while maintaining low latency on standard TCP."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Recoverable Lock-Free Locks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrent algorithms], [lock-freedom, recoverability, non-volatile memory, nested locks, transformation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hagit Attiya, Panagiota Fatourou, Eleftherios Kosmas, Yuanhao Wei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technion \u2013 Israel Institute of Technology, FORTH ICS, University of Crete, Hellenic Mediterranean University, University of British Columbia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09710",children:"https://arxiv.org/pdf/2512.09710"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a novel transformation that converts lock-based implementations into recoverable and lock-free versions. It provides a substitution for lock acquire and release operations, supporting nested locks and ensuring correctness. The method enables concurrent algorithms to tolerate thread failures and recover from crashes using non-volatile memory."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] SynthPix: A lightspeed PIV images generator"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [JAX, synthetic data generation, Particle Image Velocimetry (PIV), parallelism, accelerators]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Antonio Terpin, Alan Bonomi, Francesco Banelli, Raffaello D'Andrea"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ETH Z\xfcrich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09664",children:"https://arxiv.org/pdf/2512.09664"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SynthPix is a high-performance synthetic image generator for Particle Image Velocimetry (PIV) implemented in JAX, designed for parallelism on accelerators. It achieves orders of magnitude higher throughput in image-pair generation compared to existing tools. The tool is intended to support the training of data-hungry reinforcement learning methods for flow estimation and accelerate development in real-time PIV feedback systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [GPU prewarming, universal GPU workers, evict-aware model placement, proactive prewarming, zero-overhead memory switching]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chiheng Lou, Sheng Qi, Rui Kang, Yong Zhang, Chen Sun, Pengcheng Wang, Bingyang Liu, Xuanzhe Liu, Xin Jin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Huawei Technologies Co., Ltd"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09472",children:"https://arxiv.org/pdf/2512.09472"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," WarmServe introduces universal GPU workers to enable one-for-many GPU prewarming, using workload prediction to proactively load models. It employs an evict-aware placement strategy and a zero-overhead memory switching mechanism to manage GPU memory efficiently. The system significantly improves time-to-first-token (TTFT) and request throughput compared to existing autoscaling and GPU-sharing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [optimization algorithms], [Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA), Whale Optimization Algorithm (WOA), Seagull Optimization Algorithm (SOA), Halton sequence initialization, Pareto-guided mutation, dynamic VM load redistribution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhi Zhao, Hang Xiao, Wei Rang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shandong Normal University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09568",children:"https://arxiv.org/pdf/2512.09568"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes PHWSOA, a hybrid optimization algorithm combining Whale and Seagull algorithms with Pareto dominance to schedule tasks in cloud computing, optimizing makespan, load balancing, and cost. Experiments show it significantly outperforms baseline methods in these metrics, demonstrating its potential for efficient cloud resource management."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [expert parallelism, token routing, load balancing, memory-bound regime, allGather, Mixture of Experts]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yanpeng Yu, Haiyue Ma, Krish Agarwal, Nicolai Oswald, Qijing Huang, Hugo Linsenmaier, Chunhui Mei, Ritchie Zhao, Ritika Borkar, Bita Darvish Rouhani, David Nellans, Ronny Krashinsky, Anurag Khandelwal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Yale University, Princeton University, Carnegie Mellon University, NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09277",children:"https://arxiv.org/pdf/2512.09277"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes METRO, a token-routing algorithm for MoE model serving that balances the number of activated experts per GPU instead of tokens, which is crucial in memory-bound decode phases. It achieves this with a novel allGather scheme for global top-k knowledge, reducing communication overhead. The method significantly reduces decode latency and improves throughput compared to prior token-balancing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Scalable Construction of Spiking Neural Networks using up to thousands of GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [spiking neural networks, MPI, multi-GPU, exascale computing, cortical models, point-to-point communication, collective communication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bruno Golosio, Gianmarco Tiddia, Jos\xe9 Villamar, Luca Pontisso, Luca Sergi, Francesco Simula, Pooja Babu, Elena Pastorelli, Abigail Morrison, Markus Diesmann, Alessandro Lonardo, Pier Stanislao Paolucci, Johanna Senk"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Cagliari, Istituto Nazionale di Fisica Nucleare (INFN), J\xfclich Research Centre, RWTH Aachen University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09502",children:"https://arxiv.org/pdf/2512.09502"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a novel network construction method for simulating large-scale spiking neural networks on multi-GPU clusters using MPI. The method enables each process to build local connectivity and prepare data structures for efficient spike exchange across the cluster. The authors demonstrate the scaling performance of this approach using two cortical models with different communication strategies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [embedded Rust, multi-core parallelization, TinyML, microcontroller, inference optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhaolan Huang, Kaspar Schleiser, Gyungmin Myung, Emmanuel Baccelli"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Freie Universit\xe4t Berlin, KAIST, Inria Saclay"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09800",children:"https://arxiv.org/pdf/2512.09800"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Ariel-ML, a toolkit that automates parallelized inference for TinyML models on multi-core microcontrollers using embedded Rust. It combines a generic TinyML pipeline with a software platform to leverage multi-core architectures like Arm Cortex-M and RISC-V. The authors show that Ariel-ML reduces inference latency compared to prior art while maintaining memory footprints comparable to C/C++ toolkits."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [distributed offloading, edge-cloud continuum, Vision Transformer (ViT), Segment Anything Model (SAM), privacy protection, data partitioning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zihao Ding, Mufeng Zhu, Zhongze Tang, Sheng Wei, Yao Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rutgers University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09309",children:"https://arxiv.org/pdf/2512.09309"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a distributed framework that enhances privacy for Vision Transformers by using a trusted edge device to partition and offload image data across multiple independent cloud servers, preventing any single server from reconstructing the complete image. It demonstrates the framework on the Segment Anything Model, showing that it maintains near-baseline segmentation performance while significantly reducing the risk of content reconstruction and data exposure."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251211] Link-Sharing Backpressure Routing In Wireless Multi-Hop Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless networking], [backpressure routing, link sharing, Lyapunov drift, MaxUtility, SP-BP]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhongyuan Zhao, Yujun Ming, Ananthram Swami, Kevin Chan, Fikadu Dagefu, Santiago Segarra"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rice University, US Army DEVCOM Army Research Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09902",children:"https://arxiv.org/pdf/2512.09902"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Maximum Utility (MaxU) link-sharing method for backpressure routing in wireless multi-hop networks, which allows multiple commodities to share a link instead of using exclusive selection. By revisiting Lyapunov drift theory, the method mitigates the last-packet problem and slightly expands network capacity without increasing control overhead."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 13'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Learning Unmasking Policies for Diffusion Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09106",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08950",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09682",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Training Multi-Image Vision Agents via End2End Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08980",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09566",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09368",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09487",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09410",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Financial Instruction Following Evaluation (FIFE) ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08965",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09829",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09872",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] STACHE: Local Black-Box Explanations for Reinforcement Learning Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09909",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08948",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 12'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08983",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09427",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09013",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] FBA",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msup,{children:[(0,s.jsx)(e.mrow,{}),(0,s.jsx)(e.mn,{children:"2"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"^2"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsx)(e.span,{className:"vlist-t",children:(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,s.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]}),"D: Frequency-based Black-box Attack for AI-generated Image Detection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09264",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08967",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09006",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Towards Lossless Ultimate Vision Token Compression for VLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09010",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09410",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09202",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] 3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.08987",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09829",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251211] Fast Factorized Learning: Powered by In-Memory Database Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09836",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-12",children:"2025-12-12"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 18"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [formal methods, fault localization, SMT solvers, large language models, automated program repair, access control policies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bethel Hall, Owen Ungaro, William Eiers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stevens Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09957",children:"https://arxiv.org/pdf/2512.09957"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces CloudFix, a framework that combines formal methods for fault localization with Large Language Models (LLMs) to generate and verify repairs for cloud access control policies. It shows that this hybrid approach improves repair accuracy over a baseline, demonstrating the effectiveness of LLMs for automated policy repair in cloud security."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, gradient scheduling algorithm, resource allocation, fluid sample path analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Phuong Tran, Tzu-Hao Liu, Long Tan Le, Tung-Anh Nguyen, Van Quan La, Eason Yu, Han Shu, Choong Seon Hong, Nguyen H. Tran"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Kyung Hee University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09963",children:"https://arxiv.org/pdf/2512.09963"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces GOODSPEED, a distributed inference framework that uses adaptive speculative decoding with a central verification server coordinating heterogeneous draft servers. It employs a gradient scheduling algorithm to dynamically allocate verification tasks, maximizing a logarithmic utility function for proportional fairness. The analysis shows GOODSPEED converges to optimal goodput allocation and maintains near-optimal performance with bounded error under dynamic workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] When Quantum Federated Learning Meets Blockchain in 6G Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [quantum federated learning, blockchain, 6G networks, decentralized learning, smart contracts]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dinh C. Nguyen, Md Bokhtiar Al Zami, Ratun Rahman, Shaba Shaon, Tuy Tan Nguyen, Fatemeh Afghah"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Alabama in Huntsville, Florida State University, Clemson University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09958",children:"https://arxiv.org/pdf/2512.09958"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes QFLchain, a framework that integrates Quantum Federated Learning (QFL) with blockchain technology to enable secure and scalable distributed AI model training in 6G networks. It investigates key challenges like communication overhead and security vulnerabilities. The authors conclude that this integration offers potential advantages in training performance for decentralized, intelligent 6G systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] A study of the spectrum resource leasing method based on ERC4907 extension"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain systems], [ERC4907, smart contract, M-ERC4907, dynamic spectrum sharing, multi-slot authorization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhiming Liang, Bin Chen, Litao Ye, Chen Sun, Shuo Wang, Zhe Peng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shenzhen University, Sony (China) Limited, The Hong Kong Polytechnic University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09942",children:"https://arxiv.org/pdf/2512.09942"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Multi-slot ERC4907 (M-ERC4907) extension method to overcome the single-user, single-time-slot limitation of the original standard. The method enables batch configuration of multiple time slots and simultaneous authorization for multiple users. Experimental results show it reduces on-chain transactions and Gas consumption, improving scalability and resource allocation efficiency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] ELANA: A Simple Energy and Latency Analyzer for LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [profiling, latency analysis, energy measurement, key-value cache, quantization, model compression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hung-Yueh Chiang, Bokun Wang, Diana Marculescu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09946",children:"https://arxiv.org/pdf/2512.09946"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces ELANA, a lightweight profiling tool for benchmarking the latency and energy consumption of large language models (LLMs) during inference. It measures metrics like time-to-first-token, time-per-output-token, and joules-per-token across various hardware platforms. The tool is designed to be academic-friendly and compatible with Hugging Face models, addressing the need for a unified framework to evaluate LLM efficiency for deployment and research."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, MILP, dynamic scheduling, GPU scheduling, heterogeneous clusters]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shruti Dongare, Redwan Ibne Seraj Khan, Hadeel Albahar, Nannan Zhao, Diego Melendez Maita, Ali R. Butt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech, Kuwait University, Northwestern Polytechnical University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10271",children:"https://arxiv.org/pdf/2512.10271"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents RLTune, a hybrid scheduling framework that uses reinforcement learning for job prioritization and MILP for job-to-node mapping to manage DL workloads on heterogeneous GPU clusters. It is trained on production traces and does not require per-job profiling. The method improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens job completion time by up to 70%."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cryptography], [zk-SNARKs, zk-STARKs, Groth16, zero-knowledge proofs, trusted setup, post-quantum security, proof generation, verification latency, proof size, CPU profiling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ayush Nainwal, Atharva Kamble, Nitin Awathare"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology, Jodhpur"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10020",children:"https://arxiv.org/pdf/2512.10020"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an empirical, implementation-level comparison of the zk-SNARK (specifically Groth16) and zk-STARK proof systems on a consumer ARM platform. The main conclusion is that zk-SNARKs offer significantly faster proof generation and smaller proof sizes, while zk-STARKs provide faster verification, transparency (no trusted setup), and post-quantum security, with the performance trade-offs heavily influenced by their underlying execution models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Apache Spark, PyTorch, XGBoost, TF-IDF, Multi-Layer Perceptron, Batch Normalization, Dropout, Adam optimization, Grid Search]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Julian Rodriguez, Piotr Lopez, Emiliano Lerma, Rafael Medrano, Jacobo Hernandez"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universidad de Guanajuato"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10312",children:"https://arxiv.org/pdf/2512.10312"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper implements a series of big data analyses using machine learning and deep learning models, including an MLP, text classification, and an XGBoost regressor, across local and distributed computing environments. The core method involves processing high-dimensional datasets (Epsilon, RestMex, IMDb) with techniques like GPU acceleration and Apache Spark for distributed computing. The main conclusion is that the progression from binary classification to sentiment analysis and continuous prediction demonstrates a versatile and robust framework for handling different data scales and types in big data projects."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [DMA offloading, finer-grain overlap, FiCCO, compute-communication overlap, shard-level parallelism, execution schedules, heuristics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shagnik Pal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam, Lizy K. John"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Advanced Micro Devices Inc., The University of Texas at Austin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10236",children:"https://arxiv.org/pdf/2512.10236"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes FiCCO, a method for finer-grain compute-communication overlap that decomposes operations beyond the shard level to unlock performance gains in distributed ML training. It explores a design space of execution schedules, characterizes inefficiency trade-offs, and develops heuristics to select optimal schedules, while offloading communication to GPU DMA engines to reduce contention. The approach achieves up to 1.6x speedup in realistic deployments, with heuristics providing accurate guidance in 81% of unseen scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Decentralized Oracle Network (DON), Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC), Proof of Cooperative Learning (PoCL) consensus]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinyu Chen, Long Shi, Taotao Wang, Jiaheng Wang, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University of Science and Technology, Shenzhen University, Southeast University, Purple Mountain Laboratories, The University of New South Wales"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.09961",children:"https://arxiv.org/pdf/2512.09961"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes TDC-Cache, a two-layer framework for Web3.0 that uses a Decentralized Oracle Network (DON) for trusted caching and a Deep Reinforcement Learning (DRL) approach to dynamically optimize caching strategies, alongside a Proof of Cooperative Learning (PoCL) consensus for decision consistency. The experimental results show that the framework reduces average access latency by 20%, increases cache hit rate by up to 18%, and improves consensus success rate by 10% compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud security], [co-location attack, micro-architectural side-channel, serverless scheduling, vulnerability assessment, mitigation strategy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wei Shao, Najmeh Nazari, Behnam Omidi, Setareh Rafatirad, Houman Homayoun, Khaled N. Khasawneh, Chongzhou Fang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Davis, George Mason University, Rochester Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10361",children:"https://arxiv.org/pdf/2512.10361"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a methodology to uncover vulnerabilities in serverless cloud schedulers and demonstrates how to construct co-location attacks through normal user interfaces. It successfully achieves instance co-location on open-source infrastructures and Microsoft Azure Functions, highlighting critical security weaknesses. The work also proposes a mitigation strategy to defend against such attacks in serverless computing environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Clustered Federated Learning with Hierarchical Knowledge Distillation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [clustered federated learning, hierarchical knowledge distillation, multi-teacher knowledge distillation, bi-level aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sabtain Ahmad, Meerzhan Kanatbekova, Ivona Brandic, Atakan Aral"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Wien, University of Vienna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10443",children:"https://arxiv.org/pdf/2512.10443"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes CFLHKD, a method that enhances Clustered Federated Learning by integrating hierarchical knowledge distillation and bi-level aggregation to enable inter-cluster knowledge sharing. It demonstrates that this approach outperforms existing baselines, achieving significant improvements in both cluster-specific and global model accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed storage systems], [Cascaded Parity LRCs, wide stripes, erasure coding, fault tolerance, repair algorithms]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fan Yu, Guodong Li, Si Wu, Weijun Fang, Sihuang Hu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shandong University, Quan Cheng Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10425",children:"https://arxiv.org/pdf/2512.10425"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Cascaded Parity LRCs (CP-LRCs), a new family of erasure codes for wide stripes that embeds structured dependencies between local and global parity blocks to enable cooperative repair. This design reduces repair bandwidth and time for both single-node and multi-node failures while preserving high reliability. Evaluations on Alibaba Cloud show repair time reductions of up to 41% for single-node failures and 26% for two-node failures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [federated learning, blockchain, smart contracts, Byzantine robustness, incentive compatibility, game-theoretic analysis, YODA protocol, Corrected OSMD]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yash Srivastava, Shalin Jain, Sneha Awathare, Nitin Awathare"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology Jodhpur, Eastern University Pennsylvania"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10372",children:"https://arxiv.org/pdf/2512.10372"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes D2M, a decentralized data marketplace that integrates federated learning, blockchain smart contracts for auctions and arbitration, and an off-chain compute network (CONE) to enable privacy-preserving collaborative learning. It introduces mechanisms like a modified YODA protocol and Corrected OSMD to ensure Byzantine robustness and incentive compatibility. The evaluation shows the framework maintains high model accuracy under adversarial conditions and scales efficiently, providing a practical solution for secure data sharing."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [offloading, latent-cache management, sparse attention, batch size scaling, pd disaggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinhang Chen, Chao Zhang, Jiahuan He, Wei Liu, Jianming Zhang, Wenlong Zhou, Xiao Li, Pai Zeng, Shiyong Li, Yuanpan Qian, Dong Li, Zhaogeng Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Baidu Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10576",children:"https://arxiv.org/pdf/2512.10576"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ESS, an offload-centric system architecture that selectively moves the Latent-Cache from GPU to CPU memory to free up GPU resources for DeepSeek-V3.2-Exp. This decouples batch-size scaling from GPU memory constraints, significantly improving decode-stage throughput. High-fidelity simulations show throughput improvements of 69.4% at 32K context length and up to 123% at 128K."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain interoperability], [multi-bridge architecture, Proof of Success and Reward Distribution (PSCRD), Gini index, Nakamoto coefficient]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Damilare Peter Oyinloye, Mohd Sameen Chishti, Jingyue Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," NTNU (Norwegian University of Science and Technology)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10667",children:"https://arxiv.org/pdf/2512.10667"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes the Proof of Success and Reward Distribution (PSCRD) protocol, a novel incentive mechanism for coordinating multiple bridges in cross-chain communication to enhance decentralization and security. The protocol uses a fair reward distribution system to incentivize honest bridge participation. Mathematical analysis and simulations using the Gini index and Nakamoto coefficient demonstrate that PSCRD significantly improves fairness and decentralization without substantially increasing user costs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [differential privacy, laplace noise, gaussian noise, hybrid noise mechanism, blockchain, edge computing, k-means, logistic regression, random forest, naive bayes]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," N Mangala, Murtaza Rangwala, S Aishwarya, B Eswara Reddy, Rajkumar Buyya, KR Venugopal, SS Iyengar, LM Patnaik"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," JNTU Anantapur, University of Melbourne, UVCE Bangalore, Florida International University, National Institute of Advanced Studies Bangalore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10426",children:"https://arxiv.org/pdf/2512.10426"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a multi-layer IoT-Edge-Cloud architecture for healthcare, integrating a differential privacy framework with machine learning models to protect patient data. It introduces a hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation, which provides a balanced privacy-utility trade-off, reducing inference attacks and data reconstruction correlation while maintaining model accuracy. The hierarchical architecture, secured with blockchain, significantly reduces latency for emergency responses, validating its effectiveness for time-critical healthcare operations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251212] TriHaRd: Higher Resilience for TEE Trusted Time"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [trusted execution environments], [TriHaRd, trusted time protocol, Byzantine-resilient clock updates, consistency checks, Intel SGX, TEE]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Matthieu Bettinger, Sonia Ben Mokhtar, Pascal Felber, Etienne Rivi\xe8re, Valerio Schiavoni, Anthony Simonet-Boulogne"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INSA Lyon, CNRS, Universit\xe9 Claude Bernard Lyon 1, University of Neuch\xe2tel, UCLouvain, iExec Blockchain Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10732",children:"https://arxiv.org/pdf/2512.10732"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes TriHaRd, a new trusted time protocol for Trusted Execution Environments (TEEs) that uses Byzantine-resilient clock updates and consistency checks to achieve higher resilience against clock speed and offset manipulations by a malicious host. It empirically demonstrates that TriHaRd mitigates known attacks against the previous state-of-the-art protocol, Triad."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10099",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Latent Action World Models for Control with Unlabeled Trajectories ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10016",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] An exploration for higher efficiency in multi objective optimisation with reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10208",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10042",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10341",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10414",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10492",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10510",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10534",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Multi-Objective Reward and Preference Optimization: Theory and Algorithms ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10601",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10691",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10698",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10739",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10756",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10835",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Iterative Compositional Data Generation for Robot Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10891",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10925",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10934",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10949",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Robust Gradient Descent via Heavy-Ball Momentum with Predictive Extrapolation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10033",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10058",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10394",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10534",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10932",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251212] Bidirectional Normalizing Flow: From Data to Noise and Back ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.10953",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,t.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>a,x:()=>o});var r=n(6540);const s={},t=r.createContext(s);function a(i){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:a(i.components),r.createElement(t.Provider,{value:e},i.children)}}}]);