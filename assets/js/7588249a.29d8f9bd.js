"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1699],{11542:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_MM/20260105-20260111","title":"20260105-20260111 (cs.MM)","description":"2026-01-05","source":"@site/docs/daily/cs_MM/20260105-20260111.md","sourceDirName":"daily/cs_MM","slug":"/daily/csmm/20260105-20260111","permalink":"/ai_toutiao/daily/csmm/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/csmm/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.MM)","permalink":"/ai_toutiao/daily/csmm/20251229-20260104"},"next":{"title":"cs.MS","permalink":"/ai_toutiao/daily/csms"}}');var t=i(74848),a=i(28453);const r={slug:"/daily/csmm/20260105-20260111"},o="20260105-20260111 (cs.MM)",l={},c=[{value:"2026-01-05",id:"2026-01-05",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"20260105-20260111-csmm",children:"20260105-20260111 (cs.MM)"})}),"\n",(0,t.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Qifu Technology, Fudan University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00150",children:"https://arxiv.org/pdf/2601.00150"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u91d1\u878d\u4fe1\u8d37\u9886\u57df\u4e13\u7528\u591a\u6a21\u6001\u57fa\u51c6/Lack of domain-specific multimodal benchmark for financial credit]\n    C --\x3e C1[\u6784\u5efa\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210-\u91c7\u96c6\u7ba1\u9053/Build privacy-compliant synthesis-capture pipeline]\n    C --\x3e C2[\u8bbe\u8ba1\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]\n    D --\x3e D1[\u8bc4\u4f3023\u4e2aVLM/Evaluate 23 VLMs]\n    D --\x3e D2[Qfin-VL-Instruct\u6027\u80fd\u6700\u4f73/Qfin-VL-Instruct achieves top score]\n    D --\x3e D3[\u9c81\u68d2\u6027\u6311\u6218/Robustness remains a challenge]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Timed text extraction from Taiwanese Kua-\xe1-h\xec TV series"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [other], [music information retrieval], [OCR, Speech and Music Activity Detection, subtitle extraction, traditional Chinese, archival video processing]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Tzu-Hung Huang, Yun-En Tsai, Yun-Ning Hung, Chih-Wei Wu, I-Chieh Wei, Li Su"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Academia Sinica, National Taiwan University, Music AI, University of Auckland"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00299",children:"https://arxiv.org/pdf/2601.00299"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/z-huang/ocr-subtitle-editor",children:"https://github.com/z-huang/ocr-subtitle-editor"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Developed an interactive system for real-time OCR correction to handle low-quality archival video subtitles. 2. Proposed a two-step workflow integrating OCR-driven segmentation with Speech and Music Activity Detection (SMAD) to identify vocal segments. 3. Created a dataset of vocal segments and corresponding lyrics from Taiwanese opera TV series to support MIR tasks."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9645950400de9f4b82b5447edc80e161ca3823766faec9633bb49af2f67eccfb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9645950400de9f4b82b5447edc80e161ca3823766faec9633bb49af2f67eccfb_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of extracting timed text and lyrics from low-quality archival Taiwanese opera TV series. The authors propose a method combining an interactive OCR correction tool with a two-step segmentation approach using OCR and SMAD to efficiently create a dataset of vocal segments and lyrics. The resulting dataset is intended to support Music Information Retrieval tasks like lyrics identification and tune retrieval."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[Timed text extraction from Taiwanese Kua-\xe1-h\xec TV series] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f4e\u8d28\u91cf\u6863\u6848\u89c6\u9891\u5b57\u5e55\u63d0\u53d6\u56f0\u96be/Low-quality archival video subtitle extraction is difficult]\n    C --\x3e C1[\u4ea4\u4e92\u5f0fOCR\u5b9e\u65f6\u6821\u6b63/Interactive real-time OCR correction]\n    C --\x3e C2[OCR\u5206\u5272\u4e0e\u8bed\u97f3\u97f3\u4e50\u6d3b\u52a8\u68c0\u6d4b\u4e24\u6b65\u6cd5/Two-step OCR segmentation & SMAD]\n    D --\x3e D1[\u521b\u5efa\u5e26\u6b4c\u8bcd\u7684\u4eba\u58f0\u7247\u6bb5\u6570\u636e\u96c6/Created dataset of vocal segments with lyrics]\n    D --\x3e D2[\u652f\u6301\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1/Supports MIR tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI) / Computer-Supported Cooperative Work (CSCW)], [Mixed Reality, Digital Audio Workstation, Collaborative Looping, Musical Metaverse, Speculative Design]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Torin Hopkins, Shih-Yu Ma, Suibi Che-Chuan Weng, Ming-Yuan Pai, Ellen Yi-Luen Do, Luca Turchet"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, University of Trento, SolJAMM Research"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00326",children:"https://arxiv.org/pdf/2601.00326"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Proposed and developed MR-DAW, a novel Mixed Reality system enabling multiple remote users to control a single, shared DAW instance while moving freely in their physical space. 2. Introduced a hands-free, collaborative interaction paradigm using physical foot pedals for remote, real-time looping control within the shared virtual session. 3. Conducted a qualitative study with 20 musicians to analyze current DAW practices, evaluate the MR-DAW system\'s usability, and provide a speculative outlook on the future of collaborative music-making in the "Musical Metaverse".']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aca28144427dc6983d2e29776070b060b46281ee589677349257b7b925ad500c_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates using Mixed Reality (MR) to overcome the limitations of traditional Digital Audio Workstations (DAWs), which tether musicians to a desk and hinder remote collaboration. The authors propose MR-DAW, a networked MR system that allows geographically dispersed musicians to control a shared DAW and use foot pedals for collaborative looping. The study with 20 musicians highlights MR's potential for unencumbered musical interaction and provides a speculative vision for future remote collaborative DAWs in the Musical Metaverse."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    Root[MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[DAWs\u675f\u7f1a\u97f3\u4e50\u5bb6\u5de5\u4f5c\u6d41/DAWs encumber musician workflow]\n    Problem --\x3e P2[\u8fdc\u7a0b\u534f\u4f5c\u56f0\u96be/Remote collaboration is challenging]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f00\u53d1MR-DAW\u8bbe\u8ba1\u63a2\u9488/Developed MR-DAW design probe]\n    Method --\x3e M2[\u4f7f\u7528\u811a\u8e0f\u677f\u8fdb\u884c\u534f\u4f5c\u5faa\u73af/Used foot pedal for collaborative looping]\n    Method --\x3e M3[\u5b9a\u6027\u7814\u7a76\u4e0e\u7cfb\u7edf\u8bc4\u4f30/Qualitative study & system evaluation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[MR\u652f\u6301\u65e0\u675f\u7f1a\u4ea4\u4e92/MR affords unencumbered interaction]\n    Results --\x3e R2[\u5c55\u671b\u97f3\u4e50\u5143\u5b87\u5b99\u672a\u6765/Speculative outlook on Musical Metaverse]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Effects of Limited Field of View on Musical Collaboration Experience with Avatars in Extended Reality"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [other], [human-computer interaction], [extended reality, field of view, musical collaboration, co-presence, gesture recognition]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Suibi Che-Chuan Weng, Torin Hopkins, Shih-Yu Ma, Amy Banic, Ellen Yi-Luen Do"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder, University of Wyoming, SolJAMM Research"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00333",children:"https://arxiv.org/pdf/2601.00333"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Conducted a comparative study investigating the specific impact of a limited field of view (FOV) on key aspects of musical collaboration in XR, such as co-presence and reaction time. 2. Proposed and evaluated a novel notification system ("Mini Musicians") designed to mitigate the negative effects of a limited FOV in AR-based musical collaboration. 3. Provided empirical evidence that while limited FOV degrades the collaborative experience, interface interventions like notifications can improve performance metrics like reaction time.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b86b1bc6e18987027d6a96d6343c561f4f8304599001862d66c5108a3cb6f3ad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b86b1bc6e18987027d6a96d6343c561f4f8304599001862d66c5108a3cb6f3ad_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies how the limited field of view (FOV) in XR head-mounted displays affects musical collaboration. It compares an unrestricted holographic setup (HoloJam) with a limited-FOV AR setup and tests a notification system (Mini Musicians) to improve awareness. The results show that limited FOV reduces co-presence and enjoyment, but notification systems can help improve reaction times."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Effects of Limited FOV on Musical Collaboration in XR<br/>XR\u4e2d\u6709\u9650\u89c6\u573a\u5bf9\u97f3\u4e50\u534f\u4f5c\u7684\u5f71\u54cd] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Limited FOV in XR disrupts visual cues for musicians<br/>XR\u4e2d\u7684\u6709\u9650\u89c6\u573a\u5e72\u6270\u4e86\u97f3\u4e50\u5bb6\u7684\u89c6\u89c9\u7ebf\u7d22]\n    C --\x3e C1[Compared HoloJam (unrestricted FOV) vs. AR glasses (52\xb0 FOV)<br/>\u6bd4\u8f83HoloJam(\u65e0\u9650\u5236\u89c6\u573a)\u4e0eAR\u773c\u955c(52\xb0\u89c6\u573a)]\n    C --\x3e C2[Tested AR notification system "Mini Musicians"<br/>\u6d4b\u8bd5AR\u901a\u77e5\u7cfb\u7edf"Mini Musicians"]\n    D --\x3e D1[HoloJam: higher co-presence & enjoyment<br/>HoloJam: \u66f4\u9ad8\u7684\u5171\u5728\u611f\u4e0e\u6109\u60a6\u5ea6]\n    D --\x3e D2[Mini Musicians: reduced reaction time<br/>Mini Musicians: \u964d\u4f4e\u4e86\u53cd\u5e94\u65f6\u95f4]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," KAIST, NTU Singapore, DeepAuto.ai"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00664",children:"https://arxiv.org/pdf/2601.00664"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://taekyungki.github.io/AvatarForcing",children:"https://taekyungki.github.io/AvatarForcing"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u771f\u6b63\u4e92\u52a8/Lacks truly interactive communication]\n    Problem --\x3e P2[\u5355\u5411\u53cd\u5e94\u7f3a\u4e4f\u60c5\u611f/One-way responses lack emotional engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u9a71\u52a8\u6846\u67b6/Diffusion forcing framework]\n    Method --\x3e M2[\u65e0\u6807\u7b7e\u76f4\u63a5\u504f\u597d\u4f18\u5316/Label-free direct preference optimization]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u4ea4\u4e92/Low-latency real-time interaction (~500ms)]\n    Results --\x3e R2[6.8\u500d\u52a0\u901f/6.8x speedup]\n    Results --\x3e R3[80%\u7528\u6237\u504f\u597d/Over 80% user preference]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);