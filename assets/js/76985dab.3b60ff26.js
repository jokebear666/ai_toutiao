"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8025],{2091:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251201-20251207","title":"20251201-20251207","description":"2025-12-01","source":"@site/docs/daily/20251201-20251207.md","sourceDirName":"daily","slug":"/daily/20251201-20251207","permalink":"/ai_toutiao/daily/20251201-20251207","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766477410000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251124-20251130","permalink":"/ai_toutiao/daily/20251124-20251130"},"next":{"title":"20251208-20251214","permalink":"/ai_toutiao/daily/20251208-20251214"}}');var s=n(4848),a=n(8453);const t={},o="20251201-20251207",l={},c=[{value:"2025-12-01",id:"2025-12-01",level:2},{value:"2025-12-02",id:"2025-12-02",level:2},{value:"2025-12-03",id:"2025-12-03",level:2},{value:"2025-12-04",id:"2025-12-04",level:2},{value:"2025-12-05",id:"2025-12-05",level:2}];function h(i){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251201-20251207",children:"20251201-20251207"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-01",children:"2025-12-01"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 23"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] An Empirical Study of Cross-Language Interoperability in Replicated Data Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [replicated data libraries, foreign-function interface, common data format, cross-language interoperability, empirical study]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Provakar Mondal, Eli Tilevich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22010",children:"https://arxiv.org/pdf/2511.22010"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper empirically compares two strategies for cross-language interoperability in replicated data systems: foreign-function interface (FFI) and common data format (CDF). The study found that CDF-based integration provides better software quality, lower latency, reduced memory consumption, and higher throughput. The authors validated their findings by implementing a CDF-based replicated data library that supports mixed language environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [prefix-aware attention, multi-tile kernel, KV cache optimization, pack-forward-merge, vLLM integration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tianjin University, Stevens Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22333",children:"https://arxiv.org/pdf/2511.22333"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," PAT introduces a prefix-aware attention kernel that organizes execution using a pack-forward-merge paradigm to reduce redundant KV cache loading. It employs multi-tile kernels and query packing to optimize resource utilization during LLM decoding. Evaluation shows PAT reduces attention latency by 67.4% on average and improves throughput compared to state-of-the-art attention kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [latency-disaggregated architecture, bottleneck-based scheduler, Roofline-based performance model, fast preemption mechanism, Prefill/Decode disaggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Siyu Wu, Zihan Tang, Yuting Zeng, Hui Chen, Guiguang Ding, Tongxuan Liu, Ke Zhang, Hailong Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University, Tsinghua University, University of Science and Technology of China, JD Company"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21862",children:"https://arxiv.org/pdf/2511.21862"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a latency-disaggregated architecture that separates cluster resources into latency-strict and latency-relaxed pools for co-locating online and offline LLM workloads. It introduces a bottleneck-based scheduler with Roofline modeling and fast preemption mechanism to maintain online SLOs while improving resource utilization. Experiments show the method achieves up to 3\xd7 higher offline throughput while preserving online performance compared to existing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, weighted momentum, sharpness-aware minimization, non-IID convergence, cosine-similarity adaptive rule]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tianle Li, Yongzhi Huang, Linshan Jiang, Chang Liu, Qipeng Xie, Wenfeng Du, Lu Wang, Kaishun Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shenzhen University, The Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Nanyang Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22080",children:"https://arxiv.org/pdf/2511.22080"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FedWMSAM, a federated learning method that combines weighted momentum and sharpness-aware minimization to address local-global curvature misalignment and momentum-echo oscillation. It introduces momentum-guided global perturbation and a two-phase training schedule to improve optimization. Experimental results demonstrate the method's effectiveness in achieving fast convergence and robust generalization across non-IID data distributions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed consensus], [epistemic logic, knowledge-based programs, simultaneous agreement, crash failures, limited information exchange]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kaya Alpturer, Ron van der Meyden, Sushmita Ruj, Godfrey Wong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Princeton University, UNSW Sydney"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22380",children:"https://arxiv.org/pdf/2511.22380"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops optimal fault-tolerant simultaneous consensus protocols using epistemic logic and knowledge-based programming with limited information exchange. The authors introduce a new information exchange approach that achieves decisions at most one round later than the optimal Dwork-Moses protocol while reducing computation cost and space requirements. They derive protocols that are optimal for various limited information exchanges from the literature, including FloodSet variants and failure-counting approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [biophotonics simulation], [ray-tracing, GPU acceleration, Monte Carlo method, hardware RT-cores, OptiX platform]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shijie Yan, Douglas Dwyer, David R. Kaeli, Qianqian Fang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Northeastern University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22779",children:"https://arxiv.org/pdf/2511.22779"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces RT-MMC, a mesh-based Monte Carlo method that leverages modern GPU ray-tracing hardware for accelerated light transport simulations. By using NVIDIA's OptiX platform and RT-cores, the approach eliminates complex mesh generation while achieving 1.5x to 4.5x speed improvements over traditional methods. The hardware-based ray-tracing significantly simplifies simulation workflows and enhances practicality for biophotonics applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [Heard-Of model, asynchronous message-passing, model equivalence, colorless tasks, colored tasks, crash failures, message omissions, silencing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hagit Attiya, Armando Casta\xf1eda, Dhrubajyoti Ghosh, Thomas Nowak"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technion \u2013 Israel Institute of Technology, Instituto de Matem\xe1ticas, Universidad Nacional Aut\xf3noma de M\xe9xico, Universit\xe9 Paris-Saclay, CNRS, ENS Paris-Saclay, Laboratoire M\xe9thodes Formelles, Institut Universitaire de France"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21859",children:"https://arxiv.org/pdf/2511.21859"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper analyzes the relationship between the asynchronous message-passing model (AMP_f) and the Heard-Of model (HO_f) through bidirectional simulations and an intermediate model. It concludes that the models are equivalent for solving colorless tasks when n > 2f, but for colored tasks, equivalence holds only for f=1 due to the issue of silenced processes in HO_f."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Bayesian optimization, deep learning, active learning, design space exploration, numerical simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ahmad Tarraf, Koutaiba Kassem-Manthey, Seyed Ali Mohammadi, Philipp Martin, Lukas Moj, Semih Burak, Enju Park, Christian Terboven, Felix Wolf"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Darmstadt, GNS Gesellschaft f\xfcr numerische Simulation mbH, RWTH Aachen University, GNS Systems GmbH"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22302",children:"https://arxiv.org/pdf/2511.22302"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an AI-assisted workflow that combines deep learning and Bayesian optimization to automate the tuning of design parameters in sheet metal forming simulations. The method reduces expert involvement and accelerates design space exploration by using a deep learning model for initial parameter estimation followed by iterative refinement. The approach demonstrates significant potential to shorten design iterations and lower computational costs in simulation-driven industrial processes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Areon: Latency-Friendly and Resilient Multi-Proposer Consensus"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [directed acyclic graph, proof-of-stake, multi-proposer, fork choice rule, VRF-based eligibility, sliding window]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," \xc1lvaro Castro-Castilla, Marcin Pawlowski, Hong-Sheng Zhou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nomos Institute of Free Technology, Jagiellonian University, Virginia Commonwealth University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23025",children:"https://arxiv.org/pdf/2511.23025"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Areon introduces a multi-proposer consensus protocol that organizes blocks into a directed acyclic graph (DAG) with a sliding window reference mechanism and weighted fork choice rule. The protocol achieves bounded-latency finality with lower reorganization frequency compared to chain-based baselines. Experimental results show consistent performance improvements across various adversarial conditions and network delays."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing], [retry policy, distributed framework, analytical model, microservices, auto-scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tel Aviv University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23278",children:"https://arxiv.org/pdf/2511.23278"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces RetryGuard, a distributed framework that uses an analytical model to manage retry policies across microservices and prevent retry storms. Experimental results show RetryGuard significantly reduces resource usage and costs compared to AWS retry policies in both standard and complex Kubernetes deployments with Istio service mesh."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [network security], [history covert channels, timing patterns, relative pointers, covert amplification factor, silent history protocol]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Christoph Weissenborn, Steffen Wendzel"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal Office for Information Security, Ulm University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22259",children:"https://arxiv.org/pdf/2511.22259"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel covert communication method called History Covert Channels (HCC), which embeds hidden messages by using relative pointers to past network timing patterns instead of directly manipulating traffic. This approach reduces reliance on centralized timekeeping and aims to evade standard detection tools. The authors' experiments demonstrate that their method achieves a higher bitrate compared to prior work."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Closing the Generalization Gap in Parameter-efficient Federated Edge Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [model pruning, client selection, joint resource management, generalization analysis, alternating optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong (Shenzhen), KTH Royal Institute of Technology, Shenzhen University, University of Science and Technology of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23282",children:"https://arxiv.org/pdf/2511.23282"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a parameter-efficient federated edge learning framework that jointly optimizes model pruning and client selection with communication-computation resources. The method formulates a generalization-aware optimization problem solved via alternating optimization. Experiments show the approach achieves superior learning performance compared to state-of-the-art baselines by coupling generalization analysis with system-level optimization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [content-oblivious communication, leader election, topology knowledge, graph symmetry, message complexity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi-Jun Chang, Lyuting Chen, Haoran Zhou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23297",children:"https://arxiv.org/pdf/2511.23297"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies leader election in content-oblivious networks where nodes can only send asynchronous, content-less pulses. It shows that with topology knowledge, leader election is possible in many non-2-edge-connected graphs like asymmetric trees, but impossible in graphs symmetric about an edge or when topology knowledge is insufficient. The work provides both impossibility results and algorithms with specific message complexity bounds for different graph classes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed ledger technology], [ZipperChain, atomic broadcast, trustless, third-party services, distributed consensus, immutability, agreement, availability, pipeline, fast data center network]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BLOCKY, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21969",children:"https://arxiv.org/pdf/2511.21969"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ZipperChain is a blockchain that replaces distributed consensus with a pipeline of specialized services on a small number of nodes, transferring trust from established third-party services to provide correctness guarantees. This approach enables high transaction throughput near network line speeds and block finality around 500 ms, without needing a native token for incentives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [storage systems], [Clock2Q+, cache replacement algorithm, metadata cache, correlated references, S3-FIFO, three queues, correlation window]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Carnegie Mellon University, Bandung Institute of Technology, Broadcom Inc., Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21958",children:"https://arxiv.org/pdf/2511.21958"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Clock2Q+, a cache replacement algorithm designed for metadata caches that uses three queues and a correlation window to mitigate the negative impact of correlated references. It demonstrates superior performance, achieving up to a 28.5% lower miss ratio than S3-FIFO on metadata traces, while maintaining low overhead and ease of implementation for large-scale storage systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [blockchain, proof-of-work, trust rating, Bayesian-time-decay, high-performance cluster computing, reward incentive]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Murat Yaslioglu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Istanbul University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21844",children:"https://arxiv.org/pdf/2511.21844"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel blockchain-based framework that integrates high-performance cluster computing with AI, using an evolved proof-of-work consensus linked to computational effort and a dynamic Bayesian-time-decay trust rating for node selection. This mechanism aims to optimize resource use, incentivize broad participation, and create a merit-based system for block generation. The main conclusion is that this approach fosters a more sustainable, equitable, and energy-efficient environment for AI development by balancing computational power with inclusivity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture-of-Experts scheduling, sparse attention acceleration, prefill-decode disaggregation, KV-cache reuse, continuous batching, load-aware scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Technologies Co., Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22481",children:"https://arxiv.org/pdf/2511.22481"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," OmniInfer is a system-level acceleration framework built on vLLM that integrates three components\u2014OmniPlacement, OmniAttn, and OmniProxy\u2014to optimize LLM serving through expert placement, sparse attention, and disaggregation-aware scheduling. It achieves performance gains by adaptively disaggregating resources, exploiting sparsity, and coordinating prefill and decode phases. Evaluated on a 10-node cluster, it significantly reduces time-per-output-token and time-to-first-token while increasing query throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] DisCEdge: Distributed Context Management for Large Language Models at the Edge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [distributed context management, tokenization, geo-distributed storage, edge computing, data replication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohammadreza Malekabbasi, Minghe Wang, David Bermbach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Berlin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22599",children:"https://arxiv.org/pdf/2511.22599"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes DisCEdge, a system for managing LLM user context by storing and replicating it in tokenized form across distributed edge nodes. This approach reduces redundant computation and enables efficient synchronization. The evaluation shows it improves response times, lowers synchronization overhead, and significantly reduces client request sizes compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, aggregation methods, personalization, optimization, robustness, heterogeneity, privacy-preserving, IID, non-IID]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Meriem Arbaoui, Mohamed-el-Amine Brahmia, Abdellatif Rahmoun, Mourad Zghal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," LabRi-SBA Laboratory, CESI LINEACT UR 7527"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22616",children:"https://arxiv.org/pdf/2511.22616"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This survey paper provides a multi-level taxonomy of Federated Learning (FL) aggregation techniques, combining bibliometric analysis and systematic review to classify research in personalization, optimization, and robustness. It concludes by comparing aggregation methods under IID and non-IID data distributions and outlines future research directions to advance the field."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [LoRA, dynamic adapter placement, GPU Direct RDMA, multi-tenant serving, rank heterogeneity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor R\xfchle, Josep Torrellas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Microsoft, National Technical University of Athens"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22880",children:"https://arxiv.org/pdf/2511.22880"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents LoRAServe, a framework that dynamically places and routes heterogeneous LoRA adapters across GPUs to address performance skew in multi-tenant LLM inference. It uses workload-aware rebalancing and GPU Direct RDMA for remote access to improve resource utilization. The evaluation shows LoRAServe achieves higher throughput and lower latency while using fewer GPUs compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [split learning, pipeline parallelism, wireless edge networks, communication-computation overlap, alternating optimization, micro-batches]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23167",children:"https://arxiv.org/pdf/2511.23167"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes C\xb2P\xb2SL, a method that applies pipeline parallelism to split learning in wireless edge networks to overlap communication and computation processes, thereby reducing training latency. It formulates a joint optimization problem for task split and resource allocation, solved via alternating optimization. Experiments show the method reduces system training time by over 38% while maintaining accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Probabilistic Forward Pass, Bayesian Neural Networks, Gaussian propagation, TVM compiler, code generation, operator tuning, uncertainty estimation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fr\xf6ning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Heidelberg University, Graz University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23440",children:"https://arxiv.org/pdf/2511.23440"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an end-to-end pipeline for deploying Bayesian Neural Networks (BNNs) using a Probabilistic Forward Pass (PFP), which approximates inference by propagating Gaussian distributions through the network in a single deterministic pass. The method is implemented via custom operators in the TVM compiler and optimized for ARM CPUs, achieving speedups of up to 4200x compared to traditional sampling-based methods while maintaining comparable accuracy and uncertainty estimation. The results demonstrate that combining Bayesian approximations with code generation enables efficient BNN deployment on resource-constrained embedded systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph theory and optimization], [lasso, LARS algorithm, ADMM, bi-directional Dijkstra, \u21131-regularized regression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology, University of Washington, University of California, Irvine"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22745",children:"https://arxiv.org/pdf/2511.22745"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formulates the shortest path problem in graphs as an \u21131-regularized regression (lasso). It connects this formulation, solved via the LARS algorithm, to the bi-directional Dijkstra algorithm and highlights the applicability of ADMM for efficient updates to network topology changes."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 33'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21934",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22101",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Factors That Support Grounded Responses in LLM Conversations: A Rapid Review ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21762",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22176",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Representative Action Selection for Large Action Space: From Bandits to MDPs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22104",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22105",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22210",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22226",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22018",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22235",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21726",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21928",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22108",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22076",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22138",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GPS: General Per-Sample Prompter ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21714",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22570",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22715",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ORION: Teaching Language Models to Reason Efficiently in the Language of Thought ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22891",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Switching-time bioprocess control with pulse-width-modulated optogenetics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22893",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Language-conditioned world model improves policy generalization by reading environmental descriptions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22963",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Evolutionary Discovery of Heuristic Policies for Traffic Signal Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23122",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23148",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23158",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23193",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23262",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23315",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23442",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ThetaEvolve: Test-time Learning on Open Problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23473",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22321",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23310",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21759",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ResearchArcade: Graph Interface for Academic Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22036",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21736",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21901",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21848",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Co-Evolving Agents: Learning from Failures as Hard Negatives ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22254",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21920",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22293",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Cacheback: Speculative Decoding With Nothing But Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21699",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22434",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22460",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23203",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23274",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21747",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Automated Statistical and Machine Learning Platform for Biological Research ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21770",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Generative models for crystalline materials ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22652",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Escaping Barren Plateaus in Variational Quantum Algorithms Using Negative Learning Rate in Quantum Internet of Things ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22861",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-02",children:"2025-12-02"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 21"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] An optimization framework for task allocation in the edge/hub/cloud paradigm"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing optimization], [binary integer linear programming, task flow graph, latency optimization, energy optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andreas Kouloumpris, Georgios L. Stavrinides, Maria K. Michael, Theocharis Theocharides"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Cyprus"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00029",children:"https://arxiv.org/pdf/2512.00029"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a binary integer linear programming (BILP) framework for optimal task allocation in edge/hub/cloud architectures, aiming to minimize either latency or energy consumption. The method is evaluated with real-world and synthetic benchmarks, showing it provides optimal and scalable results for design space exploration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [k-core decomposition, parallel computing, shared-memory, Rust, message passing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Davide Rucci, Sebastian Parfeniuc, Matteo Mordacchini, Emanuele Carlini, Alfredo Cuzzocrea, Patrizio Dazzi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ISTI-CNR, University of Pisa, IIT-CNR, University of Calabria"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00233",children:"https://arxiv.org/pdf/2512.00233"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper adapts a distributed k-core decomposition algorithm for shared-memory systems and implements three optimized versions in Rust. The fastest version, FastK, significantly reduces synchronization overhead and outperforms baseline sequential and parallel implementations, achieving up to an 11x speedup on 16 threads and being orders of magnitude faster than a reference Python implementation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, semantic communication, joint source-channel coding, global representation alignment, domain-aware aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Loc X. Nguyen, Ji Su Yoon, Huy Q. Le, Yu Qiao, Avi Deb Raha, Eui-Nam Huh, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Kyung Hee University, Virginia Tech, Nanyang Technological University, University of Houston"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00711",children:"https://arxiv.org/pdf/2512.00711"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel federated learning framework to train semantic communication models across different data domains. It introduces global representation alignment to preserve domain semantics and a domain-aware aggregation method to prevent bias from dominant clients. The approach outperforms existing methods in image reconstruction quality, especially as channel conditions improve."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless networks], [multi-hop clustering, mobility-aware clustering, energy-centric CH selection, GS-assisted cluster maintenance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Basilis Mamalis, Marios Perlitis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of West Attica, Democritus University of Thrace"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00623",children:"https://arxiv.org/pdf/2512.00623"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel multi-hop clustering algorithm for FANETs that constructs stable clusters by selecting cluster heads based on high stability and energy, and employs a ground station-assisted maintenance mechanism. The method aims to enhance cluster longevity and communication efficiency. Experimental results demonstrate that the approach significantly outperforms existing schemes in terms of cluster stability, communication overhead, and security resilience."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [dynamic random walks, rejection sampling, reservoir sampling, runtime adaptation, cost model, compile-time specialization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Seongyeon Park, Jaeyong Song, Changmin Shin, Sukjin Kim, Junguk Hong, Jinho Lee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Seoul National University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00705",children:"https://arxiv.org/pdf/2512.00705"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FlexiWalker is a GPU framework that introduces high-performance kernels for rejection and reservoir sampling to efficiently execute dynamic random walks. It uses a lightweight runtime cost model to select the optimal sampling kernel per node and a compile-time component to specialize user logic. The framework significantly outperforms existing CPU and GPU baselines and can handle workloads prior systems cannot support."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [GPU parallelization, memory management, multi-threaded framework, pipeline parallelism]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bingzheng Xia, Zujie Ren, Kuang Ma, Xiaoqian Li, Wenda Li, Shuibing He"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Chinese Academy of Sciences, Zhejiang Lab, Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00398",children:"https://arxiv.org/pdf/2512.00398"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Heimdall++, an optimized GPU-accelerated tool for single-pulse detection in radio astronomy. It improves upon the original Heimdall by implementing fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple processing stages and reduce GPU stalls. The results show that Heimdall++ achieves up to 2.66x speedup in processing while maintaining result consistency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [multi-objective orchestration, agent-based routing, tiered island groups, typed placeholder sanitization, reversible anonymization, data locality routing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bala Siva Sai Akhil Malepati"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00595",children:"https://arxiv.org/pdf/2512.00595"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper presents IslandRun, a system for distributed AI inference that treats computational resources as autonomous "islands" and uses agent-based routing and reversible anonymization to orchestrate tasks across personal devices, edge servers, and the cloud. Its core method involves policy-constrained multi-objective optimization to balance performance, privacy, cost, and trust. The main conclusion is that this establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous computing ecosystems.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [hyperparameter optimization, genetic algorithms, optuna, apache spark, transformer embeddings, roberta, bayesian target encoding, svm, logistic regression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gonz\xe1lez Trigueros Jes\xfas Eduardo, Alonso S\xe1nchez Alejandro, Mu\xf1oz Rivera Emilio, Pe\xf1ar\xe1n Prieto Mariana Jaqueline, Mendoza Gonz\xe1lez Camila Natalia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universidad de Guanajuato"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00298",children:"https://arxiv.org/pdf/2512.00298"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper compares classification strategies for structured and unstructured big data, using evolutionary/Bayesian hyperparameter optimization for numerical data and distributed Apache Spark processing for text. It finds a "complexity paradox" where optimized linear models outperform complex ones on structured data, while for text, robust feature engineering with Transformer embeddings allows simpler models to generalize better. The work provides a framework for algorithm selection based on data nature and infrastructure.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [cache arbitration, thread throttling, MSHR contention, KV Cache, hybrid simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhongchun Zhou, Chengtao Lai, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00083",children:"https://arxiv.org/pdf/2512.00083"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LLaMCAT, a method that optimizes LLM inference by combining Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to reduce stalls in KV Cache access. It demonstrates significant speedups over baselines, particularly when systems are bottlenecked by miss handling throughput or limited cache size, offering a practical hardware-level solution for accelerating LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cryptographic audit trails], [post-quantum signatures, hash-and-sign, QROM, hybrid signatures, re-signing, Merkle-root anchoring, Q-Audit Integrity, Q-Non-Equivocation, Q-Binding]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leo Kao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Codebat Technologies Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00110",children:"https://arxiv.org/pdf/2512.00110"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formalizes quantum-adversary-resilient security notions for constant-size cryptographic evidence structures used in AI audit trails and analyzes a post-quantum hash-and-sign instantiation. It proposes and evaluates three migration strategies\u2014hybrid signatures, re-signing, and Merkle-root anchoring\u2014for transitioning existing logs. The study concludes that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can extend the evidentiary lifetime of deployments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [functional connectivity, anomaly detection, root cause analysis, microservices, time-varying dependencies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Giles Winchester, George Parisis, Luc Berthouze"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Sussex"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00844",children:"https://arxiv.org/pdf/2512.00844"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FC-ADL, a method for microservice anomaly detection and localization by analyzing time-varying functional connectivity between service metrics. It shows this approach can effectively detect faults and identify root causes while being more scalable than causal inference methods. The method is validated on large-scale real-world deployments like Alibaba's."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [sampling, decision plane, tensor parallelism, pipeline parallelism, sequence-parallel sampling, speculative hot-vocab sampling, CPU-based algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bohan Zhao, Zane Cao, Yongchao He"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Not explicitly stated in the provided text. Author affiliations are not included."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00719",children:"https://arxiv.org/pdf/2512.00719"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SIMPLE, a method that disaggregates the sampling step from GPU inference by moving it to a CPU-side service. This approach uses sequence-parallel sampling and speculative hot-vocab sampling to reduce the bottleneck caused by sampling in distributed LLM serving. The evaluation shows that SIMPLE significantly improves throughput and reduces latency while being compatible with existing data-plane optimizations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Delta Sum Learning: an approach for fast and global convergence in Gossip Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gossip learning, delta sum learning, open application model, decentralized orchestration, federated averaging]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tom Goethals, Merlijn Sebrechts, Stijn De Schrijver, Filip De Turck, Bruno Volckaert"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Ghent University - imec, IDLab"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01549",children:"https://arxiv.org/pdf/2512.01549"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Delta Sum Learning, a new aggregation method to improve global convergence in decentralized Gossip Learning. It also implements the method within a decentralized orchestration framework based on the Open Application Model. Evaluation shows that Delta Sum Learning maintains strong global convergence and scales better than alternatives, with a logarithmic versus linear accuracy loss as the network size increases."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, LoRA, mixture of experts, knowledge reuse, parameter-efficient fine-tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yebo Wu, Jingguang Li, Zhijiang Guo, Li Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Macau, HKUST, HKUST (Guangzhou)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00902",children:"https://arxiv.org/pdf/2512.00902"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SmartFed, a federated fine-tuning framework that reuses knowledge from existing LoRA modules via a Mixture of Rank-Wise Experts (MoRE) and an Elastic Expert Quota Allocation (EEQA) mechanism to reduce computational and communication costs. It demonstrates that this approach significantly improves model performance and training efficiency compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] StarDist: A Code Generator for Distributed Graph Algorithms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed graph processing], [MPI, Remote Memory Access (RMA), code generation, communication aggregation, bulk-reduction substrate]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Barenya Kumar Nandy, Rupesh Nasre"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology, Madras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01646",children:"https://arxiv.org/pdf/2512.01646"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents StarDist, an analysis-transformation framework that optimizes the distributed backend of the StarPlat DSL compiler for graph algorithms. It aggregates communication by reordering neighborhood accesses and uses an optimized bulk-reduction substrate built on Open MPI's passive RMA. The optimized system outperforms d-Galois and DRONE in Single Source Shortest Path computations on large graphs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [GPU memory reuse, unified GPU memory pool, on-demand KV cache allocation, GPU-affinity-aware scheduling, cold-start optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wenbin Zhu, Zhaoyan Shen, Zili Shao, Hongjun Dai, Feng Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shandong University, The Chinese University of Hong Kong, Indiana University Bloomington"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01357",children:"https://arxiv.org/pdf/2512.01357"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Tangram, a system that accelerates serverless LLM loading by reusing GPU memory to retain model parameters, reducing cold-start latency. Its key techniques include a unified memory pool for tensor sharing, dynamic KV cache management, and affinity-aware scheduling. Experiments show Tangram achieves up to 6.2x faster loading and reduces Time-To-First-Token by 23\u201355% compared to state-of-the-art methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, energy harvesting, client scheduling, version age of information, semantics-aware communication, feature-based proxy, non-IID data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Eunjeong Jeong, Giovanni Perin, Howard H. Yang, Nikolaos Pappas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Link\xf6ping University, University of Brescia, Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01983",children:"https://arxiv.org/pdf/2512.01983"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a lightweight client scheduling framework for Energy-Harvesting Federated Learning that uses a feature-based proxy to efficiently estimate the Version Age of Information, a semantics-aware metric. This approach reduces the computational cost of predicting update significance, avoiding redundant local training. Experiments show the method achieves superior learning performance and energy reduction compared to baseline policies under extreme non-IID data and scarce energy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dynamic graph re-partitioning, model-aware capacity profiling, distributed split inference, adaptive orchestration, runtime-resolved placement]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aladin Djuhera, Fernando Koch, Alecio Binotto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Munich, Florida Atlantic University, Carl Zeiss AG"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01039",children:"https://arxiv.org/pdf/2512.01039"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a framework for dynamically partitioning and placing foundation model layers across heterogeneous edge nodes at runtime, formalized as a constrained optimization problem. It aims to adapt to fluctuating network and compute resources by integrating model-aware profiling with reactive graph re-partitioning. The main conclusion is that this approach enables efficient, low-latency inference for large models in volatile edge environments like 6G MEC, overcoming the limitations of static split inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Morphling: Fast, Fused, and Flexible GNN Training at Scale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [domain-specific code synthesis, architecture-aware primitives, sparsity-aware execution, OpenMP, CUDA, MPI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anubhab, Rupesh Nasre"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IIT Madras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01678",children:"https://arxiv.org/pdf/2512.01678"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Morphling, a domain-specific code synthesizer that compiles high-level GNN specifications into optimized, portable implementations for CPUs and GPUs. It uses a library of architecture-aware primitives and a runtime engine to dynamically choose dense or sparse execution paths. The results show that Morphling significantly outperforms existing frameworks in training throughput and memory efficiency on diverse datasets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Trace-based, time-resolved analysis of MPI application performance using standard metrics"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [MPI performance analysis], [trace-based analysis, time-resolved metrics, Paraver, critical path reconstruction, load balance, serialisation efficiency, transfer efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kingshuk Haldar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," High Performance Computing Center Stuttgart (HLRS), University of Stuttgart"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01764",children:"https://arxiv.org/pdf/2512.01764"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a method for analyzing MPI application performance by calculating time-resolved standard metrics (like load balance and transfer efficiency) from execution traces, using fixed or adaptive time windows. It robustly processes Paraver traces to reconstruct critical paths and handle event anomalies. The approach reveals transient performance bottlenecks that are hidden by global, time-aggregated metrics, offering a scalable alternative to full trace visualization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] UNIQ: Communication-Efficient Distributed Quantum Computing via Unified Nonlinear Integer Programming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed quantum computing], [non-linear integer programming, greedy algorithm, JIT (Just-In-Time), EPR pair generation, qubit allocation, network scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hui Zhong, Jiachen Shen, Lei Fan, Xinyue Zhang, Hao Wang, Miao Pan, Zhu Han"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Houston"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00401",children:"https://arxiv.org/pdf/2512.00401"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes UNIQ, a unified optimization framework for distributed quantum computing that integrates qubit allocation, entanglement management, and network scheduling into a single non-linear integer programming model. It uses a greedy algorithm for qubit mapping and a JIT approach for parallel EPR pair generation to minimize communication costs and runtime. The method is shown to substantially outperform existing approaches across diverse circuits and topologies."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 61'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00119",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00545",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00049",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Gradient Inversion in Federated Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00303",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] List Replicable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00553",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00319",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00352",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning Causal States Under Partial Observability and Perturbation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00357",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00351",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00601",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00048",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00079",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00499",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] G-KV: Decoding-Time KV Cache Eviction with Global Attention ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00504",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00709",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00042",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00030",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00249",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00050",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00724",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] AI Agent for Source Finding by SoFiA-2 for SKA-SDC2 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00769",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] What Is Preference Optimization Doing, How and Why? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00778",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00831",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00908",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00915",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00961",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00968",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01046",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Automating the Refinement of Reinforcement Learning Specifications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01047",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01054",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] World Model Robustness via Surprise Recognition ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01119",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Mode-Conditioning Unlocks Superior Test-Time Scaling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach for Joint Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01202",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01224",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] On the Tension Between Optimality and Adversarial Robustness in Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01228",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01282",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01311",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Extending NGU to Multi-Agent RL: A Preliminary Study ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01321",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Directed evolution algorithm drives neural prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01362",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01366",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01374",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Multi-Path Collaborative Reasoning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01485",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01661",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] How Does RL Post-training Induce Skill Composition? A Case Study on Countdown ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01775",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01878",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01882",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Rectifying LLM Thought from Lens of Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01925",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Agentic Policy Optimization via Instruction-Policy Co-Evolution ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01945",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01952",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learned-Rule-Augmented Large Language Model Evaluators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01958",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Forecasting in Offline Reinforcement Learning for Non-stationary Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01987",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01993",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning Sim-to-Real Humanoid Locomotion in 15 Minutes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01996",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Diffusion Model Framework for Maximum Entropy Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02019",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00243",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] How do trout regulate patterns of muscle contraction to optimize propulsive efficiency during steady swimming ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01218",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Formal Verification of Noisy Quantum Reinforcement Learning Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01502",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 29'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00049",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00396",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Comprehensive Survey on Surgical Digital Twin ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00019",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00044",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Rosetta Stone for AI Benchmarks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00193",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00059",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00499",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00055",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Faster Verified Explanations for Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00164",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00722",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00829",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00931",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Diffusion Model in Latent Space for Medical Image Segmentation Task ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01292",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] The Necessity of Imperfection",":Reversing"," Model Collapse via Simulating Cognitive Boundedness ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01354",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01374",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RE-LLM: Integrating Large Language Models into Renewable Energy Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01392",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01463",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01702",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01881",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01888",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02010",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00299",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Building Trustworthy AI for Materials Discovery: From Autonomous Laboratories to Z-scores ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01080",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01565",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Cuffless Blood Pressure Estimation from Six Wearable Sensor Modalities in Multi-Motion-State Scenarios ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01653",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-03",children:"2025-12-03"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 9"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPUDirect Async, vector similarity search, GPU cluster, HNSW, CAGRA, pipelining, overlapping computation and communication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi Liu, Chen Qian"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California Santa Cruz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02278",children:"https://arxiv.org/pdf/2512.02278"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Fantasy, a system for large-scale vector similarity search that pipelines search and data transfer across a GPU cluster using GPUDirect Async. It overlaps computation and network communication to improve throughput and support large query batches for graphs that exceed single-GPU memory. The main conclusion is that this approach significantly enhances search performance for massive vector datasets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [memory disaggregation], [data object level offloading, dual-buffer prefetch, remote memory access, quantitative analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Haoyu Zheng, Shouwei Gao, Jie Ren, Wenqian Dong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Oregon State University, College of William & Mary"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02300",children:"https://arxiv.org/pdf/2512.02300"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," DOLMA is a framework that performs memory disaggregation for HPC applications by intelligently offloading data objects to remote memory and using a dual-buffer design for prefetching. It balances local and remote memory usage to minimize performance impact. The evaluation shows it reduces local memory usage by up to 63% while keeping performance degradation under 16%."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Theoretical analysis of beaconless geocast protocols in 1D"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless networking], [beaconless geocast, routing protocols, mobile ad-hoc networks, wireless sensor networks, probabilistic analysis, network load]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Joachim Gudmundsson, Irina Kostitsyna, Maarten L\xf6ffler, Tobias M\xfcller, Vera Sacrist\xe1n, Rodrigo I. Silveira"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Sydney, Eindhoven University of Technology, Universiteit Utrecht, Groningen University, Universitat Polit\xe8cnica de Catalunya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02663",children:"https://arxiv.org/pdf/2512.02663"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper provides a formal theoretical analysis of six beaconless geocast routing protocols in one-dimensional mobile ad-hoc networks, focusing on the maximum number of messages a node can receive as a measure of network load. The analysis, which includes involved probabilistic methods for some protocols, confirms behaviors previously observed only through simulation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Solutions for Distributed Memory Access Mechanism on HPC Clusters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC and distributed systems], [MPI, Infiniband, Slingshot, RDMA, LD_PRELOAD, Lustre, remote memory access]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jan Meizner, Maciej Malawski"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sano Centre for Computational Medicine, AGH University of Krakow"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02546",children:"https://arxiv.org/pdf/2512.02546"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates mechanisms for remote memory access in HPC clusters, comparing solutions based on shared storage and MPI (over Infiniband and Slingshot) to local access. It proposes using LD_PRELOAD to intercept memory calls for distributed access without kernel modifications. The main finding is that remote access performance, especially when backed by MPI, is similar to local memory access."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] TokenPowerBench: Benchmarking the Power Consumption of LLM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [power consumption measurement, phase-aligned metrics, prefill and decode stages, batch size, context length, parallelism, quantization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chenxu Niu, Wei Zhang, Jie Li, Yongjian Zhao, Tongyang Wang, Xi Wang, Yong Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Texas Tech University, Texas Advanced Computing Center, Southeast University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03024",children:"https://arxiv.org/pdf/2512.03024"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces TokenPowerBench, a benchmark for measuring the power consumption of LLM inference. It provides a configurable interface, a measurement layer for GPU/node/system power, and a pipeline to attribute energy to prefill and decode stages. The authors demonstrate its use on several model series and release it as open-source to help forecast operating expenses and meet sustainability goals."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Sampling on Metric Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [stochastic processes], [Euler-Maruyama discretization, timestep splitting, CUDA kernels, Langevin diffusion, metric graphs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rajat Vadiraj Dwaraknath, Lexing Ying"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02175",children:"https://arxiv.org/pdf/2512.02175"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the first algorithm for simulating Brownian motions and sampling on metric graphs using a timestep splitting Euler-Maruyama discretization of the corresponding stochastic differential equation. It provides theoretical convergence guarantees and demonstrates a highly parallelizable custom CUDA kernel implementation that achieves speedups of up to ~8000x over a PyTorch GPU baseline on simple graphs and ~1500x on a real vascular network model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high performance computing], [FAIR principles, workflow components, metadata, EOSC, HPC infrastructure]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sean R. Wilkinson, Patrick Widener, Sarp Oral, Rafael Ferreira da Silva"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Oak Ridge National Laboratory (OLCF)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02818",children:"https://arxiv.org/pdf/2512.02818"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a component-based model for designing FAIR (Findable, Accessible, Interoperable, Reusable) workflows tailored for High Performance Computing centers, building on the architecture of the European Open Science Cloud. It concludes that HPC centers should foster cross-disciplinary FAIR ecosystems by focusing on making individual workflow components reusable, rather than entire workflows, to maximize long-term value and reduce duplication of effort."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Distributed and Autonomic Minimum Spanning Trees"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed algorithms], [spanning tree, VCube, fault-tolerance, broadcast, autonomic algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Luiz A. Rodrigues, Elias P. Duarte Jr., Luciana Arantes"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Western Parana State University (UNIOESTE), Federal University of Parana (UFPR), Sorbonne Universit\xe9 \u2013 CNRS/LIP6"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02683",children:"https://arxiv.org/pdf/2512.02683"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an autonomic algorithm for building and maintaining a scalable spanning tree in a distributed system, using the VCube virtual topology as a failure detector. The algorithm ensures logarithmic degree and depth, supports dynamic reconstruction under process failures and recoveries, and enables efficient broadcast. Simulation results demonstrate its effectiveness compared to other alternatives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251203] Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [active storage, computing continuum, dataClay, workload offloading, distributed systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alex Barcel\xf3, Sebasti\xe1n A. Cajas Ordo\xf1ez, Jaydeep Samanta, Andr\xe9s L. Su\xe1rez-Cetrulo, Romila Ghosh, Ricardo Sim\xf3n Carbajo, Anna Queralt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Barcelona Supercomputing Center (BSC), Ireland\u2019s Centre for Artificial Intelligence (CeADAR), Universitat Polit\xe8cnica de Catalunya (UPC)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02646",children:"https://arxiv.org/pdf/2512.02646"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a software architecture that uses active storage systems, specifically the dataClay platform, to offload and distribute AI workloads across the computing continuum (IoT, edge, cloud). The method embeds computation into storage to reduce data movement overhead. The results show that this approach significantly improves memory efficiency and training speed while maintaining model accuracy, making distributed AI deployments more scalable and resource-efficient."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02486",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02386",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Synthetic Error Injection Fails to Elicit Self-Correction In Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02389",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02581",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Deep Research: A Systematic Survey ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02038",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02350",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Zero-Shot Instruction Following in RL via Structured LTL Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02633",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02358",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Improved Training Mechanism for Reinforcement Learning via Online Model Selection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02214",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Modelling the Doughnut of social and planetary boundaries with frugal machine learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02200",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02631",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02409",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02810",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02435",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02834",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02835",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02551",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] VACoT: Rethinking Visual Data Augmentation with VLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02361",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 14'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Model Recovery at the Edge under Resource Constraints for Physical AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02283",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02403",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] The Impact of Artificial Intelligence on Enterprise Decision-Making Process ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02048",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03035",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02636",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] CONFIDE: Hallucination Assessment for Reliable Biomolecular Structure Prediction and Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02033",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02337",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02342",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02444",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02812",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02409",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] Fast Gaussian Process Approximations for Autocorrelated Data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02925",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02465",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251203] EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02932",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-04",children:"2025-12-04"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 12"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] A Chronological Analysis of the Evolution of SmartNICs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [network hardware], [SmartNIC, DPU, FPGA, HPC, network offloading, data processing unit, system-on-chip]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Olasupo Ajayi, Ryan Grant"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Queen's University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04054",children:"https://arxiv.org/pdf/2512.04054"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper conducts a chronological analysis of SmartNICs (SNICs) by reviewing 370 articles published between 2010 and 2024. The main conclusion is to provide insights into the evolution, manufacturers, use cases, and application domains of SNICs, which are designed to offload tasks from host CPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [HPC energy efficiency], [energy profiling, Likwid, Nvidia profiling tools, MPI parallelism, synthetic benchmarks, Gromacs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rafael Ravedutti Lucio Machado, Jan Eitzinger, Georg Hager, Gerhard Wellein"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Erlangen National High Performance Computing Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03697",children:"https://arxiv.org/pdf/2512.03697"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes the energy efficiency of HPC systems by evaluating synthetic benchmarks and the Gromacs application on CPU and GPU clusters using profiling tools like Likwid. It identifies challenges and pitfalls in conducting such energy analysis. The study concludes by suggesting best practices for future energy efficiency research in HPC."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [SIMD vectorization, dynamic tuning, molecular dynamics, AutoPas, force calculation, neighbor identification]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Luis Gall, Samuel James Newcome, Fabio Alexander Gratl, Markus M\xfchlh\xe4u\xdfer, Manish Kumar Mishra, Hans-Joachim Bungartz"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Munich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03565",children:"https://arxiv.org/pdf/2512.03565"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper extends the AutoPas particle simulation library by introducing a dynamic tuning mechanism to select the optimal SIMD vectorization order for pairwise force calculations in Molecular Dynamics simulations. The method considers runtime parameters like particle density and neighbor identification algorithms. Benchmarks show this approach yields significant performance improvements over the library's previous static method."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, multi-frequency learning, head-worn sensors, human activity recognition, privacy-aware machine learning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dario Fenoglio, Mohan Li, Davide Casnici, Matias Laporte, Shkurta Gashi, Silvia Santini, Martin Gjoreski, Marc Langheinrich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe0 della Svizzera italiana, ETH Zurich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03287",children:"https://arxiv.org/pdf/2512.03287"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a multi-frequency Federated Learning (FL) approach for Human Activity Recognition (HAR) using head-worn sensors to enable privacy-aware model training across devices with different sampling rates. It demonstrates that this method outperforms frequency-specific approaches on two datasets, showing promise for multi-frequency FL in HAR tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [asynchronous checkpointing, state management, fast failover, surplus network capacity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bohan Zhao, Yuanhong Wang, Chenglin Liu, Jiagi Pan, Guang Yang, Ruitao Liu, Tingrui Zhang, Kai Luo, Wei Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03644",children:"https://arxiv.org/pdf/2512.03644"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FFTrainer, a system that leverages surplus network capacity to quickly save and load training states, enabling fast failover in large language model training. This approach aims to prevent costly rollbacks and accelerate recovery from node failures. The main conclusion is that FFTrainer significantly reduces recovery time and mitigates GPU utilization loss compared to prior checkpointing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing], [computation offloading, resource allocation, alternating optimization, layered approach, multi-access edge computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhen Wang, Bin Lin, Qiang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Dalian Maritime University, Dalian Neusoft University of Information, University of Calgary"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03487",children:"https://arxiv.org/pdf/2512.03487"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a double-edge-assisted scheme for space-air-marine integrated networks, using an alternating optimization method and a layered approach to jointly optimize offloading mode, volume, and resource allocation. It concludes that the proposed algorithm effectively minimizes system energy consumption under latency constraints compared to benchmark methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [computational physics], [Parallel Tempering, Metropolis-Hastings, OpenMP, CUDA, Markov Chain Monte Carlo]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aingeru Ramos, Jose A Pascual, Javier Navaridas, Ivan Coluzza"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Basque Center on Material, Applications and Nanostructures (BCMaterials), University of the Basque Country, Rice University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03825",children:"https://arxiv.org/pdf/2512.03825"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a parallel implementation of the Metropolis-Hastings algorithm with Parallel Tempering to accelerate Markov Chain Monte Carlo sampling. The implementation uses OpenMP for CPU and CUDA for GPU parallelization, achieving speed-ups of up to 52x and 986x, respectively. The results provide a benchmark for future quantum implementations of the algorithm."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [on-demand expert loading, edge-distributed inference, expert activation prediction, cacheless MoE, expert offloading]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Liujianfu Wang, Yuyang Du, Yuchen Pan, Soung Chang Liew, Jiacheng Liu, Kexin Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03927",children:"https://arxiv.org/pdf/2512.03927"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces OD-MoE, a distributed inference framework for Mixture-of-Experts models that eliminates the need for GPU expert caches by using on-demand expert loading. Its key innovations are parallelizing expert loading/computation across edge nodes and an accurate predictor to forecast expert activations in advance. The system achieves high prediction accuracy and enables efficient MoE inference on edge devices with very limited GPU memory."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Token Velocity, Convertible Decoders, autoscaling, prefill/decode disaggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ruiqi Lai, Hongrui Liu, Chengzhi Lu, Zonghao Liu, Siyu Cao, Siyang Shao, Yixin Zhang, Luo Mai, Dmitrii Ustiugov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University (NTU Singapore), Georgia Institute of Technology, Alibaba Group, University of Edinburgh"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03416",children:"https://arxiv.org/pdf/2512.03416"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces TokenScale, an autoscaling framework for disaggregated LLM serving that uses a novel predictive metric called Token Velocity and a flexible resource design with Convertible Decoders. This approach enables proactive scaling and rapid absorption of traffic bursts. The evaluation shows that TokenScale significantly improves SLO attainment and reduces costs compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, adaptive layer-freezing, encoder freezing, Green AI, MRI-to-CT conversion]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ciro Benito Raggio, Lucia Migliorelli, Nils Skupien, Mathias Krohmer Zabaleta, Oliver Blanck, Francesco Cicone, Giuseppe Lucio Cascini, Paolo Zaffino, Maria Francesca Spadea"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Karlsruhe Institute of Technology, Universit\xe0 Degli Studi Di Teramo, University Medical Center Schleswig-Holstein, Magna Graecia University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03054",children:"https://arxiv.org/pdf/2512.03054"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an adaptive encoder-freezing strategy for Federated Learning to reduce the computational and energy costs of training models for MRI-to-CT image conversion. The method selectively freezes encoder weights based on minimal updates between training rounds, achieving up to 23% reductions in training time, energy, and emissions. The core conclusion is that this Green AI approach maintains model performance while significantly improving the sustainability and accessibility of federated healthcare AI."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [quantum computing], [distributed quantum computing, multipartite entanglement, GHZ states, fan-out operations, qudits, global gates, GMS gates]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Seng W. Loke"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Deakin University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03685",children:"https://arxiv.org/pdf/2512.03685"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper explores the use of multipartite entanglement resources like GHZ states and qudits to implement distributed fan-out operations, specifically for realizing challenging distributed global gates (GMS gates). It concludes that such an approach can lead to more efficient distributed quantum circuit execution compared to using only distributed two-qubit gates, with implications for quantum circuit compilation and data center design."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251204] Integrating High Performance In-Memory Data Streaming and In-Situ Visualization in Hybrid MPI+OpenMP PIC MC Simulations Towards Exascale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high performance computing], [openPMD, ADIOS2, MPI+OpenMP, in-memory data streaming, in-situ visualization, Sustainable Staging Transport (SST), Particle-in-Cell Monte Carlo (PIC MC)]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jeremy J. Williams, Stefan Costea, Daniel Medeiros, Jordy Trilaksono, Pratibha Hegde, David Tskhakaya, Leon Kos, Ales Podolnik, Jakub Hromadka, Kevin A. Huck, Allen D. Malony, Frank Jenko, Erwin Laure, Stefano Markidis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology, University of Ljubljana, Max Planck Institute for Plasma Physics, University of Oregon, University of Vienna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03914",children:"https://arxiv.org/pdf/2512.03914"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper enhances the BIT1 plasma simulation code by integrating OpenMP task-based parallelism, the openPMD API, and ADIOS2's in-memory SST engine for data streaming. The core method replaces traditional file I/O with in-memory streaming and in-situ visualization to reduce bottlenecks. The main conclusion is that this approach significantly improves runtime, data accessibility, and enables real-time analysis for exascale plasma simulations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 26'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+(",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsx)(e.mi,{children:"\u03bb"})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03bb"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.6944em"}}),(0,s.jsx)(e.span,{className:"mord mathnormal",children:"\u03bb"})]})})]}),",",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsx)(e.mi,{children:"\u03bb"})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03bb"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.6944em"}}),(0,s.jsx)(e.span,{className:"mord mathnormal",children:"\u03bb"})]})})]}),"))-GA ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03805",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Better World Models Can Lead to Better Post-Training Performance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03400",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03911",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03847",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Hierarchical Vision Language Action Model Using Success and Failure Demonstrations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03913",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Adaptive sampling using variational autoencoder and reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03525",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Multi-Agent Reinforcement Learning with Communication-Constrained Priors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03528",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03882",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] A Learning-based Control Methodology for Transitioning VTOL UAVs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03548",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03973",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03194",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03783",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03891",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03429",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] A Multi-Agent, Policy-Gradient approach to Network Routing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03211",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03759",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03794",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03244",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03736",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03795",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Multimodal Reinforcement Learning with Agentic Verifier for AI Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03438",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03729",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03059",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] SkillFactory: Self-Distillation For Learning Cognitive Behaviors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04072",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03208",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 14'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Robust Tabular Foundation Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03307",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03394",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03476",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Scalable Decision Focused Learning via Online Trainable Surrogates ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03861",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Adaptive Identification and Modeling of Clinical Pathways with Process Mining ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03787",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03736",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03050",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03451",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] The promising potential of vision language models for the generation of textual weather forecasts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03623",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Irresponsible AI: big tech's influence on AI research and associated impacts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03077",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] AtomDisc: An Atom-level Tokenizer that Boosts Molecular LLMs and Reveals Structure--Property Associations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03080",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] An AI Implementation Science Study to Improve Trustworthy Data in a Large Healthcare System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.03098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251204] Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04047",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-05",children:"2025-12-05"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 13"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [data-sharing systems], [energy profiling, pipeline modeling, simulation experiments, stage reuse, configuration estimation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sepideh Masoudi, Sebastian Werner, Pierluigi Plebani, Stefan Tai"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, Politecnico di Milano"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04086",children:"https://arxiv.org/pdf/2512.04086"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines and identifies reuse potential in shared stages to reduce energy. The method is validated through simulation experiments, showing promising potential for cross-organizational pipeline optimization and establishing a foundation for energy-conscious execution strategies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Toward Sustainability-Aware LLM Inference on Edge Clusters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [edge computing, carbon-aware routing, latency-aware routing, benchmarking, batch processing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kolichala Rajashekar, Nafiseh Sharghivand, Radu Prodan, Reza Farahani"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Innsbruck, University of Klagenfurt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04088",children:"https://arxiv.org/pdf/2512.04088"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes sustainability-aware LLM inference on edge clusters using carbon- and latency-aware routing strategies based on empirical benchmarking of energy and execution time. It finds that a batch size of four prompts offers a good trade-off between throughput and energy efficiency, while larger batches risk GPU memory saturation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [analytical performance model, JIT compilation, cache hierarchy modeling, matrix multiplication, Triton]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ryan Swann, Muhammad Osama, Xiaohu Guo, Bryant Nelson, Lixun Zhang, Alex Brown, Yen Ong, Ali Yazdani, Sean Siddens, Ganesh Dasika, Alex Underwood"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Advanced Micro Devices, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04226",children:"https://arxiv.org/pdf/2512.04226"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces tritonBLAS, a GEMM library that uses an analytical model of GPU architecture and algorithmic blocking to predict and generate high-performance kernels without autotuning. It achieves over 95% of the performance of autotuned solutions while eliminating tuning time, making it a practical replacement for empirical methods in HPC and ML workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Scaling MPI Applications on Aurora"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [MPI, Slingshot interconnect, dragonfly topology, HPL, HPCG, Graph500]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Huda Ibeid, Anthony-Trung Nguyen, Aditya Nishtala, Premanand Sakarda, Larry Kaplan, Nilakantan Mahadevan, Michael Woodacre, Victor Anisimov, Kalyan Kumaran, JaeHyuk Kwack, Vitali Morozov, Servesh Muralidharan, Scott Parker"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Intel Corporation, Hewlett Packard Enterprise, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04291",children:"https://arxiv.org/pdf/2512.04291"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper details the system design and network validation of the Aurora exascale supercomputer, focusing on its HPE Slingshot fabric and dragonfly topology. It demonstrates the system's performance through MPI and application benchmarks, concluding that Aurora provides the necessary throughput, latency, and bandwidth to scale applications to large node counts for breakthrough science."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] A Structure-Aware Irregular Blocking Method for Sparse LU Factorization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing], [sparse LU factorization, irregular blocking, load balancing, dependency tree, diagonal block-based feature]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhen Hu, Dongliang Xiong, Kai Huang, Changjun Wu, Xiaowen Jiang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04389",children:"https://arxiv.org/pdf/2512.04389"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a structure-aware irregular blocking method for sparse LU factorization, which introduces a diagonal block-based feature to characterize local nonzero distribution and adjusts block sizes accordingly to balance workloads. The method uses fine-grained blocks in dense regions and coarse-grained blocks in sparse regions. Experiments show it achieves significant speedups over existing methods like PanguLU and SuperLU_DIST on NVIDIA A100 GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed messaging systems], [Remote Procedure Call, Remote Memory Access, file-based delivery, ordered delivery, at-least-once delivery]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sushant Kumar Gupta, Anil Raghunath Iyer, Chang Yu, Neel Bagora, Olivier Pomerleau, Vivek Kumar, Prunthaban Kanthakumar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Google LLC"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04096",children:"https://arxiv.org/pdf/2512.04096"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Fast ACS, a file-based ordered message delivery system that uses Remote Procedure Call for inter-cluster and Remote Memory Access for intra-cluster communication to achieve low-latency, scalable delivery. The system successfully provides ordering and at-least-once delivery guarantees to thousands of consumers across global clusters with sub-second p99 latency at low resource cost."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [service placement, resource provisioning, task scheduling, resource allocation, instance selection, AI-driven optimization, quantum computing, serverless computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ali Akbar Vali, Sadoon Azizi, Mohammad Shojafar, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Kurdistan, University of Surrey, The University of Melbourne"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04093",children:"https://arxiv.org/pdf/2512.04093"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper surveys state-of-the-art energy-efficient resource management strategies in microservices-based fog and edge computing, reviewing and classifying over 136 studies from 2020-2024. It identifies a lack of synergy among core resource management components and outlines future research directions, including AI-driven optimization and quantum computing, to develop more integrated and sustainable solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [serverless computing], [WebAssembly, WASI, AOT compilation, JIT compilation, cold-start, warm-start, serverless workflow]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mario Colosi, Reza Farahani, Lauri Loven, Radu Prodan, Massimo Villari"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Messina, University of Klagenfurt, University of Oulu, University of Innsbruck"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04089",children:"https://arxiv.org/pdf/2512.04089"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper evaluates the performance of serverless workflows using WebAssembly (Wasm) across browser, edge, and cloud environments. It finds that Ahead-of-Time (AOT) compilation and instance warming significantly reduce startup latency, and while browsers perform well with small payloads, edge and cloud nodes outperform them for compute- and memory-intensive tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [gpuFLOPBench, CUDA kernels, FLOP counting, static performance analysis, HeCBench]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gregory Bolet, Giorgis Georgakoudis, Konstantinos Parasyris, Harshitha Menon, Niranjan Hasabnis, Kirk W. Cameron, Gal Oren"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech, Lawrence Livermore National Laboratory, Codemetal, Stanford University, Technion"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04355",children:"https://arxiv.org/pdf/2512.04355"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces gpuFLOPBench, a benchmark for evaluating Large Language Models' ability to statically predict the floating-point operation (FLOP) counts of CUDA kernels without executing them. The results show that while modern LLMs perform well on straightforward kernels, they make significant errors when FLOPs depend on implicit compiler or runtime behaviors like division or intrinsic functions. This reveals a core limitation in current LLMs' reasoning about hardware-specific performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] VLCs: Managing Parallelism with Virtualized Libraries"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Virtual Library Contexts, resource partitioning, library virtualization, OpenMP, OpenBLAS, LibTorch]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yineng Yan, William Ruys, Hochan Lee, Ian Henriksen, Arthur Peters, Sean Stephens, Bozhi You, Henrique Fingler, Martin Burtscher, Milos Gligoric, Keshav Pingali, Mattan Erez, George Biros, Christopher J. Rossbach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Texas at Austin, Texas State University, Microsoft"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04320",children:"https://arxiv.org/pdf/2512.04320"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Virtual Library Contexts (VLCs), a method to manage parallelism by creating process subunits that encapsulate libraries and their resource allocations without modifying library code. This allows users to partition resources between libraries to prevent contention or load multiple library copies for parallel execution of thread-unsafe code. Experiments with C++ and Python prototypes show VLCs can achieve speedups of up to 2.85x on benchmarks using libraries like OpenMP, OpenBLAS, and LibTorch."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [VLSI physical design acceleration], [FPGA-CPU synergy, task partition, multi-granularity pipelining, cell shifting optimization, mixed-cell-height legalization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xingyu Liu, Jiawei Liang, Linfeng Du, Yipu Zhang, Chaofang Ma, Hanwei Fan, Jiang Xu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04527",children:"https://arxiv.org/pdf/2512.04527"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents FLEX, an accelerator for VLSI legalization that uses an optimized FPGA-CPU task partition and a multi-granularity pipelining technique to speed up the critical cell placement process. It demonstrates significant speedups over CPU-GPU and multi-threaded CPU baselines while also improving legalization quality."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Offloading to CXL-based Computational Memory"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [memory systems], [CXL, computational memory, near-memory processing, asynchronous back-streaming, offloading, KAI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Suyeon Lee, Kangkyu Park, Kwangsik Shin, Ada Gavrilovska"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Georgia Institute of Technology, SK hynix"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04449",children:"https://arxiv.org/pdf/2512.04449"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel 'Asynchronous Back-Streaming' protocol and a system called KAI to improve operation offloading to CXL-based Computational Memory (CCM). KAI enables asynchronous data movement and lightweight pipelining between the host and CCM. The system significantly reduces end-to-end runtime and idle times, demonstrating improved performance and efficiency for disaggregated memory systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251205] Federated Learning for Terahertz Wireless Communication"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, terahertz communications, multicarrier stochastic framework, beam squint, molecular absorption, SNR-weighted aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," O. Tansel Baydas, Ozgur B. Akan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Cambridge, Ko\xe7 University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04984",children:"https://arxiv.org/pdf/2512.04984"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper develops a multicarrier stochastic framework to analyze the impact of realistic Terahertz channel impairments, like beam squint and molecular absorption, on Federated Learning optimization dynamics. It identifies a critical "diversity trap" where convergence error is driven by the harmonic mean of subcarrier SNRs, and proposes an SNR-weighted aggregation strategy to recover convergence in high-squint regimes where standard averaging fails.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 27'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Toward Virtuous Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04246",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04368",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] The Geometry of Benchmarks: A New Path Toward AGI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04276",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Towards better dense rewards in Reinforcement Learning Applications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04302",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04341",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Data-regularized Reinforcement Learning for Diffusion Models at Scale ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04332",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04359",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Learning to Orchestrate Agents in Natural Language with the Conductor ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04388",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04277",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04124",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] MARL Warehouse Robots ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04463",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] GTM: Simulating the World of Tools for AI Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04535",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04552",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04653",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] TRINITY: An Evolved LLM Coordinator ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04695",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04752",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04773",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04779",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04793",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04918",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04949",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04958",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Structured Document Translation via Format Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05100",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05105",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04405",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Continuous-time reinforcement learning for optimal switching over multiple regimes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04697",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 11'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04283",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04252",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04273",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Towards better dense rewards in Reinforcement Learning Applications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04302",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Decoding Large Language Diffusion Models with Foreseeing Movement ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04135",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04562",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04632",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04752",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Arbitrage: Efficient Reasoning via Advantage-Aware Speculation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05033",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.04716",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251205] Meta-Learning for Quantum Optimization via Quantum Sequence Model ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.05058",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>o});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);