"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4311],{27460:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_LG/20260105-20260111","title":"20260105-20260111 (cs.LG)","description":"2026-01-05","source":"@site/docs/daily/cs_LG/20260105-20260111.md","sourceDirName":"daily/cs_LG","slug":"/daily/cslg/20260105-20260111","permalink":"/ai_toutiao/daily/cslg/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cslg/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.LG)","permalink":"/ai_toutiao/daily/cslg/20251229-20260104"},"next":{"title":"cs.LO","permalink":"/ai_toutiao/category/cslo"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/cslg/20260105-20260111"},o="20260105-20260111 (cs.LG)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260105-20260111-cslg",children:"20260105-20260111 (cs.LG)"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [anomaly detection], [class imbalance, synthetic dataset, generalization error, unsupervised methods, semi-supervised methods]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," McMaster University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00005",children:"https://arxiv.org/pdf/2601.00005"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comprehensive, problem-agnostic evaluation of 14 anomaly detectors under simulated industrial constraints of extreme class imbalance. 2. Identified that the best-performing detector depends critically on the absolute number of faulty examples available, not just the imbalance ratio, and provided thresholds for method selection. 3. Demonstrated the nuanced impact of feature dimensionality on method performance, showing semi-supervised methods gain advantage in higher dimensions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d98d7533bc130ddf43f0469391265ce51a72fe31fdeb441a3c4b553d4e3dd35_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates anomaly detection algorithms for industrial problems with extreme class imbalance using a synthetic dataset. It benchmarks 14 detectors across varying anomaly rates and training sizes, finding that the optimal detector depends on the absolute number of faulty examples, with unsupervised methods best for very few faults and supervised/semi-supervised methods improving with 30-50 faults. The study highlights performance drops on smaller datasets and provides practical deployment insights."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Extreme class imbalance in industrial applications due to limited faulty data]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Benchmark 14 detectors on synthetic hyperspherical dataset with varying anomaly rates and training sizes]\n    D[\u5173\u952e\u7ed3\u679c/Results: Best detector depends on number of faulty examples; Unsupervised dominates with <20 faults; Supervised/semi-supervised improve with 30-50 faults]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [logistics optimization], [workload balancing, evolutionary algorithms, k-means, last-mile delivery, hybrid algorithm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Luis M. Moreno-Saavedra, Silvia Jimenez-Fernandez, Antonio Portilla-Figueras, David Casillas-Perez, Sancho Salcedo-Sanz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad de Alcal\xe1, Universidad Rey Juan Carlos"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00023",children:"https://arxiv.org/pdf/2601.00023"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a multi-algorithm methodology for operational human resources workload balancing in last-mile delivery, moving beyond simple geographical assignment. 2. Introduces and combines several algorithmic approaches, including different versions of k-means, evolutionary algorithms, recursive assignments, and a hybrid evolutionary ensemble. 3. Validates the proposed approach by applying it to a real-world case study of a delivery workforce in Azuqueca de Henares, Spain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43bf05ee106d97a42fa05d1af33419810885c8fff72d16b95af434c71d43f4ff_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unbalanced workload distribution among delivery workers in last-mile urban logistics. It proposes a multi-algorithm approach that uses a combination of distance and workload considerations, including k-means variants and evolutionary algorithms, to assign packages and balance daily effort. The method was successfully tested on a real-world delivery system in Spain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A multi-algorithm approach for operational human resources workload balancing<br>\u591a\u7b97\u6cd5\u65b9\u6cd5\u7528\u4e8e\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u7684\u4eba\u529b\u8d44\u6e90\u8d1f\u8f7d\u5747\u8861"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Unbalanced workload among delivery workers in last-mile systems<br>\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u5feb\u9012\u5458\u5de5\u4f5c\u91cf\u4e0d\u5747\u8861"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Multi-algorithm approach combining k-means, evolutionary algorithms, and hybrid ensembles<br>\u7ed3\u5408k-means\u3001\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u96c6\u6210\u7684\u591a\u7b97\u6cd5\u65b9\u6cd5"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Successfully applied to a real-world delivery workforce, balancing workload<br>\u6210\u529f\u5e94\u7528\u4e8e\u73b0\u5b9e\u914d\u9001\u56e2\u961f\uff0c\u5b9e\u73b0\u4e86\u5de5\u4f5c\u91cf\u5747\u8861"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [mental health language modeling], [large language models, fine-tuning, PHQ-9, Nigerian Pidgin, depression screening]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Isaac Iyinoluwa Olufadewa, Miracle Ayomikun Adesina, Ezekiel Ayodeji Oladejo, Uthman Babatunde Usman, Owen Kolade Adeniyi, Matthew Tolulope Olawoyin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Artificial Intelligence for Low-Resource Public Health Application (ALPHA) Centre, Slum and Rural Health Initiative; University of Ibadan; University of Ilorin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00004",children:"https://arxiv.org/pdf/2601.00004"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Created a novel, annotated dataset of 432 Nigerian Pidgin audio responses for depression screening aligned with PHQ-9 items. 2. Fine-tuned and evaluated three LLMs (Phi-3-mini, Gemma-3-4B-it, GPT-4.1) for automated depression screening in a low-resource language. 3. Demonstrated that fine-tuned GPT-4.1 achieved high accuracy (94.5%) and cultural appropriateness for PHQ-9 severity scoring in Nigerian Pidgin."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/849aa2e2bc1566aab5f5b5e5d072677a6a66426e677648e836adf89c32b964e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of depression screening in Nigeria by fine-tuning large language models for Nigerian Pidgin English. The authors collected and annotated a dataset of audio responses, then fine-tuned three LLMs to predict PHQ-9 severity scores. The fine-tuned GPT-4.1 model achieved the best performance, providing a foundation for AI-mediated mental health tools in linguistically diverse, resource-constrained settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Finetuning LLMs for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited depression screening in Nigeria due to language barriers and lack of clinicians]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Fine-tune LLMs on annotated Nigerian Pidgin dataset for PHQ-9 scoring]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>GPT-4.1 achieved 94.5% accuracy and best cultural appropriateness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [policy gradient, self-play, Markov Decision Process, Advantage Actor-Critic, ablation study]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nicholas A. Pape"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00007",children:"https://arxiv.org/pdf/2601.00007"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates the classic stochastic combinatorial game Yahtzee as a Markov Decision Process and establishes it as a mid-scale RL benchmark. 2. Conducts a comprehensive empirical study comparing REINFORCE, A2C, and PPO under a fixed training budget, identifying A2C as the most robust method. 3. Achieves a median score within 5% of the optimal dynamic programming solution, while analyzing persistent challenges like long-horizon credit assignment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfca1ffc9ad8523e303bf57755dcad543948f47e5e9f3c5a9d726a359bd3fe5b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates using deep reinforcement learning to play the full solitaire version of Yahtzee. It trains self-play agents with policy gradient methods (REINFORCE, A2C, PPO) and performs ablation studies on various design choices. The main finding is that A2C robustly achieves near-optimal performance, while all methods struggle with long-term strategic elements like securing the upper section bonus."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Yahtzee as a mid-scale RL benchmark with delayed rewards & combinatorial complexity] --\x3e B1[\u76ee\u6807/Objectives<br>Can RL achieve near-optimal performance via self-play?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Formulate as MDP, train self-play agents with policy gradient methods (REINFORCE, A2C, PPO)] --\x3e C1[\u6280\u672f/Techniques<br>Ablation on encodings, architecture, estimators, entropy]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>A2C is robust & achieves median score within 5% of optimal DP score] --\x3e D1[\u6311\u6218/Challenges<br>Agents struggle with long-horizon strategy (upper bonus)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [ferroelectric synapses, spiking neural networks, EEG signal processing, adaptive learning, neuromorphic computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura B\xe9gon-Lours"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00020",children:"https://arxiv.org/pdf/2601.00020"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: EEG\u4fe1\u53f7\u975e\u5e73\u7a33\u6027\u9650\u5236\u6a21\u578b\u6cdb\u5316\uff0c\u9700\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e2a\u6027\u5316\u9002\u5e94/Non-stationary EEG signals limit model generalization, requiring personalized adaptation on resource-constrained platforms]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5728\u94c1\u7535\u5fc6\u963b\u7a81\u89e6\u4e0a\u90e8\u7f72SNN\uff0c\u91c7\u7528\u8bbe\u5907\u611f\u77e5\u7684\u6743\u91cd\u66f4\u65b0\u7b56\u7565/Deploy SNNs on ferroelectric memristive synapses with a device-aware weight-update strategy]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u4e24\u79cd\u90e8\u7f72\u7b56\u7565\u6027\u80fd\u5ab2\u7f8e\u8f6f\u4ef6SNN\uff0c\u7279\u5b9a\u5bf9\u8c61\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u51c6\u786e\u7387/Two deployment strategies achieve performance comparable to software SNNs, subject-specific transfer learning improves accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [tokenizer transplant, model composition, supply-chain vulnerability, sparse solver, spectral mimicry]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiaoze Liu, Weichen Yu, Matt Fredrikson, Xiaoqian Wang, Jing Gao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Purdue University, Carnegie Mellon University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00065",children:"https://arxiv.org/pdf/2601.00065"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/xz-liu/tokenforge",children:"https://github.com/xz-liu/tokenforge"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies tokenizer transplant as a novel attack surface in the LLM composition supply chain, 2. Introduces the concept of a "breaker token"\u2014a single, engineered token that is inert in a donor model but maliciously activates after transplant, 3. Formalizes and instantiates the attack as a dual-objective optimization problem solved with a sparse solver, demonstrating its training-free nature, stealth, and persistence.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bac901d101e76e81a328892233109e7f05c25f328de683add53ccd17ae81590e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper identifies a security vulnerability in the tokenizer transplant step required for composing different LLMs. The authors propose a method to engineer a single "breaker token" that, when added to a donor model, remains harmless but sabotages a base model after transplant by exploiting coefficient reuse. The attack is stealthy, training-free, and persistent, revealing a hidden risk in modular AI pipelines.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Tokenizer transplant introduces a supply-chain vulnerability for LLM composition]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Engineer a single "breaker token" exploiting coefficient reuse via sparse solver]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Stealthy, training-free attack that persists against fine-tuning and merging]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [mode collapse, noise optimization, frequency characteristics, text-to-image generation, inference-time scaling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UC Berkeley, University of T\xfcbingen (T\xfcbingen AI Center), Technical University of Munich (MCML)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00090",children:"https://arxiv.org/pdf/2601.00090"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\nRoot("It\'s Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Mode collapse in text-to-image models")\nRoot --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Noise optimization with frequency analysis")\nRoot --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Improved generation diversity and quality")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatio-temporal graph neural networks], [spatio-temporal graph neural network, dynamic graphs, temporal attention, graph convolutional network, network forensics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Swetha Varadarajan, Abhishek Ray, Lumina Albert"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Cape Town, George Mason University, Colorado State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00075",children:"https://arxiv.org/pdf/2601.00075"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://anonymous.4open.science/r/IMB-GNN-F022",children:"https://anonymous.4open.science/r/IMB-GNN-F022"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IMBWatch, a novel ST-GNN framework for detecting Illicit Massage Businesses by modeling them as dynamic, heterogeneous graphs from open-source data. 2. Introduces a method combining graph convolutions with temporal attention to capture evolving spatio-temporal patterns like intercity movement and coordinated advertising. 3. Demonstrates superior performance over baseline models on real-world data and provides an interpretable, scalable tool for proactive anti-trafficking interventions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef349259f296853dd5ad0527274fd725d811fdf18b16b020222413bfeb7f6f93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef349259f296853dd5ad0527274fd725d811fdf18b16b020222413bfeb7f6f93_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces IMBWatch, a spatio-temporal graph neural network framework designed to detect Illicit Massage Businesses by modeling their operations as dynamic graphs from online ads and records. The method uses graph convolutions and temporal attention to learn evolving patterns, and it outperforms baseline models in accuracy and F1-score on real-world data, offering a scalable tool for law enforcement."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[IMBWatch: A Spatio-Temporal Graph Neural Network Framework for Detecting Illicit Massage Businesses] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Detecting covert Illicit Massage Businesses is difficult due to encoded ads and dynamic operations.)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Constructs dynamic graphs from open-source data and uses ST-GNN with temporal attention to model spatio-temporal evolution.)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms baseline models (GCN, GAT, etc.) on real data, offering higher accuracy, interpretability, and scalability.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [LLM Security], [Go-Explore, Prompt Injection, Adversarial Testing, Agent Safety, Multi-Hop Attacks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Manish Bhatt, Adrian Wood, Idan Habler, Ammar Al-Kahfah"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," OWASP, Amazon, Dropbox, CISCO, AWS"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00042",children:"https://arxiv.org/pdf/2601.00042"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/mbhatt1/competitionscratch",children:"https://github.com/mbhatt1/competitionscratch"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Adapted the Go-Explore reinforcement learning algorithm for systematic security testing of LLM agents. 2. Conducted a large-scale empirical study revealing that random seed variance dominates algorithmic parameter choices in this domain. 3. Provided actionable insights for practitioners, such as the ineffectiveness of reward shaping and the benefits of using ensembles and simple state signatures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fd9dc0076e96f0b8282984cab768d0355248ad4e2af52c17ac7798bb446d49_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper adapts the Go-Explore algorithm to test the security of safety-trained LLM agents against prompt injection attacks. Through 28 experimental runs on GPT-4o-mini, the study finds that random seed variance is a major factor, reward shaping is harmful, and simple state signatures work best. The results suggest that managing seed variance and applying domain knowledge are more critical than algorithmic sophistication for effective security testing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing<br/>\u5927\u578b\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\uff1a\u7528\u4e8eAI\u7ea2\u961f\u6d4b\u8bd5\u7684Go-Explore"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Testing security of safety-trained LLM agents<br/>\u6d4b\u8bd5\u7ecf\u8fc7\u5b89\u5168\u8bad\u7ec3\u7684LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027"] --\x3e P1["Prompt Injection<br/>\u63d0\u793a\u6ce8\u5165"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Adapt Go-Explore algorithm<br/>\u6539\u7f16Go-Explore\u7b97\u6cd5"] --\x3e M1["Systematic exploration from archive<br/>\u4ece\u5b58\u6863\u8fdb\u884c\u7cfb\u7edf\u63a2\u7d22"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Key Findings<br/>\u5173\u952e\u53d1\u73b0"] --\x3e R1["Seed variance dominates<br/>\u79cd\u5b50\u65b9\u5dee\u5360\u4e3b\u5bfc"]\n    Results --\x3e R2["Reward shaping harms performance<br/>\u5956\u52b1\u5851\u5f62\u635f\u5bb3\u6027\u80fd"]\n    Results --\x3e R3["Simple signatures outperform<br/>\u7b80\u5355\u7b7e\u540d\u6548\u679c\u66f4\u597d"]\n    Results --\x3e R4["Ensembles provide diversity<br/>\u96c6\u6210\u63d0\u4f9b\u591a\u6837\u6027"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Exploration in the Limit"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [bandit algorithms], [best arm identification, asymptotic error control, confidence sequences, nonparametric, sample complexity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Brian M. Cho, Nathan Kallus"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cornell University, Netflix"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00084",children:"https://arxiv.org/pdf/2601.00084"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a relaxed, asymptotic formulation for fixed-confidence best arm identification (BAI) that requires valid error control only after a minimum sample size, aligning with long-horizon practical settings. 2. Develops a novel asymptotic anytime-valid confidence sequence for arm indices and uses it to design a new BAI algorithm that flexibly incorporates covariates for variance reduction in fully nonparametric settings. 3. Provides asymptotic sample complexity bounds, showing the worst-case complexity matches the best-case complexity of Gaussian BAI under exact guarantees, and demonstrates reduced average sample complexity in experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a27e11a98452705127752d5c92465faa383d4d4bc6f162ce25d8230c0bd85bbd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a27e11a98452705127752d5c92465faa383d4d4bc6f162ce25d8230c0bd85bbd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses limitations in existing best arm identification (BAI) methods by proposing an asymptotic framework that relaxes the requirement for exact error control to an asymptotic one, enabling the use of tighter bounds and handling nonparametric distributions with covariates. It introduces a new algorithm based on asymptotic anytime-valid confidence sequences. Experiments show this approach reduces average sample complexity while maintaining error control."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Exploration in the Limit] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709BAI\u65b9\u6cd5\u4e0d\u5b9e\u7528/Existing BAI methods impractical]\n    B1 --\x3e B2[\u4e25\u683c\u8bef\u5dee\u63a7\u5236\u5bfc\u81f4\u9650\u5236/Stringent exact error control causes restrictions]\n    C --\x3e C1[\u6e10\u8fd1\u8bef\u5dee\u63a7\u5236\u6846\u67b6/Asymptotic error control framework]\n    C1 --\x3e C2[\u65b0\u9896\u7f6e\u4fe1\u5e8f\u5217/Novel confidence sequences]\n    C2 --\x3e C3[\u65b0BAI\u7b97\u6cd5/New BAI algorithm]\n    D --\x3e D1[\u6837\u672c\u590d\u6742\u5ea6\u5339\u914d\u6700\u4f73\u60c5\u51b5/Sample complexity matches best-case]\n    D1 --\x3e D2[\u5b9e\u9a8c\u51cf\u5c11\u5e73\u5747\u6837\u672c\u91cf/Experiments reduce average sample size]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [symbolic regression], [Bayesian Optimization, Instruction Tuning, Partial Differential Equation Discovery, LLM Prompting]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junqi Qu, Yan Zhang, Shangqian Gao, Shibo Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00088",children:"https://arxiv.org/pdf/2601.00088"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and formalizes the problem of "instruction brittleness" in LLMs for equation discovery, where model outputs are highly sensitive to prompt phrasing. 2. Proposes NeuroSymBO, a novel framework that reframes prompt engineering as a sequential decision problem, using Bayesian Optimization to dynamically select optimal instructions from a library at each step. 3. Demonstrates through experiments on PDE discovery benchmarks that adaptive instruction selection significantly outperforms static prompts, achieving higher recovery rates and more parsimonious solutions.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d0309ae3b94e9b3ab379ec45397a3f052c2722939b39c6750528988b21974_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d0309ae3b94e9b3ab379ec45397a3f052c2722939b39c6750528988b21974_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the problem of "instruction brittleness" in LLMs used for discovering partial differential equations, where fixed prompts lead to suboptimal results. It proposes NeuroSymBO, a framework that uses Bayesian Optimization to dynamically select the best instruction at each step of the generation process. Experiments show this adaptive approach outperforms static prompting, yielding more accurate and simpler equations.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: LLM\u6307\u4ee4\u8106\u5f31\u6027<br/>Instruction Brittleness in LLMs] --\x3e P1[\u9759\u6001\u63d0\u793a\u5bfc\u81f4\u6b21\u4f18\u89e3<br/>Static prompts cause suboptimal solutions]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: NeuroSymBO\u6846\u67b6<br/>NeuroSymBO Framework] --\x3e M1[\u5c06\u63d0\u793a\u5de5\u7a0b\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898<br/>Reframes prompt engineering as sequential decision] --\x3e M2[\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u52a8\u6001\u9009\u62e9\u6307\u4ee4<br/>Uses Bayesian Optimization for adaptive instruction selection]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5b9e\u9a8c\u8bc4\u4f30<br/>Experimental Evaluation] --\x3e R1[\u81ea\u9002\u5e94\u9009\u62e9\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u63d0\u793a<br/>Adaptive selection significantly outperforms fixed prompts] --\x3e R2[\u66f4\u9ad8\u7684\u6062\u590d\u7387\u4e0e\u66f4\u7b80\u7ea6\u7684\u89e3<br/>Higher recovery rates & more parsimonious solutions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Metric Interval Temporal Logic (MITL), Timed Limit-Deterministic Generalized B\xfcchi Automata (Timed-LDGBA), Q-learning, POMDP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Iowa, Talus Renewables, Inc., Roma Tre University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00087",children:"https://arxiv.org/pdf/2601.00087"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement learning with timed constraints for robotics motion planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u590d\u6742\u4efb\u52a1\u5e8f\u5217\u4e0e\u4e25\u683c\u65f6\u95f4\u7ea6\u675f/Complex task sequences & strict temporal constraints]\n    B --\x3e B2[\u968f\u673a\u52a8\u6001\u4e0e\u90e8\u5206\u53ef\u89c2\u6d4b\u6027/Stochastic dynamics & partial observability]\n    C --\x3e C1[MITL\u516c\u5f0f\u8f6c\u6362\u4e3aTimed-LDGBA/MITL to Timed-LDGBA translation]\n    C --\x3e C2[\u6784\u5efa\u4ea7\u54c1\u5b9a\u65f6\u6a21\u578b\u4e0eQ\u5b66\u4e60/Construct product timed models for Q-learning]\n    C --\x3e C3[\u7b80\u5355\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5956\u52b1\u7ed3\u6784/Simple yet expressive reward structure]\n    D --\x3e D1[\u6ee1\u8db3\u65f6\u95f4\u7ea6\u675f\u7684\u7b56\u7565/Policies satisfy time-bounded requirements]\n    D --\x3e D2[\u6269\u5c55\u5230\u66f4\u5927\u72b6\u6001\u7a7a\u95f4/Scales to larger state spaces]\n    D --\x3e D3[\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u6709\u6548/Effective in partially observable environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [geometric reinforcement learning, Hamiltonian optimization, simultaneous navigation and mapping, local sensory observations, path differential]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00116",children:"https://arxiv.org/pdf/2601.00116"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/",children:"https://github.com/"}),' (as per the abstract "The code is publicly available on Github." The specific URL is not provided in the given text, only a placeholder link.)']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRL-SNAM: Geometric RL for SNAM] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Simultaneous Navigation and Mapping in mapless environments]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Geometric RL with Path Differential Hamiltonians, local energy landscapes]\n    D[\u5173\u952e\u7ed3\u679c/Results: High-quality navigation with minimal exploration, generalizes to unseen layouts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement Learning with Function Approximation for Non-Markov Processes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [non-Markov processes, linear function approximation, policy evaluation, Q-learning, partially observed MDPs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ali Devran Kara"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00151",children:"https://arxiv.org/pdf/2601.00151"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proved convergence of policy evaluation with linear function approximation under ergodic non-Markov processes, linking the limit to a fixed point of a joint projection-Bellman operator. 2. Established convergence for a special case of Q-learning with linear approximation where basis functions are based on quantization maps under similar ergodicity conditions. 3. Applied the theoretical results to Partially Observed MDPs (POMDPs) using finite-memory state representations and derived explicit error bounds for the learning algorithm limits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies reinforcement learning with linear function approximation for non-Markov processes. It proves convergence for policy evaluation and a special case of Q-learning under ergodicity conditions, and applies the theory to POMDPs to derive error bounds."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Reinforcement Learning with Function Approximation for Non-Markov Processes] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>RL with linear function approximation for non-Markov processes] --\x3e P1[\u975e\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b/Non-Markov Processes]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Theoretical analysis under ergodicity conditions] --\x3e M1[\u7b56\u7565\u8bc4\u4f30/Policy Evaluation]\n    Method --\x3e M2[Q\u5b66\u4e60/Q-learning]\n    M2 --\x3e M2_1[\u7279\u6b8a\u60c5\u51b5:\u57fa\u4e8e\u91cf\u5316\u7684\u57fa\u51fd\u6570/Special Case: Quantization-based basis]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6536\u655b\u6027\u8bc1\u660e/Convergence Proofs]\n    Results --\x3e R2[\u5e94\u7528\u4e8ePOMDPs/Application to POMDPs]\n    R2 --\x3e R2_1[\u663e\u5f0f\u8bef\u5dee\u754c/Explicit Error Bounds]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [predictive modeling], [XGBoost, feature importance, class imbalance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yann Bellec, Rohan Kaman, Siwen Cui, Aarav Agrawal, Calvin Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California San Diego"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00152",children:"https://arxiv.org/pdf/2601.00152"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A large-scale empirical analysis using an XGBoost model to predict traffic accident severity from environmental, temporal, and spatial factors. 2. A feature importance analysis revealing that time of day, location, temperature, and wind speed are strong predictors, while precipitation and visibility show limited predictive power, suggesting driver behavioral adaptation. 3. Identification of dataset limitations (class imbalance, predominance of mid-level severity) and proposed future directions including alternative sampling and enhanced feature engineering."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27e52d086cf3d330117b8cda24994123a6a67bf935b7650263f6ca2e120a3ed2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27e52d086cf3d330117b8cda24994123a6a67bf935b7650263f6ca2e120a3ed2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study uses an XGBoost classifier on a dataset of 500,000 US traffic accidents to predict accident severity. The model achieves 78% accuracy, and feature analysis shows that while time, location, and some weather factors are predictive, precipitation and visibility are not, potentially due to driver adaptation. The findings highlight the limitations of current data for predicting extreme cases and suggest future research directions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[Predict traffic accident severity/\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u7a0b\u5ea6]\n    Method --\x3e M1[XGBoost Classifier/XGBoost\u5206\u7c7b\u5668]\n    Method --\x3e M2[Randomized Search CV/\u968f\u673a\u641c\u7d22\u4ea4\u53c9\u9a8c\u8bc1]\n    Method --\x3e M3[Class Weighting/\u7c7b\u522b\u52a0\u6743]\n    Results --\x3e R1[78% Accuracy/78%\u51c6\u786e\u7387]\n    Results --\x3e R2[Precipitation low importance/\u964d\u6c34\u9884\u6d4b\u529b\u4f4e]\n    Results --\x3e R3[Dataset limitations identified/\u8bc6\u522b\u6570\u636e\u96c6\u5c40\u9650]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junkai Luo, Yinglun Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Riverside"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00167",children:"https://arxiv.org/pdf/2601.00167"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Online Finetuning Decision Transformers with Pure RL Gradients] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u5728\u7ebf\u5fae\u8c03DT\u65f6\uff0c\u7eafRL\u68af\u5ea6\u65b9\u6cd5\u672a\u88ab\u63a2\u7d22/Pure RL gradients for online DT finetuning unexplored]\nB --\x3e B2[\u540e\u89c1\u4e4b\u76ca\u56de\u62a5\u91cd\u6807\u6ce8\u4e0eRL\u7b97\u6cd5\u4e0d\u517c\u5bb9/Hindsight return relabeling incompatible with RL]\nC --\x3e C1[\u9002\u914dGRPO\u81f3DT/Adapt GRPO to DTs]\nC --\x3e C2[\u5f15\u5165\u5173\u952e\u4fee\u6539: \u5b50\u8f68\u8ff9\u4f18\u5316\u7b49/Introduce key modifications]\nD --\x3e D1[\u8d85\u8d8a\u73b0\u6709\u5728\u7ebfDT\u57fa\u7ebf/Outperform online DT baselines]\nD --\x3e D2[\u5b9e\u73b0SOTA\u6027\u80fd/Achieve SOTA performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Reservoir Computing, Sequential Architecture, Spatiotemporal Forecasting, High-dimensional Data, Training Efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ata Akbari Asanjan, Filip Wudarski, Daniel O'Connor, Shaun Geaney, Elena Strbac, P. Aaron Lott, Davide Venturelli"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," USRA Research Institute for Advanced Computer Science (RIACS), Standard Chartered Bank"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00172",children:"https://arxiv.org/pdf/2601.00172"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a Sequential Reservoir Computing architecture that decomposes a large reservoir into smaller, interconnected ones to reduce computational and memory costs. 2. Demonstrates superior performance with longer forecast horizons and lower error metrics on chaotic and high-dimensional physical systems compared to RNN/LSTM baselines. 3. Achieves up to three orders of magnitude lower training cost, maintaining RC's efficiency while improving scalability for high-dimensional forecasting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Sequential Reservoir Computing, a novel architecture that breaks a large reservoir into a sequence of smaller ones to efficiently forecast high-dimensional spatiotemporal systems. It outperforms traditional RNNs and LSTMs in forecast horizon and accuracy while drastically reducing training costs, offering a path to real-time, energy-efficient forecasting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>RNN/LSTM\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4f20\u7edfRC\u6269\u5c55\u6027\u5dee"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u987a\u5e8f\u50a8\u5c42\u8ba1\u7b97\u67b6\u6784"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u9884\u6d4b\u66f4\u957f\uff0c\u8bef\u5dee\u66f4\u4f4e\uff0c\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u964d\u4f4e"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [semantic communication], [reinforcement learning, unequal error protection, adaptive repetition coding, semantic distortion metric, per-dimension protection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Moirangthem Tiken Singh, Adnan Arif"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00186",children:"https://arxiv.org/pdf/2601.00186"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5e26\u5bbd\u53d7\u9650\u4e0b\u4fdd\u6301\u8bed\u4e49/Bandwidth-constrained Semantic Preservation]\n    C --\x3e C1[\u57fa\u4e8eRL\u7684\u81ea\u9002\u5e94\u91cd\u590d\u7f16\u7801/RL-based Adaptive Repetition Coding]\n    C --\x3e C2[\u590d\u5408\u8bed\u4e49\u5931\u771f\u5ea6\u91cf/Composite Semantic Distortion Metric]\n    D --\x3e D1[\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u5747\u5300\u4fdd\u62a4/Significant Gains Over Uniform Protection]\n    D --\x3e D2[\u6311\u6218\u4f20\u7edf\u4fe1\u9053\u7f16\u7801\u8303\u5f0f/Challenges Traditional Channel Coding]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [clinical prediction], [XGBoost, electronic health records (EHR), FIB-4 score, liver cirrhosis, early prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhuqi Miao, Sujan Ravi, Abdulaziz Ahmed"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Oklahoma State University, University of Alabama at Birmingham"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00175",children:"https://arxiv.org/pdf/2601.00175"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed and validated machine learning models for predicting liver cirrhosis 1, 2, and 3 years before diagnosis using routine EHR data. 2. Established a rigorous benchmark by comparing the ML models' performance directly against the traditional clinical FIB-4 score across multiple time horizons. 3. Demonstrated that ML models consistently outperform FIB-4, with performance gains increasing for longer-term (3-year) predictions, enabling earlier clinical risk stratification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10f123bbaaf0097372e87215cac6fce63e113f7c0a6ea463635fbc4532641048_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10f123bbaaf0097372e87215cac6fce63e113f7c0a6ea463635fbc4532641048_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops XGBoost models using electronic health record data to predict liver cirrhosis 1-3 years before diagnosis. The models consistently outperformed the standard FIB-4 score, with performance gains increasing for longer prediction horizons. The study concludes that these ML models can be integrated into clinical workflows as automated decision-support tools for earlier and more accurate risk stratification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Early Prediction of Liver Cirrhosis Up to Three Years in Advance] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u65e9\u671f\u9884\u6d4b\u809d\u786c\u5316 / Early Prediction of Liver Cirrhosis]\nB --\x3e B2[\u8d85\u8d8aFIB-4\u8bc4\u5206 / Benchmarking Against FIB-4]\nC --\x3e C1[\u4f7f\u7528EHR\u6570\u636e / Use EHR Data]\nC --\x3e C2[\u6784\u5efa\u9884\u6d4b\u573a\u666f / Construct Prediction Scenarios]\nC --\x3e C3[\u8bad\u7ec3XGBoost\u6a21\u578b / Train XGBoost Models]\nD --\x3e D1[ML\u6a21\u578b\u6027\u80fd\u66f4\u4f18 / ML Models Outperform FIB-4]\nD --\x3e D2[AUC: 0.81, 0.73, 0.69 / AUC: 0.81, 0.73, 0.69]\nD --\x3e D3[\u652f\u6301\u65e9\u671f\u98ce\u9669\u5206\u5c42 / Supports Early Risk Stratification]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [semi-supervised learning], [Generative Adversarial Network, Swin Transformer, spike classification, semi-supervised learning, Bayesian optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Danial Sharifrazi, Nouman Javed, Mojtaba Mohammadi, Seyede Sana Salehi, Roohallah Alizadehsani, Prasad N. Paradkar, U. Rajendra Acharya, Asim Bhatti"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Deakin University, CSIRO Health and Biosecurity, Islamic Azad University, University of Southern Queensland"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00189",children:"https://arxiv.org/pdf/2601.00189"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel semi-supervised GAN architecture (SSI-GAN) with a Swin-inspired, shifted-window discriminator for neuronal spike classification. 2. Introduced a transformer-based generator and a flat, window-based transformer discriminator with multi-head self-attention to capture sparse, high-frequency spike features. 3. Demonstrated state-of-the-art performance with 99.93% accuracy using only 1-3% labeled data, reducing manual labeling effort by 97-99% compared to supervised methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49a3916713727a5d92dd8fe976e78d600a974d9217a0ccc868f8608ec8453524_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the labor-intensive problem of classifying mosquito neuronal spikes for arboviral disease detection by proposing SSI-GAN, a semi-supervised GAN that combines a Swin-inspired discriminator with a transformer-based generator. Using only 1-3% labeled data from over 15 million spike samples, it achieved up to 99.93% accuracy in classifying Zika-infected, dengue-infected, or uninfected categories, significantly reducing labeling effort while outperforming baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SSI-GAN: \u534a\u76d1\u7763Swin\u542f\u53d1\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7528\u4e8e\u795e\u7ecf\u5143\u5c16\u5cf0\u5206\u7c7b] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u868a\u866b\u795e\u7ecf\u5143\u5c16\u5cf0\u6a21\u5f0f\u624b\u52a8\u5206\u7c7b\u52b3\u52a8\u5bc6\u96c6\u4e14\u6602\u8d35\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5168\u6807\u8bb0\u6570\u636e\u548c\u9ad8\u5ea6\u9884\u5904\u7406]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faSSI-GAN\uff0c\u4f7f\u7528Swin\u542f\u53d1\u7684\u79fb\u4f4d\u7a97\u53e3\u5224\u522b\u5668\u548c\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5668\uff0c\u4ec5\u97001-3%\u6807\u8bb0\u6570\u636e]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u8fbe\u523099.93%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6807\u8bb0\u5de5\u4f5c\u91cf\u51cf\u5c1197-99%\uff0c\u5728\u6240\u6709\u611f\u67d3\u9636\u6bb5\u4fdd\u6301\u9ad8\u7cbe\u5ea6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [hybrid feature engineering, wavelet decomposition, graph-theoretic descriptors, linear separability, model compression]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Moirangthem Tiken Singh, Manibhushan Yaikhom"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00192",children:"https://arxiv.org/pdf/2601.00192"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 \xb5s latency) for real-time arrhythmia detection on edge devices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection<br>\u4f18\u5316\u7684\u6df7\u5408\u7279\u5f81\u5de5\u7a0b\u7528\u4e8e\u8d44\u6e90\u9ad8\u6548\u7684\u5fc3\u5f8b\u5931\u5e38\u68c0\u6d4b"] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B["Problem: Deep learning models are too heavy for edge devices.<br>\u6838\u5fc3\u95ee\u9898: \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927"]\n    C["Method: Hybrid feature engineering (wavelet + graph) for linear separability.<br>\u4e3b\u8981\u65b9\u6cd5: \u6df7\u5408\u7279\u5f81\u5de5\u7a0b\uff08\u5c0f\u6ce2+\u56fe\u8bba\uff09\u5b9e\u73b0\u7ebf\u6027\u53ef\u5206\u6027"]\n    D["Results: 98.44% accuracy, 8.54 KB model, 0.46 \xb5s latency.<br>\u5173\u952e\u7ed3\u679c: 98.44% \u51c6\u786e\u7387, 8.54 KB \u6a21\u578b, 0.46 \xb5s \u5ef6\u8fdf"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [LSTM, Transformer, Stock Prediction, Time Series Forecasting, Attention]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shaswat Mohanty"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00197",children:"https://arxiv.org/pdf/2601.00197"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Presents an enhanced StockBot architecture for systematic evaluation of modern time-series forecasting models (attention-based, convolutional, recurrent) in a unified setting. 2. Demonstrates empirically that a carefully constructed vanilla LSTM model consistently outperforms transformer-based models in stock price forecasting accuracy and decision-making stability under default hyperparameters. 3. Highlights the robustness, data efficiency, and importance of architectural inductive bias of recurrent models for financial forecasting, especially in data-limited scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/131078c30708f9e13fee0c529bee858ed184717ff3b0bf5f762eda2a9370616c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents StockBot 2.0, a framework for evaluating time-series models for stock prediction. It finds that a vanilla LSTM model, despite its simplicity, outperforms more complex transformer-based models in forecasting accuracy and trading decision stability when trained with default settings, emphasizing the value of recurrent inductive biases for financial data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Forecasting financial markets is challenging due to complexity and volatility."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Enhanced StockBot architecture for systematic evaluation of attention, CNN, and RNN models."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Vanilla LSTM achieves superior accuracy and stable decisions compared to transformers."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Unknown Aware AI-Generated Content Attribution"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image attribution], [generative model attribution, constrained optimization, open world setting, CLIP features, unknown generators]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ellie Thieu, Jifan Zhang, Haoyue Bai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Wisconsin\u2013Madison (UW\u2013Madison)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00218",children:"https://arxiv.org/pdf/2601.00218"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Establishes a strong baseline for target generator attribution using CLIP features and a linear classifier with limited labeled data. 2. Proposes a novel constrained optimization method that leverages unlabeled "wild" data from the internet to improve robustness to unseen generators. 3. Demonstrates that incorporating unlabeled wild data substantially improves attribution performance on challenging, unseen, and newly released generative models.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53bb1bd3ca2eb8682e06279d4402b6c4441fc76d44f3b86a930479ba697b4e06_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53bb1bd3ca2eb8682e06279d4402b6c4441fc76d44f3b86a930479ba697b4e06_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of attributing AI-generated images to their specific source model in an open-world setting where new, unseen generators constantly emerge. The authors propose a constrained optimization approach that uses unlabeled data collected from the internet to encourage the classifier to treat unknown samples as non-target, while maintaining performance on known labeled data. The method significantly improves attribution accuracy on challenging, unseen generative models compared to a baseline trained only on limited labeled data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Unknown Aware AI-Generated Content Attribution") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Attribution in open world with unseen generators")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Constrained optimization with unlabeled wild data")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Improved performance on unseen generators")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Robust Graph Fine-Tuning with Adversarial Graph Prompting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [Adversarial Graph Prompting, Parameter-Efficient Fine-Tuning, Graph Prompt Learning, Adversarial Learning, Robust Fine-Tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ziyan Zhang, Bo Jiang, Jin Tang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Anhui University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00229",children:"https://arxiv.org/pdf/2601.00229"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Adversarial Graph Prompting (AGP) framework that integrates adversarial learning into graph prompting for robust parameter-efficient fine-tuning of pre-trained GNNs. 2. Formulates AGP as a min-max optimization problem and develops an alternating optimization scheme, featuring a Joint Projected Gradient Descent (JointPGD) algorithm for generating adversarial noise and a module for learning optimal node prompts. 3. Provides theoretical analysis demonstrating that AGP can handle both graph topology and node feature noise, confirming its versatility and robustness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/337af6d04faa59592d198afa70192352bc013ab310b674dbdaed1185325d78c7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/337af6d04faa59592d198afa70192352bc013ab310b674dbdaed1185325d78c7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the vulnerability of Parameter-Efficient Fine-Tuning (PEFT) methods for Graph Neural Networks (GNNs) to noise and attacks. It proposes a novel Adversarial Graph Prompting (AGP) framework that formulates robust fine-tuning as a min-max optimization problem, using adversarial noise generation and prompt learning to counteract it. The method is theoretically sound and experimentally validated to be more robust than state-of-the-art approaches across various graph noises."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Graph Fine-Tuning with Adversarial Graph Prompting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709PEFT\u65b9\u6cd5\u5bf9\u56fe\u566a\u58f0\u548c\u653b\u51fb\u8106\u5f31/Existing PEFT methods are vulnerable to graph noise & attacks]\n    C --\x3e C1[\u5bf9\u6297\u56fe\u63d0\u793a(AGP)\u6846\u67b6/Adversarial Graph Prompting (AGP) Framework]\n    C1 --\x3e C2[\u6700\u5c0f-\u6700\u5927\u4f18\u5316/Min-Max Optimization]\n    C2 --\x3e C3[\u5185\u5c42: JointPGD\u751f\u6210\u5bf9\u6297\u566a\u58f0/Inner: JointPGD for adversarial noise]\n    C2 --\x3e C4[\u5916\u5c42: \u5b66\u4e60\u6700\u4f18\u8282\u70b9\u63d0\u793a/Outer: Learn optimal node prompts]\n    D --\x3e D1[\u7406\u8bba\u8bc1\u660e\u5904\u7406\u62d3\u6251\u548c\u8282\u70b9\u566a\u58f0/Theoretically handles topology & node noise]\n    D --\x3e D2[\u5b9e\u9a8c\u9a8c\u8bc1\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027/Experiments validate robustness & effectiveness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [LoRA, K-FAC, Parameter-Efficient Fine-Tuning, Fisher Information, Dynamic Rank Adaptation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pritish Saha, Chandrav Rajbangshi, Rudra Goyal, Mohit Goyal, Anurag Deo, Biswajit Roy, Ningthoujam Dhanachandra Singh, Raxit Goswami, Amitava Das"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," RAAPID Lab, Pragya Lab (BITS Pilani, Goa)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00231",children:"https://arxiv.org/pdf/2601.00231"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces K-FAC-based gradient preconditioning in the low-rank subspace for more geometry-aware updates. 2. Proposes periodic Fisher-guided reprojection of the LoRA basis to suppress parameter drift. 3. Implements dynamic rank adaptation to concentrate capacity on high-signal directions, reducing the number of trainable parameters."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e9515d46161014bf32c8c1a74f165b3980c9984817a7e1ca0778ce4d66219f5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that standard LoRA/QLoRA methods are geometry-agnostic, leading to inefficient updates and parameter drift. It proposes GRIT, a new LoRA procedure that uses K-FAC preconditioning, Fisher-guided reprojection, and dynamic rank adaptation to make updates more curvature-aware. This approach matches or surpasses baseline performance while reducing trainable parameters by an average of 46% and achieving lower drift."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRIT: Geometry-Aware PEFT] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6807\u51c6LoRA/QLoRA\u5ffd\u7565\u66f2\u7387/Standard LoRA/QLoRA ignores curvature]\n    B --\x3e B2[\u5bfc\u81f4\u4f4e\u6548\u66f4\u65b0\u4e0e\u53c2\u6570\u6f02\u79fb/Causes inefficient updates & parameter drift]\n    C --\x3e C1[K-FAC\u9884\u6761\u4ef6\u68af\u5ea6/K-FAC preconditioned gradients]\n    C --\x3e C2[Fisher\u5f15\u5bfc\u91cd\u6295\u5f71/Fisher-guided reprojection]\n    C --\x3e C3[\u52a8\u6001\u79e9\u9002\u5e94/Dynamic rank adaptation]\n    D --\x3e D1[\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d/Matches or surpasses baselines]\n    D --\x3e D2[\u53c2\u6570\u51cf\u5c11~46%/Reduces parameters by ~46%]\n    D --\x3e D3[\u6f02\u79fb\u66f4\u4f4e/Lower drift]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chao Yang, Haoyuan Zheng, Yue Ma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi\u2019an Jiaotong Liverpool University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00237",children:"https://arxiv.org/pdf/2601.00237"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: \u7ea2\u5916\u6570\u636e\u7a00\u7f3a / IR Data Scarcity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: CycleGAN\u8de8\u6a21\u6001\u751f\u6210 + YOLOv8\u68c0\u6d4b / CycleGAN Cross-modal Generation + YOLOv8 Detection"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63a5\u8fd1\u5168\u76d1\u7763\u57fa\u51c6 / Performance Significantly Improved, Approaches Fully Supervised Benchmark"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [neuromorphic computing, state-space models, sparse attention, surrogate gradients, local learning rules]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Osvaldo Simeone"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University London (Intelligent Networked Systems Institute - INSI)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00245",children:"https://arxiv.org/pdf/2601.00245"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u80fd\u8017\u589e\u957f / Escalating AI Energy Requirements]\n    C --\x3e C1[\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u539f\u5219 / Neuromorphic Computing Principles]\n    C1 --\x3e C2[\u79bb\u6563\u7a00\u758f\u6fc0\u6d3b / Discrete & Sparse Activations]\n    C1 --\x3e C3[\u5faa\u73af\u52a8\u6001 / Recurrent Dynamics]\n    C --\x3e C4[\u5904\u7406\u6846\u67b6: \u4ee4\u724c\u5185\u4e0e\u4ee4\u724c\u95f4 / Processing Framework: Intra-Token vs. Inter-Token]\n    D --\x3e D1[\u73b0\u4ee3AI\u4f53\u73b0\u795e\u7ecf\u5f62\u6001\u539f\u5219 / Modern AI Embodies Neuromorphic Principles]\n    D --\x3e D2[\u8fde\u63a5SNN\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001Transformer / Connects SNNs, State-Space Models, Transformers]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Rectifying Adversarial Examples Using Their Vulnerabilities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial defense], [adversarial examples, label rectification, re-attack, white-box attack, black-box attack]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fumiya Morimoto, Ryuto Morita, Satoshi Ono"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kagoshima University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00270",children:"https://arxiv.org/pdf/2601.00270"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel adversarial example rectification method based on "re-attacking" AEs to move them beyond the decision boundary for correct label estimation. 2. The method is designed to be straightforward, requiring only AEs as input without parameter adjustments or preliminary training, enabling it to address diverse attack types. 3. Demonstrates consistent performance and superior stability against various attacks, including targeted and black-box attacks, compared to conventional rectification and input transformation methods.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the problem of rectifying adversarial examples (AEs) to recover the correct labels of the original inputs, which is crucial for applications like autonomous driving. The proposed method works by "re-attacking" the AEs to push them across the model\'s decision boundary. The results show that this method performs consistently across different attack types and is more stable than existing approaches.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Rectifying Adversarial Examples Using Their Vulnerabilities] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: DNNs misclassify adversarial examples, needing correct label recovery)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Re-attack AEs to move them beyond decision boundary)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Consistent performance across attacks, outperforms conventional methods in stability)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00282",children:"https://arxiv.org/pdf/2601.00282"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Large Language Models Still Explain Themselves?<br/>\u5927\u8bed\u8a00\u6a21\u578b\u8fd8\u80fd\u89e3\u91ca\u81ea\u5df1\u5417\uff1f"] --\x3e Problem["Quantization\'s effect on Self-Explanations is unknown.<br/>\u91cf\u5316\u5bf9\u81ea\u6211\u89e3\u91ca\u7684\u5f71\u54cd\u672a\u77e5"]\n    Root --\x3e Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>\u8bc4\u4f30\u91cf\u5316\u540eLLM\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u53cd\u4e8b\u5b9e\u793a\u4f8b"]\n    Root --\x3e Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>\u8d28\u91cf/\u5fe0\u5b9e\u5ea6\u9002\u5ea6\u4e0b\u964d\uff1b\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [theoretical machine learning], [kernel evolution, spectral truncation, label rank compression, Laplacian spectral filtering, Neural Tangent Kernel]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hongxi Li, Chunlin Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00276",children:"https://arxiv.org/pdf/2601.00276"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Derives a kernel ODE revealing a "water-filling" spectral law for supervised learning, showing the kernel is compressed into a low-rank subspace bounded by the number of classes. 2. Proves that any stable steady state under L2-regularization and linear readout inherently exhibits label-driven rank compression, independent of the fast-readout approximation. 3. Demonstrates that SGD noise is confined to a low-rank subspace (O(C)), unifying deterministic and stochastic views of alignment, and contrasts this with expansive self-supervised representations.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03dc1352545d41fe155dce9a675ce3e4dfa7f83f33a1829acc745f5fa2954776_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03dc1352545d41fe155dce9a675ce3e4dfa7f83f33a1829acc745f5fa2954776_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops a kernel-centric theory for feature learning in wide neural networks. It shows that supervised learning inherently compresses the kernel into a low-rank subspace bounded by the number of classes, and that SGD noise is similarly confined, contrasting this compressive behavior with the expansive representations of self-supervised learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Task-Driven Kernel Flows<br>\u4efb\u52a1\u9a71\u52a8\u7684\u6838\u6d41] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[Understanding kernel evolution in feature learning<br>\u7406\u89e3\u7279\u5f81\u5b66\u4e60\u4e2d\u7684\u6838\u6f14\u5316]\nC --\x3e C1[Kernel ODE & algebraic analysis<br>\u6838ODE\u4e0e\u4ee3\u6570\u5206\u6790]\nD --\x3e D1[Supervised learning is compressive (rank \u2264 C)<br>\u76d1\u7763\u5b66\u4e60\u662f\u538b\u7f29\u6027\u7684 (\u79e9 \u2264 C)]\nD --\x3e D2[SGD noise is low-rank (O(C))<br>SGD\u566a\u58f0\u662f\u4f4e\u79e9\u7684 (O(C))]\nD --\x3e D3[Contrast with expansive self-supervision<br>\u4e0e\u6269\u5f20\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5bf9\u6bd4]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Optimal Transport Improve Federated Inverse Reinforcement Learning?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Inverse Reinforcement Learning, Federated Learning, Optimal Transport, Wasserstein Barycenter, Maximum Entropy IRL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," David Millard, Ali Baheri"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rochester Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00309",children:"https://arxiv.org/pdf/2601.00309"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an optimal transport-based approach for federating learned reward functions in Inverse Reinforcement Learning (IRL). 2. Proposes using a Wasserstein barycenter for reward fusion, which accounts for the geometric structure of the reward landscape, as opposed to simple parameter averaging. 3. Provides a theoretical proof that the barycentric fusion yields a more faithful global reward estimate than conventional federated averaging methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of learning a shared reward function across heterogeneous agents in privacy-sensitive, communication-limited settings. It proposes a federated IRL framework where agents perform local Maximum Entropy IRL and then fuse their reward functions via a Wasserstein barycenter. The authors prove this method provides a more accurate global reward estimate than standard parameter averaging, offering a principled and efficient solution for multi-agent systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Optimal Transport Improve Federated Inverse Reinforcement Learning?"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Heterogeneous agents need a shared reward, but data pooling is impractical due to privacy, dynamics differences, and limited bandwidth."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Local lightweight MaxEnt IRL followed by reward fusion via Wasserstein barycenter."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Barycentric fusion yields a more faithful global reward estimate than parameter averaging."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Quantum King-Ring Domination in Chess: A QAOA Approach"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [quantum optimization], [QAOA, benchmark, constraint-preserving mixers, warm-start, CVaR]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gerhard Stenzel, Michael K\xf6lle, Tobias Rohe, Julian Hager, Leo S\xfcnkel, Maximilian Zorn, Claudia Linnhoff-Popien"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," LMU Munich"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00318",children:"https://arxiv.org/pdf/2601.00318"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduction of the Quantum King-Ring Domination (QKRD) benchmark, a structured, NISQ-scale testbed derived from chess with 5,000 instances. 2. Systematic evaluation of QAOA design choices, showing the advantages of constraint-preserving mixers and warm-start strategies. 3. Demonstration that structured benchmarks reveal performance insights for problem-informed QAOA techniques that are obscured in random synthetic instances."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0b58913db95e3acf390bd659716f38782d77c5b0f4c58736c855c0b4940575b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0b58913db95e3acf390bd659716f38782d77c5b0f4c58736c855c0b4940575b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces a new structured benchmark called Quantum King-Ring Domination (QKRD) based on chess to evaluate the Quantum Approximate Optimization Algorithm (QAOA). Using this benchmark, the authors systematically test various QAOA design choices and find that constraint-preserving mixers and warm-start strategies significantly improve performance. The results show that structured benchmarks are crucial for revealing the advantages of problem-informed quantum optimization techniques."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Quantum King-Ring Domination in Chess: A QAOA Approach] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u8bed\u4e49\u7ed3\u6784/Existing benchmarks lack semantic structure]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u7684QKRD\u57fa\u51c6/Propose chess-based QKRD benchmark]\n    C --\x3e C2[\u7cfb\u7edf\u8bc4\u4f30QAOA\u8bbe\u8ba1\u9009\u62e9/Systematically evaluate QAOA design choices]\n    D --\x3e D1[\u7ea6\u675f\u4fdd\u6301\u6df7\u9891\u5668\u6536\u655b\u66f4\u5feb/Constraint-preserving mixers converge faster]\n    D --\x3e D2[\u70ed\u542f\u52a8\u7b56\u7565\u663e\u8457\u6539\u8fdb/Warm-start strategies yield significant improvement]\n    D --\x3e D3[\u7ed3\u6784\u5316\u57fa\u51c6\u63ed\u793a\u9690\u85cf\u4f18\u52bf/Structured benchmarks reveal obscured advantages]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Smart Fault Detection in Nanosatellite Electrical Power System"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [fault diagnosis], [neural network, PCA classification, decision tree, KNN]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alireza Rezaee, Niloofar Nobahari, Amin Asgarifar, Farshid Hajati"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Tehran, University of New England"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00335",children:"https://arxiv.org/pdf/2601.00335"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new fault detection method for nanosatellite electrical power systems operating without an Attitude Determination Control Subsystem (ADCS) in LEO orbit. 2. Uses a neural network to simulate the fault-free system behavior using solar radiation and panel temperature as inputs to predict current and load. 3. Applies multiple machine learning classifiers (neural network, PCA, decision tree, KNN) to diagnose specific fault patterns and types in the power subsystem."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab383b2f3d676c3a09faf3b55b3c7d4dd74c574ecc0a93d40c28f82897ffe1bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab383b2f3d676c3a09faf3b55b3c7d4dd74c574ecc0a93d40c28f82897ffe1bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning-based method for detecting faults in nanosatellite electrical power systems. It first simulates normal system behavior using a neural network and then employs various classifiers to identify specific fault types. The approach aims to diagnose common faults like line-to-line shorts and open circuits without relying on an ADCS."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Smart Fault Detection in Nanosatellite Electrical Power System] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LEO\u8f68\u9053\u65e0ADCS\u7684\u7eb3\u536b\u661f\u7535\u6e90\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b/Fault detection for nanosatellite EPS without ADCS in LEO]\n    C --\x3e C1[\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u65e0\u6545\u969c\u7cfb\u7edf/Simulate fault-free system with neural network]\n    C --\x3e C2[\u5e94\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u8fdb\u884c\u6545\u969c\u8bca\u65ad/Apply multiple ML classifiers for fault diagnosis]\n    D --\x3e D1[\u8bca\u65ad\u5149\u4f0f\u3001\u8f6c\u6362\u5668\u3001\u7535\u6c60\u7b49\u7279\u5b9a\u6545\u969c/Diagnose specific faults in PV, converter, battery]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual representation learning], [Joint Embedding Predictive Architecture (JEPA), BERT, CLS token, language-agnostic embedding, multilingual benchmarks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taj Gillin, Adam Lalani, Kenneth Zhang, Marcel Mateos Salles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Brown University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00366",children:"https://arxiv.org/pdf/2601.00366"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BERT-JEPA (BEPA), a novel training paradigm that adds a JEPA objective to BERT-style models to reorganize the [CLS] embedding space. 2. Demonstrates that BEPA finetuning transforms the [CLS] embedding space into a semantic-first, language-agnostic space, shifting its PCA representation from low-rank to fuller-rank. 3. Shows that this reorganization improves performance on multilingual tasks with little to no loss in English performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cd5b4a28e22d003dd88f59a4d1a1a55a97fa54c9c398a578c710764342d3180_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the problem that BERT\'s [CLS] embeddings fail to capture language-invariant semantics. It proposes BERT-JEPA (BEPA), a method that adds a Joint Embedding Predictive Architecture (JEPA) objective during training to reorganize the [CLS] embedding space into a language-agnostic "thought space". The main conclusion is that this approach significantly improves performance on multilingual benchmarks while maintaining English task performance.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: CLS embeddings are not language-invariant and fail to capture true sentence semantics."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Add JEPA training objective to BERT to create a language-agnostic embedding space."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Improved multilingual benchmark performance; reorganized, semantic-first CLS space."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deterministic Coreset for Lp Subspace"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [randomized algorithms, numerical linear algebra, data summarization], [coreset, subspace embedding, \u2113p regression, deterministic algorithm, iterative algorithm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rachit Chhaya, Anirban Dasgupta, Dan Feldman, Supratim Shit"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dhirubhai Ambani University, IIT Gandhinagar, University of Haifa, IIIT-Delhi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00361",children:"https://arxiv.org/pdf/2601.00361"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the first iterative algorithm for constructing a deterministic \u03b5-coreset for \u2113p subspace embedding for any p in [1,\u221e). 2. Achieves an optimal coreset size of O(d^{max(1,p/2)}/\u03b5\xb2), removing long-standing logarithmic factors. 3. Provides a deterministic guarantee for the coreset, enabling its use for approximately solving \u2113p regression deterministically."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b23fc2c49cea022bba3ab19cb79b7de330860628aab1b669177840bc826ecda1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a new iterative algorithm for constructing a deterministic coreset that provides an \u2113p subspace embedding for any p\u22651. The method ensures bounded loss in each iteration, leading to a coreset whose size is optimal and free of logarithmic factors. The result solves a long-standing open problem and enables deterministic approximate solutions to \u2113p regression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Deterministic Coreset for Lp Subspace") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u9700\u8981\u4e3a \u2113p \u5b50\u7a7a\u95f4\u5d4c\u5165\u6784\u5efa\u786e\u5b9a\u6027\u6838\u5fc3\u96c6 / Need deterministic coreset for \u2113p subspace embedding")\n    Problem --\x3e P2("\u73b0\u6709\u6838\u5fc3\u96c6\u5b58\u5728\u5bf9\u6570\u56e0\u5b50 / Existing coresets have log factors")\n    Method --\x3e M1("\u8fed\u4ee3\u7b97\u6cd5 / Iterative algorithm")\n    Method --\x3e M2("\u786e\u4fdd\u6709\u754c\u635f\u5931 / Ensures bounded loss")\n    Results --\x3e R1("\u6838\u5fc3\u96c6\u5927\u5c0f: O(d^{max(1,p/2)}/\u03b5\xb2) / Coreset size: O(d^{max(1,p/2)}/\u03b5\xb2)")\n    Results --\x3e R2("\u79fb\u9664\u5bf9\u6570\u56e0\u5b50 / Removes log factors")\n    Results --\x3e R3("\u786e\u5b9a\u6027\u4fdd\u8bc1 / Deterministic guarantee")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [cyber-physical systems security], [intrusion detection system, anomaly detection, G-code manipulation, transformer encoder, self-attention autoencoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Mahbub Hasan, Marcus Sternhagen, Krishna Chandra Roy"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New Mexico Institute of Mining and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00384",children:"https://arxiv.org/pdf/2601.00384"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Investigation of stealthy Man-in-the-Middle (MitM) attack vectors targeting the CAD-to-machine interface in Fused Deposition Modeling (FDM) 3D printers. 2. Proposal of an unsupervised Intrusion Detection System (IDS) that uses a frozen Transformer-based encoder and contrastive learning to create anomaly-sensitive embeddings from machine logs. 3. Demonstration of effective anomaly classification using a combination of clustering and a self-attention autoencoder on real 3D printing systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b296e2b64944be1e0ab79e116131c11acbb4a662a6a9c3e8adaf377db0c3190e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates stealthy cyberattacks that manipulate G-code in additive manufacturing systems, leading to structurally defective parts. To detect these attacks, the authors propose an unsupervised intrusion detection system that uses a Transformer-based encoder and contrastive learning to analyze machine logs. Their method successfully distinguishes between normal and compromised printing executions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[AM\u7cfb\u7edf\u7684\u65b0\u653b\u51fb\u9762 / New Attack Surfaces in AM]\nB --\x3e B2[\u9690\u79d8\u7684\u4e2d\u95f4\u4eba\u653b\u51fb / Stealthy MitM Attacks]\nC --\x3e C1[\u57fa\u4e8e\u65e5\u5fd7\u7684\u65e0\u76d1\u7763IDS / Unsupervised IDS from Logs]\nC --\x3e C2[Transformer\u7f16\u7801\u5668 / Transformer Encoder]\nC --\x3e C3[\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u81ea\u6ce8\u610f\u529b / Contrastive Learning & Self-Attention]\nD --\x3e D1[\u6709\u6548\u533a\u5206\u6b63\u5e38\u4e0e\u653b\u51fb / Effectively Distinguishes Benign & Compromised]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [aerial video, optical flow, convolutional neural network, hierarchical extreme learning machine, UCF-ARG dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nouar AlDahoul, Aznul Qalid Md Sabri, Ali Mohammed Mansoor"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Malaya"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00391",children:"https://arxiv.org/pdf/2601.00391"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a framework combining optical flow with three different deep learning models (S-CNN, pretrained CNN, H-ELM) for human detection in challenging aerial videos. 2. Conducts a comparative performance analysis of the models on the UCF-ARG dataset, evaluating accuracy and training speed across five human actions. 3. Demonstrates the effectiveness of automatic feature learning over handcrafted features for handling dynamic events like camera jitter and scale variation in aerial footage."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/104ed4332f4aebfb8e2617a89656868c3e4e5fae6201be63d4509d5b080a2f50_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/104ed4332f4aebfb8e2617a89656868c3e4e5fae6201be63d4509d5b080a2f50_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of human detection in aerial videos with a non-static camera. It proposes using optical flow combined with three deep learning models (S-CNN, a pretrained CNN, and H-ELM) for automatic feature learning. The experiments on the UCF-ARG dataset show that the pretrained CNN achieves the highest accuracy (98.09%), successfully demonstrating the method's robustness to dynamic conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u52a8\u6001\u4e8b\u4ef6\u6311\u6218<br/>Dynamic Event Challenges]\n    B1 --\x3e B2[\u5149\u7167\u53d8\u5316, \u76f8\u673a\u6296\u52a8, \u76ee\u6807\u5c3a\u5bf8\u53d8\u5316<br/>Illumination Changes, Camera Jitter, Object Size Variation]\n    C --\x3e C1[\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5<br/>Feature Learning Methods]\n    C1 --\x3e C2[\u7ed3\u5408\u5149\u6d41\u4e0e\u4e09\u79cd\u6df1\u5ea6\u6a21\u578b<br/>Combine Optical Flow with Three Deep Models]\n    C2 --\x3e C3[\u76d1\u7763CNN, \u9884\u8bad\u7ec3CNN, \u5206\u5c42\u6781\u9650\u5b66\u4e60\u673a<br/>S-CNN, Pretrained CNN, H-ELM]\n    D --\x3e D1[\u9884\u8bad\u7ec3CNN\u51c6\u786e\u7387\u6700\u9ad8<br/>Pretrained CNN Highest Accuracy]\n    D1 --\x3e D2[\u5e73\u5747\u51c6\u786e\u738798.09%<br/>Average Accuracy 98.09%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [network intrusion detection], [timing-controlled evasion, weighted fair queueing (WFQ), network-optimised spiking (NOS), metadata-only detection, streaming IDS]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Muhammad Bilal, Omer Tariq, Hasan Ahmed"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Lancaster University, Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00389",children:"https://arxiv.org/pdf/2601.00389"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed NOS-Gate, a lightweight, streaming IDS for consumer gateways that uses a two-state unit derived from Network-Optimised Spiking dynamics per flow. 2. Introduced a queue-aware, reversible mitigation action that temporarily reduces a flagged flow's weight under Weighted Fair Queueing (WFQ). 3. Developed an executable 'worlds' benchmark for evaluating IDS under timing-controlled evasion, specifying benign processes, attacker budgets, and enabling packet-level WFQ replay."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16e859896027b80a597ad555df2beab42dc7cf683d8aef57af2a60ff1820126c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16e859896027b80a597ad555df2beab42dc7cf683d8aef57af2a60ff1820126c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of detecting intrusions in encrypted traffic on resource-constrained consumer gateways, where attackers can evade detection by manipulating timing patterns. It proposes NOS-Gate, a lightweight streaming IDS that uses metadata features and a novel mitigation strategy integrated with queue management. The evaluation shows NOS-Gate achieves higher detection recall and reduces queueing delays compared to baselines, with low computational overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[NOS-Gate: Queue-Aware Streaming IDS] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u52a0\u5bc6\u6d41\u91cf\u4e2d\u65f6\u5e8f\u6a21\u5f0f\u6cc4\u9732/Timing patterns leak through encryption]\n    Problem --\x3e P2[\u653b\u51fb\u8005\u8fdb\u884c\u65f6\u5e8f\u63a7\u5236\u89c4\u907f/Attacker uses timing-controlled evasion]\n    Problem --\x3e P3[\u7f51\u5173\u8d44\u6e90\u4e25\u683c\u53d7\u9650/Gateway has tight CPU & latency budget]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u8f7b\u91cf\u7ea7\u53cc\u6001\u5355\u5143/Lightweight two-state NOS unit per flow]\n    Method --\x3e M2[\u57fa\u4e8e\u5143\u6570\u636e\u7a97\u53e3\u7684\u8bc4\u5206/Score fixed-length metadata windows]\n    Method --\x3e M3[\u53ef\u9006\u7684WFQ\u6743\u91cd\u7f13\u89e3/Reversible WFQ weight mitigation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9ad8\u4e8b\u4ef6\u53ec\u56de\u7387/High incident recall (0.952)]\n    Results --\x3e R2[\u964d\u4f4e\u6392\u961f\u5ef6\u8fdf/Reduced p99.9 queueing delay]\n    Results --\x3e R3[\u4f4e\u8ba1\u7b97\u5f00\u9500/Low scoring cost (~2.09 \xb5s)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [time-warp emulation, CUDA interception, virtual time coordination, performance modeling, discrete-event simulation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00397",children:"https://arxiv.org/pdf/2601.00397"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GPU\u96c6\u7fa4\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u6162/Evaluating serving configs on GPU clusters is slow and expensive]\n    B --\x3e B2[\u6a21\u62df\u5668\u9700\u8981\u91cd\u5199\u63a7\u5236\u903b\u8f91/Simulators require re-implementing complex control logic]\n    C --\x3e C1[\u62e6\u622aCUDA API\u8c03\u7528/Intercept CUDA API calls]\n    C --\x3e C2[\u865a\u62df\u65f6\u95f4\u8df3\u8dc3/Virtual time jumps based on kernel predictions]\n    C --\x3e C3[\u5206\u5e03\u5f0f\u534f\u8c03\u534f\u8bae/Distributed coordination protocol]\n    D --\x3e D1[<5%\u9884\u6d4b\u8bef\u5dee/<5% prediction error]\n    D --\x3e D2[5-17\u500d\u52a0\u901f/5-17x faster than real execution]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [Privacy-preserving data aggregation], [unanimous-release confidentiality, consensus locking, malicious deviation detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Prajwal Panth, Sahaj Raj Malla"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KIIT University, Kathmandu University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00418",children:"https://arxiv.org/pdf/2601.00418"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (\u03c3_S, \u03c3_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework's properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Secure, Verifiable, and Scalable Multi-Client Data Sharing<br>\u5b89\u5168\u3001\u53ef\u9a8c\u8bc1\u3001\u53ef\u6269\u5c55\u7684\u591a\u5ba2\u6237\u7aef\u6570\u636e\u5171\u4eab] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Secure multi-client data aggregation with privacy and verifiability<br>\u5b89\u5168\u3001\u53ef\u9a8c\u8bc1\u7684\u591a\u5ba2\u6237\u7aef\u9690\u79c1\u6570\u636e\u805a\u5408)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Consensus-Based Privacy-Preserving Data Distribution (CPPDD)<br>\u57fa\u4e8e\u5171\u8bc6\u7684\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5206\u53d1)\n    C --\x3e C1(Affine Masking & Consensus Locking<br>\u4eff\u5c04\u63a9\u7801\u4e0e\u5171\u8bc6\u9501\u5b9a)\n    C --\x3e C2(Step/Data Checksums (\u03c3_S, \u03c3_D)<br>\u6b65\u9aa4/\u6570\u636e\u6821\u9a8c\u548c)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Linear scalability, 100% deviation detection, lower FLOPs vs MPC/HE<br>\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff0c100%\u5f02\u5e38\u68c0\u6d4b\uff0c\u76f8\u6bd4MPC/HE\u66f4\u4f4e\u7684\u8ba1\u7b97\u91cf)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [efficient transformers], [astrocyte-inspired computing, long-term plasticity (LTP), short-term plasticity (STP), memory compression, Long Range Arena (LRA)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Pennsylvania State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00426",children:"https://arxiv.org/pdf/2601.00426"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[RMAAT: Astrocyte-Inspired Memory Compression and Replay] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6/Quadratic Complexity of Self-Attention]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u542f\u53d1\u7684\u5faa\u73af\u8bb0\u5fc6\u67b6\u6784/Astrocyte-Inspired Recurrent Memory Architecture]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728LRA\u57fa\u51c6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387/Competitive Accuracy & Efficiency on LRA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Delta Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00417",children:"https://arxiv.org/pdf/2601.00417"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/yifanzhang-pro/deep-delta-learning",children:"https://github.com/yifanzhang-pro/deep-delta-learning"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Deep Delta Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6b8b\u5dee\u7f51\u7edc\u9650\u5236/ResNet Limitation]\n    B1 --\x3e B2["\u521a\u6027\u76f8\u52a0\u504f\u7f6e/Rigid Additive Bias"]\n    B2 --\x3e B3["\u9650\u5236\u590d\u6742\u72b6\u6001\u8f6c\u6362/Limits Complex State Transitions"]\n    C --\x3e C1[Delta \u7b97\u5b50/Delta Operator]\n    C1 --\x3e C2["\u79e9-1\u6270\u52a8/ Rank-1 Perturbation"]\n    C2 --\x3e C3["\u53ef\u5b66\u4e60\u51e0\u4f55\u53d8\u6362/Learnable Geometric Transform"]\n    C3 --\x3e C4["\u52a8\u6001\u95e8\u63a7/Dynamic Gating (\u03b2)"]\n    D --\x3e D1["\u8c31\u5206\u6790/Spectral Analysis"]\n    D1 --\x3e D2["\u63d2\u503c\u8eab\u4efd/\u6295\u5f71/\u53cd\u5c04/Interpolates Identity/Projection/Reflection"]\n    D --\x3e D3["\u540c\u6b65\u79e9-1\u6ce8\u5165/Synchronous Rank-1 Injection"]\n    D3 --\x3e D4["\u63a7\u5236\u8f6c\u6362\u8c31/Controls Transition Spectrum"]\n    D4 --\x3e D5["\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3/Preserves Stable Training"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00423",children:"https://arxiv.org/pdf/2601.00423"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/shengjun-zhang/VisualGRPO",children:"https://github.com/shengjun-zhang/VisualGRPO"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u53bb\u566a\u6b65\u4e0a\u4f18\u5316\uff0c\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u6a21\u7cca/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n    C --\x3e C1[\u63d0\u51faE-GRPO: \u71b5\u611f\u77e5\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n    C1 --\x3e C2[\u5408\u5e76\u4f4e\u71b5\u6b65\u4e3a\u9ad8\u71b5SDE\u91c7\u6837\u6b65\uff0c\u5176\u4ed6\u6b65\u7528ODE\u91c7\u6837/Merge low-entropy steps for SDE, use ODE elsewhere]\n    C1 --\x3e C3[\u5f15\u5165\u591a\u6b65\u5206\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u8ba1\u7b97/Introduce multi-step group normalized advantage]\n    D --\x3e D1[\u5728\u4e0d\u540c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027/Method effectiveness demonstrated across different reward settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Comparative Analysis of Interpretable Machine Learning Methods"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretable machine learning], [Explainable Boosting Machines (EBMs), Symbolic Regression (SR), Generalized Optimal Sparse Decision Trees (GOSDT), Interpretable Generalized Additive Neural Networks (IGANNs), tabular data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mattia Billa, Giovanni Orlandi, Veronica Guidetti, Federica Mandreoli"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Modena and Reggio Emilia"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00428",children:"https://arxiv.org/pdf/2601.00428"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a large-scale comparative evaluation of 16 inherently interpretable methods across 216 real-world tabular datasets. 2. Stratified performance analysis based on structural dataset characteristics (dimensionality, sample size, linearity, class imbalance) and assessed training time and robustness under distributional shifts. 3. Provided empirical findings on performance hierarchies and context-dependent model suitability, offering practical guidance for balancing interpretability and predictive performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b45f5348139a7a9be289787000b2099d616e153113ae1190dc4749f573df65bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b45f5348139a7a9be289787000b2099d616e153113ae1190dc4749f573df65bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the scarcity of systematic evaluations for inherently interpretable machine learning models on tabular data by conducting a large-scale benchmark of 16 methods. The study analyzes performance across 216 datasets, considering structural characteristics and robustness. The results show that EBMs are strong for regression, while SR and IGANNs excel in non-linear settings, and GOSDT is sensitive to class imbalance, providing context-dependent guidance for practitioners."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Comparative Analysis of Interpretable Machine Learning Methods<br/>\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6bd4\u8f83\u5206\u6790] --\x3e B[Problem: Need for systematic evaluation of inherently interpretable models for tabular data<br/>\u95ee\u9898: \u7f3a\u4e4f\u5bf9\u8868\u683c\u6570\u636e\u56fa\u6709\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u7cfb\u7edf\u8bc4\u4f30]\n    A --\x3e C[Method: Large-scale benchmark of 16 interpretable methods across 216 datasets, stratified by data characteristics<br/>\u65b9\u6cd5: \u5728216\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf916\u79cd\u65b9\u6cd5\u8fdb\u884c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6309\u6570\u636e\u7279\u5f81\u5206\u5c42]\n    A --\x3e D[Results: EBMs strong for regression; SR/IGANNs good for non-linear data; GOSDT sensitive to imbalance<br/>\u7ed3\u679c: EBMs\u5728\u56de\u5f52\u4e2d\u8868\u73b0\u597d\uff1bSR/IGANNs\u5728\u975e\u7ebf\u6027\u6570\u636e\u4e2d\u8868\u73b0\u597d\uff1bGOSDT\u5bf9\u4e0d\u5e73\u8861\u654f\u611f]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series anomaly detection], [time series foundation models, parameter-efficient fine-tuning, anomaly detection, LoRA, AUC-PR]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Miseon Park, Kijung Yoon"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hanyang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00446",children:"https://arxiv.org/pdf/2601.00446"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematically evaluated the use of Time Series Foundation Models (TSFMs) as universal backbones for anomaly detection, showing they outperform task-specific models. 2. Compared multiple adaptation strategies (zero-shot, full fine-tuning, PEFT) for TSFMs across benchmarks, highlighting their versatility. 3. Demonstrated that Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA can match or exceed full fine-tuning performance while being computationally cheaper."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94e39b065d01be71b9920224b4a74b3db8c40a04646e3c3b4b83dce81d7aa0c3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/94e39b065d01be71b9920224b4a74b3db8c40a04646e3c3b4b83dce81d7aa0c3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether pretrained Time Series Foundation Models (TSFMs) can be effectively adapted for anomaly detection. It compares different adaptation strategies, including zero-shot inference and parameter-efficient fine-tuning (PEFT). The results show that TSFMs, especially when adapted with PEFT methods like LoRA, outperform traditional task-specific models, offering a scalable and efficient solution for time series anomaly detection."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection<br>\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u9002\u5e94\u7b56\u7565\u7684\u6bd4\u8f83\u7814\u7a76"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Most anomaly detection methods need extensive task-specific training.<br>\u5927\u591a\u6570\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7279\u5b9a\u4efb\u52a1\u7684\u8bad\u7ec3\u3002"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Adapt TSFMs using zero-shot, full fine-tuning, and PEFT (e.g., LoRA).<br>\u4f7f\u7528\u96f6\u6837\u672c\u3001\u5168\u5fae\u8c03\u548cPEFT\uff08\u5982LoRA\uff09\u6765\u9002\u5e94TSFMs\u3002"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>TSFMs outperform baselines; PEFT is efficient and effective.<br>TSFMs\u8d85\u8d8a\u57fa\u7ebf\uff1bPEFT\u9ad8\u6548\u4e14\u6709\u6548\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Controllable Concept Bottleneck Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [explainable ai], [Concept Bottleneck Models, Model Editing, Influence Functions, Machine Unlearning, Incremental Learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hongbin Lin, Chenyang Ren, Juangui Xu, Zhengyu Hu, Cheng-Long Wang, Yao Shu, Hui Xiong, Jingfeng Zhang, Di Wang, Lijie Hu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Based on the author list and affiliations (Hui Xiong, Fellow, IEEE), the primary institution is likely Rutgers University. Other affiliations may be present but are not explicitly listed in the provided content."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00451",children:"https://arxiv.org/pdf/2601.00451"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Controllable Concept Bottleneck Models (CCBMs) that support three granularities of model editing (concept-label, concept, and data-level) for dynamic maintenance. 2. Derives mathematically rigorous closed-form approximations for editing operations using influence functions, eliminating the need for retraining from scratch. 3. Demonstrates the efficiency and adaptability of CCBMs through experiments, validating their practical value for creating dynamic and trustworthy models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8610f217ce88f4c2d5513fec62b4fe3c5b8a65762f2a3fae9fcf60d3e539a686_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8610f217ce88f4c2d5513fec62b4fe3c5b8a65762f2a3fae9fcf60d3e539a686_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of efficiently editing Concept Bottleneck Models (CBMs) in dynamic real-world scenarios without retraining. It proposes Controllable Concept Bottleneck Models (CCBMs), which use influence functions to provide closed-form approximations for edits at concept-label, concept, and data levels. Experimental results show that CCBMs are efficient and adaptable, making them practical for maintaining trustworthy AI systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Controllable Concept Bottleneck Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9759\u6001CBM\u96be\u4ee5\u52a8\u6001\u7f16\u8f91/Static CBMs are hard to edit dynamically]\n    C --\x3e C1[CCBM\u4e0e\u5f71\u54cd\u51fd\u6570/CCBMs with Influence Functions]\n    D --\x3e D1[\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a/Efficient and Adaptable]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Imitation from Observations with Trajectory-Level Generative Embeddings"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [imitation learning], [imitation learning from observations, offline learning, diffusion models, trajectory embedding, surrogate reward]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yongtao Qu, Shangzhe Li, Weitong Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of North Carolina at Chapel Hill"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00452",children:"https://arxiv.org/pdf/2601.00452"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TGE, a trajectory-level generative embedding method for offline imitation learning from observations. 2. Introduces a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model. 3. Demonstrates that the method effectively bridges distributional gaps and outperforms prior methods on D4RL benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8023941a359742c0280304bc5c6d6f3a9e7332f5f298e3867930992f450af6fb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8023941a359742c0280304bc5c6d6f3a9e7332f5f298e3867930992f450af6fb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses offline imitation learning from observations where expert data is scarce and offline data is suboptimal. It proposes TGE, a method that uses a temporal diffusion model to create a smooth trajectory embedding for robust reward estimation. The approach outperforms existing methods on standard locomotion and manipulation benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Imitation from Observations with Trajectory-Level Generative Embeddings] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\uff0c\u4e13\u5bb6\u6570\u636e\u7a00\u7f3a\uff0c\u79bb\u7ebf\u6570\u636e\u4e0d\u7406\u60f3/Offline LfO, scarce expert data, imperfect offline data]\n    C --\x3e C1[\u63d0\u51faTGE\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5d4c\u5165\u6784\u5efa\u5e73\u6ed1\u7684\u8f68\u8ff9\u7ea7\u5956\u52b1/Propose TGE, constructs smooth trajectory-level reward via diffusion embedding]\n    D --\x3e D1[\u5728D4RL\u57fa\u51c6\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Matches or outperforms prior methods on D4RL benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Networks Learn Deep Hierarchical Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [theoretical machine learning], [hierarchical models, residual networks, layerwise SGD, efficient learnability, teacher-student framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amit Daniely"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hebrew University of Jerusalem, Google Research Tel Aviv"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00455",children:"https://arxiv.org/pdf/2601.00455"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proves that layerwise SGD on residual networks can efficiently learn a class of hierarchical models with polynomial depth, surpassing previous learnable models limited to log-depth. 2. Introduces a formal model where the existence of human teachers, providing granular labels, naturally reveals a hierarchical structure that facilitates learning. 3. Suggests that the learnability of deep hierarchical models could form a theoretical basis for understanding why deep learning works."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1faa5ee1ce6f2d929632472a3fef6f2a37f968b78b881ef42a244f56de38679_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper shows that layerwise stochastic gradient descent (SGD) on residual networks can efficiently learn a class of hierarchical models where labels are structured in increasingly complex levels. This class is more expressive, requiring polynomial depth, than previously known learnable models. The authors argue that this learnability, supported by a formal model of teaching, provides a potential theoretical foundation for understanding deep learning's success."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Deep Networks Learn Deep Hierarchical Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u7406\u89e3\u6df1\u5ea6\u7f51\u7edc\u4e3a\u4f55\u6709\u6548/Understanding why deep networks work]\nC --\x3e C1[\u5c42\u7ea7SGD\u4e0e\u6b8b\u5dee\u7f51\u7edc/Layerwise SGD on ResNets]\nC --\x3e C2[\u5f62\u5f0f\u5316\u6559\u5e08-\u5b66\u751f\u6a21\u578b/Formal teacher-student model]\nD --\x3e D1[\u53ef\u9ad8\u6548\u5b66\u4e60\u591a\u9879\u5f0f\u6df1\u5ea6\u6a21\u578b/Efficiently learn polynomial-depth models]\nD --\x3e D2[\u8d85\u8d8a\u5bf9\u6570\u6df1\u5ea6\u7535\u8def/Surpasses log-depth circuits]\nD --\x3e D3[\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u57fa\u7840/Provides basis for understanding deep learning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Laplacian Kernelized Bandit"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-armed bandits], [graph Laplacian, reproducing kernel Hilbert space (RKHS), Gaussian process, regret bound, multi-user contextual bandits]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuang Wu, Arash A. Amini"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00461",children:"https://arxiv.org/pdf/2601.00461"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a principled joint penalty combining graph smoothness and individual roughness, proving it is equivalent to the squared norm in a unified multi-user RKHS. 2. Explicitly derived the reproducing kernel for this RKHS, which fuses the graph Laplacian with a base arm kernel. 3. Designed two algorithms (LK-GP-UCB and LK-GP-TS) based on this kernel and provided theoretical regret bounds scaling with the kernel's effective dimension."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4eb4360cdf936fab9711bd6722ad2b67b34b282bf409e4e900beb17c23a0b2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4eb4360cdf936fab9711bd6722ad2b67b34b282bf409e4e900beb17c23a0b2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses multi-user contextual bandits where users are connected by a graph and have non-linear reward functions. It proposes a new kernel that combines graph structure with arm features, enabling the design of efficient Gaussian Process-based algorithms for exploration. The method provides strong theoretical regret guarantees and outperforms baselines in non-linear settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Laplacian Kernelized Bandit] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Multi-user contextual bandits with graph homophily and non-linear rewards]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified multi-user RKHS with Laplacian-fused kernel; Algorithms LK-GP-UCB & LK-GP-TS]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Regret bounds scale with effective kernel dimension; Outperforms baselines empirically]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Mixture-of-Experts, Orthogonality Regularization, Weight-Activation Gap, Sparse Activation, Expert Diversity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hyunjun Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00457",children:"https://arxiv.org/pdf/2601.00457"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Showed that orthogonality regularization fails to reduce weight-space overlap and yields inconsistent effects on model performance across different datasets. 2. Identified a significant disconnect between weight-space and activation-space orthogonality, with no significant correlation between the two. 3. Demonstrated that weight-space regularization is an unreliable optimization target for improving expert diversity in MoE models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/297630182c2a41472ac5ae250b1200161c3b50705c9ba5130f166dda38b1a979_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the effectiveness of applying orthogonality loss to enforce expert diversity in Mixture-of-Experts (MoE) models. The analysis reveals that this geometric regularization fails to reduce weight-space overlap, does not translate to activation-space orthogonality, and leads to inconsistent performance changes. The findings demonstrate that weight-space regularization is unsuitable for achieving MoE diversity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Geometric Regularization in MoEs: The Disconnect Between Weights and Activations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u4e13\u5bb6\u591a\u6837\u6027\u7684\u51e0\u4f55\u6b63\u5219\u5316\u4f5c\u7528\u4e0d\u660e\u786e/The role of geometric regularization in expert specialization is unclear)\n    C --\x3e C1(\u5e94\u7528\u6b63\u4ea4\u6027\u635f\u5931\u4ee5\u5f3a\u5236\u4e13\u5bb6\u591a\u6837\u6027/Apply orthogonality loss to enforce expert diversity)\n    D --\x3e D1(\u6743\u91cd\u7a7a\u95f4\u91cd\u53e0\u672a\u51cf\u5c11/Weight-space overlap not reduced)\n    D --\x3e D2(\u6fc0\u6d3b\u7a7a\u95f4\u91cd\u53e0\u4fdd\u6301\u9ad8\u4f4d/Activation-space overlap remains high)\n    D --\x3e D3(\u6027\u80fd\u5f71\u54cd\u4e0d\u4e00\u81f4/Inconsistent effects on performance)\n    D --\x3e D4(\u6743\u91cd\u4e0e\u6fc0\u6d3b\u6b63\u4ea4\u6027\u65e0\u663e\u8457\u76f8\u5173/No significant correlation between weight and activation orthogonality)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [biomedical signal processing], [EEG, UNet, data augmentation, spike wave discharges, seizure detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Virginia"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00459",children:"https://arxiv.org/pdf/2601.00459"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Comprehensive comparison of 14 machine learning classifiers on a large, manually annotated EEG dataset for SWD detection, identifying a 1D UNet as the best performer. 2. Enhancement of the 1D UNet model through data augmentation, with scaling identified as the most beneficial augmentation technique, resulting in the AugUNet1D model. 3. Public release of the AugUNet1D model (both pretrained and untrained) and demonstration of its superior performance against a state-of-the-art algorithmic method ("Twin Peaks").']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/284cd89a49b52c6de51bc92974e7ebd74725aa53109914c27ce628c85db7b46c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/284cd89a49b52c6de51bc92974e7ebd74725aa53109914c27ce628c85db7b46c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the time-consuming manual labeling of spike wave discharges (SWDs) in EEG recordings by proposing an automated detection method. The authors developed and evaluated a 1D UNet model enhanced with data augmentation (AugUNet1D) on a large mouse EEG dataset, finding it outperformed other classifiers and a recent algorithmic approach. The main conclusion is that AugUNet1D provides a superior, publicly available tool for accurate SWD detection."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Detecting SWD using 1D Residual UNet<br/>\u4f7f\u7528\u4e00\u7ef4\u6b8b\u5deeUNet\u68c0\u6d4bSWD] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[Problem: Manual EEG labeling is time-consuming<br/>\u95ee\u9898: \u624b\u52a8\u6807\u8bb0EEG\u8017\u65f6] --\x3e B1[Target: Automate SWD detection<br/>\u76ee\u6807: \u81ea\u52a8\u5316SWD\u68c0\u6d4b]\n    C[Method: 1D UNet with data augmentation (AugUNet1D)<br/>\u65b9\u6cd5: \u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7684\u4e00\u7ef4UNet] --\x3e C1[Compared 14 classifiers<br/>\u6bd4\u8f83\u4e8614\u79cd\u5206\u7c7b\u5668]\n    D[Results: AugUNet1D outperforms other methods<br/>\u7ed3\u679c: AugUNet1D\u6027\u80fd\u6700\u4f18] --\x3e D1[Model made public<br/>\u6a21\u578b\u5df2\u516c\u5f00]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Chains and Discrete Dynamical Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [neural chains, physics-informed neural networks (PINNs), finite-difference methods, Burgers equation, Eikonal equation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sauro Succi, Abhisek Ganguly, Santosh Ansumali"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Italian Institute of Technology, Jawaharlal Nehru Centre for Advanced Scientific Research (JNCASR), University of Roma Tre, Harvard University, Cornell University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00473",children:"https://arxiv.org/pdf/2601.00473"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and analyzes the analogy between transformer-based neural chains (without self-attention) and discrete dynamical systems from discretized neural integral/PDEs. 2. Conducts a comparative analysis between standard numerical discretization (finite-difference) and PINN learning for solving Burgers and Eikonal equations, showing they converge to similar dynamical knowledge. 3. Identifies that PINNs explore a vast space of random matrices, unlike the structured matrices of finite-difference methods, leading to more parameters, higher training costs, and reduced explainability for 1D problems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/484d1f1718d109a31c34806ec956587c6c88440887cfe665bb5795e2c2cad940_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the connection between neural chains (transformers without self-attention) and discrete dynamical systems. It compares solving PDEs like Burgers and Eikonal equations using standard finite-difference methods versus Physics-Informed Neural Networks (PINNs), finding both methods yield similar solutions but PINNs use many more random, less interpretable parameters. The authors conclude that for these 1D problems, PINNs offer no efficiency advantage over traditional methods, though their potential for high-dimensional problems remains open."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Neural Chains and Discrete Dynamical Systems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5206\u6790\u795e\u7ecf\u94fe\u4e0e\u79bb\u6563\u52a8\u529b\u7cfb\u7edf\u7684\u7c7b\u6bd4/Analyze analogy between neural chains and discrete dynamical systems]\n    B --\x3e B2[\u6bd4\u8f83PINN\u4e0e\u4f20\u7edf\u6570\u503c\u65b9\u6cd5/Compare PINNs vs. traditional numerical methods]\n    C --\x3e C1[\u5c06\u6570\u503c\u79bb\u6563\u5316\u8868\u8ff0\u4e3a\u795e\u7ecf\u94fe/Cast numerical discretization as neural chains]\n    C --\x3e C2[\u4f7f\u7528PINN\u6c42\u89e3\u65b9\u7a0b/Use PINNs to solve equations]\n    C --\x3e C3[\u6bd4\u8f83\u77e9\u9635\u7ed3\u6784\u4e0e\u53c2\u6570\u7a7a\u95f4/Compare matrix structure and parameter space]\n    D --\x3e D1[\u4e24\u79cd\u65b9\u6cd5\u83b7\u5f97\u76f8\u540c\u52a8\u529b\u5b66\u77e5\u8bc6/Both methods acquire same dynamical knowledge]\n    D --\x3e D2[PINN\u4f7f\u7528\u66f4\u591a\u968f\u673a\u53c2\u6570/PINNs use more random parameters]\n    D --\x3e D3[1D\u95ee\u9898\u4e2dPINN\u65e0\u6548\u7387\u4f18\u52bf/No PINN efficiency advantage for 1D problems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Noise-Aware Named Entity Recognition for Historical VET Documents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [named entity recognition], [Noise-Aware Training (NAT), OCR Noise, Data Augmentation, Transfer Learning, Multi-stage Fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alexander M. Esser, Jens D\xf6rpinghaus"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Federal Institute for Vocational Education and Training (BIBB), University of Koblenz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00488",children:"https://arxiv.org/pdf/2601.00488"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a robust NER approach for historical VET documents using Noise-Aware Training with synthetic OCR errors. 2. Systematically compares three complementary training strategies (noisy, clean, and artificial data). 3. Demonstrates that domain-specific and noise-aware fine-tuning significantly improves robustness and accuracy under noisy conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb3e9604f5f72e56feeecdc196004299094bc184a404fe77959f2a61d9e4da2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles Named Entity Recognition in noisy, historical Vocational Education and Training documents by proposing a method using Noise-Aware Training with synthetic OCR errors, transfer learning, and multi-stage fine-tuning. The approach, one of the first to recognize multiple entity types in this domain, shows that domain-specific and noise-aware fine-tuning substantially increases model robustness and accuracy. The method is applied to German but is designed to be transferable to other languages."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Noise-Aware NER for Historical VET Documents] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem: NER in noisy historical VET documents)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method: Noise-Aware Training with synthetic OCR errors, transfer learning, multi-stage fine-tuning)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results: Increased robustness and accuracy under noisy conditions)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [Retrieval-Augmented Generation, CodeQL, KLEE, Self-Repair, Symbolic Execution]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vidyut Sriram, Sawan Pandita, Achintya Lakshmanan, Aneesh Shamraj, Suman Saha"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Pennsylvania State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00509",children:"https://arxiv.org/pdf/2601.00509"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a retrieval-augmented, multi-tool repair workflow integrating compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution for iterative LLM self-repair. 2. Utilizes a lightweight embedding model for semantic retrieval of security-focused repair examples to guide code generation. 3. Demonstrates significant robustness improvements, reducing security vulnerabilities by up to 96% for DeepSeek-Coder and from 58.55% to 22.19% for CodeLlama."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdcdc91c311cf046454507445fa7f6baef39a437738ec8e32b2eb087f6fbfa91_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdcdc91c311cf046454507445fa7f6baef39a437738ec8e32b2eb087f6fbfa91_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLM-generated code containing security vulnerabilities and errors. It proposes a method where a code-generating LLM iteratively refines its output using feedback from multiple tools (compiler, CodeQL, KLEE) and retrieval of past successful repairs. The results show this approach significantly reduces security defects, even for larger, more stubborn models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Improving LLM-Assisted Secure Code Generation<br>\u63d0\u5347LLM\u8f85\u52a9\u5b89\u5168\u4ee3\u7801\u751f\u6210"] --\x3e Problem["LLM\u751f\u6210\u4ee3\u7801\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e<br>LLM-generated code has security vulnerabilities"]\n    Root --\x3e Method["\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u5de5\u5177\u81ea\u4fee\u590d\u5de5\u4f5c\u6d41<br>Retrieval-Augmented Multi-Tool Self-Repair Workflow"]\n    Root --\x3e Results["\u5b89\u5168\u6f0f\u6d1e\u663e\u8457\u51cf\u5c11<br>Security vulnerabilities significantly reduced"]\n    Problem --\x3e P1["\u903b\u8f91\u4e0d\u4e00\u81f4<br>Logical inconsistencies"]\n    Problem --\x3e P2["\u7f16\u8bd1\u9519\u8bef<br>Compilation errors"]\n    Method --\x3e M1["\u68c0\u7d22\u4fee\u590d\u793a\u4f8b<br>Retrieve repair examples"]\n    Method --\x3e M2["\u5de5\u5177\u53cd\u9988(\u7f16\u8bd1\u5668/CodeQL/KLEE)<br>Tool feedback (Compiler/CodeQL/KLEE)"]\n    Method --\x3e M3["\u8fed\u4ee3\u81ea\u4fee\u590d<br>Iterative self-repair"]\n    Results --\x3e R1["DeepSeek\u6f0f\u6d1e\u51cf\u5c1196%<br>DeepSeek vulnerabilities reduced 96%"]\n    Results --\x3e R2["CodeLlama\u5173\u952e\u7f3a\u9677\u738722.19%<br>CodeLlama critical defect rate 22.19%"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [reasoning integrity, retrieval-augmented generation, process verification, small language models, neural classifier]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Laksh Advani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00513",children:"https://arxiv.org/pdf/2601.00513"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),'  1. Introduces the "Right-for-Wrong-Reasons" (RWR) phenomenon and the Reasoning Integrity Score (RIS), a novel process-based metric for evaluating the trustworthiness of small language model agents, validated with high inter-rater agreement. 2. Empirically demonstrates that while retrieval-augmented generation (RAG) significantly improves reasoning integrity, meta-cognitive interventions like self-critique often degrade it in small models, providing mechanistic explanations for these effects. 3. Distills the verification capability into a lightweight neural classifier that achieves high performance (0.86 F1-score) and efficiency (100x speedup), enabling practical process-based verification for deployment.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b874fd2b8e4599623bc1f3efd5491de7179346cb166566b9907ab13137ae7a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b874fd2b8e4599623bc1f3efd5491de7179346cb166566b9907ab13137ae7a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies a critical reliability problem where small language models (7-9B parameters) often produce correct answers based on flawed reasoning, a phenomenon invisible to standard accuracy metrics. To address this, the authors introduce a process-based evaluation metric (RIS) and analyze the impact of interventions like RAG and self-critique, finding RAG improves reasoning while self-critique harms it in small models. They conclude that process verification is essential for trustworthy agents and demonstrate a fast neural classifier for this purpose."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[When Small Models Are Right for Wrong Reasons<br>\u5f53\u5c0f\u6a21\u578b\u56e0\u9519\u8bef\u539f\u56e0\u800c\u6b63\u786e\u65f6] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Small models give correct answers with flawed reasoning<br>\u5c0f\u6a21\u578b\u57fa\u4e8e\u9519\u8bef\u63a8\u7406\u7ed9\u51fa\u6b63\u786e\u7b54\u6848]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Introduce Reasoning Integrity Score (RIS)<br>\u5f15\u5165\u63a8\u7406\u5b8c\u6574\u6027\u8bc4\u5206]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>RAG helps, self-critique hurts; Neural verifier is fast<br>RAG\u6709\u6548\uff0c\u81ea\u6211\u6279\u5224\u6709\u5bb3\uff1b\u795e\u7ecf\u9a8c\u8bc1\u5668\u901f\u5ea6\u5feb]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Siamese Recurrent Autoencoder, hybrid loss, real-time anomaly detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Laksh Advani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: University of Colorado Boulder)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00516",children:"https://arxiv.org/pdf/2601.00516"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated the ineffectiveness of standard anomaly detection methods for agent trajectory validation, establishing the need for specialized models. 2. Proposed a novel, sequence-aware Siamese Recurrent Autoencoder with a hybrid loss function for real-time trajectory anomaly detection. 3. Demonstrated that the approach is over 17x faster than LLM Judge baselines, making it suitable for real-time deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de97af87ef40e2bb7fa3067f0d8a7f8e2dc7d8fd40b77747e64af793669009c2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of detecting anomalous action plans in autonomous LLM agents, where existing methods fail to capture sequential structure and context. It proposes Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss combining contrastive learning and reconstruction for unified anomaly detection. The method achieves high F1-scores (0.88-0.94) and significantly faster inference (32 ms) than LLM-based baselines, enabling real-time safety verification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Trajectory Guard] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u4e0d\u9002\u7528/Existing methods ill-suited]\n    Problem --\x3e P2[\u9700\u8981\u5e8f\u5217\u611f\u77e5/Need for sequence-awareness]\n    Method --\x3e M1[\u5b6a\u751f\u5faa\u73af\u81ea\u7f16\u7801\u5668/Siamese Recurrent Autoencoder]\n    Method --\x3e M2[\u6df7\u5408\u635f\u5931\u51fd\u6570/Hybrid Loss Function]\n    M2 --\x3e M2a[\u5bf9\u6bd4\u5b66\u4e60/Contrastive Learning]\n    M2 --\x3e M2b[\u91cd\u5efa/Reconstruction]\n    Results --\x3e R1[\u9ad8F1\u5206\u6570/High F1-scores (0.88-0.94)]\n    Results --\x3e R2[\u4f4e\u5ef6\u8fdf/32 ms latency]\n    Results --\x3e R3[\u5b9e\u65f6\u90e8\u7f72/Real-time deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [LSTM compression, model efficiency, retail forecasting, edge computing, hidden units]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ravi Teja Pagidoju"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Campbellsville University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00525",children:"https://arxiv.org/pdf/2601.00525"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/RaviTeja444/sales-forecast-LSTM",children:"https://github.com/RaviTeja444/sales-forecast-LSTM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematic evaluation of LSTM network sizes from 16 to 128 hidden units on real retail data. 2. Discovery that moderate compression (to 64 units) actually improves forecast accuracy. 3. Practical guidelines for model selection based on the accuracy-efficiency trade-off."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bda8da07103a6f89178a84bdce230c27d6cea8328b8c1fcb749e7912c516181_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies LSTM model compression for resource-constrained retail sales forecasting by reducing the number of hidden units. The method involves systematically pruning the LSTM from 128 to 16 hidden units. The main conclusion is that reducing the model to 64 units not only makes it 73% smaller but also improves accuracy by 47%, showing larger models are not always better."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Optimizing LSTM for Resource-Constrained Retail Forecasting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LSTM\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u4e2d\u5c0f\u578b\u96f6\u552e\u5546\u96be\u4ee5\u90e8\u7f72/LSTM models are computationally expensive for mid-to-small retailers)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u901a\u8fc7\u9010\u6b65\u51cf\u5c11\u9690\u85cf\u5355\u5143\u8fdb\u884c\u6a21\u578b\u538b\u7f29/Model compression by reducing hidden units from 128 to 16)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: 64\u5355\u5143\u6a21\u578b\u66f4\u5c0f(76KB vs 280KB)\u4e14\u66f4\u51c6\u786e(MAPE 12.4% vs 23.6%)/64-unit model is smaller and more accurate)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [sparse-attention, cross-attention, class-balanced focal loss, multimodal fusion, explainable AI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dristi Datta, Tanmoy Debnath, Minh Chau, Manoranjan Paul, Gourab Adhikary, Md Geaur Rahman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Charles Sturt University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00519",children:"https://arxiv.org/pdf/2601.00519"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for multimodal Parkinson's disease profiling. 2. Introduced a sparsity-constrained attention-gating fusion layer to dynamically prioritize informative modalities from heterogeneous data (MRI, clinical, demographic). 3. Employed a Class-Balanced Focal Loss to effectively handle dataset imbalance without synthetic oversampling, achieving high performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0104d1bebca16cde5f74be8dbe6bfbcfb976784343fbe1f1f8025e57eed10_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0104d1bebca16cde5f74be8dbe6bfbcfb976784343fbe1f1f8025e57eed10_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SAFN, an interpretable deep learning model that integrates MRI and clinical data using sparse-attention and cross-attention mechanisms for Parkinson's disease severity profiling. It addresses challenges in multimodal fusion and class imbalance. The model achieves high accuracy and interpretability, aligning clinical assessment importance with diagnostic principles."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[SAFN: Parkinson's Disease Severity Profiling<br/>SAFN: \u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u5206\u6790] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br/>Multimodal fusion, interpretability, class imbalance<br/>\u591a\u6a21\u6001\u878d\u5408\u3001\u53ef\u89e3\u91ca\u6027\u3001\u7c7b\u522b\u4e0d\u5e73\u8861] --\x3e P1[\u6311\u6218/Challenges<br/>Integrate imaging & clinical data<br/>\u6574\u5408\u5f71\u50cf\u4e0e\u4e34\u5e8a\u6570\u636e]\n    Problem --\x3e P2[\u76ee\u6807/Goal<br/>Robust & interpretable PD profiling<br/>\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684PD\u5206\u6790]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Class-Weighted Sparse-Attention Fusion Network<br/>\u7c7b\u522b\u52a0\u6743\u7a00\u758f\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc] --\x3e M1[\u6280\u672f/Techniques<br/>Modality-specific encoders, symmetric cross-attention<br/>\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u3001\u5bf9\u79f0\u4ea4\u53c9\u6ce8\u610f\u529b]\n    Method --\x3e M2[\u521b\u65b0/Innovations<br/>Sparsity-constrained attention gating, Class-Balanced Focal Loss<br/>\u7a00\u758f\u7ea6\u675f\u6ce8\u610f\u529b\u95e8\u63a7\u3001\u7c7b\u522b\u5e73\u8861\u7126\u70b9\u635f\u5931]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br/>Evaluation on PPMI dataset<br/>\u5728PPMI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30] --\x3e R1[\u6027\u80fd/Performance<br/>Accuracy: 0.98\xb10.02, PR-AUC: 1.00\xb10.00<br/>\u51c6\u786e\u7387: 0.98\xb10.02, PR-AUC: 1.00\xb10.00]\n    Results --\x3e R2[\u53ef\u89e3\u91ca\u6027/Interpretability<br/>~60% weight to clinical assessments<br/>~60%\u6743\u91cd\u5206\u914d\u7ed9\u4e34\u5e8a\u8bc4\u4f30]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [federated learning, prefix-tuning, large model customization, efficient fine-tuning, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00526",children:"https://arxiv.org/pdf/2601.00526"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Federated Customization of Large Models: Approaches, Experiments, and Insights] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u5b9a\u5236\u5927\u6a21\u578b\u7684\u6311\u6218/Challenges of customizing large models within FL]\n    C --\x3e C1[\u56de\u987e\u5927\u6a21\u578b\u5b9a\u5236\u6280\u672f/Review LM customization techniques]\n    C --\x3e C2[\u8ba8\u8bba\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0/Discuss FL implementations]\n    C --\x3e C3[\u5b9e\u9a8c\u8054\u90a6\u524d\u7f00\u8c03\u4f18/Experiment with federated prefix-tuning]\n    D --\x3e D1[\u9a8c\u8bc1\u8054\u90a6\u524d\u7f00\u8c03\u4f18\u53ef\u884c\u6027/Validate feasibility of federated prefix-tuning]\n    D --\x3e D2[\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u65b9\u6cd5/Performance close to centralized]\n    D --\x3e D3[\u5c55\u793a\u7ade\u4e89\u529b\u4e0e\u9c81\u68d2\u6027/Show competitive performance & robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [diffusion models, cloud-native architecture, edge deployment, constraint satisfaction, planogram generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ravi Teja Pagidoju, Shriya Agarwal"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Campbellsville University, University of the Cumberlands"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00527",children:"https://arxiv.org/pdf/2601.00527"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel cloud-native architecture for automated planogram synthesis using diffusion models, combining AWS for training and edge deployment for real-time inference. 2. A diffusion model that integrates retail-specific constraints through a modified loss function to generate store-specific layouts. 3. A comprehensive simulation-based and economic analysis demonstrating significant reductions in design time (98.3%) and cost (97.5%) with high constraint satisfaction (94.4%) and linear scalability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b868081df5651946090e341fbb8526b8b99531ee775ea4b316dc9682b146e22e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b868081df5651946090e341fbb8526b8b99531ee775ea4b316dc9682b146e22e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a cloud-native system using diffusion models to automate the generation of retail planograms. The method learns from successful shelf arrangements across multiple stores and incorporates business constraints into the model. The results show it drastically reduces design time and cost while maintaining high constraint satisfaction, proving the viability of generative AI for retail space optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Cloud-Native Generative AI for Planogram Synthesis] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Manual planogram creation is slow and expensive] --\x3e P1[\u6311\u6218/Challenge: 30 hours per layout]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Cloud-native diffusion model] --\x3e M1[\u8bad\u7ec3/Training: Cloud-based (AWS)]\n    Method --\x3e M2[\u63a8\u7406/Inference: Edge deployment]\n    Method --\x3e M3[\u6a21\u578b/Model: Constraint-integrated diffusion]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6548\u7387/Efficiency: 98.3% time reduction]\n    Results --\x3e R2[\u6548\u679c/Effectiveness: 94.4% constraint satisfaction]\n    Results --\x3e R3[\u7ecf\u6d4e/Economic: 97.5% cost reduction]\n    Results --\x3e R4[\u53ef\u6269\u5c55\u6027/Scalability: Linear scaling to 10k stores]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Entropy Production in Machine Learning Under Fokker-Planck Probability Flow"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Fokker-Planck equation, Kullback-Leibler divergence, entropy production, data drift, retraining trigger]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lennon Shikhman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00554",children:"https://arxiv.org/pdf/2601.00554"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel entropy-based retraining framework for machine learning models grounded in nonequilibrium statistical physics, modeling data drift as probability flow governed by a Fokker-Planck equation. 2. Derives a dynamical interpretation of model-data mismatch by showing its time derivative decomposes into an entropy-balance equation featuring a nonnegative entropy production term. 3. Introduces and validates an entropy-triggered retraining strategy that maintains high predictive performance while significantly reducing retraining frequency compared to daily or label-based policies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53bb0b8d5d5942ee17365e19e7b865b31c2701e9fcbcde1a5d6b9dc2ccc5b0e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53bb0b8d5d5942ee17365e19e7b865b31c2701e9fcbcde1a5d6b9dc2ccc5b0e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses performance degradation in ML models due to data drift in nonstationary environments. It proposes a principled retraining framework based on nonequilibrium dynamics, using a Fokker-Planck model of drift and an entropy-production trigger. The method achieves performance comparable to frequent retraining while drastically reducing the number of retraining events."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Entropy Production in Machine Learning Under Fokker-Planck Probability Flow") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Model performance degrades due to data drift in nonstationary environments.")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Entropy-based retraining framework using Fokker-Planck dynamics and KL divergence.")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Achieves high performance with order-of-magnitude fewer retraining events.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Adversarial Samples Are Not Created Equal"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [adversarial robustness], [adversarial samples, non-robust features, adversarial bugs, ensemble-based metric, sharpness-aware minimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jennifer Crawford, Amol Khanna, Fred Lu, Amy R. Wagoner, Stella Biderman, Andre T. Nguyen, Edward Raff"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Scale AI, CrowdStrike, Booz Allen Hamilton, EleutherAI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00577",children:"https://arxiv.org/pdf/2601.00577"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations. 2. Introduces the concept of "adversarial bugs" to differentiate adversarial samples that do not rely on non-robust features. 3. Uses this new perspective to re-examine phenomena like the impact of sharpness-aware minimization and the robustness gap in adversarially trained models.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e880259ebf04545c0ce64ed1e802b2caa6a3b401669986a52d0c3ccb0f063371_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e880259ebf04545c0ce64ed1e802b2caa6a3b401669986a52d0c3ccb0f063371_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper argues that not all adversarial samples are created equal, differentiating between those that exploit brittle "non-robust" features and those that do not ("adversarial bugs"). It proposes an ensemble-based metric to identify this distinction and uses it to analyze adversarial attacks, offering a new lens to re-examine existing robustness phenomena.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Adversarial Samples Are Not Created Equal<br>\u5bf9\u6297\u6027\u6837\u672c\u5e76\u975e\u751f\u800c\u5e73\u7b49") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u89e3\u91ca\u6240\u6709\u5bf9\u6297\u6837\u672c<br>Existing theory doesn\'t explain all adversarial samples")\n    Method --\x3e M1("\u63d0\u51fa\u57fa\u4e8e\u96c6\u6210\u7684\u5ea6\u91cf<br>Propose ensemble-based metric")\n    Method --\x3e M2("\u533a\u5206\'\u5bf9\u6297\u6027\u6f0f\u6d1e\'<br>Differentiate \'adversarial bugs\'")\n    Results --\x3e R1("\u91cd\u65b0\u5ba1\u89c6\u9c81\u68d2\u6027\u73b0\u8c61<br>Re-examine robustness phenomena")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Learning to be Reproducible: Custom Loss Design for Robust Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [training stability & reproducibility], [custom loss function, training robustness, reproducibility, stochastic factors, weight initialization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Waqas Ahmed, Sheeba Samuel, Kevin Coakley, Birgitta Koenig-Ries, Odd Erik Gundersen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Friedrich Schiller University Jena, University of Technology Chemnitz, Norwegian University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00578",children:"https://arxiv.org/pdf/2601.00578"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and empirically analyzes the critical gap in ensuring consistent performance across training runs due to stochastic factors like weight initialization and data shuffling. 2. Proposes a novel Custom Loss Function (CLF) designed to explicitly balance predictive accuracy with training stability, reducing sensitivity to these stochastic factors. 3. Demonstrates through extensive experiments on diverse architectures and tasks (image classification, time series forecasting) that CLF significantly improves training robustness without sacrificing predictive performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0accb01febbfd00a3b2c4650b921cc29a716544cb9a8fbc39d5a8ebe82c21bf6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of inconsistent model performance across training runs due to stochastic factors. It proposes a Custom Loss Function (CLF) to explicitly balance accuracy and stability, which is shown to improve training robustness without harming predictive performance in experiments on image classification and time series forecasting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Learning to be Reproducible: Custom Loss Design for Robust Neural Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u6a21\u578b\u6027\u80fd\u4e0d\u4e00\u81f4/Inconsistent Model Performance")\n    Problem --\x3e P2("\u5bf9\u968f\u673a\u56e0\u7d20\u654f\u611f/Sensitive to Stochastic Factors")\n    Method --\x3e M1("\u63d0\u51fa\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570/Propose Custom Loss Function (CLF)")\n    Results --\x3e R1("\u63d0\u9ad8\u8bad\u7ec3\u9c81\u68d2\u6027/Improves Training Robustness")\n    Results --\x3e R2("\u4fdd\u6301\u9884\u6d4b\u6027\u80fd/Maintains Predictive Performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Mixture-of-Experts, federated fine-tuning, resource-aware, expert selection, sparsity-aware aggregation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zihan Fang, Zheng Lin, Senkang Hu, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong, The University of Hong Kong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00583",children:"https://arxiv.org/pdf/2601.00583"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a method to identify expert importance based on contributions to fine-tuning performance, enabling informed expert selection. 2. Proposes an adaptive expert subset selection mechanism from an information bottleneck perspective to align with heterogeneous client computing budgets. 3. Designs a sparsity-aware model aggregation strategy that weights updates from actively fine-tuned experts and gating parameters to mitigate destructive interference during global aggregation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a4349d6a95221a61360e261ad370cd60546e55c0ae19bef19bfe4f05de72ff4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes HFedMoE, a heterogeneous federated learning framework for fine-tuning large language models using Mixture-of-Experts. It addresses challenges in expert selection, resource heterogeneity, and aggregation interference by customizing expert subsets per client and using importance-weighted aggregation. Experiments show HFedMoE outperforms state-of-the-art methods in accuracy and convergence speed."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u8054\u90a6\u5fae\u8c03\u4e2dMoE\u9762\u4e34\u7684\u6311\u6218/Challenges in MoE for FL fine-tuning]\n    B1 --\x3e B2[\u4e13\u5bb6\u9009\u62e9\u56f0\u96be/Difficulty in expert selection]\n    B1 --\x3e B3[\u5ba2\u6237\u7aef\u8d44\u6e90\u5f02\u6784\u6027/Client resource heterogeneity]\n    B1 --\x3e B4[\u805a\u5408\u5e72\u6270/Aggregation interference]\n    C --\x3e C1[\u5b9a\u5236\u5316\u4e13\u5bb6\u5b50\u96c6/Customized expert subset per client]\n    C1 --\x3e C2[\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u4e13\u5bb6\u9009\u62e9/Importance-based expert selection]\n    C1 --\x3e C3[\u4fe1\u606f\u74f6\u9888\u89c6\u89d2\u7684\u9002\u914d/Adaptation via information bottleneck]\n    C --\x3e C4[\u7a00\u758f\u611f\u77e5\u7684\u6a21\u578b\u805a\u5408/Sparsity-aware model aggregation]\n    D --\x3e D1[\u66f4\u9ad8\u7684\u8bad\u7ec3\u7cbe\u5ea6/Higher training accuracy]\n    D --\x3e D2[\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6/Faster convergence speed]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [sports analytics], [Lasso regression, training load (CTL/ATL), N-of-1 study, feature engineering, route topology]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Francisco Aguilera Moreno"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," None specified (Inferred from author's email domain: gmail.com)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00604",children:"https://arxiv.org/pdf/2601.00604"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a personalized machine learning model for cycling time prediction that uses route topology and athlete fitness state, avoiding complex physics-based parameters. 2. Implements and validates the approach using an N-of-1 study design with rigorous feature engineering to prevent data leakage. 3. Demonstrates that integrating fitness metrics (CTL, ATL) reduces prediction error by 14% compared to using route features alone, highlighting the importance of physiological state."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9697eaafe078b4416e1e63fcfa2f99ad60e0031eff1e90380ad4c7d7042e046_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9697eaafe078b4416e1e63fcfa2f99ad60e0031eff1e90380ad4c7d7042e046_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a personalized machine learning approach to predict cycling race times by combining route topology features with the athlete's training load-derived fitness state. The method, evaluated on a single-athlete dataset, uses Lasso regression and achieves high accuracy (MAE=6.60 min, R\xb2=0.922), showing that fitness metrics significantly improve predictions over topology alone."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Cycling Race Time Prediction] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Physics-based models are impractical<br>\u7269\u7406\u6a21\u578b\u4e0d\u5b9e\u7528]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Personalized ML with Route & Fitness<br>\u4e2a\u6027\u5316ML\u7ed3\u5408\u8def\u7ebf\u4e0e\u4f53\u80fd]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results<br>Fitness reduces error by 14%<br>\u4f53\u80fd\u6307\u6807\u964d\u4f4e14%\u8bef\u5dee]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Graph Neural Network, Q-learning, traffic-aware optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sonia Khetarpaul, P Y Sharan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shiv Nadar Institution of Eminence"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00607",children:"https://arxiv.org/pdf/2601.00607"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel traffic-aware, graph-based reinforcement learning framework for optimal taxi placement that integrates real-time traffic data (e.g., congestion scores) with historical demand patterns. 2. Employs Graph Neural Network (GNN) embeddings to encode spatial-temporal dependencies within the urban road network, enhancing the agent's understanding of network topology and dynamics. 3. Designs a multi-objective reward mechanism that jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance, leading to significant performance improvements over a baseline."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inefficient taxi supply-demand matching in smart cities by proposing a framework that models the urban road network as a graph and uses Graph Neural Networks combined with Q-learning to recommend optimal taxi placement hotspots. The method integrates real-time traffic conditions and historical data to optimize for passenger waiting time and driver travel distance. Experiments on a simulated Delhi dataset show the model reduces passenger waiting time by 56% and travel distance by 38% compared to a stochastic baseline."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Traffic-Aware Optimal Taxi Placement<br>Using Graph Neural Network-Based Reinforcement Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Conventional taxi hotspot models overlook dynamic traffic influences.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Graph-based RL with GNN embeddings for spatial-temporal dependencies.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Reduced passenger waiting time by 56% and travel distance by 38%.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Stronger Approximation Guarantees for Non-Monotone \u03b3-Weakly DR-Submodular Maximization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [submodular optimization], [weakly DR-submodular, continuous-greedy, Frank-Wolfe, approximation algorithm, down-closed convex body]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hareshkumar Jadav, Ranveer Singh, Vaneet Aggarwal"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IIT Indore, Purdue University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00611",children:"https://arxiv.org/pdf/2601.00611"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel approximation algorithm for maximizing non-monotone \u03b3-weakly DR-submodular functions over down-closed convex bodies. 2. A smooth approximation guarantee that recovers the 0.401 factor for DR-submodular (\u03b3=1) and degrades gracefully for \u03b3<1, improving upon prior bounds. 3. A hybrid algorithmic framework combining Frank-Wolfe-guided continuous-greedy with a \u03b3-aware double-greedy step to handle non-monotonicity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3fb1d9788f036c8398d4ed9b82b8201cf4616c9a51d4916ebe91884a9996b77_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of maximizing non-monotone, nonnegative \u03b3-weakly DR-submodular functions over down-closed convex bodies. The authors propose a new algorithm that integrates a Frank-Wolfe-guided continuous-greedy approach with a \u03b3-aware double-greedy step. This method achieves state-of-the-art approximation guarantees that depend smoothly on the parameter \u03b3, improving upon previous results for this class of functions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Stronger Approximation Guarantees for Non-Monotone \u03b3-Weakly DR-Submodular Maximization<br>\u975e\u5355\u8c03\u03b3-\u5f31DR-\u5b50\u6a21\u6700\u5927\u5316\u7684\u66f4\u5f3a\u8fd1\u4f3c\u4fdd\u8bc1] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Maximize non-monotone \u03b3-weakly DR-submodular function<br>\u6700\u5927\u5316\u975e\u5355\u8c03\u03b3-\u5f31DR-\u5b50\u6a21\u51fd\u6570]\n    B --\x3e B2[Over down-closed convex body<br>\u5728\u4e0b\u5c01\u95ed\u51f8\u4f53\u4e0a]\n    C --\x3e C1[Frank-Wolfe-guided continuous-greedy<br>Frank-Wolfe\u5f15\u5bfc\u7684\u8fde\u7eed\u8d2a\u5fc3]\n    C --\x3e C2[\u03b3-aware double-greedy step<br>\u03b3\u611f\u77e5\u7684\u53cc\u8d2a\u5fc3\u6b65\u9aa4]\n    D --\x3e D1[Smooth approximation guarantee based on \u03b3<br>\u57fa\u4e8e\u03b3\u7684\u5e73\u6ed1\u8fd1\u4f3c\u4fdd\u8bc1]\n    D --\x3e D2[Recovers 0.401 for \u03b3=1 (DR-submodular)<br>\u03b3=1\u65f6\u6062\u590d0.401\u56e0\u5b50]\n    D --\x3e D3[Improves prior bounds for \u03b3<1<br>\u6539\u8fdb\u4e86\u03b3<1\u65f6\u7684\u73b0\u6709\u754c\u9650]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Do Chatbot LLMs Talk Too Much? The YapBench Benchmark"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [verbosity, benchmark, brevity, over-generation, evaluation metric]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vadim Borisov, Michael Gr\xf6ger, Mina Mikhael, Richard H. Schreiber"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," tabularis.ai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00624",children:"https://arxiv.org/pdf/2601.00624"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://huggingface.co/datasets/tabularisai/yapbench",children:"https://huggingface.co/datasets/tabularisai/yapbench"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces YapBench, a benchmark with over 300 prompts to quantify LLM over-generation in brevity-ideal scenarios. 2. Proposes YapScore, a tokenizer-agnostic metric based on character count to measure excess response length. 3. Establishes a live leaderboard and provides analysis revealing an order-of-magnitude spread in verbosity across 76 evaluated LLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b7bd8c8557d407f813af012f276e8f3d15707b3fbc649fe498992f3cbf4d01_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b7bd8c8557d407f813af012f276e8f3d15707b3fbc649fe498992f3cbf4d01_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLMs generating unnecessarily long and verbose responses to simple prompts. It introduces the YapBench benchmark and YapScore metric to measure this over-generation. The evaluation of 76 models shows significant variation in verbosity, highlighting a common failure mode in current assistant LLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Do Chatbot LLMs Talk Too Much? The YapBench Benchmark] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs often give unnecessarily long responses, increasing cognitive load and cost.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce YapBench benchmark and YapScore metric to measure excess verbosity.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Large variation in verbosity found across 76 models; benchmark and leaderboard released.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [Hypergraph Neural Network, Learning Using Privileged Information, Knowledge Distillation, Severed Graph Strategy, dual-stream distillation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuren Gabriel Yu, Sikang Ren, Yongji Tian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Beijing Tiantan Hospital"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00626",children:"https://arxiv.org/pdf/2601.00626"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HyperPriv-EPN] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Pre-op prognosis lacks semantic insights from post-op reports]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hypergraph LUPI with Severed Graph Strategy & dual-stream distillation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA accuracy & survival stratification on 311 patients]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretable machine learning], [Bi-objective Optimization, Temporal Integrated Gradients, Optimal Path Oracle, Directed Acyclic Graph, Structured Regularization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kasra Fouladi, Hamta Rahmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated; inferred from email domains as independent researchers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00655",children:"https://arxiv.org/pdf/2601.00655"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the IGBO framework that trains interpretable models by formalizing the task as a bi-objective optimization problem, jointly optimizing for accuracy and adherence to domain knowledge constraints. 2. Introduces an Optimal Path Oracle to generate data-manifold-aware integration paths, addressing the Out-of-Distribution problem in Temporal Integrated Gradients computation. 3. Provides theoretical analysis proving convergence properties and robustness to mini-batch noise, and demonstrates empirical effectiveness on time-series data with minimal accuracy loss."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/797ceec0d9225c3c9714a73c2ff308494df8996ce6022916738679549e999bd1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains models to be both accurate and interpretable by jointly optimizing a task loss and an interpretability loss derived from domain knowledge encoded as a DAG. It addresses a key challenge in gradient-based attribution (the OOD problem) by learning an Optimal Path Oracle. Empirical results show IGBO effectively enforces interpretability constraints with minimal impact on accuracy, outperforming standard regularization methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Interpretability-Guided Bi-objective Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/Black-box models lack interpretability]\n    B --\x3e B2[\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u7ea6\u675f/Post-hoc methods don't guarantee constraints]\n    B --\x3e B3[\u68af\u5ea6\u5f52\u56e0\u5b58\u5728OOD\u95ee\u9898/Gradient attribution has OOD problem]\n    C --\x3e C1[\u53cc\u76ee\u6807\u4f18\u5316\u6846\u67b6/Bi-objective Optimization Framework]\n    C --\x3e C2[\u4f7f\u7528DAG\u7f16\u7801\u9886\u57df\u77e5\u8bc6/Encode knowledge via DAG]\n    C --\x3e C3[\u6700\u4f18\u8def\u5f84\u9884\u8a00\u673a/Optimal Path Oracle]\n    D --\x3e D1[\u7406\u8bba\u6536\u655b\u6027\u8bc1\u660e/Theoretical convergence proof]\n    D --\x3e D2[\u5b9e\u8bc1\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf/Empirically outperforms baselines]\n    D --\x3e D3[\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931/Minimal accuracy loss]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KAIST, NTU Singapore, DeepAuto.ai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00664",children:"https://arxiv.org/pdf/2601.00664"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://taekyungki.github.io/AvatarForcing",children:"https://taekyungki.github.io/AvatarForcing"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u771f\u6b63\u4e92\u52a8/Lacks truly interactive communication]\n    Problem --\x3e P2[\u5355\u5411\u53cd\u5e94\u7f3a\u4e4f\u60c5\u611f/One-way responses lack emotional engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u9a71\u52a8\u6846\u67b6/Diffusion forcing framework]\n    Method --\x3e M2[\u65e0\u6807\u7b7e\u76f4\u63a5\u504f\u597d\u4f18\u5316/Label-free direct preference optimization]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u4ea4\u4e92/Low-latency real-time interaction (~500ms)]\n    Results --\x3e R2[6.8\u500d\u52a0\u901f/6.8x speedup]\n    Results --\x3e R3[80%\u7528\u6237\u504f\u597d/Over 80% user preference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Three factor delay learning rules for spiking neural networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [spiking neural networks, delay learning, three-factor learning, online learning, neuromorphic processors]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Luke Vassallo, Nima Taherinejad"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Heidelberg University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00668",children:"https://arxiv.org/pdf/2601.00668"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced learnable synaptic and axonal delays into LIF-based SNNs and proposed novel three-factor learning rules for online, simultaneous learning of both weights and delays. 2. Employed a smooth Gaussian surrogate gradient exclusively for eligibility trace calculation to enable gradient-equivalent delay parameter updates. 3. Demonstrated significant improvements in model efficiency, achieving up to 6.6x model size reduction and 67% lower inference latency with minimal accuracy loss, enabling on-device learning for resource-constrained environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited temporal learning capability of Spiking Neural Networks (SNNs) by introducing learnable synaptic and axonal delays and proposing online three-factor learning rules to train them. The method uses a Gaussian surrogate gradient for eligibility traces and achieves competitive accuracy on temporal tasks like speech recognition while drastically reducing model size and latency. The findings facilitate efficient, on-device learning for power and area-constrained neuromorphic hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Three factor delay learning rules for spiking neural networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[SNNs lack temporal parameters/SNNs\u7f3a\u4e4f\u65f6\u95f4\u53c2\u6570]\n    B --\x3e B2[Existing delay learning is offline & large/\u73b0\u6709\u5ef6\u8fdf\u5b66\u4e60\u662f\u79bb\u7ebf\u7684\u4e14\u6a21\u578b\u5927]\n    C --\x3e C1[Learnable synaptic & axonal delays/\u53ef\u5b66\u4e60\u7684\u7a81\u89e6\u548c\u8f74\u7a81\u5ef6\u8fdf]\n    C --\x3e C2[Three-factor online learning rules/\u4e09\u56e0\u7d20\u5728\u7ebf\u5b66\u4e60\u89c4\u5219]\n    C --\x3e C3[Gaussian surrogate for eligibility trace/\u7528\u4e8e\u8d44\u683c\u8ff9\u7684\u9ad8\u65af\u4ee3\u7406\u68af\u5ea6]\n    D --\x3e D1[Accuracy improved up to 20%/\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe20%]\n    D --\x3e D2[Model size reduced 6.6x/\u6a21\u578b\u5927\u5c0f\u51cf\u5c116.6\u500d]\n    D --\x3e D3[Latency reduced 67%/\u5ef6\u8fdf\u964d\u4f4e67%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [operator learning, finite element methods, sparse networks, computational efficiency, stability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Seungchan Ko, Jiyeon Kim, Dongwook Shin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Inha University, Ajou University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00672",children:"https://arxiv.org/pdf/2601.00672"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel sparse network architecture for FEONet, leveraging finite-element local sparsity to reduce computational cost and memory usage. 2. Provides theoretical analysis demonstrating the sparse architecture's approximation capability and stability for reliable training. 3. Validates the method through extensive numerical experiments, showing substantial efficiency gains while maintaining accuracy comparable to the original FEONet."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ca7dbad16ad9ca12a77087fa33228fbe27facf787daefb7dcf7928c77caf2bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ca7dbad16ad9ca12a77087fa33228fbe27facf787daefb7dcf7928c77caf2bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Sparse FEONet, a memory-efficient variant of the Finite Element Operator Network, to address the scalability issues of the original model. By incorporating a sparse architecture inspired by finite element structures, it significantly reduces computational cost and memory footprint while preserving accuracy. Theoretical and experimental results confirm its effectiveness and stability for solving parametric PDEs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Sparse FEONet<br>\u7a00\u758fFEONet"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>FEONet\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5927\u89c4\u6a21\u95ee\u9898\u6709\u6311\u6218<br>High computational cost of FEONet for large-scale problems"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51fa\u57fa\u4e8e\u6709\u9650\u5143\u5c40\u90e8\u7a00\u758f\u6027\u7684\u65b0\u7f51\u7edc\u67b6\u6784<br>Propose new sparse network architecture via finite-element local sparsity"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u8ba1\u7b97\u6210\u672c\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u4fdd\u6301\u7cbe\u5ea6\uff0c\u7406\u8bba\u4fdd\u8bc1<br>Substantial improvements in cost/efficiency, maintained accuracy, theoretical guarantees"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00677",children:"https://arxiv.org/pdf/2601.00677"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[IRPO: Scaling the Bradley-Terry Model via RL] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Pairwise GRMs create O(n\xb2) bottleneck in RL/\u6210\u5bf9GRM\u5728RL\u4e2d\u9020\u6210O(n\xb2)\u74f6\u9888]\n    C --\x3e C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: \u5c06Bradley-Terry\u878d\u5165GRPO\u5b9e\u73b0\u9010\u70b9\u8bc4\u5206]\n    D --\x3e D1[SOTA among pointwise GRMs/\u5728\u9010\u70b9GRM\u4e2d\u8fbe\u5230SOTA]\n    D --\x3e D2[Outperforms pairwise GRMs in post-training/\u5728\u8bad\u7ec3\u540e\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6210\u5bf9GRM]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, spike-driven language models (SLMs), memory footprint, tiered search, embedded systems]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University (NYU) Abu Dhabi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00679",children:"https://arxiv.org/pdf/2601.00679"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: SLMs\u5185\u5b58\u5360\u7528\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907/SLMs have large memory footprints, challenging for resource-constrained embedded deployment]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u81ea\u52a8\u5316\u5206\u5c42\u91cf\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u7f51\u7edc\u5c42\u6b21\u3001\u5c42\u654f\u611f\u6027\u548c\u591a\u76ee\u6807\u6743\u8861\u51fd\u6570/Automated tiered quantization strategy using network hierarchy, layer sensitivity, and multi-objective trade-off]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u5360\u7528\u51cf\u5c11\u9ad8\u8fbe86.5%\uff0c\u529f\u8017\u964d\u4f4e\u9ad8\u8fbe20%\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b/Memory footprint reduced by up to 86.5%, power by up to 20%, performance close to original model]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cost Optimization in Production Line Using Genetic Algorithm"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [combinatorial optimization], [genetic algorithm, task scheduling, production line, chromosome encoding, JGAP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alireza Rezaee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Tehran"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00689",children:"https://arxiv.org/pdf/2601.00689"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and compares two chromosome encoding strategies (station-based and task-based) for a GA applied to a production line scheduling problem. 2. Adapts standard GA operators (crossover, mutation, etc.) to preserve solution feasibility under precedence and capacity constraints. 3. Empirically demonstrates that the task-based encoding yields smoother convergence and more reliable cost minimization, especially for problems with a large number of valid schedules."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper applies a genetic algorithm to optimize task scheduling in a production line to minimize cost. It investigates two different ways to represent the schedule (encoding) within the algorithm and finds that a task-based encoding performs better, converging more smoothly to lower-cost solutions. The study shows GAs are advantageous for this type of complex, constrained scheduling problem."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cost Optimization in Production Line Using Genetic Algorithm] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6700\u5c0f\u5316\u751f\u4ea7\u7ebf\u6210\u672c/Minimize Production Line Cost]\n    B --\x3e B2[\u4efb\u52a1\u8c03\u5ea6\u4e0e\u7ea6\u675f/Task Scheduling with Constraints]\n    C --\x3e C1[\u9057\u4f20\u7b97\u6cd5/Genetic Algorithm]\n    C --\x3e C2[\u4e24\u79cd\u7f16\u7801\u7b56\u7565/Two Encoding Strategies]\n    C2 --\x3e C21[\u57fa\u4e8e\u5de5\u4f4d\u7684\u7f16\u7801/Station-based Encoding]\n    C2 --\x3e C22[\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7801/Task-based Encoding]\n    D --\x3e D1[\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7801\u6027\u80fd\u66f4\u4f18/Task-based Encoding Performs Better]\n    D --\x3e D2[\u66f4\u5e73\u6ed1\u7684\u6536\u655b/Smoother Convergence]\n    D --\x3e D3[\u66f4\u53ef\u9760\u7684\u4f18\u5316/More Reliable Optimization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [ticket troubleshooting, retrieval-augmented generation, instruction-tuning, domain-specific ranking, large language models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mohamed Trabelsi, Huseyin Uzunalioglu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nokia Bell Labs"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00691",children:"https://arxiv.org/pdf/2601.00691"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TeleDoCTR, an end-to-end system for telecom ticket troubleshooting integrating classification, retrieval, and generation tasks. 2. Introduces a domain-specific and contextual approach combining ranking and generative models tailored for the telecom domain. 3. Demonstrates superior performance over state-of-the-art methods on a real-world telecom dataset, enhancing troubleshooting accuracy and efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151b34be88d61fea64f35727dce1917583308996e63a1822af1e6ad8863fe6a8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TeleDoCTR, a system that automates telecom ticket troubleshooting by integrating domain-specific models for ticket classification, retrieval of similar historical tickets, and generation of fault analysis reports. It is evaluated on a real-world telecom dataset and shows improved performance over existing methods, making the troubleshooting process more accurate and efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Telecom ticket troubleshooting is complex, time-consuming, and human-intensive.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Integrates domain-specific ranking and generative models for classification, retrieval, and generation tasks.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Superior performance over SOTA methods on real-world data, enhancing accuracy and efficiency.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ARISE: Adaptive Reinforcement Integrated with Swarm Exploration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [swarm intelligence, policy gradient, adaptive exploration, non-stationary rewards, particle swarm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rajiv Chaitanya M, D R Ramesh Babu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dayananda Sagar College of Engineering"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00693",children:"https://arxiv.org/pdf/2601.00693"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ARISE, a lightweight framework that augments standard policy-gradient RL methods with a swarm-based exploration layer., 2. Proposes an adaptive mechanism that modulates exploration intensity based on reward-variance cues., 3. Demonstrates significant performance improvements and robustness, particularly in challenging and non-stationary environments, without altering core algorithmic structures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces ARISE, a framework that enhances reinforcement learning by integrating a swarm-based exploration layer with standard policy-gradient methods to improve exploration. It adaptively blends policy actions with particle-driven proposals and modulates exploration using reward variance. The method shows substantial performance gains on complex tasks and improved robustness in non-stationary environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[ARISE: Adaptive Reinforcement Integrated with Swarm Exploration] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[RL\u63a2\u7d22\u6311\u6218/RL Exploration Challenge]\n    Problem --\x3e P2[\u975e\u5e73\u7a33\u5956\u52b1/Non-stationary Rewards]\n    Method --\x3e M1[\u7fa4\u4f53\u667a\u80fd\u63a2\u7d22\u5c42/Swarm-based Exploration Layer]\n    Method --\x3e M2[\u81ea\u9002\u5e94\u8c03\u8282/Adaptive Modulation]\n    Method --\x3e M3[\u7b56\u7565-\u7c92\u5b50\u6df7\u5408/Policy-Particle Blending]\n    Results --\x3e R1[\u6027\u80fd\u663e\u8457\u63d0\u5347/Substantial Performance Gains]\n    Results --\x3e R2[\u9c81\u68d2\u6027\u589e\u5f3a/Enhanced Robustness]\n    Results --\x3e R3[\u67b6\u6784\u65e0\u5173/Architecture-agnostic]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [B-spline tokenization, adaptive segmentation, Rotary Positional Embedding (RoPE), long-term forecasting, transformer efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Maximilian Reinwardt, Michael Eichelbeck, Matthias Althoff"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Munich"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00698",children:"https://arxiv.org/pdf/2601.00698"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the B-Spline Adaptive Tokenizer (BSAT), a parameter-free method for adaptively segmenting time series by fitting B-splines and placing tokens in high-curvature regions. 2. Proposes a hybrid positional encoding strategy combining additive learnable encoding with a novel L-RoPE (layer-wise learnable base Rotary Positional Embedding). 3. Demonstrates that the model achieves competitive performance at high compression rates, making it suitable for memory-constrained use cases."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6206fb30f6a9eddbc2ada96d43c221bc41515951535d8d579b2df705186641a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6206fb30f6a9eddbc2ada96d43c221bc41515951535d8d579b2df705186641a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of transformers for long-term time series forecasting by introducing BSAT, an adaptive tokenizer that uses B-splines to create variable-length tokens aligned with data semantics, and a novel hybrid positional encoding. The method achieves strong performance with high compression, making it effective under memory constraints."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Quadratic self-attention complexity & rigid uniform patching in transformers for time series"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>B-Spline Adaptive Tokenizer (BSAT) & Hybrid Positional Encoding (L-RoPE)"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Competitive performance at high compression rates, suitable for memory-constrained use"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Bayesian Inverse Games with High-Dimensional Multi-Modal Observations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [inverse reinforcement learning], [Bayesian inference, variational autoencoder, Nash equilibrium, inverse games, multimodal observations]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yash Jain, Xinjie Liu, Lasse Peters, David Fridovich-Keil, Ufuk Topcu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin, Delft University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00696",children:"https://arxiv.org/pdf/2601.00696"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Bayesian inference framework for inverse games to quantify uncertainty in estimating agent objectives, addressing the overconfidence of point-estimate methods. 2. Introduces a structured variational autoencoder with an embedded differentiable Nash game solver, enabling posterior sampling without requiring labeled objective data. 3. Demonstrates that multimodal inference reduces uncertainty when trajectory data is insufficient, leading to safer downstream planning decisions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inferring agents' hidden objectives in multi-agent interactions, where existing maximum likelihood methods produce overconfident point estimates. The authors propose a Bayesian inverse game framework using a structured variational autoencoder with a differentiable Nash solver to generate posterior samples from multimodal observations. Experiments show the method improves inference quality, quantifies uncertainty, and enables safer autonomous decision-making compared to prior approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Bayesian Inverse Games with High-Dimensional Multi-Modal Observations") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("MLE\u65b9\u6cd5\u53ea\u63d0\u4f9b\u70b9\u4f30\u8ba1\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u88ab\u5ffd\u7565/MLE methods provide only point estimates, ignoring uncertainty")\n    Problem --\x3e P2("\u4e0b\u6e38\u89c4\u5212\u53ef\u80fd\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u52a8\u4f5c/Downstream planning can be overconfident, leading to unsafe actions")\n    Method --\x3e M1("\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6/Approximate Bayesian inference framework")\n    Method --\x3e M2("\u7ed3\u6784\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5d4c\u5165\u53ef\u5fae\u7eb3\u4ec0\u6c42\u89e3\u5668/Structured VAE with embedded differentiable Nash solver")\n    Method --\x3e M3("\u5229\u7528\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e/Utilizes multi-modal observation data")\n    Results --\x3e R1("\u6210\u529f\u5b66\u4e60\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03/Successfully learns prior and posterior distributions")\n    Results --\x3e R2("\u63a8\u7406\u8d28\u91cf\u4f18\u4e8eMLE\u65b9\u6cd5/Improves inference quality over MLE")\n    Results --\x3e R3("\u591a\u6a21\u6001\u63a8\u7406\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027/Multimodal inference further reduces uncertainty")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [reinforcement learning, precision autotuning, contextual bandit, mixed-precision, linear solvers]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Erin Carson, Xinye Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Charles University, Sorbonne Universit\xe9, CNRS, LIP6"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00728",children:"https://arxiv.org/pdf/2601.00728"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel reinforcement learning framework formulated as a contextual bandit problem for adaptive precision tuning of numerical algorithms. 2. Applies the framework to iterative refinement for linear solvers, using a Q-table and epsilon-greedy strategy to dynamically select precision configurations based on system features. 3. Demonstrates the framework's effectiveness and generalization, reducing computational cost while maintaining accuracy comparable to double-precision baselines on unseen data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning framework for adaptive precision tuning, formulated as a contextual bandit problem, to optimize the trade-off between computational cost and accuracy in linear solvers. The method dynamically selects precision configurations based on system features using a Q-learning approach. Empirical results show it reduces cost while maintaining accuracy, and it generalizes well to unseen data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root(Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u80fd\u8017\u9ad8/High-precision computing is energy-intensive)\n    Problem --\x3e P2(\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u53ef\u80fd\u4e0d\u7a33\u5b9a/Reduced-precision can be unstable)\n    Method --\x3e M1(\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a/Formulate as Contextual Bandit)\n    Method --\x3e M2(\u4f7f\u7528Q\u8868\u548cepsilon-greedy\u7b56\u7565/Use Q-table & epsilon-greedy)\n    Method --\x3e M3(\u5e94\u7528\u4e8e\u7ebf\u6027\u6c42\u89e3\u5668\u7684\u8fed\u4ee3\u7cbe\u5316/Apply to iterative refinement for linear solvers)\n    Results --\x3e R1(\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c/Effectively reduces computational cost)\n    Results --\x3e R2(\u4fdd\u6301\u4e0e\u53cc\u7cbe\u5ea6\u76f8\u5f53\u7684\u7cbe\u5ea6/Maintains accuracy comparable to double-precision)\n    Results --\x3e R3(\u6cdb\u5316\u5230\u672a\u89c1\u6570\u636e/Generalizes to unseen data)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," U\u011furcan \xd6zalp"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Turkish Aerospace"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00737",children:"https://arxiv.org/pdf/2601.00737"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Critic\u7f51\u7edc\u7cfb\u7edf\u6027\u9ad8\u4f30\u4ef7\u503c/Critic Networks Systematically Overestimate Value Estimates]\n    C --\x3e C1[\u4f7f\u7528\u5355\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u65f6\u5e8f\u5076\u7136\u4e0d\u786e\u5b9a\u6027/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]\n    C --\x3e C2[\u5728TD\u66f4\u65b0\u4e2d\u5e94\u7528\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u60b2\u89c2\u504f\u5dee/Apply Uncertainty-Based Pessimistic Bias in TD Updates]\n    C --\x3e C3[\u5bf9\u8bc4\u8bba\u5bb6\u548c\u884c\u52a8\u8005\u7f51\u7edc\u4f7f\u7528Dropout\u6b63\u5219\u5316/Use Dropout Regularization on Critic and Actor Networks]\n    D --\x3e D1[\u7f13\u89e3\u9ad8\u4f30\u504f\u5dee/Mitigates Overestimation Bias]\n    D --\x3e D2[\u5728\u968f\u673a\u73af\u5883\u4e2d\u4ea7\u751f\u98ce\u9669\u89c4\u907f\u884c\u4e3a/Leads to Risk-Averse Behavior in Stochastic Environments]\n    D --\x3e D3[\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027/Improves Computational Efficiency and Training Stability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [distributional creative reasoning, diversity collapse, gradient flow, variational objective, reasoning paths]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Max Ruiz Luyten, Mihaela van der Schaar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Cambridge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00747",children:"https://arxiv.org/pdf/2601.00747"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Distributional Creative Reasoning (DCR), a unified variational framework that models training as gradient flow through probability measures on solution traces, encompassing methods like STaR, GRPO, and DPO as special cases. 2. Proves the diversity decay theorem, which explains how correctness-focused objectives lead to distinct modes of diversity collapse in different algorithms. 3. Provides principled designs and actionable recipes to ensure convergence to a stable and diverse policy, preventing creative collapse while maintaining correctness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fbb639faaebe1f394144b6e1a9a7bbb2ea6b005c8194a324964adf46349c164_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fbb639faaebe1f394144b6e1a9a7bbb2ea6b005c8194a324964adf46349c164_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that bootstrapped reasoning loops in LLMs, which optimize for correctness, lead to a collapse in the diversity of reasoning paths, harming creative problem-solving. To address this, it proposes Distributional Creative Reasoning (DCR), a unified theoretical framework that explains this collapse and offers designs to prevent it. The main conclusion is that DCR provides the first principled recipe for training LLMs that are both correct and creative."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving<br>\u63a8\u7406-\u521b\u9020\u529b\u6743\u8861\uff1a\u8fc8\u5411\u521b\u9020\u529b\u9a71\u52a8\u7684\u95ee\u9898\u89e3\u51b3"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Correctness-focused training causes diversity collapse in reasoning paths.<br>\u4ee5\u6b63\u786e\u6027\u4e3a\u6838\u5fc3\u7684\u8bad\u7ec3\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u7684\u591a\u6837\u6027\u5d29\u6e83\u3002"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Introduce Distributional Creative Reasoning (DCR), a unified variational framework.<br>\u63d0\u51fa\u5206\u5e03\u521b\u9020\u6027\u63a8\u7406\uff08DCR\uff09\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u53d8\u5206\u6846\u67b6\u3002"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Diversity decay theorem, designs to prevent collapse, actionable recipes.<br>\u591a\u6837\u6027\u8870\u51cf\u5b9a\u7406\uff0c\u9632\u6b62\u5d29\u6e83\u7684\u8bbe\u8ba1\uff0c\u53ef\u64cd\u4f5c\u7684\u65b9\u6848\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [sports analytics], [covariate-dependent Hidden Markov Model, defensive credit attribution, role-conditioned ghosting]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sean Groom, Shuo Wang, Francisco Belo, Axl Rice, Liam Anderson"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Birmingham, Nottingham Forest Football Club"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00748",children:"https://arxiv.org/pdf/2601.00748"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a covariate-dependent Hidden Markov Model (CDHMM) to infer time-resolved defensive role assignments (man-marking and zonal) from player tracking data during corner kicks. 2. Proposes a novel framework for defensive credit attribution based on the inferred role assignments. 3. Develops a role-conditioned ghosting method for context-aware counterfactual analysis of off-ball defensive performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829cb6bc9dbc2a288f24bae908e436437a33ee4cddc07525ba2d5196a191500c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829cb6bc9dbc2a288f24bae908e436437a33ee4cddc07525ba2d5196a191500c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of evaluating off-ball defensive performance in football, which is not captured by traditional metrics. The authors propose a covariate-dependent Hidden Markov Model to automatically infer defensive roles from tracking data and use this to create a new framework for credit attribution and counterfactual analysis. The method provides an interpretable, context-aware evaluation of defensive contributions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football<br>\u8db3\u7403\u4e2d\u65e0\u7403\u9632\u5b88\u89d2\u8272\u4e0e\u6027\u80fd\u8bc4\u4f30\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u8bc4\u4f30\u65e0\u7403\u9632\u5b88\u8868\u73b0<br>\u73b0\u6709\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u7f3a\u4e4f\u6218\u672f\u80cc\u666f"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f15\u5165\u534f\u53d8\u91cf\u4f9d\u8d56\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b(CDHMM)<br>\u63d0\u51fa\u57fa\u4e8e\u89d2\u8272\u7684\u9632\u5b88\u8d21\u732e\u5f52\u56e0\u4e0e\u5e7b\u5f71\u65b9\u6cd5"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u3001\u60c5\u5883\u611f\u77e5\u7684\u9632\u5b88\u8d21\u732e\u8bc4\u4f30"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Memory Bank Compression for Continual Adaptation of Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [memory bank compression, codebook optimization, online resetting mechanism, Key-Value Low-Rank Adaptation (KV-LoRA)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Thomas Katraouras, Dimitrios Rafailidis"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Thessaly"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00756",children:"https://arxiv.org/pdf/2601.00756"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Thomkat/MBC",children:"https://github.com/Thomkat/MBC"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed MBC, a model that compresses the memory bank for continual learning via a codebook optimization strategy. 2. Introduced an online resetting mechanism to prevent codebook collapse and ensure stable learning. 3. Employed Key-Value Low-Rank Adaptation (KV-LoRA) in the LLM's attention layers to efficiently utilize the compressed memory representations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4b16d7a0bdd1f6fa0e71da5c21a92629d07342bdc56905e10612ee07549b8dd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of memory bank growth in continual learning for LLMs by proposing MBC, which compresses the memory bank using codebook optimization and an online resetting mechanism. The method integrates KV-LoRA for efficient adaptation and achieves a 99.7% reduction in memory bank size while maintaining high accuracy on question-answering tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Memory Bank Compression for Continual Adaptation of Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u77e5\u8bc6\u8fc7\u65f6 / LLMs' knowledge becomes outdated]\n    B --\x3e B2[\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8 / Catastrophic forgetting in continual learning]\n    B --\x3e B3[\u5185\u5b58\u5e93\u65e0\u9650\u589e\u957f / Memory bank grows unbounded]\n    C --\x3e C1[\u5185\u5b58\u5e93\u538b\u7f29 / Memory bank compression]\n    C --\x3e C2[\u7801\u672c\u4f18\u5316\u7b56\u7565 / Codebook optimization strategy]\n    C --\x3e C3[\u5728\u7ebf\u91cd\u7f6e\u673a\u5236 / Online resetting mechanism]\n    C --\x3e C4[KV-LoRA / Key-Value Low-Rank Adaptation]\n    D --\x3e D1[\u5185\u5b58\u5e93\u5927\u5c0f\u51cf\u5c11\u81f30.3% / Memory bank size reduced to 0.3%]\n    D --\x3e D2[\u4fdd\u6301\u9ad8\u7cbe\u5ea6 / Maintains high retention accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Categorical Reparameterization with Denoising Diffusion models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [categorical reparameterization, gradient estimation, denoising diffusion, continuous relaxation, Gumbel-Softmax]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Samson Gourevitch, Alain Durmus, Eric Moulines, Jimmy Olsson, Yazid Janati"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," CMAP, Ecole polytechnique; Mohamed Bin Zayed University of AI; KTH Royal Institute of Technology; Institute of Foundation Models, MBZUAI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00781",children:"https://arxiv.org/pdf/2601.00781"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel diffusion-based soft reparameterization for categorical distributions, extending the family of continuous relaxations. 2. Shows that the denoiser for categorical distributions under a Gaussian noising process has a closed-form, efficient solution, enabling a training-free diffusion sampler. 3. Demonstrates that the proposed method yields competitive or improved optimization performance on various benchmarks compared to existing gradient estimators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07551087da54975e4db6aaddfb0b58a659ed27c7a4b438249f27613f2153d75f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07551087da54975e4db6aaddfb0b58a659ed27c7a4b438249f27613f2153d75f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of gradient-based optimization with categorical variables by proposing a new diffusion-based reparameterization method. It leverages the closed-form denoiser for categorical distributions to create a differentiable, training-free sampler. Experimental results show this approach provides effective gradient estimates, outperforming or matching existing methods on several benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Categorical Reparameterization with Denoising Diffusion models] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u68af\u5ea6\u4f18\u5316\u4e0e\u5206\u7c7b\u53d8\u91cf/Gradient optimization with categorical variables]\n    P1 --\x3e P2[\u73b0\u6709\u4f30\u8ba1\u5668\u5b58\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861/Existing estimators have bias-variance trade-off]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u57fa\u8f6f\u91cd\u53c2\u6570\u5316/Diffusion-based soft reparameterization]\n    M1 --\x3e M2[\u95ed\u5f0f\u53bb\u566a\u5668/Closed-form denoiser for categorical distributions]\n    M2 --\x3e M3[\u514d\u8bad\u7ec3\u53ef\u5fae\u5206\u91c7\u6837\u5668/Training-free differentiable sampler]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u7ade\u4e89\u6027\u6216\u6539\u8fdb\u7684\u4f18\u5316\u6027\u80fd/Competitive or improved optimization performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning verification], [spectral graph analysis, attention patterns, Fiedler value, high-frequency energy ratio, sliding window attention]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Valentin No\xebl"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Devoteam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00791",children:"https://arxiv.org/pdf/2601.00791"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a training-free method for detecting valid mathematical reasoning in LLMs by performing spectral analysis on attention matrices treated as dynamic graphs. 2. Identified four interpretable spectral diagnostics (Fiedler value, HFER, smoothness, entropy) that show significant statistical differences between valid and invalid proofs across multiple model families. 3. Discovered that the method captures logical coherence rather than formal verifier acceptance and revealed an architectural dependency where different attention mechanisms (e.g., Sliding Window Attention) shift the primary discriminative spectral feature."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b1f2999ddcf2e05565198a4f51efee7b71422414b78038f132537978df2e4e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a training-free method to detect valid mathematical reasoning in large language models by analyzing the spectral properties of attention patterns. The method identifies key spectral signatures that effectively distinguish between valid and invalid proofs with high accuracy. The findings show the method captures logical coherence and its effectiveness depends on the model's attention architecture."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Detecting valid mathematical reasoning in LLMs")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Spectral analysis of attention patterns as dynamic graphs")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: High classification accuracy, detects logical coherence, architectural dependency identified")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sunny Gupta, Amit Sethi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Bombay"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00785",children:"https://arxiv.org/pdf/2601.00785"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/sunnyinAI/FedHypeVAE"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FedHypeVAE] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u975eIID\u6570\u636e\u4e0e\u9690\u79c1\u6311\u6218/non-IID & Privacy]\n    C --\x3e C1[\u8d85\u7f51\u7edc\u751f\u6210\u6761\u4ef6VAE/Hypernetwork-Generated Conditional VAE]\n    C --\x3e C2[\u5dee\u5206\u9690\u79c1\u8bad\u7ec3/Differentially Private Training]\n    C --\x3e C3[MMD\u5bf9\u9f50\u4e0e\u6b63\u5219\u5316/MMD Alignment & Regularization]\n    D --\x3e D1[\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u7edf\u4e00/Unified Personalization & Privacy]\n    D --\x3e D2[\u53ef\u63a7\u591a\u57df\u5408\u6210/Controllable Multi-Domain Synthesis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [U-Net, layer normalization, instance-batch normalization, left ventricle segmentation, cardiac MRI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenhui Chu, Nikolaos V. Tsekos"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Houston"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00794",children:"https://arxiv.org/pdf/2601.00794"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies\u2014layer normalization and a combined instance-batch normalization\u2014to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5de6\u5fc3\u5ba4\u5206\u5272\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981/Left ventricle segmentation is critical for clinical diagnosis]\n    C --\x3e C1[\u63d0\u51faLNU-Net\u548cIBU-Net/Propose LNU-Net and IBU-Net]\n    C1 --\x3e C2[\u57fa\u4e8eU-Net\uff0c\u91c7\u7528\u4e0d\u540c\u5f52\u4e00\u5316\u7b56\u7565/Based on U-Net with different normalization strategies]\n    D --\x3e D1[\u5728805\u5f20MRI\u56fe\u50cf\u4e0a\u8bc4\u4f30/Evaluated on 805 MRI images]\n    D1 --\x3e D2[\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Outperforms state-of-the-art approaches]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [scientific machine learning], [Bayesian operator inference, active learning, reduced-order models, parametric systems, adaptive sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shane A. McQuarrie, Mengwu Guo, Anirban Chaudhuri"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Brigham Young University, Lund University, The University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00038",children:"https://arxiv.org/pdf/2601.00038"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a probabilistic version of parametric operator inference by casting the learning problem as Bayesian linear regression. 2. Designed a sequential adaptive sampling scheme that uses prediction uncertainties from the probabilistic ROM to select new training parameters. 3. Demonstrated through numerical experiments that the active learning framework yields more stable and accurate ROMs than random sampling under the same computational budget."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c29bb4bdd9fe6e4385983b4dab88a4de7d95bf126d6c1b64568426082175c1b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c29bb4bdd9fe6e4385983b4dab88a4de7d95bf126d6c1b64568426082175c1b6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an active learning framework to improve data-driven reduced-order models (ROMs) for parametric dynamical systems. The method uses Bayesian operator inference to create probabilistic ROMs and leverages their prediction uncertainties to adaptively select the most informative training parameters. The results show this approach consistently produces more stable and accurate ROMs compared to training with random parameter samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Active learning for data-driven reduced models<br>\u6570\u636e\u9a71\u52a8\u7684\u964d\u9636\u6a21\u578b\u4e3b\u52a8\u5b66\u4e60") --\x3e Problem("ROM quality depends on training data<br>ROM\u8d28\u91cf\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e")\n    Root --\x3e Method("Bayesian OpInf + Adaptive Sampling<br>\u8d1d\u53f6\u65af\u7b97\u5b50\u63a8\u65ad + \u81ea\u9002\u5e94\u91c7\u6837")\n    Root --\x3e Results("More stable & accurate ROMs vs. random sampling<br>\u76f8\u6bd4\u968f\u673a\u91c7\u6837\uff0c\u83b7\u5f97\u66f4\u7a33\u5b9a\u51c6\u786e\u7684ROM")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [quantum computing], [quantum dot, charge stability diagram, automated characterization, machine learning, image processing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00067",children:"https://arxiv.org/pdf/2601.00067"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method's effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Automated electrostatic characterization of quantum dot devices<br>\u91cf\u5b50\u70b9\u5668\u4ef6\u81ea\u52a8\u9759\u7535\u8868\u5f81] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Manual interpretation of CSDs is slow and error-prone<br>CSD\u624b\u52a8\u89e3\u91ca\u6162\u4e14\u6613\u9519]\n    C --\x3e C1[Integrates ML, image processing, object detection<br>\u96c6\u6210ML\u3001\u56fe\u50cf\u5904\u7406\u3001\u76ee\u6807\u68c0\u6d4b]\n    D --\x3e D1[Enables rapid extraction of device parameters (lever arms, couplings)<br>\u5b9e\u73b0\u5668\u4ef6\u53c2\u6570\u5feb\u901f\u63d0\u53d6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Group Cross-Correlations with Faintly Constrained Filters"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [group theory, harmonic analysis, topological dynamics], [group cross-correlations, G-equivariant filters, non-compact stabilizers, non-transitive actions, unimodularity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Benedikt Fluhr"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cannot be inferred from the provided content."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00045",children:"https://arxiv.org/pdf/2601.00045"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a new notion of group cross-correlations with less constrained filters, 2. Resolves incompatibilities for group actions with non-compact stabilizers, 3. Generalizes results to non-transitive group actions and weakens the unimodularity assumption."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/140a9a6a7cb64898feb16f84d9ad4ac9bc745bc82682b538beb6dd66d1f871c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/140a9a6a7cb64898feb16f84d9ad4ac9bc745bc82682b538beb6dd66d1f871c8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a generalized framework for group cross-correlations by relaxing the constraints on the filters used. This new approach resolves previous theoretical limitations for group actions with non-compact stabilizers and extends applicability to non-transitive actions without requiring unimodularity. The work provides a more flexible mathematical foundation for equivariant transformations on vector bundles."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Group Cross-Correlations with Faintly Constrained Filters] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5148\u524d\u6ee4\u6ce2\u5668\u7ea6\u675f\u8fc7\u7d27 / Previous filter constraints too tight]\n    B --\x3e B2[\u4e0e\u975e\u7d27\u7a33\u5b9a\u5b50\u7fa4\u4e0d\u517c\u5bb9 / Incompatible with non-compact stabilizers]\n    C --\x3e C1[\u63d0\u51fa\u5f31\u7ea6\u675f\u6ee4\u6ce2\u5668\u6982\u5ff5 / Propose faintly constrained filters]\n    C --\x3e C2[\u63a8\u5e7f\u5230\u975e\u4f20\u9012\u7fa4\u4f5c\u7528 / Generalize to non-transitive actions]\n    D --\x3e D1[\u89e3\u51b3\u4e0d\u517c\u5bb9\u6027\u95ee\u9898 / Resolves incompatibility]\n    D --\x3e D2[\u653e\u5bbd\u5355\u6a21\u6027\u5047\u8bbe / Weakens unimodularity assumption]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Convolutional Neural Network, Transfer Learning, Chest X-ray, Pediatric Pneumonia, RegNet]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zahedan University of Medical Sciences, Guilan University of Medical Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00041",children:"https://arxiv.org/pdf/2601.00041"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Deep Learning Approach for Pediatric Pneumonia Diagnosis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u513f\u7ae5\u80ba\u708e\u8bca\u65ad\u56f0\u96be / Pediatric Pneumonia Diagnosis is Challenging]\n    C --\x3e C1[\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60 / Use Pretrained CNNs with Transfer Learning]\n    C --\x3e C2[\u8bc4\u4f30ResNetRS, RegNet, EfficientNetV2 / Evaluate ResNetRS, RegNet, EfficientNetV2]\n    D --\x3e D1[RegNet\u6027\u80fd\u6700\u4f73 / RegNet Achieved Best Performance]\n    D --\x3e D2[\u51c6\u786e\u738792.4%, \u654f\u611f\u5ea690.1% / Accuracy 92.4%, Sensitivity 90.1%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ben Gurion University of the Negev, Tel Aviv University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00012",children:"https://arxiv.org/pdf/2601.00012"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Shaharak88/neural-brain-fields",children:"https://github.com/Shaharak88/neural-brain-fields"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>EEG data challenges: low SNR, variability, limited datasets"] --\x3e P1["\u5177\u4f53\u6311\u6218/Challenges<br>Varying length, low SNR, participant differences"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>NeRF-inspired neural network for EEG"] --\x3e M1["\u6838\u5fc3\u7c7b\u6bd4/Core Analogy<br>Viewpoints (NeRF) \u2194 Electrodes (EEG)"]\n    Method --\x3e M2["\u6280\u672f\u5b9e\u73b0/Technique<br>Train on single sample to get fixed weight vector"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Enables continuous visualization & reconstruction"] --\x3e R1["\u529f\u80fd\u4e00/Capability 1<br>Render signal at unseen times/positions"]\n    Results --\x3e R2["\u529f\u80fd\u4e8c/Capability 2<br>Simulate non-existent electrodes"]\n    Results --\x3e R3["\u5b9e\u8bc1\u7ed3\u679c/Empirical Result<br>Improves standard EEG network performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [health informatics], [deep learning, Holter ECG, explainable AI, time series analysis, risk prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Eran Zvuloni, Ronit Almog, Michael Glikson, Shany Brimer Biton, Ilan Green, Izhar Laufer, Offer Amir, Joachim A. Behar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technion - Israel Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00014",children:"https://arxiv.org/pdf/2601.00014"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed DeepHHF, a deep learning model that uses full 24-hour single-lead ECG recordings for heart failure risk prediction, outperforming models using short segments and clinical scores. 2. Created and utilized the large-scale Technion-Leumit Holter ECG (TLHE) dataset, comprising 69,663 recordings from 47,729 patients collected over 20 years. 3. Provided explainability analysis showing the model focuses on arrhythmias and heart abnormalities, with key attention patterns during daytime hours (8 AM to 3 PM), linking model decisions to clinically relevant features."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa3d768712a10f4b2d45732f0f8e0d64f70a07add2581bf81c12c996da11b77_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DeepHHF, a deep learning model that analyzes 24-hour single-lead ECG data to predict the 5-year risk of heart failure. The model achieved an AUC of 0.80, outperforming baseline methods, and identified high-risk individuals with a two-fold increased chance of hospitalization or death. The study demonstrates the feasibility of using long-term, continuous ECG data and explainable AI for non-invasive and accessible heart failure risk prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9884\u6d4b\u5fc3\u529b\u8870\u7aed\u98ce\u9669/Predict Heart Failure Risk]\n    B --\x3e B2[\u4f7f\u7528\u957f\u65f6\u7a0bECG\u6570\u636e/Using Long-term ECG Data]\n    C --\x3e C1[\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bDeepHHF/Deep Learning Model DeepHHF]\n    C --\x3e C2[\u5206\u679024\u5c0f\u65f6\u5355\u5bfc\u8054ECG/Analyze 24-hour Single-lead ECG]\n    C --\x3e C3[\u53ef\u89e3\u91ca\u6027\u5206\u6790/Explainability Analysis]\n    D --\x3e D1[AUC\u8fbe\u52300.80/AUC of 0.80]\n    D --\x3e D2[\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53/Identify High-risk Individuals]\n    D --\x3e D3[\u5173\u6ce8\u5fc3\u5f8b\u5931\u5e38\u4e0e\u65e5\u95f4\u6a21\u5f0f/Focus on Arrhythmias & Daytime Patterns]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [biomedical sensing and instrumentation], [electrical bioimpedance, physics-informed neural network, cuffless blood pressure, hemodynamic monitoring, wearable device]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Henry Crandall, Tyler Schuessler, Filip B\u011bl\xedk, Albert Fabregas, Barry M. Stults, Alexandra Boyadzhiev, Huanan Zhang, Jim S. Wu, Aylin R. Rodan, Stephen P. Juraschek, Ramakrishna Mukkamala, Alfred K. Cheung, Stavros G. Drakos, Christel Hohenegger, Braxton Osting, Benjamin Sanchez"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Utah, University of Illinois Chicago"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00081",children:"https://arxiv.org/pdf/2601.00081"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. 2. Established a multiscale biophysical modeling framework to elucidate the relationship between BioZ and blood pressure, identifying key influencing parameters. 3. Created a signal-tagged physics-informed neural network that incorporates fluid dynamics principles for calibration-free estimation of blood pressure and blood velocity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ba7d002ced9d6e8a70d39b806f9e6640ce2f6db3f9aa85c2884e1ce15b38a91_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ba7d002ced9d6e8a70d39b806f9e6640ce2f6db3f9aa85c2884e1ce15b38a91_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops a smartwatch-based system using electrical bioimpedance sensing and a physics-informed machine learning model to enable cuffless, calibration-free monitoring of blood pressure and blood velocity. The method addresses the theoretical limitations of existing wearable technologies by grounding the model in biophysical principles. The approach was successfully validated across diverse populations and settings, demonstrating its clinical feasibility."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Cuffless, Calibration-Free Hemodynamic Monitoring<br>\u65e0\u8896\u5e26\u3001\u514d\u6821\u51c6\u8840\u6d41\u52a8\u529b\u5b66\u76d1\u6d4b"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u73b0\u6709\u65e0\u8896\u5e26\u8bbe\u5907\u4f9d\u8d56\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u7684\u65b9\u6cd5\uff0c\u6613\u53d7\u751f\u7406\u548c\u5b9e\u9a8c\u6df7\u6742\u56e0\u7d20\u5f71\u54cd<br>Existing cuffless devices lack theoretical foundation and are vulnerable to confounders"]\n    Method["\u5f00\u53d1\u5177\u6709\u5b9e\u65f6\u751f\u7269\u7535\u963b\u6297\u4f20\u611f\u7684\u667a\u80fd\u624b\u8868\uff0c\u5e76\u4f7f\u7528\u7ed3\u5408\u6d41\u4f53\u52a8\u529b\u5b66\u539f\u7406\u7684\u4fe1\u53f7\u6807\u8bb0\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc<br>Developed smartwatch with real-time BioZ sensing and a signal-tagged physics-informed neural network"]\n    Results["\u5728\u5065\u5eb7\u4e2a\u4f53\u548c\u60a3\u8005\u4e2d\u6210\u529f\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6280\u672f\u7528\u4e8e\u65e0\u8896\u5e26\u8840\u538b\u548c\u8840\u6d41\u901f\u5ea6\u76d1\u6d4b\u7684\u53ef\u884c\u6027<br>Successfully tested in healthy individuals and patients, demonstrating feasibility for cuffless BP and velocity monitoring"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Detecting Unobserved Confounders: A Kernelized Regression Approach"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal inference], [unobserved confounders, kernel regression, reproducing kernel hilbert space, single-environment, causal effect estimation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yikai Chen, Yunxin Mao, Chunyuan Zheng, Hao Zou, Shanzhi Gu, Shixuan Liu, Yang Shi, Wenjing Yang, Kun Kuang, Haotian Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00200",children:"https://arxiv.org/pdf/2601.00200"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel method (KRCD) for detecting unobserved confounders in nonlinear observational data under single-environment conditions, bridging a key gap in existing methods. 2. Provided theoretical guarantees, proving that in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist, and that finite-sample differences converge to a zero-mean Gaussian distribution. 3. Demonstrated superior performance and computational efficiency over existing baselines through extensive experiments on synthetic benchmarks and the Twins dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34f526abd65dea1ab2c42ba3db6b7f0a13399cb09990eded28483079693cf4f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d34f526abd65dea1ab2c42ba3db6b7f0a13399cb09990eded28483079693cf4f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Kernel Regression Confounder Detection (KRCD), a method to detect unobserved confounders in nonlinear, single-environment observational data by comparing standard and higher-order kernel regressions. Theoretically, it proves key properties about the test statistic's behavior. Experiments show KRCD outperforms existing baselines in both detection performance and computational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Detecting Unobserved Confounders: A Kernelized Regression Approach") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Detecting unobserved confounders in nonlinear, single-environment data")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Kernel Regression Confounder Detection (KRCD)")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines, proven theoretical guarantees, high computational efficiency")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Minimum Weight Perfect Matching for Quantum Error Codes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Quantum Error Correction, Minimum Weight Perfect Matching, Graph Neural Networks, Transformer, Hybrid Decoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yotam Peled, David Zenati, Eliya Nachmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ben-Gurion University of the Negev"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00242",children:"https://arxiv.org/pdf/2601.00242"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid decoder (NMWPM) that integrates GNNs and Transformers to predict dynamic edge weights for the MWPM algorithm. 2. Formulated a novel proxy loss function to enable end-to-end training through the non-differentiable MWPM algorithm. 3. Demonstrated a significant reduction in Logical Error Rate (LER) compared to standard baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e507723ea614845b9a945c6d086d7f6413ab64a2e5cbfa21b28ceaa1777ffa60_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a neural-enhanced decoder for quantum error correction called Neural Minimum Weight Perfect Matching (NMWPM). It uses a hybrid architecture of Graph Neural Networks and Transformers to predict dynamic edge weights for the classical MWPM decoder, trained with a novel proxy loss. The method significantly reduces the Logical Error Rate, showing the advantage of combining neural networks with classical algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Neural Minimum Weight Perfect Matching for Quantum Error Codes"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Quantum error correction requires effective decoding."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Hybrid GNN+Transformer predicts weights for MWPM."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Significant reduction in Logical Error Rate."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [astronomical image analysis], [Low-Rank Adaptation (LoRA), photometric redshift, spectroscopic redshift, CNN, transfer learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vikram Seenivasan, Srinath Saikrishnan, Andrew Lizarraga, Jonathan Soriano, Bernie Boscoe, Tuan Do"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA), Southern Oregon University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00146",children:"https://arxiv.org/pdf/2601.00146"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Applied Low-Rank Adaptation (LoRA), a technique from large language models, to fine-tune CNN-based regression models for photometric redshift prediction in astrophysics. 2. Demonstrated that LoRA effectively combines datasets with different ground truths (photometric and spectroscopic redshifts) to improve model performance, outperforming traditional transfer learning with significantly reduced bias and scatter. 3. Showed that LoRA provides a computationally efficient middle ground between full model retraining and no retraining, enabling the leveraging of pre-trained models for data-sparse tasks in astrophysics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7c8dfd9d9ff319ca34f37730c3de7d103e6a39edc2131c3e4ca2711ce7fd7e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7c8dfd9d9ff319ca34f37730c3de7d103e6a39edc2131c3e4ca2711ce7fd7e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper applies Low-Rank Adaptation (LoRA) to fine-tune a CNN model for predicting galaxy redshifts, combining a less accurate but broad photometric redshift dataset with a more accurate but limited spectroscopic redshift dataset. The LoRA-based method outperforms traditional transfer learning, achieving lower bias and scatter, and offers a computationally efficient compromise between full retraining and no adaptation. The work demonstrates LoRA's utility for improving regression models in astrophysics by efficiently integrating datasets with different quality ground truths."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Combining datasets with different ground truths using LoRA for photometric redshift prediction] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u7ed3\u5408\u4e0d\u540c\u7cbe\u5ea6\u7684\u7ea2\u79fb\u6570\u636e\u96c6\u4ee5\u63d0\u5347CNN\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1f<br>How to combine redshift datasets of different accuracy to improve CNN model generalization?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u5728\u5149\u8c31\u7ea2\u79fb\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u57fa\u4e8e\u6d4b\u5149\u7ea2\u79fb\u8bad\u7ec3\u7684\u57fa\u6a21\u578b<br>Using Low-Rank Adaptation (LoRA) to fine-tune a base model (trained on photometric redshifts) on a spectroscopic redshift dataset.]\n    D[\u5173\u952e\u7ed3\u679c/Results: LoRA\u6a21\u578b\u6bd4\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u504f\u5dee\u548c\u6563\u5c04\u66f4\u5c0f\uff1b\u63d0\u4f9b\u5168\u91cd\u8bad\u7ec3\u4e0e\u4e0d\u8bad\u7ec3\u4e4b\u95f4\u7684\u9ad8\u6548\u6298\u4e2d\u65b9\u6848<br>LoRA model has less bias/scatter than traditional transfer learning; offers an efficient middle ground between full retraining and no retraining.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [computational fluid dynamics], [physics-informed neural networks, multi-stage training, unbounded domain, coordinate transformation, compressible potential equation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xuehui Qian, Hongkai Tao, Yongji Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Washington University in St. Louis, University of Notre Dame, Central South University, Stanford University, New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00342",children:"https://arxiv.org/pdf/2601.00342"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel PINN framework to solve the full nonlinear compressible potential equation in an infinite domain, overcoming a key limitation of traditional methods. 2. Introduces a coordinate transformation and embeds physical asymptotic constraints to address the unbounded-domain and convergence challenges inherent in standard PINNs. 3. Employs a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70afc10edc6a8a0b3606afe357472e00dfb9e25867b67948c402e19271e24608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70afc10edc6a8a0b3606afe357472e00dfb9e25867b67948c402e19271e24608_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel framework using Physics-Informed Neural Networks (PINNs) to solve the full nonlinear subsonic compressible flow equation in an infinite domain. The method addresses domain and convergence challenges via coordinate transformation and a multi-stage training process. The results demonstrate high-fidelity solutions and quantify errors from traditional domain truncation and linearization, especially at higher Mach numbers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Solving Nonlinear Subsonic Flow in Infinite Domain via MS-PINNs<br>\u57fa\u4e8e\u591a\u9636\u6bb5\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u65e0\u9650\u57df\u975e\u7ebf\u6027\u4e9a\u97f3\u901f\u6d41\u52a8] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>\u4f20\u7edf\u65b9\u6cd5\u5728\u65e0\u9650\u57df\u6c42\u89e3\u975e\u7ebf\u6027\u4e9a\u97f3\u901f\u6d41\u52a8\u5b58\u5728\u8bef\u5dee] --\x3e P1[\u7ebf\u6027\u5316\u6216\u622a\u65ad\u57df\u5f15\u5165\u8bef\u5dee<br>Linearization/Truncation Causes Error]\n    Problem --\x3e P2[\u6807\u51c6PINNs\u5728\u65e0\u9650\u57df\u6536\u655b\u56f0\u96be<br>Standard PINNs Struggle in Unbounded Domain]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51fa\u65b0\u7684PINN\u6846\u67b6] --\x3e M1[\u5750\u6807\u53d8\u6362\u5904\u7406\u65e0\u9650\u57df<br>Coordinate Transformation for Unbounded Domain]\n    Method --\x3e M2[\u7f51\u7edc\u5d4c\u5165\u7269\u7406\u6e10\u8fd1\u7ea6\u675f<br>Embed Physical Asymptotic Constraints]\n    Method --\x3e M3[\u591a\u9636\u6bb5\u8bad\u7ec3\u63d0\u9ad8\u7cbe\u5ea6<br>Multi-Stage Training for High Accuracy]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u9a8c\u8bc1\u6846\u67b6\u5e76\u91cf\u5316\u8bef\u5dee] --\x3e R1[\u5728\u5706\u548c\u692d\u5706\u51e0\u4f55\u4e0a\u9a8c\u8bc1<br>Validated on Circular/Elliptical Geometries]\n    Results --\x3e R2[\u7cbe\u5ea6\u63a5\u8fd1\u673a\u5668\u7cbe\u5ea6<br>Accuracy Approaches Machine Precision]\n    Results --\x3e R3[\u91cf\u5316\u622a\u65ad\u548c\u7ebf\u6027\u5316\u8bef\u5dee<br>Quantifies Truncation/Linearization Errors]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretable machine learning], [quantum-mechanical properties, CatBoost, explainable AI, binding features, electronic descriptors]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Li Chen, Leonardo Medrano Sandonas, Shirong Huang, Alexander Croy, Gianaurelio Cuniberti"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," TUD Dresden University of Technology, Friedrich Schiller University Jena"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00503",children:"https://arxiv.org/pdf/2601.00503"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Development of the MORE-ML computational framework integrating quantum-mechanical data with machine learning for predicting sensing properties. 2. Expansion of the dataset to MORE-QX, providing extensive electronic binding features for body odor volatilome adsorption. 3. Demonstration that CatBoost models with explainable AI methods offer superior performance and mechanistic insights for rational receptor design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19c57b3a84042598ddfad14eee8131675c83c4b83807b904281ff6859ca8850_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19c57b3a84042598ddfad14eee8131675c83c4b83807b904281ff6859ca8850_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of designing sensing materials for complex body odor detection by developing MORE-ML, a framework that uses quantum-mechanical property data of molecular building blocks as inputs to train tree-based ML models (like CatBoost) to predict binding features. The approach, validated on an expanded dataset, shows that CatBoost models perform well, especially on unseen compounds, and explainable AI methods identify key quantum properties influencing predictions, providing a foundation for rational design of artificial sensing materials."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Challenges in extending e-noses to complex body odor volatilome (BOV)"] --\x3e P1["\u7f3a\u4e4f\u673a\u7406\u7406\u89e3/Lack of mechanistic insight"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>MORE-ML framework"] --\x3e M1["\u6570\u636e\u96c6/Dataset<br>Expand MORE-Q to MORE-QX"]\n    Method --\x3e M2["\u6a21\u578b/Model<br>Tree-based ML (CatBoost) with QM descriptors"]\n    Method --\x3e M3["\u5206\u6790/Analysis<br>Explainable AI methods"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>CatBoost outperforms, provides design principles"] --\x3e R1["\u9ad8\u53ef\u8f6c\u79fb\u6027/High transferability"]\n    Results --\x3e R2["\u53ef\u89e3\u91ca\u7684\u89c1\u89e3/Interpretable insights"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [machine learning interatomic potential], [TensorNet2, force field, drug-like compounds, charged states, DFT-level accuracy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Stephen E. Farr, Stefan Doerr, Antonio Mirarchi, Francesc Sabanes Zariquiey, Gianni De Fabritiis"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Acellera Labs, Universitat Pompeu Fabra, ICREA"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00581",children:"https://arxiv.org/pdf/2601.00581"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/acellera/aceff-paper",children:"https://github.com/acellera/aceff-paper"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces AceFF, a pre-trained MLIP specifically optimized for small molecule drug discovery. 2. Employs a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds to improve generalizability. 3. Achieves a balance of high-throughput inference speed and DFT-level accuracy, explicitly supporting essential medicinal chemistry elements and charged states."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf03a6da5bff3c9952736adc7ba7fd161d48187c1623ade05d2174437fd823b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf03a6da5bff3c9952736adc7ba7fd161d48187c1623ade05d2174437fd823b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces AceFF, a machine learning interatomic potential designed for small molecule drug discovery. It uses a refined TensorNet2 architecture trained on drug-like compounds to achieve DFT-level accuracy with fast inference. The authors demonstrate its state-of-the-art performance on organic molecules through rigorous benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: MLIPs lack generalizability across diverse chemical spaces for drug discovery.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Refined TensorNet2 architecture trained on comprehensive dataset of drug-like compounds.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves DFT-level accuracy with high-throughput speed, establishing a new SOTA for organic molecules.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Generative Conditional Missing Imputation Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [missing data imputation], [Generative Neural Network, Missing Imputation, Multiple Imputation by Chained Equations, MCAR, MAR]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," George Sun, Yi-Hui Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," North Carolina State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00517",children:"https://arxiv.org/pdf/2601.00517"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Generative Conditional Missing Imputation Networks (GCMI) with a theoretical foundation for MCAR and MAR mechanisms., 2. Enhances GCMI's robustness and accuracy by integrating a multiple imputation framework using chained equations., 3. Empirically demonstrates the superior performance of the proposed method compared to other leading imputation techniques."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c2fe27f0abc6f988f3b6e7f4d03378bacf1dab0ce55f725ea0b5d5b1991f7df_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c2fe27f0abc6f988f3b6e7f4d03378bacf1dab0ce55f725ea0b5d5b1991f7df_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Generative Conditional Missing Imputation Networks (GCMI), a generative method for imputing missing data. The method is theoretically grounded for MCAR and MAR scenarios and is further improved by integrating a multiple imputation approach. Experiments show it outperforms other state-of-the-art imputation techniques."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Generative Conditional Missing Imputation Networks] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem: Missing Data Imputation)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method: GCMI with Multiple Imputation)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results: Superior Performance)\n    Problem --\x3e SubP1(\u7f3a\u5931\u673a\u5236/Missing Mechanisms: MCAR, MAR)\n    Method --\x3e SubM1(\u7406\u8bba\u57fa\u7840/Theoretical Foundation: GCMI)\n    Method --\x3e SubM2(\u589e\u5f3a\u6846\u67b6/Enhanced Framework: Multiple Imputation by Chained Equations)\n    Results --\x3e SubR1(\u8bc4\u4f30\u65b9\u5f0f/Evaluation: Simulations & Benchmark Datasets)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);