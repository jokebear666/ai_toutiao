"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8299],{6266:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_CL/20251215-20251221","title":"20251215-20251221 (cs.CL)","description":"2025-12-18","source":"@site/docs/daily/cs_CL/20251215-20251221.md","sourceDirName":"daily/cs_CL","slug":"/daily/cs_CL/20251215-20251221","permalink":"/ai_toutiao/daily/cs_CL/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766562086000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.CL","permalink":"/ai_toutiao/daily/cscl"},"next":{"title":"20251222-20251228 (cs.CL)","permalink":"/ai_toutiao/daily/cscl/20251222-20251228"}}');var r=i(4848),a=i(8453);const t={},o="20251215-20251221 (cs.CL)",l={},c=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function h(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"20251215-20251221-cscl",children:"20251215-20251221 (cs.CL)"})}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Krunal Jesani, Dmitry Ignatov, Radu Timofte"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of W\xfcrzburg"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14706",children:"https://arxiv.org/pdf/2512.14706"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ryan Cartularo"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14712",children:"https://arxiv.org/pdf/2512.14712"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] SoMe: A Realistic Benchmark for LLM-based Social Media Agents"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [SoMe benchmark, social media agents, LLM-based agents, agent tools, task evaluation, quantitative analysis, qualitative analysis]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Dizhan Xue, Jing Cui, Shengsheng Qian, Chuanrui Hu, Changsheng Xu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tianjin University of Technology; Nanjing University of Posts and Telecommunications; Peng Cheng Laboratory"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14720",children:"https://arxiv.org/pdf/2512.14720"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SoMe, a comprehensive benchmark for evaluating LLM-based social media agents across diverse tasks using real social media data and agent tools. The evaluation shows that current LLMs, both closed and open-source, perform unsatisfactorily on these realistic social media agent tasks. SoMe serves as a challenging testbed to advance future social media agent development."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] NoveltyRank: Estimating Conceptual Novelty of AI Papers"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [binary classification, pairwise comparison, semantic similarity, SPECTER2, fine-tuning, Qwen3-4B-Instruct-2507, SciBERT]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zhengxu Yan, Han Li, Yuming Feng"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14738",children:"https://arxiv.org/pdf/2512.14738"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes NoveltyRank, a model that estimates the conceptual novelty of AI papers using title, abstract, and semantic similarity to prior literature, evaluated through binary classification and pairwise comparison tasks. It fine-tunes Qwen3-4B-Instruct-2507 and SciBERT, benchmarking against GPT-5.1, and finds that task formulation and modeling choices significantly impact performance, with the implementation made publicly available."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Revisiting the Reliability of Language Models in Instruction-Following"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [llm evaluation], [instruction-following, reliability, data augmentation, benchmark, IFEval++, reliable@k]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jianshuo Dong, Yutong Zhang, Yan Liu, Zhenyu Zhong, Tao Wei, Chao Zhang, Han Qiu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Ant Group"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14754",children:"https://arxiv.org/pdf/2512.14754"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper introduces a new metric, reliable@k, and an automated data augmentation pipeline to generate "cousin prompts" for evaluating nuance-oriented reliability in LLMs, constructing the IFEval++ benchmark. It finds that current LLMs show significant performance drops (up to 61.8%) with nuanced prompt variations, highlighting a crucial gap in real-world reliability.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [large language models], [transformer architecture, structural hallucination, Licensing Oracle, hybrid systems, statistical ontology]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Richard Ackermann, Simeon Emanuilov"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," RA Software, Sofia University \u201cSt. Kliment Ohridski\u201d"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14801",children:"https://arxiv.org/pdf/2512.14801"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper argues that hallucination in LLMs is an architectural inevitability of transformers, which model token co-occurrence rather than the world, and demonstrates through a Licensing Oracle that external truth-validation modules are required for reliable abstention. It concludes that hallucination is a structural property, not a correctable incentive problem, necessitating hybrid systems to separate linguistic fluency from epistemic responsibility."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [end-to-end spoken dialogue systems, audio language models, multi-turn evaluation, speech-to-speech, audio-native benchmark, inference memory, instruction retention, self coherence, voice editing]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda U\xe7ar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Scale AI"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14865",children:"https://arxiv.org/pdf/2512.14865"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new "Voice Editing" axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Task Matrices: Linear Maps for Cross-Model Finetuning Transfer"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [model adaptation], [task matrix, linear transformation, finetuning, linear probe, embedding space]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Darrin O' Brien, Dhikshith Gajulapalli, Eric Xia"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Algoverse AI Research, Brown University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14880",children:"https://arxiv.org/pdf/2512.14880"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces the concept of a \"task matrix,\" a linear transformation that maps a base model's embedding state to a finetuned model's state. It demonstrates that applying this matrix to a base model can outperform linear probes and sometimes approach full finetuning performance across vision and text models on various datasets. The results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [large language models, knowledge graphs, viewpoint classification, fine-tuning, wikidata, semantic enrichment]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Massimiliano Fadda, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Cagliari, The Open University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14887",children:"https://arxiv.org/pdf/2512.14887"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper improves a pipeline for analyzing political viewpoints in news by fine-tuning Large Language Models for classification and enriching claim representations with semantic actor descriptions from Wikidata. The integrated approach, evaluated on UK immigration debate data, shows that combining fine-tuned LLMs with knowledge graph context yields the best performance, particularly with models capable of processing long inputs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [retrieval-augmented generation, structured drug knowledge, pharmacy question-answering, external knowledge integration]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Houman Kazemzadeh, Kiarash Mokhtari Dizaji, Seyed Reza Tavakoli, Farbod Davoodi, MohammadReza KarimiNejad, Parham Abed Azad, Ali Sabzi, Armin Khosravi, Siavash Ahmadi, Mohammad Hossein Rohban, Glolamali Aminian, Tahereh Javaheri"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tehran University of Medical Sciences, Sharif University of Technology, Amir Kabir University of Technology, Missouri University of Science and Technology, The Alan Turing Institute, Boston University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14896",children:"https://arxiv.org/pdf/2512.14896"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces DrugRAG, a three-step retrieval-augmented generation pipeline that integrates external structured drug knowledge into LLM prompts to improve accuracy on pharmacy QA tasks. It demonstrates that this external method enhances performance across multiple models without modifying their architecture, providing a practical approach for evidence-based AI in pharmacy."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [LoRA, dataset translation, visual question answering, instruction tuning, parameter-efficient fine-tuning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," George-Andrei Dima, Dumitru-Clementin Cercel"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," National University of Science and Technology POLITEHNICA Bucharest"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14926",children:"https://arxiv.org/pdf/2512.14926"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a parameter-efficient method for adapting vision-language models to Romanian by translating the Flickr30k dataset and generating QA pairs, then fine-tuning models like LLaMA, LLaVA, and Qwen2 using LoRA. The results show significant improvements in Romanian visual QA and image captioning, with the Qwen2-VL-RoVQA model achieving the best performance and reduced grammatical errors."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [knowledge distillation, byte-pair encoding, cross-tokenizer likelihood scoring, vocabulary misalignment, next-token probability]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Buu Phan, Ashish Khisti, Karen Ullrich"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Toronto, Meta AI"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14954",children:"https://arxiv.org/pdf/2512.14954"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a method for cross-tokenizer likelihood scoring to enable knowledge distillation between language models with different vocabularies. It leverages the recursive structure of Byte-Pair Encoding to compute exact or approximate next-token probabilities. The approach reduces memory footprint and improves model performance on tasks like mathematical reasoning compared to baseline distillation methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Prompt Repetition Improves Non-Reasoning LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [prompt repetition, causal language model, attention mechanism, non-reasoning tasks]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yaniv Leviathan, Matan Kalman, Yossi Matias"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Google Research"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14982",children:"https://arxiv.org/pdf/2512.14982"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a simple method of repeating the input prompt to improve the performance of LLMs on non-reasoning tasks. This technique allows all prompt tokens to attend to each other within the causal attention mechanism, addressing order sensitivity. The authors demonstrate that this method boosts accuracy for models like Gemini, GPT, Claude, and Deepseek without increasing output length or latency."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [multimodal reasoning, chain-of-thought prompting, ablation studies, occlusion-based interpretability, benchmark evaluation, modality fusion]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yiming Cui, Xin Yao, Yuxuan Qin, Xin Li, Shijin Wang, Guoping Hu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," State Key Laboratory of Cognitive Intelligence, iFLYTEK AI Research"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.14989",children:"https://arxiv.org/pdf/2512.14989"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically evaluates 40 multimodal large language models on a benchmark of chemistry Olympiad questions requiring visual and textual reasoning. The core method involves using chain-of-thought prompting and interpretability techniques like ablation and occlusion. The main conclusion is that current models struggle with modality fusion, but chain-of-thought improves both accuracy and visual grounding, revealing critical limitations in scientific reasoning."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [Process Reward Model (PRM), Chain-of-Function, meta-learning, label correction, bi-level optimization, test-time scaling, LiveCodeBench]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ruiyi Zhang, Peijia Qin, Qi Cao, Pengtao Xie"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of California, San Diego"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15000",children:"https://arxiv.org/pdf/2512.15000"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes DreamPRM-Code, a process reward model for coding that treats functions as reasoning steps using a Chain-of-Function strategy and employs a meta-learning-based label correction mechanism to refine noisy intermediate training labels. It achieves state-of-the-art performance on LiveCodeBench, surpassing OpenAI o4-mini."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15047",children:"https://arxiv.org/pdf/2512.15047"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Meta-Prompting Protocol, Adversarial Trinity, DSPy, TextGrad, textual gradients, semantic computation graph]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Fanzhe Fu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15053",children:"https://arxiv.org/pdf/2512.15053"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper introduces the Meta-Prompting Protocol, a framework that formalizes LLM orchestration as a programmable system using an adversarial topology (Generator, Auditor, Optimizer) to treat prompts as differentiable variables. It leverages textual critiques as gradients within a semantic computation graph to mitigate hallucination and improve reliability. The authors demonstrate its theoretical viability with tools like DSPy and TextGrad, proposing a foundation for deterministic "Observable Software Engineering" for probabilistic models.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [neuron-level intervention, expertise-weighted soft suppression, MM-TOXIC-QA, white-box detoxification, SGM*]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hongbo Wang, MaungMaung AprilPyone, Isao Echizen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Tokyo, National Institute of Informatics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15052",children:"https://arxiv.org/pdf/2512.15052"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SGM, a neuron-level intervention method that selectively recalibrates toxic expert neurons in multimodal large language models (MLLMs) using expertise-weighted soft suppression to reduce harmful outputs. Experiments show SGM significantly cuts toxicity rates from 48.2% to 2.5% while preserving model fluency and reasoning, and it can be combined with other methods for stronger safety."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [retrieval-augmented generation], [conformal prediction, semantic similarity, natural language inference, hallucination detection, text embeddings]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Debu Sinha"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Independent Researcher"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15068",children:"https://arxiv.org/pdf/2512.15068"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper applies conformal prediction to provide statistical guarantees for hallucination detection in RAG systems, rigorously evaluating embedding-based methods. It finds that while these methods work on synthetic data, they fail on real benchmarks due to the "semantic illusion," where plausible hallucinations remain semantically similar to source documents. The study concludes that embedding-based detection is insufficient for production, as reasoning-based methods like GPT-4 as a judge perform significantly better.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Quantifying Return on Security Controls in LLM Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [retrieval-augmented generation (RAG), Monte Carlo simulation, loss exceedance curves, Laplace's Rule of Succession, adversarial probing, Garak, attribute-based access control (ABAC), named entity recognition (NER) redaction, NeMo Guardrails]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Richard Helder Moulton, Austin O'Brien, John D. Hastings"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Dakota State University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15081",children:"https://arxiv.org/pdf/2512.15081"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a framework to quantify the financial return on security controls for LLM systems by simulating attacks on a RAG service, estimating attack success probabilities, and modeling potential losses with Monte Carlo methods. The main conclusion is that controls like ABAC and NER redaction significantly reduce expected financial losses and offer high return-on-control, whereas NeMo Guardrails provides minimal benefit in the tested scenarios."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [interpretability], [sparse autoencoders, sparse probes, concept disentanglement, steering experiments, multi-concept evaluation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, Patrik Reizinger"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Boston University, Harvard University, Mila \u2013 Quebec AI Institute, Goodfire, University of T\xfcbingen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15134",children:"https://arxiv.org/pdf/2512.15134"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a multi-concept evaluation framework to test whether interpretability methods like sparse autoencoders and sparse probes recover disentangled and independently manipulable concept representations. It finds that features often correspond to single concepts, but concepts are distributed across many features, and steering one feature typically affects multiple concepts, indicating a lack of true independence. The results highlight that correlational metrics are insufficient for proving disentanglement and underscore the need for compositional evaluations in interpretability research."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [MCP (Model Context Protocol), safety benchmark, multi-turn evaluation, multi-server workflows, attack taxonomy, agentic systems]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xuanjun Zong, Zhiqi Shen, Lei Wang, Yunshi Lan, Chao Yang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," East China Normal University, National University of Singapore, Singapore Management University, Shanghai AI Laboratory"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15163",children:"https://arxiv.org/pdf/2512.15163"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces MCP-SafetyBench, a benchmark built on real Model Context Protocol servers to evaluate the safety of LLMs operating as agents across tools and services. It systematically tests models on multi-step, multi-server tasks across five domains, revealing significant safety vulnerabilities that escalate with task complexity. The results highlight the urgent need for improved defenses in real-world LLM agent deployments."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [knowledge-aware question answering], [knowledge graph, chain-of-thought, few-shot in-context learning, relation-driven adaptive hop-count selection, path guidance]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chao Zhang, Minghan Li, Tianrui Lv, Guodong Zhou"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Soochow University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15219",children:"https://arxiv.org/pdf/2512.15219"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes RFKG-CoT, a method that enhances knowledge-aware question answering by dynamically selecting reasoning steps in a knowledge graph based on relations and using few-shot examples to guide large language models in understanding reasoning paths. It improves answer accuracy over previous methods by making the integration of knowledge graph evidence more adaptive and guided. Experiments show significant accuracy gains on multiple benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [fine-tuning, LoRA, zero-shot prompting, few-shot prompting, supervised fine-tuning, transformer models]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yash Bhaskar, Parameswari Krishnamurthy"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," IIIT Hyderabad"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15226",children:"https://arxiv.org/pdf/2512.15226"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper explores various methods for low-resource Indic language translation, including fine-tuning models like mT5 and IndicBart, using LoRA with IndicTrans2 and Llama 3, and prompting LLMs like Llama 3 and Mixtral. The results highlight the challenges of data scarcity and demonstrate the potential of fine-tuned large language models for these translation tasks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [reinforcement learning with verifiable rewards, progressive prefix-token policy optimization, beginning lock-in effect, prefix optimization, continuation accumulated reward]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yiliu Sun, Zicheng Zhao, Yang Wei, Yanfang Zhang, Chen Gong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nanjing University of Science and Technology, North University of China, Shanghai Jiao Tong University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15274",children:"https://arxiv.org/pdf/2512.15274"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Progressive Prefix-token Policy Optimization (PPPO), a reinforcement learning method that focuses on optimizing the initial prefix tokens of an LLM's reasoning output, based on the identified Beginning Lock-in Effect. It introduces strategies like Progressive Prefix Retention and Continuation Accumulated Reward to improve training efficiency. The method achieves significant accuracy improvements on reasoning tasks while using far fewer training tokens compared to standard approaches."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [multimodal reasoning, perception-cognition gap, calculation-conceptualization discrepancy, process hallucination, OCR, AI-resistant questions]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Seok-Hyun Ga, Chun-Yen Chang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute for Research Excellence in Learning Sciences, National Taiwan Normal University, Seoul National University, Universitas Negeri Malang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15298",children:"https://arxiv.org/pdf/2512.15298"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This study evaluates the multimodal scientific reasoning of LLMs like GPT-4o and Gemini on the Korean CSAT Earth Science I exam under different input conditions. It finds that models suffer from fundamental cognitive flaws, such as a perception-cognition gap and calculation-conceptualization discrepancy, even with optimized inputs. The paper concludes by suggesting these vulnerabilities can be exploited to design AI-resistant assessment questions to ensure academic integrity."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [scientific information extraction], [zero-shot prompting, few-shot prompting, event-specific prompting, reflection-based prompting, in-context learning, event extraction, argument extraction]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Charan Prakash Rathore, Saumi Ray, Dhruv Kumar"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15312",children:"https://arxiv.org/pdf/2512.15312"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically evaluates six state-of-the-art LLMs using four prompting strategies (zero-shot, few-shot, event-specific, reflection-based) for extracting structured event and argument information from zeolite synthesis procedures. It finds that while LLMs achieve strong performance on high-level event classification, they show modest results on fine-grained parameter extraction, with advanced prompting offering minimal gains over zero-shot approaches. The conclusion is that precise scientific information extraction requires domain-adapted models, as current LLMs have fundamental limitations in capturing synthesis-specific nuances."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Adversarial versification in portuguese as a jailbreak operator in LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [adversarial versification, poetry jailbreak, guardrail vulnerabilities, semiotic-formal variation, latent region displacement]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Joao Queiroz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Federal University of Juiz de Fora"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15353",children:"https://arxiv.org/pdf/2512.15353"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper investigates using versification, or rewriting prompts as poetry, as a method to bypass safety guardrails in aligned large language models. It concludes that this structural adversarial technique exploits a model's over-reliance on surface patterns, causing significant safety failures, and highlights a critical research gap for Portuguese due to its linguistic complexity."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Emotion Recognition in Signers"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [cross-lingual transfer, temporal segment selection, hand motion features, textual emotion recognition, facial expression analysis]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kotaro Funakoshi, Yaoxiong Zhu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute of Science Tokyo"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15376",children:"https://arxiv.org/pdf/2512.15376"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a new dataset for emotion recognition in Japanese Sign Language and addresses the challenges of overlapping grammatical/affective expressions and data scarcity using cross-lingual transfer from textual emotion recognition in spoken language. The authors demonstrate that selecting specific temporal segments and incorporating hand motion features significantly improves emotion recognition performance in signers, establishing a stronger baseline than spoken language LLMs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Tracking Temporal Dynamics of Vector Sets with Gaussian Process"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [temporal analysis], [Gaussian Process, Random Fourier Features, vector sets, temporal dynamics, low-dimensional visualization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Taichi Aida, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tokyo Metropolitan University, Hitotsubashi University, National Institute for Japanese Language and Linguistics, National Institute of Advanced Industrial Science and Technology, The Institute of Statistical Mathematics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15538",children:"https://arxiv.org/pdf/2512.15538"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a method to model time-varying vector sets using infinite-dimensional Gaussian Processes, approximating the latent function with Random Fourier Features to obtain compact, comparable representations over time. It demonstrates effectiveness in capturing temporal dynamics in crime distributions and word embeddings, providing interpretable, low-dimensional visualizations of structural changes."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Evaluating Metrics for Safety with LLM-as-Judges"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [LLM-as-Judges, weighted metrics, confidence thresholds, context sensitivity, safety evaluation, human review]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kester Clegg, Richard Hawkins, Ibrahim Habli, Tom Lawton"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of York, Bradford Royal Infirmary"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15617",children:"https://arxiv.org/pdf/2512.15617"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a safety evaluation method for LLMs in critical applications by focusing on evidence from LLM-as-Judges frameworks. It suggests using a basket of weighted metrics and context-sensitive error severity to lower risk, with low-confidence judgments triggering human review. The main conclusion is that such an approach can enhance the reliability of LLMs in safety-critical information flows."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [Low-Rank Adaptation (LoRA), supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), rank sweep, representational drift, attention patterns]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Darshita Rathore, Vineet Kumar, Chetna Bansal, Anindya Moitra"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," PayPal"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15634",children:"https://arxiv.org/pdf/2512.15634"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper comprehensively evaluates the trade-offs between full supervised fine-tuning (SFT) and Low-Rank Adaptation (LoRA) for fine-tuning large language models. It finds that LoRA, especially at specific rank values, can achieve competitive or even superior performance to SFT on reasoning tasks, while also analyzing the structural changes in model representations."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [continual learning], [energy-based model, progressive parameter selection, pseudo-sample generation, catastrophic forgetting mitigation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Mayo Clinic, Google, Texas A&M University, The University of Texas at Dallas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15658",children:"https://arxiv.org/pdf/2512.15658"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces PPSEBM, a framework combining an Energy-Based Model with Progressive Parameter Selection to address catastrophic forgetting in continual learning for NLP tasks. It uses task-specific parameters and generates pseudo-samples from prior tasks to retain past knowledge. Experimental results show PPSEBM outperforms state-of-the-art methods in mitigating forgetting."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Explaining the Reasoning of Large Language Models Using Attribution Graphs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [interpretability], [attribution methods, context attribution, attribution graph, CAGE, faithfulness]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Chase Walker, Rickard Ewetz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Florida"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15663",children:"https://arxiv.org/pdf/2512.15663"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the CAGE framework, which uses an attribution graph to explain autoregressive LLMs by quantifying how each generated token is influenced by both the prompt and all prior tokens, preserving causality and row stochasticity. This approach improves the faithfulness of context attributions by accounting for inter-generational influences, achieving average gains of up to 40% over existing methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-text compression, VTCBench, DeepSeek-OCR, Glyph, VTC-Retrieval, VTC-Reasoning, VTC-Memory]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS; Tencent Hunyuan Team"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15649",children:"https://arxiv.org/pdf/2512.15649"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces VTCBench, the first benchmark to evaluate Vision-Language Models' ability to understand long context using Vision-Text Compression (VTC), a technique that converts long text into dense 2D images for token efficiency. The study systematically tests models on retrieval, reasoning, and memory tasks with VTC-compressed inputs. The main conclusion is that most VLMs perform poorly on long-context understanding with VTC, despite good OCR decoding, failing to capture long-range associations in the compressed visual context."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [llm interpretability], [LatentQA, Activation Oracles, activation analysis, fine-tuning detection, natural language queries]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Adam Karvonen, James Chua, Cl\xe9ment Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," MATS, Truthful AI, EPFL, ENS Paris-Saclay, Northeastern University, Anthropic"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15674",children:"https://arxiv.org/pdf/2512.15674"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Activation Oracles, models trained using the LatentQA approach to answer natural language questions about the internal activations of other LLMs. The core finding is that these oracles, especially when trained on diverse datasets, can generalize to out-of-distribution tasks and effectively verbalize hidden information, such as knowledge from fine-tuning, often matching or exceeding prior white-box interpretability methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251218] Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [predictive concept decoder, communication bottleneck, sparse concept list, encoder-decoder, auto-interp score, fine-tuning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Transluce"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15712",children:"https://arxiv.org/pdf/2512.15712"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Predictive Concept Decoders (PCDs), an end-to-end trained architecture where an encoder compresses a model's internal activations into a sparse list of concepts, and a decoder uses this list to answer questions about the model's behavior. The method is pretrained on large datasets and then finetuned, showing that the interpretability and downstream performance of the bottleneck concepts improve with more data."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Value Lens: Using Large Language Models to Understand Human Values"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [large language models, value detection, generative AI, dual-LLM approach, expert verification]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Eduardo de la Cruz Fern\xe1ndez, Marcelo Karanik, Sascha Ossowski"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Universidad Polit\xe9cnica de Madrid, Universidad Rey Juan Carlos"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15722",children:"https://arxiv.org/pdf/2512.15722"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Value Lens, a two-stage model that uses Large Language Models (LLMs) to detect human values in text. The first stage uses an LLM to conceptualize a value theory verified by experts, and the second stage employs a dual-LLM approach for detection and critical review. The results show that Value Lens performs comparably to or better than other models in similar tasks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] LLaDA2.0: Scaling Up Diffusion Language Models to 100B"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [discrete diffusion language model, block-level WSD training, mixture-of-experts, knowledge inheritance, parallel decoding, SFT, DPO]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Ling Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, Yihong Zhuang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Ant Group, Renmin University of China, Zhejiang University, Westlake University, HongKong University of Science and Technology"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15745",children:"https://arxiv.org/pdf/2512.15745"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LLaDA2.0, a method for converting pre-trained auto-regressive language models into large-scale discrete diffusion models (dLLMs) using a novel three-phase block-level training scheme. The resulting instruction-tuned models, including a 100B-parameter variant, achieve superior performance and efficiency through parallel decoding. The work establishes a new paradigm for frontier-scale model deployment by enabling efficient scaling and knowledge inheritance from existing models."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [CLIP, Stable Diffusion XL, zero-shot classification, demographic bias mitigation, data generation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Javon Hickmon"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Washington"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15747",children:"https://arxiv.org/pdf/2512.15747"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Auto-Tuning Safety Guardrails for Black-Box Large Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [hyperparameter optimization, system prompts, content filters, jailbreak detection, malware generation, Optuna, ModernBERT, grid search]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Perry Abdulkadir"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of St. Thomas"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15782",children:"https://arxiv.org/pdf/2512.15782"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes treating the design of safety guardrails (like system prompts and content filters) for a frozen black-box LLM as a hyperparameter optimization problem. Using a proof-of-concept with Mistral-7B-Instruct and ModernBERT, it shows that a black-box optimizer (Optuna) can efficiently find safe configurations, matching the best grid search results with far fewer evaluations and less time. The conclusion is that this auto-tuning approach is a feasible method to harden LLM deployments under practical constraints."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] A Systematic Analysis of Biases in Large Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [fairness and bias analysis], [news summarization, stance classification, UN voting patterns, multilingual story completion, World Values Survey]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Xulang Zhang, Rui Mao, Erik Cambria"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15792",children:"https://arxiv.org/pdf/2512.15792"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically analyzes biases in large language models (LLMs) across political, ideological, alliance, language, and gender dimensions using experiments like news summarization and stance classification. The main conclusion is that despite being aligned for neutrality, the studied LLMs still exhibit various types of biases and affinities."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [ai ethics evaluation], [Model Cards, ALTAI, FactSheets, Harms Modeling, literature survey, interviews]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jhessica Silva, Diego A. B. Moreira, Gabriel O. dos Santos, Alef Ferreira, Helena Maia, Sandra Avila, Helio Pedrini"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Universidade Estadual de Campinas (UNICAMP), Universidade Federal de Goi\xe1s (UFG)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15791",children:"https://arxiv.org/pdf/2512.15791"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a methodology to evaluate AI Ethics Tools (AIETs) for language models by selecting four tools (Model Cards, ALTAI, FactSheets, Harms Modeling) and applying them to Portuguese language models, with developer interviews. The results indicate that these tools help guide general ethical considerations but fail to address language-specific aspects like idiomatic expressions or identify negative impacts for Portuguese."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [ethical ai], [contrastive learning, social norms generation, moral reasoning, explainable ai, valence prediction]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Hong Kong Baptist University, Singapore Management University, Singapore University of Technology and Design"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15793",children:"https://arxiv.org/pdf/2512.15793"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces ClarityEthic, a method that enhances ethical assessment of human actions by generating conflicting social norms to explain and predict valence (support/oppose). It uses a contrastive learning strategy to strengthen the moral reasoning of language models. Experiments show the method outperforms baselines and human evaluations confirm the generated norms provide plausible explanations."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [self-supervised learning, vision-language alignment, I-JEPA, JARVIS, masked predictive loss, frozen vision foundation models]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Modena and Reggio Emilia, AMD Silo AI"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15885",children:"https://arxiv.org/pdf/2512.15885"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] DSO: Direct Steering Optimization for Bias Mitigation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [fairness and bias mitigation], [activation steering, reinforcement learning, linear transformations, inference-time control]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Apple, Carnegie Mellon University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15926",children:"https://arxiv.org/pdf/2512.15926"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Social Story Frames: Contextual Reasoning about Narrative Intent and Reception"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [SocialStoryFrames, SSF-Generator, SSF-Classifier, narrative theory, linguistic pragmatics, taxonomy, reader response]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Joel Mire, Maria Antoniak, Steven R. Wilson, Zexin Ma, Achyutarama R. Ganti, Andrew Piper, Maarten Sap"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Carnegie Mellon University, University of Colorado Boulder, University of Michigan-Flint, University of Connecticut, McGill University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15925",children:"https://arxiv.org/pdf/2512.15925"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SocialStoryFrames, a formalism and computational framework for modeling nuanced reader responses to social media stories, including inferences about author intent and affective reactions. It develops two models (SSF-Generator and SSF-Classifier) and applies them to a corpus of online narratives to analyze storytelling practices across communities. The main conclusion is that this approach enables scalable, context-sensitive research into the social dynamics of online storytelling."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] BRAID: Bounded Reasoning for Autonomous Inference and Decisions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [bounded reasoning, structured prompting, instruction graphs, mermaid, chain-of-thought]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Arma\u011fan Amcalar, Eyup Cinar"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," OpenServ Labs, Eskisehir Osmangazi University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15959",children:"https://arxiv.org/pdf/2512.15959"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces BRAID, a bounded reasoning framework that uses Mermaid-based instruction graphs to structure prompts for LLMs, enabling them to reason structurally instead of through unbounded natural language. The method is shown to substantially increase reasoning accuracy and cost efficiency across multiple GPT model tiers on several benchmark datasets."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [reinforcement learning, low-rank approximation, dynamic rank selection, matrix perturbation theory, singular value decomposition]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Caner Erden"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Sakarya University of Applied Sciences"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.15973",children:"https://arxiv.org/pdf/2512.15973"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Dynamic Rank Reinforcement Learning (DR-RL), a framework that uses a reinforcement learning agent to dynamically select low-rank approximations for Multi-Head Self-Attention in LLMs during inference, balancing accuracy and computational cost. It employs online matrix perturbation theory for efficient updates. Experiments show the method maintains accuracy equivalent to full-rank attention while significantly reducing FLOPs, especially for long sequences."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Cross-Language Bias Examination in Large Language Models"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [fairness and bias evaluation], [multilingual bias evaluation, BBQ benchmark, prompt-based Implicit Association Test, explicit bias, implicit bias]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuxuan Liang, Marwa Mahmoud"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Georgia Institute of Technology, University of Glasgow"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16029",children:"https://arxiv.org/pdf/2512.16029"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a multilingual bias evaluation framework that combines explicit bias assessment using the BBQ benchmark with implicit bias measurement via a prompt-based Implicit Association Test, applied across five languages. The results show significant variation in bias across languages, with Arabic and Spanish exhibiting higher stereotype bias, and reveal contrasting patterns between explicit and implicit bias, such as age having low explicit but high implicit bias. The study highlights the importance of cross-lingual bias analysis for developing equitable multilingual LLMs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Are We on the Right Way to Assessing LLM-as-a-Judge?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [LLM-as-a-Judge, Sage evaluation suite, local self-consistency, global logical consistency, situational preference, panel-based judge, deep reasoning, finetuned LLM-as-a-Judge]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuanning Feng, Sinan Wang, Zhengxiang Cheng, Yao Wan, Dongping Chen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Huazhong University of Science and Technology, University of Maryland"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16041",children:"https://arxiv.org/pdf/2512.16041"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Sage, a novel evaluation suite that assesses LLM-as-a-Judge without human annotation by measuring local self-consistency and global logical consistency. It finds that even top LLMs exhibit significant reliability problems, attributing this to situational preference, and shows that finetuning, panel-based judging, and deep reasoning can improve consistency. The work also questions the reliability of human annotation as a gold standard."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [reinforcement learning, group relative policy optimization, margin-based cosine similarity, semantic-driven reinforcement learning, large vision-language model]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengyu Wang, Shuchang Ye, Usman Naseem, Jinman Kim"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Sydney, Macquarie University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16145",children:"https://arxiv.org/pdf/2512.16145"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a semantic-driven reinforcement learning method (MRG-R1) for medical report generation, which uses a report-level reward based on clinical findings similarity and Group Relative Policy Optimization to improve clinical correctness over token-level objectives. It achieves state-of-the-art performance on benchmark datasets, demonstrating that optimizing for semantic alignment meaningfully enhances clinical accuracy in generated reports."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [RoBERTa, multi-task learning, transformer models, code-mixed text, hate speech detection, fake narratives]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yash Bhaskar, Sankalp Bahad, Parameswari Krishnamurthy"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," IIIT Hyderabad"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16147",children:"https://arxiv.org/pdf/2512.16147"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a dual-head RoBERTa model with multi-task learning to detect hate speech driven by fake narratives (Faux-Hate) in code-mixed Hindi-English text. It addresses binary classification and target/severity prediction tasks. The system achieved competitive results, demonstrating the effectiveness of multi-task learning for this complex problem."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Science Consultant Agent"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Retrieval-Augmented Generation (RAG), fine-tuning, knowledge distillation, prompting, AutoML]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Duke University, Amazon"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16171",children:"https://arxiv.org/pdf/2512.16171"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the Science Consultant Agent, a web-based AI tool that uses structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting optimal AI modeling strategies. It aims to prevent resource misallocation by providing evidence-based guidance, moving beyond brute-force exploration or example-induced bias to accelerate development."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] An Information-Theoretic Framework for Robust Large Language Model Editing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [information bottleneck theory, gradient-based updates, knowledge editing, locate-then-edit, transformer patching]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," East China Normal University, Alibaba Group, Hefei University of Technology"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16227",children:"https://arxiv.org/pdf/2512.16227"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel model editing framework called the Information Bottleneck Knowledge Editor (IBKE), which uses information bottleneck theory to compress and isolate essential information for making precise, generalizable knowledge corrections in LLMs. The method leverages compact latent representations to guide gradient-based updates, minimizing disruption to unrelated model behaviors. The authors demonstrate that IBKE achieves state-of-the-art accuracy and improved generality and specificity of edits across multiple LLM architectures and benchmark tasks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Sigma-Moe-Tiny Technical Report"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [mixture-of-experts, load balancing, progressive sparsification, fine-grained expert segmentation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Qingguo Hu, Zhenghao Lin, Ziyue Yang, Yucheng Ding, Xiao Liu, Yuting Jiang, Ruizhe Wang, Tianyu Chen, Zhongxin Guo, Yifan Xiong, Rui Gao, Lei Qu, Jinsong Su, Peng Cheng, Yeyun Gong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Microsoft Research"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16248",children:"https://arxiv.org/pdf/2512.16248"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Sigma-MoE-Tiny, a highly sparse mixture-of-experts language model that uses fine-grained expert segmentation with up to 96 experts per layer and activates only one expert per token, achieving a 40:1 total-to-activated parameter ratio. To address load balancing challenges in such extreme sparsity, the authors propose a progressive sparsification schedule, which ensures stable training without irrecoverable loss spikes. Despite activating only 0.5B parameters, the model achieves top-tier performance compared to counterparts of similar or larger scale, setting a new benchmark for sparsity in open-source MoE models."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [sequent safety, multi-agent guard, state tracker, policy verifier, threat watcher, referee, machine-checkable rules, predicate updater]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yiliu Yang, Yilei Jiang, Qunzhong Wang, Yingshui Tan, Xiaoyong Zhu, Sherman S.M. Chow, Bo Zheng, Xiangyu Yue"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Alibaba Group"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16279",children:"https://arxiv.org/pdf/2512.16279"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes QuadSentinel, a four-agent supervisory framework that compiles natural language safety policies into machine-checkable rules using sequents and enforces them online in multi-agent systems. It demonstrates improved guardrail accuracy and rule recall while reducing false positives compared to single-agent baselines on standard benchmarks. The approach allows for safe deployment without modifying the core agents."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Adaptation of Agentic AI"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [agentic AI adaptation], [agent adaptation, tool adaptation, tool-execution-signaled, agent-output-signaled, agent-agnostic, agent-supervised, SFT, RL, VR methods, Toolformer, ToolLLM, DeepRetrieval, Prover-V2, DeepSeek-R1, Kimi-1.5, ReTool, Search-R1, HuggingGPT, ViperGPT, Subagent-as-Tool, Agentic Memory, Reflexion, Memento, SWE-Grep, Tab-RL]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," UIUC, Stanford, Princeton, Harvard, UW, Caltech, UC Berkeley, UCSD, Georgia Tech, Northwestern, TAMU, Unity"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16301",children:"https://arxiv.org/pdf/2512.16301"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a systematic framework for adapting agentic AI systems, categorizing adaptations into agent adaptation (tool-execution-signaled and agent-output-signaled) and tool adaptation (agent-agnostic and agent-supervised). It reviews representative methods in each category, analyzes their trade-offs, and provides practical guidance for selecting adaptation strategies. The work aims to offer a conceptual foundation and roadmap for building more capable, efficient, and reliable agentic AI systems."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Tools Orchestration Privacy Risk (TOP-R), TOP-Bench, Privacy Enhancement Principle (PEP), H-Score, Risk Leakage Rate (RLR)]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuxuan Qiao, Dongqin Liu, Hongchang Yang, Wei Zhou, Songlin Hu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16310",children:"https://arxiv.org/pdf/2512.16310"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper identifies and studies a new privacy risk in single-agent, multi-tool LLM architectures, termed Tools Orchestration Privacy Risk (TOP-R), where agents can synthesize sensitive information from aggregated tool outputs. It introduces a benchmark (TOP-Bench) and a mitigation method called the Privacy Enhancement Principle (PEP). The main conclusion is that TOP-R is a severe and prevalent risk in current models, but the proposed PEP method can effectively reduce leakage and improve the safety-robustness trade-off."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [SpeechLLMs, cascaded systems, speech foundation models, speech-to-text translation, benchmarking, Whisper, SeamlessM4T, Gemma, Tower+]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vil\xe9m Zouhar, Carlos Escolano, Gerard I. G\xe1llego, Jorge Iranzo-S\xe1nchez, Ahrii Kim, Dominik Mach\xe1\u010dek, Patricia Schmidtova, Maike Z\xfcfle"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fondazione Bruno Kessler, Barcelona Supercomputing Center, University of Zurich, ETH Zurich, Universitat Polit\xe8cnica de Catalunya, Universitat Polit\xe8cnica de Val\xe8ncia, AI-Bio Convergence Research Institute, Charles University, KIT"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16378",children:"https://arxiv.org/pdf/2512.16378"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper introduces the "Hearing to Translate" test suite to benchmark SpeechLLMs against cascaded and direct speech-to-text translation systems. It finds that cascaded systems, which combine speech recognition with LLM-based translation, remain the most reliable overall, while current SpeechLLMs only match them in specific scenarios. The study concludes that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Topic Modelling Black Box Optimization"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [hyperparameter optimization], [latent dirichlet allocation, black-box optimization, genetic algorithm, evolution strategy, preferential amortized black-box optimization, sharpness-aware black-box optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Roman Akramov, Artem Khamatullin, Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16445",children:"https://arxiv.org/pdf/2512.16445"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formulates selecting the number of topics in Latent Dirichlet Allocation (LDA) as a discrete black-box optimization problem. It compares evolutionary methods (GA, ES) against learned amortized optimizers (PABBO, SABBO), finding that the amortized approaches are substantially more sample- and time-efficient, with SABBO often finding a near-optimal topic number after essentially a single evaluation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [text simplification, prompt engineering, fine-tuning, Flesch-Kincaid, SMOG, SARI, BERTScore, G-Eval, Likert scale]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Primoz Kocbek, Leon Kopitar, Gregor Stiglic"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Maribor, University of Ljubljana, University of Edinburgh"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16530",children:"https://arxiv.org/pdf/2512.16530"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates using Large Language Models (LLMs) like GPT-4o to simplify biomedical texts for better health literacy, comparing methods such as prompt templates, a two-agent approach, and fine-tuning. It concludes that the GPT-4o-mini model performed best, while fine-tuning underperformed, and that the LLM-based G-Eval metric aligned well with human qualitative assessments."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [fuzzy exploratory search, benchmark, web retrieval, semantic ambiguity, agent evaluation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yumeng Wang, Tianyu Fan, Lingrui Xu, Chao Huang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tsinghua University, The University of Hong Kong"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16553",children:"https://arxiv.org/pdf/2512.16553"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper introduces the "Needle in the Web" benchmark, designed to evaluate search agents and LLMs on retrieving web pages for ambiguous, exploratory queries. The method involves generating queries of controllable difficulty based on factual claims from web content. The main conclusion is that current LLMs and search agents struggle significantly with this fuzzy retrieval task, achieving low accuracy and highlighting it as an open challenge.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Iker Garc\xeda-Ferrero, David Montero, Roman Orus"]}),"\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.strong,{children:"institution:"})}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16602",children:"https://arxiv.org/pdf/2512.16602"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [data preparation, workflow automation, pipeline construction, operator synthesis, natural-language specification, model-in-the-loop data generation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Hao Liang, Xiaochen Ma, Zhou Liu, Zhen Hao Wong, Zhengyang Zhao, Zimo Meng, Runming He, Chengyu Shen, Qifeng Cai, Zhaoyang Han, Meiyi Qiang, Yalin Feng, Tianyi Bai, Zewei Pan, Ziyi Guo, Yizhen Jiang, Jingwen Deng, Qijie You, Peichao Lai, Tianyu Guo, Chi Hsu Tsai, Hengyi Feng, Rui Hu, Wenkai Yu, Junbo Niu, Bohan Zeng, Ruichuan An, Lu Ma, Jihao Huang, Yaowei Zheng, Conghui He, Linpeng Tang, Bin Cui, Weinan E, Wentao Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Peking University, Institute for Advanced Algorithms Research, OriginHub Technology, OpenDataLab, Shanghai Artificial Intelligence Laboratory"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16676",children:"https://arxiv.org/pdf/2512.16676"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces DataFlow, an LLM-driven framework that provides system-level abstractions and a PyTorch-style API for building modular, reusable, and automated data preparation pipelines. It includes a DataFlow-Agent to translate natural language into executable workflows and demonstrates that its generated data consistently improves downstream LLM performance across multiple domains, establishing a foundation for scalable, data-centric AI development."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [natural language to temporal logic translation, grounding model, system signature, hierarchical classification, masked language models, logical equivalence]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," William English, Chase Walker, Dominic Simon, Rickard Ewetz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Florida"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16770",children:"https://arxiv.org/pdf/2512.16770"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes GinSign, a framework that grounds natural language into system signatures for temporal logic translation by using a hierarchical grounding model. This model decomposes the task into structured classification, allowing the use of smaller masked language models instead of large LLMs. Experiments show the framework achieves 95.5% grounded logical-equivalence, a 1.4x improvement over prior methods, enabling reliable downstream model checking."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [retrieval-augmented generation, deductive reasoning, conflict-aware trust-score, reasoning-trace-augmented framework, supervised fine-tuning]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani, Carnegie Mellon University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16795",children:"https://arxiv.org/pdf/2512.16795"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [Grammar Forced Translation (GraFT), temporal logic translation, solution space reduction, atomic propositions lifting, large language models]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," William English, Dominic Simon, Sumit Kumar Jha, Rickard Ewetz"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Florida, Florida International University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16814",children:"https://arxiv.org/pdf/2512.16814"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Grammar Forced Translation (GraFT), a framework that improves the translation of natural language to temporal logic by restricting the language model's valid output tokens at each step, thereby reducing the solution space. This method enhances both lifting of atomic propositions and the final translation phase. The results show that GraFT improves end-to-end and out-of-domain translation accuracy compared to state-of-the-art approaches."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [layer-wise caching, semantic similarity, fingerprinting, adaptive eviction, key-value cache, transformer acceleration]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Harsh Vardhan Bansal"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Amazon Web Services"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16843",children:"https://arxiv.org/pdf/2512.16843"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LLMCache, a model-agnostic layer-wise caching framework that accelerates transformer inference by reusing intermediate activations for semantically similar inputs. It uses a lightweight fingerprinting mechanism for matching and adaptive strategies for cache management. Experiments show up to 3.1x inference speedup with minimal accuracy degradation, demonstrating its practicality for real-world deployment."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal training], [reward models, multimodal benchmark, preference pairs, LLM-as-a-judge, Best-of-N sampling, ensemble filtering]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Meta (FAIR at Meta Superintelligence Labs)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16899",children:"https://arxiv.org/pdf/2512.16899"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Multimodal RewardBench 2 (MMRB2), a comprehensive benchmark for evaluating reward models on multimodal tasks involving interleaved text and image sequences. It finds that current models like Gemini 3 Pro achieve 75-80% accuracy, which is still below human performance (>90%), and that benchmark performance strongly correlates with downstream task success."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] In-Context Algebra"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [transformer interpretability], [transformers, in-context learning, algebraic groups, symbolic reasoning, causal tests, attention mechanisms]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Eric Todd, Jannik Brinkmann, Rohit Gandikota, David Bau"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Northeastern University, TU Clausthal"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16902",children:"https://arxiv.org/pdf/2512.16902"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper trains small transformers on a novel arithmetic task where token meanings are variable and defined only within each sequence. It finds that, instead of learning geometric embeddings, the models develop symbolic reasoning mechanisms like commutative copying and closure-based cancellation. This demonstrates that transformer reasoning strategies shift from geometric to symbolic when deprived of fixed token embeddings."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Impacts of Racial Bias in Historical Training Data for News AI"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [algorithmic auditing], [multi-label classifier, explainable AI, word2vec, New York Times Annotated Corpus]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16901",children:"https://arxiv.org/pdf/2512.16901"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic "blacks" label acts as a general "racism detector" but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [RLVR, GRPO, policy entropy, spurious rewards, clipping bias, exploration-exploitation trade-off]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Columbia University, The Chinese University of Hong Kong, Shenzhen, Alibaba DAMO Academy, New York University Stern School of Business"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16912",children:"https://arxiv.org/pdf/2512.16912"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates the exploration-exploitation trade-off in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning. It finds that spurious rewards, combined with clipping bias, reduce policy entropy to produce more confident outputs, which enhances performance, while entropy minimization alone is insufficient. The authors propose a reward-misalignment model to explain why spurious rewards can be beneficial beyond simple data contamination."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv251219] Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [adversarial reinforcement learning, process reward models, step-level rewards, reasoning chain partitioning, joint training, discriminator]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Johns Hopkins University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16917",children:"https://arxiv.org/pdf/2512.16917"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Generative Adversarial Reasoner, a framework that jointly trains an LLM reasoner and an LLM-based discriminator using adversarial reinforcement learning to provide dense, step-level rewards for improving reasoning. This method enhances sample efficiency and reasoning quality by co-evolving the models to detect and correct process errors. It demonstrates consistent performance gains on mathematical benchmarks over standard RL post-training baselines."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var s=i(6540);const r={},a=s.createContext(r);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);