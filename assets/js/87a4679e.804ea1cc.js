"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1372],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},80054:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/20260105-20260111","title":"20260105-20260111","description":"2026-01-05","source":"@site/docs/daily/20260105-20260111.md","sourceDirName":"daily","slug":"/daily/20260105-20260111","permalink":"/ai_toutiao/daily/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104","permalink":"/ai_toutiao/daily/20251229-20260104"},"next":{"title":"Paper","permalink":"/ai_toutiao/category/paper"}}');var r=i(74848),a=i(28453);const t={},o="20260105-20260111",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260105-20260111",children:"20260105-20260111"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"cs.DC total: 9"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Impact of Clustering on the Observability and Controllability of Complex Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [network controllability], [clustering, scale-free networks, observability, controllability, structured systems theory]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mohammadreza Doostmohammadian, Hamid R. Rabiee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Semnan University, Sharif University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00221",children:"https://arxiv.org/pdf/2601.00221"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Investigates and quantifies the relationship between network clustering and the requirements for observability and controllability in scale-free networks., 2. Demonstrates through simulations that densely clustered networks require fewer driver and observer nodes, offering a structural optimization principle., 3. Provides practical insights for reducing sensor/actuator placement in resource-constrained applications like social networks and intelligent transportation systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee38f16190dd3c9de93bdef0b47cb72f5c579d59a7937f107666cb7810142401_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee38f16190dd3c9de93bdef0b47cb72f5c579d59a7937f107666cb7810142401_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies how clustering affects the observability and controllability of complex scale-free networks. Using structured systems theory and Monte-Carlo simulations, it shows that higher clustering reduces the number of required driver and observer nodes. The findings suggest that network design can be optimized for control and monitoring by increasing clustering, especially in resource-limited scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root(Impact of Clustering on Observability and Controllability) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u7f51\u7edc\u53ef\u89c2\u6d4b\u6027\u4e0e\u53ef\u63a7\u6027\u9700\u6c42/Network Observability & Controllability Requirements)\n    Method --\x3e M1(\u7ed3\u6784\u5316\u7cfb\u7edf\u7406\u8bba/Structured Systems Theory)\n    Method --\x3e M2(\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e0e\u6848\u4f8b\u7814\u7a76/Monte-Carlo Simulations & Case Studies)\n    Results --\x3e R1(\u5bc6\u96c6\u805a\u7c7b\u51cf\u5c11\u9a71\u52a8\u4e0e\u89c2\u6d4b\u8282\u70b9/Dense Clustering Reduces Driver & Observer Nodes)\n    Results --\x3e R2(\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u7f51\u7edc\u8bbe\u8ba1/Optimizes Resource-Constrained Network Design)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [distributed consensus], [RAFT, replay attack, message forgery, authenticated verification, freshness check]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tamer Afifi, Abdelfatah Hegazy, Ehab Abousaif"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arab Academy for Science, Technology and Maritime Transport (AASTMT)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00273",children:"https://arxiv.org/pdf/2601.00273"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A systematic security analysis of the RAFT protocol, identifying its susceptibility to message replay and forgery attacks. 2. Examination of the practical feasibility of these attacks through simulated scenarios. 3. Proposal of a novel cryptographic approach for enhancing RAFT's security, incorporating authenticated message verification and freshness checks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f61353e1cee5179dd015df5e513ba3552de676525e8ac5e6c2fb0a48786e62f3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f61353e1cee5179dd015df5e513ba3552de676525e8ac5e6c2fb0a48786e62f3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies security vulnerabilities in the RAFT distributed consensus algorithm, specifically to replay and forgery attacks, which can disrupt consensus and cause data inconsistency. To address this, the authors propose a novel security framework using cryptography, authenticated message verification, and freshness checks. The proposed solution aims to enhance the security of RAFT implementations and guide the development of more resilient distributed systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: RAFT\u534f\u8bae\u7684\u5b89\u5168\u6f0f\u6d1e\u672a\u88ab\u5145\u5206\u8ba4\u77e5/Security vulnerabilities in RAFT are not fully recognized"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u7cfb\u7edf\u5b89\u5168\u5206\u6790\u4e0e\u57fa\u4e8e\u5bc6\u7801\u5b66\u7684\u89e3\u51b3\u65b9\u6848/Systematic security analysis & cryptographic solution"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u63d0\u51fa\u589e\u5f3a\u5b89\u5168\u6027\u7684\u6846\u67b6/Proposed a framework for enhancing security"]\n    Problem --\x3e P1["\u653b\u51fb\u7c7b\u578b/Attack Types: \u6d88\u606f\u91cd\u653e\u4e0e\u4f2a\u9020/Message replay & forgery"]\n    Problem --\x3e P2["\u540e\u679c/Consequence: \u5171\u8bc6\u7834\u574f\u4e0e\u6570\u636e\u4e0d\u4e00\u81f4/Consensus disruption & data inconsistency"]\n    Method --\x3e M1["\u5206\u6790/Analysis: \u6a21\u62df\u653b\u51fb\u573a\u666f/Simulated attack scenarios"]\n    Method --\x3e M2["\u65b9\u6848/Solution: \u8ba4\u8bc1\u3001\u9a8c\u8bc1\u4e0e\u65b0\u9c9c\u6027\u68c0\u67e5/Authentication, verification & freshness check"]\n    Results --\x3e R1["\u6210\u679c/Outcome: \u8bc6\u522b\u8bbe\u8ba1\u5f31\u70b9/Identified design weaknesses"]\n    Results --\x3e R2["\u6210\u679c/Outcome: \u63d0\u4f9b\u5b89\u5168\u589e\u5f3a\u6846\u67b6/Provided security enhancement framework"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lov\xe9n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Lule\xe5 University of Technology, Peking University, TU Wien"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00339",children:"https://arxiv.org/pdf/2601.00339"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Bio-inspired Agentic Self-healing Framework<br>\u751f\u7269\u542f\u53d1\u7684\u667a\u80fd\u4f53\u81ea\u6108\u6846\u67b6"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>DCCS\u4e2d\u7684\u590d\u6742\u6027\u4e0e\u6545\u969c\u9891\u53d1<br>Complexity & Frequent Faults in DCCS"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>ReCiSt\u6846\u67b6: \u4eff\u751f\u56db\u5c42\u4e0eLM\u667a\u80fd\u4f53<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u6570\u5341\u79d2\u5185\u81ea\u6108\uff0c\u4f4eCPU\u5f00\u9500<br>Self-healing in tens of seconds, low CPU overhead"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Word Frequency Counting Based on Serverless MapReduce"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [serverless computing], [Serverless Computing, MapReduce, Word Frequency Counting, Function as a Service (FaaS), Cloud Computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hanzhe Li, Bingchen Lin, Mengyuan Xu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi'an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00380",children:"https://arxiv.org/pdf/2601.00380"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel combination of the serverless computing paradigm (FaaS) with the MapReduce programming model for data processing tasks. 2. Investigates and determines the optimal number of Map and Reduce functions for a given workload within a serverless MapReduce framework. 3. Demonstrates through experiments that increasing the number of functions reduces execution time and improves overall efficiency for the word frequency counting task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88bebde834fdbf686ac0168c04a12e2eb20d8157d9be5e2222f9ff775a21b2ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of optimizing big data processing efficiency by integrating the serverless computing model (FaaS) with the MapReduce framework. It proposes a serverless MapReduce approach for word frequency counting and experimentally finds the optimal number of Map and Reduce functions to minimize execution time. The results show that this method improves processing efficiency, offering a cost-effective solution for cloud-based data analytics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Word Frequency Counting Based on Serverless MapReduce] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9700\u6c42: \u9ad8\u6027\u80fd\u4e0e\u9ad8\u6548\u7387\u8ba1\u7b97 / Demand: High-performance & High-efficiency Computing]\n    C --\x3e C1[\u7ed3\u5408: \u65e0\u670d\u52a1\u5668\u8ba1\u7b97(FaaS)\u4e0eMapReduce / Combine: Serverless Computing (FaaS) & MapReduce]\n    C --\x3e C2[\u4f18\u5316: Map\u4e0eReduce\u51fd\u6570\u6570\u91cf / Optimize: Number of Map & Reduce Functions]\n    D --\x3e D1[\u7ed3\u679c: \u6267\u884c\u65f6\u95f4\u51cf\u5c11 / Result: Execution Time Reduces]\n    D --\x3e D2[\u7ed3\u679c: \u7a0b\u5e8f\u6548\u7387\u63d0\u5347 / Result: Program Efficiency Improves]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [time-warp emulation, CUDA interception, virtual time coordination, performance modeling, discrete-event simulation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00397",children:"https://arxiv.org/pdf/2601.00397"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A time-warp emulator that enables performance modeling by directly executing real serving system code without physical GPUs, eliminating the need to re-implement complex control logic. 2. A system that intercepts CUDA API calls to virtualize device management and performs time jumps by fast-forwarding virtual time based on predicted kernel durations. 3. A coordination protocol that synchronizes time jumps across distributed processes while preserving causality, ensuring accurate emulation of parallel execution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87798fc4e00db06b5558e8552991b262a89ec7eea8a194407a93b93519f3357e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Revati, a time-warp emulator for efficient LLM serving configuration testing. It directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running GPU kernels, achieving less than 5% prediction error while running 5-17x faster than real GPU execution on frameworks like vLLM and SGLang."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GPU\u96c6\u7fa4\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u6162/Evaluating serving configs on GPU clusters is slow and expensive]\n    B --\x3e B2[\u6a21\u62df\u5668\u9700\u8981\u91cd\u5199\u63a7\u5236\u903b\u8f91/Simulators require re-implementing complex control logic]\n    C --\x3e C1[\u62e6\u622aCUDA API\u8c03\u7528/Intercept CUDA API calls]\n    C --\x3e C2[\u865a\u62df\u65f6\u95f4\u8df3\u8dc3/Virtual time jumps based on kernel predictions]\n    C --\x3e C3[\u5206\u5e03\u5f0f\u534f\u8c03\u534f\u8bae/Distributed coordination protocol]\n    D --\x3e D1[<5%\u9884\u6d4b\u8bef\u5dee/<5% prediction error]\n    D --\x3e D2[5-17\u500d\u52a0\u901f/5-17x faster than real execution]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [Privacy-preserving data aggregation], [unanimous-release confidentiality, consensus locking, malicious deviation detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Prajwal Panth, Sahaj Raj Malla"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KIIT University, Kathmandu University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00418",children:"https://arxiv.org/pdf/2601.00418"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the CPPDD framework, a lightweight protocol for secure multi-client data aggregation using per-client affine masking and priority-driven sequential consensus locking to enforce unanimous-release confidentiality. 2. Introduces decentralized integrity verification via step and data checksums (\u03c3_S, \u03c3_D) enabling autonomous malicious deviation detection and atomic abort without persistent coordination. 3. Formally proves the framework's properties (correctness, CDIF, IND-CPA security) and empirically demonstrates linear scalability up to 500 clients with significantly lower computational overhead compared to MPC and HE baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a94aa63b5803d44299e228782541c1d2830c11c784cb2a9d0a55df2b0c6765_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes the CPPDD framework to address the problem of secure and verifiable multi-client data sharing. The method combines affine masking and consensus locking for privacy, and uses checksums for integrity verification, enabling efficient, scalable aggregation with malicious security. The framework is proven secure and shown to be orders of magnitude more efficient than traditional cryptographic approaches like MPC and HE."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Secure, Verifiable, and Scalable Multi-Client Data Sharing<br>\u5b89\u5168\u3001\u53ef\u9a8c\u8bc1\u3001\u53ef\u6269\u5c55\u7684\u591a\u5ba2\u6237\u7aef\u6570\u636e\u5171\u4eab] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Secure multi-client data aggregation with privacy and verifiability<br>\u5b89\u5168\u3001\u53ef\u9a8c\u8bc1\u7684\u591a\u5ba2\u6237\u7aef\u9690\u79c1\u6570\u636e\u805a\u5408)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Consensus-Based Privacy-Preserving Data Distribution (CPPDD)<br>\u57fa\u4e8e\u5171\u8bc6\u7684\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5206\u53d1)\n    C --\x3e C1(Affine Masking & Consensus Locking<br>\u4eff\u5c04\u63a9\u7801\u4e0e\u5171\u8bc6\u9501\u5b9a)\n    C --\x3e C2(Step/Data Checksums (\u03c3_S, \u03c3_D)<br>\u6b65\u9aa4/\u6570\u636e\u6821\u9a8c\u548c)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Linear scalability, 100% deviation detection, lower FLOPs vs MPC/HE<br>\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff0c100%\u5f02\u5e38\u68c0\u6d4b\uff0c\u76f8\u6bd4MPC/HE\u66f4\u4f4e\u7684\u8ba1\u7b97\u91cf)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [federated learning, prefix-tuning, large model customization, efficient fine-tuning, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fuzhou University, Data61 CSIRO, La Trobe University, Nanyang Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00526",children:"https://arxiv.org/pdf/2601.00526"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a comprehensive review of large model customization techniques and discusses their implementation within a federated learning framework. 2. Proposes and experimentally validates federated prefix-tuning, which is the first application of prefix-tuning in a federated learning setting. 3. Demonstrates through comparative experiments that federated prefix-tuning achieves competitive performance, satisfactory efficiency, and consistent robustness compared to other federated customization methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fea5cfedb112a13fd696a58bea7fb932671198f51d78a63540d7922a373af9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper explores the federated customization of large models, which aims to adapt pre-trained models for specialized tasks using decentralized, private data. It proposes and validates federated prefix-tuning as a novel method, showing its performance is close to centralized approaches and competitive with other federated techniques. The work provides insights into implementing various customization methods within a federated learning framework to address privacy and data decentralization challenges."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Federated Customization of Large Models: Approaches, Experiments, and Insights] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0b\u5b9a\u5236\u5927\u6a21\u578b\u7684\u6311\u6218/Challenges of customizing large models within FL]\n    C --\x3e C1[\u56de\u987e\u5927\u6a21\u578b\u5b9a\u5236\u6280\u672f/Review LM customization techniques]\n    C --\x3e C2[\u8ba8\u8bba\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0/Discuss FL implementations]\n    C --\x3e C3[\u5b9e\u9a8c\u8054\u90a6\u524d\u7f00\u8c03\u4f18/Experiment with federated prefix-tuning]\n    D --\x3e D1[\u9a8c\u8bc1\u8054\u90a6\u524d\u7f00\u8c03\u4f18\u53ef\u884c\u6027/Validate feasibility of federated prefix-tuning]\n    D --\x3e D2[\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u65b9\u6cd5/Performance close to centralized]\n    D --\x3e D3[\u5c55\u793a\u7ade\u4e89\u529b\u4e0e\u9c81\u68d2\u6027/Show competitive performance & robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [cloud computing], [cloud benchmarking, point-of-sale systems, performance analysis, cost optimization, retail technology]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ravi Teja Pagidoju"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Campbellsville University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00530",children:"https://arxiv.org/pdf/2601.00530"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Presents a systematic, repeatable, and transparent methodology for evaluating POS workloads on cloud platforms using free-tier resources and open-source benchmarking code. 2. Provides the first comprehensive, code-driven comparison of POS-specific workloads across Google Cloud Platform and Microsoft Azure, analyzing performance metrics and cost efficiency. 3. Establishes an open benchmarking framework and offers practical insights for merchants considering cloud POS implementation, linking architectural components to observed performance-cost trade-offs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/084205dec68906e341b24595e452d764be0491b5d3d16bfccff9cf4f8d4f1eca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/084205dec68906e341b24595e452d764be0491b5d3d16bfccff9cf4f8d4f1eca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a systematic methodology to compare the performance and cost of cloud-based Point-of-Sale systems on Google Cloud Platform and Microsoft Azure using free-tier resources and open-source code. The analysis finds that GCP offers faster response times, while Azure demonstrates higher cost efficiency for steady-state operations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u96f6\u552e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5b9e\u8bc1\u7814\u7a76 / Lack of empirical research on retail workloads]\n    C --\x3e C1[\u4f7f\u7528\u514d\u8d39\u8d44\u6e90\u548c\u5f00\u6e90\u4ee3\u7801\u8fdb\u884c\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5 / Systematic benchmarking using free-tier resources and open-source code]\n    D --\x3e D1[GCP\u54cd\u5e94\u65f6\u95f4\u5feb23.0% / GCP 23.0% faster response]\n    D --\x3e D2[Azure\u6210\u672c\u6548\u7387\u9ad871.9% / Azure 71.9% higher cost efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, edge-cloud collaboration, communication-efficient inference, adaptive speculation, shared-backbone architecture]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuchen Li, Rui Kong, Zhonghao Lyu, Qiyang Li, Xinran Chen, Hengyi Cai, Lingyong Yan, Shuaiqiang Wang, Jiashu Zhao, Guangxu Zhu, Linghe Kong, Guihai Chen, Haoyi Xiong, Dawei Yin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Baidu Inc., Shanghai Jiao Tong University, KTH Royal Institute of Technology, Wilfrid Laurier University, Shenzhen Research Institute of Big Data"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00644",children:"https://arxiv.org/pdf/2601.00644"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a shared-backbone architecture enabling a single static edge-side draft model to be compatible with a family of evolving cloud-side target models, decoupling edge deployment from cloud updates. 2. Developed a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel conditions and device energy budgets. 3. Designed the FlexSpec framework, which reduces communication and maintenance costs for edge-cloud collaborative LLM inference, improving scalability and efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7429a9e45e5a64278f26ba8b33031aac6b2f433fdae9180afd7344ec56b1fffd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7429a9e45e5a64278f26ba8b33031aac6b2f433fdae9180afd7344ec56b1fffd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FlexSpec, a communication-efficient framework for edge-cloud collaborative LLM inference using speculative decoding. Its core innovation is a shared-backbone architecture that allows a frozen edge-side draft model to work with evolving cloud target models, paired with an adaptive speculation mechanism. Experiments show it achieves superior inference efficiency compared to conventional speculative decoding approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[FlexSpec: Frozen Drafts Meet Evolving Targets] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[LLM\u90e8\u7f72\u5728\u8fb9\u7f18\u53d7\u9650/LLM Deployment on Edge is Constrained]\n    Problem --\x3e P2[\u73b0\u6709\u534f\u540c\u63a8\u7406\u901a\u4fe1\u5f00\u9500\u5927/Existing Collaborative Inference has High Comm. Overhead]\n    Problem --\x3e P3[\u6a21\u578b\u9891\u7e41\u66f4\u65b0\u9650\u5236\u53ef\u6269\u5c55\u6027/Frequent Model Updates Limit Scalability]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5171\u4eab\u4e3b\u5e72\u67b6\u6784/Shared-Backbone Architecture]\n    Method --\x3e M2[\u89e3\u8026\u8fb9\u7f18\u4e0e\u4e91\u7aef\u90e8\u7f72/Decouple Edge and Cloud Deployment]\n    Method --\x3e M3[\u4fe1\u9053\u611f\u77e5\u81ea\u9002\u5e94\u63a8\u6d4b/Channel-Aware Adaptive Speculation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u51cf\u5c11\u901a\u4fe1\u4e0e\u7ef4\u62a4\u6210\u672c/Reduced Communication & Maintenance Cost]\n    Results --\x3e R2[\u63d0\u5347\u63a8\u7406\u6548\u7387/Improved Inference Efficiency]\n    Results --\x3e R3[\u4f18\u4e8e\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5/Superior to Conventional SD Approaches]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 22'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Metric Interval Temporal Logic (MITL), Timed Limit-Deterministic Generalized B\xfcchi Automata (Timed-LDGBA), Q-learning, POMDP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Iowa, Talus Renewables, Inc., Roma Tre University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00087",children:"https://arxiv.org/pdf/2601.00087"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement learning with timed constraints for robotics motion planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u590d\u6742\u4efb\u52a1\u5e8f\u5217\u4e0e\u4e25\u683c\u65f6\u95f4\u7ea6\u675f/Complex task sequences & strict temporal constraints]\n    B --\x3e B2[\u968f\u673a\u52a8\u6001\u4e0e\u90e8\u5206\u53ef\u89c2\u6d4b\u6027/Stochastic dynamics & partial observability]\n    C --\x3e C1[MITL\u516c\u5f0f\u8f6c\u6362\u4e3aTimed-LDGBA/MITL to Timed-LDGBA translation]\n    C --\x3e C2[\u6784\u5efa\u4ea7\u54c1\u5b9a\u65f6\u6a21\u578b\u4e0eQ\u5b66\u4e60/Construct product timed models for Q-learning]\n    C --\x3e C3[\u7b80\u5355\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5956\u52b1\u7ed3\u6784/Simple yet expressive reward structure]\n    D --\x3e D1[\u6ee1\u8db3\u65f6\u95f4\u7ea6\u675f\u7684\u7b56\u7565/Policies satisfy time-bounded requirements]\n    D --\x3e D2[\u6269\u5c55\u5230\u66f4\u5927\u72b6\u6001\u7a7a\u95f4/Scales to larger state spaces]\n    D --\x3e D3[\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u6709\u6548/Effective in partially observable environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [meta-reinforcement learning, constraint propagation, graph attention network, structured inference, green ai]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Iowa State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00095",children:"https://arxiv.org/pdf/2601.00095"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MetaJuLS, a meta-reinforcement learning framework for learning universal constraint propagation policies applicable across languages and tasks without task-specific retraining. 2. Formulates structured inference as adaptive constraint propagation and trains a Graph Attention Network policy via meta-learning, achieving significant speedups (1.5-2.0x) over GPU-optimized baselines with minimal accuracy loss. 3. Demonstrates rapid cross-domain adaptation (5-15 seconds) and contributes to Green AI by reducing inference carbon footprint through fewer propagation steps."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5284c89e9a9eec9c1882da4c36e7d1e9bbce97ea559fe29f19c435e5f8b8854_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5284c89e9a9eec9c1882da4c36e7d1e9bbce97ea559fe29f19c435e5f8b8854_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the inefficiency of structured inference (e.g., JSON parsing) in large language models by proposing MetaJuLS, a meta-reinforcement learning method that learns adaptive constraint propagation policies. This approach achieves up to 2x speedup over baselines while maintaining high accuracy and enables fast adaptation to new languages and tasks. The work contributes to more efficient and environmentally friendly LLM inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Universal Adaptive Constraint Propagation<br>\u901a\u7528\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["LLMs need efficient structured inference<br>LLM\u9700\u8981\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u63a8\u7406"] --\x3e P1["Wasted computation from static checking<br>\u9759\u6001\u68c0\u67e5\u5bfc\u81f4\u8ba1\u7b97\u6d6a\u8d39"]\n    Problem --\x3e P2["Need for cross-domain generalization<br>\u9700\u8981\u8de8\u9886\u57df\u6cdb\u5316"]\n    Method["MetaJuLS: Meta-RL for constraint propagation<br>MetaJuLS: \u7528\u4e8e\u7ea6\u675f\u4f20\u64ad\u7684\u5143\u5f3a\u5316\u5b66\u4e60"] --\x3e M1["Learns universal policies via meta-learning<br>\u901a\u8fc7\u5143\u5b66\u4e60\u5b66\u4e60\u901a\u7528\u7b56\u7565"]\n    Method --\x3e M2["Uses Graph Attention Network<br>\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc"]\n    Results["Key Results<br>\u5173\u952e\u7ed3\u679c"] --\x3e R1["1.5-2.0x speedup<br>1.5-2.0\u500d\u52a0\u901f"]\n    Results --\x3e R2["Fast adaptation (5-15s)<br>\u5feb\u901f\u9002\u5e94(5-15\u79d2)"]\n    Results --\x3e R3["Contributes to Green AI<br>\u52a9\u529b\u7eff\u8272AI"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [geometric reinforcement learning, Hamiltonian optimization, simultaneous navigation and mapping, local sensory observations, path differential]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00116",children:"https://arxiv.org/pdf/2601.00116"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/",children:"https://github.com/"}),' (as per the abstract "The code is publicly available on Github." The specific URL is not provided in the given text, only a placeholder link.)']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRL-SNAM: Geometric RL for SNAM] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Simultaneous Navigation and Mapping in mapless environments]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Geometric RL with Path Differential Hamiltonians, local energy landscapes]\n    D[\u5173\u952e\u7ed3\u679c/Results: High-quality navigation with minimal exploration, generalizes to unseen layouts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement Learning with Function Approximation for Non-Markov Processes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [non-Markov processes, linear function approximation, policy evaluation, Q-learning, partially observed MDPs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ali Devran Kara"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00151",children:"https://arxiv.org/pdf/2601.00151"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proved convergence of policy evaluation with linear function approximation under ergodic non-Markov processes, linking the limit to a fixed point of a joint projection-Bellman operator. 2. Established convergence for a special case of Q-learning with linear approximation where basis functions are based on quantization maps under similar ergodicity conditions. 3. Applied the theoretical results to Partially Observed MDPs (POMDPs) using finite-memory state representations and derived explicit error bounds for the learning algorithm limits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e117bcd100ad5f0daa634538585e364383717d05bbbd527d7dcc2b26e2b92b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies reinforcement learning with linear function approximation for non-Markov processes. It proves convergence for policy evaluation and a special case of Q-learning under ergodicity conditions, and applies the theory to POMDPs to derive error bounds."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Reinforcement Learning with Function Approximation for Non-Markov Processes] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>RL with linear function approximation for non-Markov processes] --\x3e P1[\u975e\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b/Non-Markov Processes]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Theoretical analysis under ergodicity conditions] --\x3e M1[\u7b56\u7565\u8bc4\u4f30/Policy Evaluation]\n    Method --\x3e M2[Q\u5b66\u4e60/Q-learning]\n    M2 --\x3e M2_1[\u7279\u6b8a\u60c5\u51b5:\u57fa\u4e8e\u91cf\u5316\u7684\u57fa\u51fd\u6570/Special Case: Quantization-based basis]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6536\u655b\u6027\u8bc1\u660e/Convergence Proofs]\n    Results --\x3e R2[\u5e94\u7528\u4e8ePOMDPs/Application to POMDPs]\n    R2 --\x3e R2_1[\u663e\u5f0f\u8bef\u5dee\u754c/Explicit Error Bounds]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Decision Transformer, online finetuning, GRPO, hindsight return relabeling, sub-trajectory optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junkai Luo, Yinglun Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Riverside"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00167",children:"https://arxiv.org/pdf/2601.00167"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies hindsight return relabeling as a fundamental obstacle to using pure RL gradients for online finetuning of Decision Transformers. 2. Proposes new algorithms that adapt GRPO to DTs with key modifications like sub-trajectory optimization, sequence-level likelihood objectives, and active sampling. 3. Demonstrates state-of-the-art performance across multiple benchmarks, showing the effectiveness of pure-RL-based online finetuning for DTs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7142aabdfb71311da4fa293bcc82f4d6d24b9dddc11ccaf74c0aaea92b54d5e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of online finetuning for Decision Transformers using pure reinforcement learning gradients, which has been largely unexplored. The authors identify and overcome the incompatibility of standard hindsight return relabeling with RL algorithms, proposing modified methods based on GRPO. Their approach outperforms existing online DT baselines, achieving new state-of-the-art results on several benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Online Finetuning Decision Transformers with Pure RL Gradients] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u5728\u7ebf\u5fae\u8c03DT\u65f6\uff0c\u7eafRL\u68af\u5ea6\u65b9\u6cd5\u672a\u88ab\u63a2\u7d22/Pure RL gradients for online DT finetuning unexplored]\nB --\x3e B2[\u540e\u89c1\u4e4b\u76ca\u56de\u62a5\u91cd\u6807\u6ce8\u4e0eRL\u7b97\u6cd5\u4e0d\u517c\u5bb9/Hindsight return relabeling incompatible with RL]\nC --\x3e C1[\u9002\u914dGRPO\u81f3DT/Adapt GRPO to DTs]\nC --\x3e C2[\u5f15\u5165\u5173\u952e\u4fee\u6539: \u5b50\u8f68\u8ff9\u4f18\u5316\u7b49/Introduce key modifications]\nD --\x3e D1[\u8d85\u8d8a\u73b0\u6709\u5728\u7ebfDT\u57fa\u7ebf/Outperform online DT baselines]\nD --\x3e D2[\u5b9e\u73b0SOTA\u6027\u80fd/Achieve SOTA performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [semantic communication], [reinforcement learning, unequal error protection, adaptive repetition coding, semantic distortion metric, per-dimension protection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Moirangthem Tiken Singh, Adnan Arif"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00186",children:"https://arxiv.org/pdf/2601.00186"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel reinforcement learning framework for per-dimension unequal error protection of quantized semantic embeddings, 2. A composite semantic distortion metric that balances global embedding similarity with entity-level preservation to guide the RL agent, 3. The demonstration that simple, intelligently allocated repetition coding can outperform conventional codes like LDPC for fine-grained semantic protection"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb4eade98aeb895832aed0b8cd026ed4c334838f42e2fd62ce3cdef3c7aea4d0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning framework to protect quantized semantic embeddings transmitted over noisy channels. The method uses adaptive repetition coding to provide unequal error protection per embedding dimension, guided by a novel semantic distortion metric. The results show that this approach significantly outperforms uniform protection, challenging traditional channel coding paradigms by aligning code structure with semantic granularity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5e26\u5bbd\u53d7\u9650\u4e0b\u4fdd\u6301\u8bed\u4e49/Bandwidth-constrained Semantic Preservation]\n    C --\x3e C1[\u57fa\u4e8eRL\u7684\u81ea\u9002\u5e94\u91cd\u590d\u7f16\u7801/RL-based Adaptive Repetition Coding]\n    C --\x3e C2[\u590d\u5408\u8bed\u4e49\u5931\u771f\u5ea6\u91cf/Composite Semantic Distortion Metric]\n    D --\x3e D1[\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u5747\u5300\u4fdd\u62a4/Significant Gains Over Uniform Protection]\n    D --\x3e D2[\u6311\u6218\u4f20\u7edf\u4fe1\u9053\u7f16\u7801\u8303\u5f0f/Challenges Traditional Channel Coding]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Omar Sharif, Eftekhar Hossain, Patrick Ng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dartmouth College, University of Central Florida"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00215",children:"https://arxiv.org/pdf/2601.00215"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: MLLMs lack visual grounding in reasoning"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: RL with reward functions & GRPO"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: 5.56% improvement on Qwen-2.5-VL-7B"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [neuromorphic computing, state-space models, sparse attention, surrogate gradients, local learning rules]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Osvaldo Simeone"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University London (Intelligent Networked Systems Institute - INSI)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00245",children:"https://arxiv.org/pdf/2601.00245"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u80fd\u8017\u589e\u957f / Escalating AI Energy Requirements]\n    C --\x3e C1[\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u539f\u5219 / Neuromorphic Computing Principles]\n    C1 --\x3e C2[\u79bb\u6563\u7a00\u758f\u6fc0\u6d3b / Discrete & Sparse Activations]\n    C1 --\x3e C3[\u5faa\u73af\u52a8\u6001 / Recurrent Dynamics]\n    C --\x3e C4[\u5904\u7406\u6846\u67b6: \u4ee4\u724c\u5185\u4e0e\u4ee4\u724c\u95f4 / Processing Framework: Intra-Token vs. Inter-Token]\n    D --\x3e D1[\u73b0\u4ee3AI\u4f53\u73b0\u795e\u7ecf\u5f62\u6001\u539f\u5219 / Modern AI Embodies Neuromorphic Principles]\n    D --\x3e D2[\u8fde\u63a5SNN\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001Transformer / Connects SNNs, State-Space Models, Transformers]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aly Sabri Abdalla, Vuk Marojevic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mississippi State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00257",children:"https://arxiv.org/pdf/2601.00257"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --\x3e P1["\u5b50\u95ee\u9898/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --\x3e M1["\u7ec4\u4ef6/Component: Semantic-aware rApp (terrain interpreter)"]\n    Method --\x3e M2["\u7ec4\u4ef6/Component: RL-enabled xApp (trajectory planner)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results: Framework enables closed-loop, AI-optimized LAE operations"] --\x3e R1["\u8bc4\u4f30/Evaluation: Feasibility and performance analysis presented"]\n    Results --\x3e R2["\u5c55\u671b/Outlook: Research challenges and standardization needs surveyed"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Optimal Transport Improve Federated Inverse Reinforcement Learning?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Inverse Reinforcement Learning, Federated Learning, Optimal Transport, Wasserstein Barycenter, Maximum Entropy IRL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," David Millard, Ali Baheri"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rochester Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00309",children:"https://arxiv.org/pdf/2601.00309"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an optimal transport-based approach for federating learned reward functions in Inverse Reinforcement Learning (IRL). 2. Proposes using a Wasserstein barycenter for reward fusion, which accounts for the geometric structure of the reward landscape, as opposed to simple parameter averaging. 3. Provides a theoretical proof that the barycentric fusion yields a more faithful global reward estimate than conventional federated averaging methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18b7dd99db471c1c90bc7b908611843e09230b1dcad68713f2c1d393a1fa86bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of learning a shared reward function across heterogeneous agents in privacy-sensitive, communication-limited settings. It proposes a federated IRL framework where agents perform local Maximum Entropy IRL and then fuse their reward functions via a Wasserstein barycenter. The authors prove this method provides a more accurate global reward estimate than standard parameter averaging, offering a principled and efficient solution for multi-agent systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Optimal Transport Improve Federated Inverse Reinforcement Learning?"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Heterogeneous agents need a shared reward, but data pooling is impractical due to privacy, dynamics differences, and limited bandwidth."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Local lightweight MaxEnt IRL followed by reward fusion via Wasserstein barycenter."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Barycentric fusion yields a more faithful global reward estimate than parameter averaging."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Offline Multi-Agent Reinforcement Learning for 6G Communications: Fundamentals, Applications and Future Directions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent reinforcement learning], [offline reinforcement learning, conservative Q-learning, meta-learning, radio resource management, UAV networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Eslam Eldeeb, Hirley Alves"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Oulu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00321",children:"https://arxiv.org/pdf/2601.00321"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel offline multi-agent reinforcement learning algorithm based on conservative Q-learning (CQL) for safe and efficient training in wireless networks., 2. Extends the offline MARL approach with meta-learning to enhance adaptability in dynamic environments., 3. Validates the proposed framework through practical use cases in radio resource management and UAV network applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cc964c00876ee81486a12f18505ed613255db0f942d692eb3a3d428f15d72c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cc964c00876ee81486a12f18505ed613255db0f942d692eb3a3d428f15d72c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the cost, safety, and scalability limitations of online multi-agent reinforcement learning (MARL) in complex 6G networks by proposing an offline MARL algorithm based on conservative Q-learning (CQL), enhanced with meta-learning for dynamic environments. The method is validated in wireless use cases like radio resource management and UAV networks. The work concludes that offline MARL is a promising direction for future wireless applications, highlighting its advantages and limitations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Offline Multi-Agent RL for 6G] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Online MARL faces cost, safety, scalability limits in 6G networks]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Novel offline MARL algorithm based on CQL + meta-learning]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Validated in RRM & UAV use cases; highlights advantages & future directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [geolocalization, vision-language models, chain of region, haversine distance, retrieval-free]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Biao Wu, Meng Fang, Ling Chen, Ke Xu, Tao Cheng, Jun Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, University of Liverpool, University College London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00388",children:"https://arxiv.org/pdf/2601.00388"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Geo-R, a retrieval-free framework for image geolocalization that uses reinforcement learning. 2. Introduces Chain of Region, a rule-based hierarchical reasoning paradigm to generate interpretable supervision from GPS coordinates. 3. Develops a lightweight RL strategy with coordinate-aligned rewards based on Haversine distance for spatially meaningful feedback."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/986e7d2e1e80caef1d7794a999a5e00e8f2a503a4cae673b68dc3cfafa02122c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/986e7d2e1e80caef1d7794a999a5e00e8f2a503a4cae673b68dc3cfafa02122c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing image geolocalization methods by proposing Geo-R, a retrieval-free framework that uses a rule-based Chain of Region for hierarchical reasoning and a reinforcement learning strategy with Haversine distance rewards. The approach improves localization accuracy, generalization, and interpretability without relying on synthetic labels or external retrieval, as validated across multiple benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Vision-Language Reasoning for Geolocalization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing methods rely on synthetic annotations or retrieval, limiting interpretability and generalization.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes Geo-R, a retrieval-free framework using Chain of Region for hierarchical reasoning and RL with Haversine distance rewards.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved accuracy, stronger generalization, and more transparent inference, establishing a new retrieval-free paradigm.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00423",children:"https://arxiv.org/pdf/2601.00423"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/shengjun-zhang/VisualGRPO",children:"https://github.com/shengjun-zhang/VisualGRPO"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u53bb\u566a\u6b65\u4e0a\u4f18\u5316\uff0c\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u6a21\u7cca/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n    C --\x3e C1[\u63d0\u51faE-GRPO: \u71b5\u611f\u77e5\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n    C1 --\x3e C2[\u5408\u5e76\u4f4e\u71b5\u6b65\u4e3a\u9ad8\u71b5SDE\u91c7\u6837\u6b65\uff0c\u5176\u4ed6\u6b65\u7528ODE\u91c7\u6837/Merge low-entropy steps for SDE, use ODE elsewhere]\n    C1 --\x3e C3[\u5f15\u5165\u591a\u6b65\u5206\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u8ba1\u7b97/Introduce multi-step group normalized advantage]\n    D --\x3e D1[\u5728\u4e0d\u540c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027/Method effectiveness demonstrated across different reward settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CPPO: Contrastive Perception for Vision Language Policy Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [contrastive perception loss, entropy shift, vision-language models, policy optimization, multimodal reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huawei Technologies Canada Co. Ltd., Huawei Cloud"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00501",children:"https://arxiv.org/pdf/2601.00501"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CPPO: Contrastive Perception for Vision Language Policy Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Disentangling perception and reasoning tokens in VLMs is difficult with prior methods requiring extra LLMs or indiscriminate rewards]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Detect perception tokens via entropy shifts under perturbed images; apply Contrastive Perception Loss (CPL) for consistency/sensitivity]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: CPPO surpasses previous perception-rewarding methods, avoids extra models, and improves training efficiency/scalability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Graph Neural Network, Q-learning, traffic-aware optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sonia Khetarpaul, P Y Sharan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shiv Nadar Institution of Eminence"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00607",children:"https://arxiv.org/pdf/2601.00607"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel traffic-aware, graph-based reinforcement learning framework for optimal taxi placement that integrates real-time traffic data (e.g., congestion scores) with historical demand patterns. 2. Employs Graph Neural Network (GNN) embeddings to encode spatial-temporal dependencies within the urban road network, enhancing the agent's understanding of network topology and dynamics. 3. Designs a multi-objective reward mechanism that jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance, leading to significant performance improvements over a baseline."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d1b24d654126d64fa77f6ff66cd948e1830612a785483db227d8986e431770d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inefficient taxi supply-demand matching in smart cities by proposing a framework that models the urban road network as a graph and uses Graph Neural Networks combined with Q-learning to recommend optimal taxi placement hotspots. The method integrates real-time traffic conditions and historical data to optimize for passenger waiting time and driver travel distance. Experiments on a simulated Delhi dataset show the model reduces passenger waiting time by 56% and travel distance by 38% compared to a stochastic baseline."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Traffic-Aware Optimal Taxi Placement<br>Using Graph Neural Network-Based Reinforcement Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Conventional taxi hotspot models overlook dynamic traffic influences.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Graph-based RL with GNN embeddings for spatial-temporal dependencies.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Reduced passenger waiting time by 56% and travel distance by 38%.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, robust adaptive control, visual pose estimation, hierarchical learning, safety supervisor]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tampere University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00610",children:"https://arxiv.org/pdf/2601.00610"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A hierarchical learning framework that decomposes the goal-reaching control problem into tightly coupled modules, including RL for planning and supervised learning for dynamics modeling. 2. Integration of a model-based robust adaptive controller with the learned dynamics model to guarantee wheel command tracking on slip-prone terrain, ensuring uniform exponential stability. 3. Design of a mathematical safety supervisor to autonomously monitor the robot, stop it on unsafe faults, and guide it back to a safe area, reducing human intervention."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/588ba572bdf33b4091ce883350b57f99b3a43b7383f0188394f0a36af791cfc3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/588ba572bdf33b4091ce883350b57f99b3a43b7383f0188394f0a36af791cfc3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hierarchical learning framework for safe, vision-based goal-reaching control of large mobile robots. The method combines reinforcement learning for motion planning, supervised learning for robot dynamics modeling, and a robust adaptive controller for stable actuation, all overseen by a safety supervisor. Experiments on a 6,000 kg robot confirm the framework's effectiveness and safety guarantees."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[RL\u63a2\u7d22\u4e0d\u5b89\u5168/Unsafe RL Exploration]\n    B --\x3e B2[\u5927\u578b\u673a\u5668\u4eba\u5e94\u7528\u53d7\u9650/Limited Application for Large Robots]\n    C --\x3e C1[\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1/Visual Pose Estimation]\n    C --\x3e C2[RL\u8fd0\u52a8\u89c4\u5212\u5668/RL Motion Planner]\n    C --\x3e C3[\u76d1\u7763\u5b66\u4e60\u52a8\u529b\u5b66\u6a21\u578b/Supervised Learning Dynamics Model]\n    C --\x3e C4[\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u5668/Robust Adaptive Controller]\n    C --\x3e C5[\u6570\u5b66\u5b89\u5168\u76d1\u7763\u5668/Mathematical Safety Supervisor]\n    D --\x3e D1[\u4fdd\u8bc1\u7a33\u5b9a\u6027/Guarantees Stability]\n    D --\x3e D2[\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027/Experimental Validation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RoboReward: General-Purpose Vision-Language Reward Models for Robotics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [vision-language models, reward modeling, reinforcement learning, data augmentation, robotics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University, UC Berkeley"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00675",children:"https://arxiv.org/pdf/2601.00675"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RoboReward, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment and RoboArena. 2. Proposes a negative examples data augmentation pipeline to generate calibrated negatives and near-misses for training. 3. Trains and deploys general-purpose 4B/8B vision-language reward models that outperform larger VLMs and improve real-robot RL policy learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3d93ea0589a39158950d54cbe397e38c6b0ab250252ddc7e117e74f8d59010_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3d93ea0589a39158950d54cbe397e38c6b0ab250252ddc7e117e74f8d59010_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of designing rewards for robotic reinforcement learning by introducing RoboReward, a dataset and benchmark for training vision-language reward models. The method includes a data augmentation pipeline to create negative examples and trains compact 4B/8B parameter models. The results show these models outperform larger VLMs on short-horizon tasks and significantly improve real-robot policy learning compared to a frontier VLM."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RoboReward: General-Purpose Vision-Language Reward Models for Robotics] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Reward design for RL is labor-intensive or brittle/RL\u5956\u52b1\u8bbe\u8ba1\u8d39\u65f6\u6216\u8106\u5f31]\n    C --\x3e C1[Build dataset & benchmark from OXE & RoboArena/\u57fa\u4e8eOXE\u548cRoboArena\u6784\u5efa\u6570\u636e\u96c6\u4e0e\u57fa\u51c6]\n    C --\x3e C2[Propose negative examples augmentation pipeline/\u63d0\u51fa\u8d1f\u6837\u672c\u6570\u636e\u589e\u5f3a\u6d41\u7a0b]\n    C --\x3e C3[Train RoboReward 4B/8B VLMs/\u8bad\u7ec3RoboReward 4B/8B VLM]\n    D --\x3e D1[No existing VLM excels across all tasks/\u73b0\u6709VLM\u65e0\u5168\u80fd\u6a21\u578b]\n    D --\x3e D2[RoboReward models outperform larger VLMs/RoboReward\u6a21\u578b\u4f18\u4e8e\u66f4\u5927VLM]\n    D --\x3e D3[Improves real-robot RL over Gemini Robotics-ER/\u5728\u771f\u5b9e\u673a\u5668\u4ebaRL\u4e2d\u5927\u5e45\u8d85\u8d8aGemini]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Generative Reward Models, Bradley-Terry Model, Group Relative Policy Optimization, Reinforcement Learning from Human Feedback, Pointwise Scoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Haonan Song, Qingchen Xie, Huan Zhu, Feng Xiao, Luxi Xing, Fuzhen Li, Liu Kang, Feng Jiang, Zhiyong Zheng, Fan Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HUJING Digital Media & Entertainment Group (XingYun Lab), Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00677",children:"https://arxiv.org/pdf/2601.00677"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IRPO, a novel RL framework that integrates the Bradley-Terry model into GRPO to scale pairwise reward models for RL training. 2. Introduces a pointwise scoring mechanism that enables efficient evaluation of many candidate responses during RL, overcoming the O(n^2) computational bottleneck. 3. Demonstrates state-of-the-art performance among pointwise GRMs and shows significant advantages over pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5cc90e0083ca0244ea3fbdd445ab9a0b8ff4478f444643d38c88fecae22744f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of pairwise Generative Reward Models (GRMs) in reinforcement learning by proposing Intergroup Relative Preference Optimization (IRPO). IRPO incorporates the Bradley-Terry model into Group Relative Policy Optimization to generate pointwise scores, enabling efficient evaluation of many candidates. The method achieves state-of-the-art performance among pointwise GRMs and outperforms pairwise GRMs in post-training evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[IRPO: Scaling the Bradley-Terry Model via RL] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Pairwise GRMs create O(n\xb2) bottleneck in RL/\u6210\u5bf9GRM\u5728RL\u4e2d\u9020\u6210O(n\xb2)\u74f6\u9888]\n    C --\x3e C1[IRPO: Integrate Bradley-Terry into GRPO for pointwise scoring/IRPO: \u5c06Bradley-Terry\u878d\u5165GRPO\u5b9e\u73b0\u9010\u70b9\u8bc4\u5206]\n    D --\x3e D1[SOTA among pointwise GRMs/\u5728\u9010\u70b9GRM\u4e2d\u8fbe\u5230SOTA]\n    D --\x3e D2[Outperforms pairwise GRMs in post-training/\u5728\u8bad\u7ec3\u540e\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6210\u5bf9GRM]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ARISE: Adaptive Reinforcement Integrated with Swarm Exploration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [swarm intelligence, policy gradient, adaptive exploration, non-stationary rewards, particle swarm]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rajiv Chaitanya M, D R Ramesh Babu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dayananda Sagar College of Engineering"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00693",children:"https://arxiv.org/pdf/2601.00693"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ARISE, a lightweight framework that augments standard policy-gradient RL methods with a swarm-based exploration layer., 2. Proposes an adaptive mechanism that modulates exploration intensity based on reward-variance cues., 3. Demonstrates significant performance improvements and robustness, particularly in challenging and non-stationary environments, without altering core algorithmic structures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da08cf28812cd5b0330c83f593897cfcd5e8e953ff273742e122d3262910e186_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces ARISE, a framework that enhances reinforcement learning by integrating a swarm-based exploration layer with standard policy-gradient methods to improve exploration. It adaptively blends policy actions with particle-driven proposals and modulates exploration using reward variance. The method shows substantial performance gains on complex tasks and improved robustness in non-stationary environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[ARISE: Adaptive Reinforcement Integrated with Swarm Exploration] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1[RL\u63a2\u7d22\u6311\u6218/RL Exploration Challenge]\n    Problem --\x3e P2[\u975e\u5e73\u7a33\u5956\u52b1/Non-stationary Rewards]\n    Method --\x3e M1[\u7fa4\u4f53\u667a\u80fd\u63a2\u7d22\u5c42/Swarm-based Exploration Layer]\n    Method --\x3e M2[\u81ea\u9002\u5e94\u8c03\u8282/Adaptive Modulation]\n    Method --\x3e M3[\u7b56\u7565-\u7c92\u5b50\u6df7\u5408/Policy-Particle Blending]\n    Results --\x3e R1[\u6027\u80fd\u663e\u8457\u63d0\u5347/Substantial Performance Gains]\n    Results --\x3e R2[\u9c81\u68d2\u6027\u589e\u5f3a/Enhanced Robustness]\n    Results --\x3e R3[\u67b6\u6784\u65e0\u5173/Architecture-agnostic]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [reinforcement learning, precision autotuning, contextual bandit, mixed-precision, linear solvers]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Erin Carson, Xinye Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Charles University, Sorbonne Universit\xe9, CNRS, LIP6"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00728",children:"https://arxiv.org/pdf/2601.00728"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel reinforcement learning framework formulated as a contextual bandit problem for adaptive precision tuning of numerical algorithms. 2. Applies the framework to iterative refinement for linear solvers, using a Q-table and epsilon-greedy strategy to dynamically select precision configurations based on system features. 3. Demonstrates the framework's effectiveness and generalization, reducing computational cost while maintaining accuracy comparable to double-precision baselines on unseen data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7720c36c9ae5fbf03bb75ea2bb7142862906819bb41cf70b683252fc884718e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning framework for adaptive precision tuning, formulated as a contextual bandit problem, to optimize the trade-off between computational cost and accuracy in linear solvers. The method dynamically selects precision configurations based on system features using a Q-learning approach. Empirical results show it reduces cost while maintaining accuracy, and it generalizes well to unseen data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root(Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u80fd\u8017\u9ad8/High-precision computing is energy-intensive)\n    Problem --\x3e P2(\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u53ef\u80fd\u4e0d\u7a33\u5b9a/Reduced-precision can be unstable)\n    Method --\x3e M1(\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a/Formulate as Contextual Bandit)\n    Method --\x3e M2(\u4f7f\u7528Q\u8868\u548cepsilon-greedy\u7b56\u7565/Use Q-table & epsilon-greedy)\n    Method --\x3e M3(\u5e94\u7528\u4e8e\u7ebf\u6027\u6c42\u89e3\u5668\u7684\u8fed\u4ee3\u7cbe\u5316/Apply to iterative refinement for linear solvers)\n    Results --\x3e R1(\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c/Effectively reduces computational cost)\n    Results --\x3e R2(\u4fdd\u6301\u4e0e\u53cc\u7cbe\u5ea6\u76f8\u5f53\u7684\u7cbe\u5ea6/Maintains accuracy comparable to double-precision)\n    Results --\x3e R3(\u6cdb\u5316\u5230\u672a\u89c1\u6570\u636e/Generalizes to unseen data)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [actor-critic, overestimation, aleatoric uncertainty, distributional critic, dropout]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," U\u011furcan \xd6zalp"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Turkish Aerospace"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00737",children:"https://arxiv.org/pdf/2601.00737"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Stochastic Actor-Critic (STAC), a novel algorithm that uses temporal aleatoric uncertainty (from stochastic transitions, rewards, and policy) to scale pessimistic bias in TD updates, instead of relying on epistemic uncertainty. 2. Demonstrates that a single distributional critic network modeling return uncertainty is sufficient to mitigate overestimation and induce risk-averse behavior. 3. Shows that applying dropout for regularization in both actor and critic networks further improves training stability and performance, enhancing computational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602bf8125f08dc0296e56b4a9e156cd6089d329c07cd83909cc1b3c96effc1bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of overestimation bias in off-policy actor-critic reinforcement learning methods. It proposes the Stochastic Actor-Critic (STAC) algorithm, which mitigates overestimation by using a single distributional critic to model temporal aleatoric uncertainty for scaling pessimistic updates, and employs dropout for regularization. The results show that this approach effectively reduces overestimation, leads to risk-averse behavior, and improves computational efficiency and training stability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[STAC: Mitigating Overestimation via Temporal Aleatoric Uncertainty] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Critic\u7f51\u7edc\u7cfb\u7edf\u6027\u9ad8\u4f30\u4ef7\u503c/Critic Networks Systematically Overestimate Value Estimates]\n    C --\x3e C1[\u4f7f\u7528\u5355\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u65f6\u5e8f\u5076\u7136\u4e0d\u786e\u5b9a\u6027/Use Single Distributional Critic to Model Temporal Aleatoric Uncertainty]\n    C --\x3e C2[\u5728TD\u66f4\u65b0\u4e2d\u5e94\u7528\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u60b2\u89c2\u504f\u5dee/Apply Uncertainty-Based Pessimistic Bias in TD Updates]\n    C --\x3e C3[\u5bf9\u8bc4\u8bba\u5bb6\u548c\u884c\u52a8\u8005\u7f51\u7edc\u4f7f\u7528Dropout\u6b63\u5219\u5316/Use Dropout Regularization on Critic and Actor Networks]\n    D --\x3e D1[\u7f13\u89e3\u9ad8\u4f30\u504f\u5dee/Mitigates Overestimation Bias]\n    D --\x3e D2[\u5728\u968f\u673a\u73af\u5883\u4e2d\u4ea7\u751f\u98ce\u9669\u89c4\u907f\u884c\u4e3a/Leads to Risk-Averse Behavior in Stochastic Environments]\n    D --\x3e D3[\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027/Improves Computational Efficiency and Training Stability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [multi-functional RIS, NOMA, energy efficiency, hybrid deep reinforcement learning, parametrized sharing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chi-Te Kuo, Li-Hsiang Shen, Jyun-Jhe Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Central University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00538",children:"https://arxiv.org/pdf/2601.00538"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates an energy efficiency maximization problem for a multi-MF-RIS-aided NOMA downlink network, jointly optimizing power, beamforming, RIS configurations, and RIS positions. 2. Proposes a Parametrized Sharing scheme for Multi-Agent Hybrid Deep Reinforcement Learning (PMHRL) that combines multi-agent PPO for continuous variables and DQN for discrete variables. 3. Demonstrates through simulations that the proposed PMHRL and multi-MF-RIS architecture achieve superior energy efficiency compared to various benchmarks and alternative system scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62f2884171c264fbe837606dbe74e9d01169f9c8afd169da9b9bed765c6ab97e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of maximizing energy efficiency in downlink NOMA networks assisted by multiple multi-functional RISs. The authors propose a novel parametrized sharing scheme for a multi-agent hybrid deep reinforcement learning algorithm (PMHRL) to jointly optimize power, beamforming, and RIS parameters. Simulation results show that the proposed method achieves the highest energy efficiency compared to other benchmarks and system configurations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u6700\u5927\u5316\u591aMF-RIS\u8f85\u52a9NOMA\u7f51\u7edc\u7684\u80fd\u6548/Maximize EE of multi-MF-RIS-aided NOMA network")\n    Method --\x3e M1("\u53c2\u6570\u5171\u4eab\u7684\u591a\u667a\u80fd\u4f53\u6df7\u5408DRL (PMHRL)/Parametrized Sharing Multi-Agent Hybrid DRL (PMHRL)")\n    M1 --\x3e M1_1("PPO\u5904\u7406\u8fde\u7eed\u53d8\u91cf/PPO for continuous variables")\n    M1 --\x3e M1_2("DQN\u5904\u7406\u79bb\u6563\u53d8\u91cf/DQN for discrete variables")\n    Results --\x3e R1("PMHRL\u80fd\u6548\u6700\u9ad8/PMHRL achieves highest EE")\n    Results --\x3e R2("\u4f18\u4e8e\u65e0\u53c2\u6570\u5171\u4eab\u3001\u7eafPPO/DQN\u57fa\u51c6/Superior to benchmarks without sharing, pure PPO/DQN")\n    Results --\x3e R3("\u591aMF-RIS NOMA\u4f18\u4e8e\u4f20\u7edfRIS\u7b49\u573a\u666f/Multi-MF-RIS NOMA outperforms traditional RIS, etc.")'}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 4'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, self-explanations, faithfulness, natural language explanations, counterfactual examples]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qianli Wang, Nils Feldhus, Pepa Atanasova, Fedor Splitt, Simon Ostermann, Sebastian M\xf6ller, Vera Schmitt"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, German Research Center for Artificial Intelligence (DFKI), University of Copenhagen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00282",children:"https://arxiv.org/pdf/2601.00282"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. First comprehensive study on the impact of quantization on the quality and faithfulness of LLM self-explanations. 2. Empirical evaluation across multiple quantization techniques, bit widths, and model sizes, revealing moderate but consistent degradation in explanation metrics. 3. Provides practical recommendations for validating self-explanations in quantized models, highlighting the greater sensitivity of natural language explanations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8576a4dae59be54ef8b0a451a49893f5b9d59eceafecb4a697aed2b3fd4a470b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how quantization affects the quality and faithfulness of self-explanations generated by large language models. The authors evaluate multiple quantization methods and find they cause moderate declines in explanation metrics, with larger models showing better faithfulness preservation. They conclude that while quantization degrades self-explanations, the impact is relatively minor and does not negate its benefits for model compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Can Large Language Models Still Explain Themselves?<br/>\u5927\u8bed\u8a00\u6a21\u578b\u8fd8\u80fd\u89e3\u91ca\u81ea\u5df1\u5417\uff1f"] --\x3e Problem["Quantization\'s effect on Self-Explanations is unknown.<br/>\u91cf\u5316\u5bf9\u81ea\u6211\u89e3\u91ca\u7684\u5f71\u54cd\u672a\u77e5"]\n    Root --\x3e Method["Evaluate NLEs & Counterfactuals from quantized LLMs.<br/>\u8bc4\u4f30\u91cf\u5316\u540eLLM\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u53cd\u4e8b\u5b9e\u793a\u4f8b"]\n    Root --\x3e Results["Moderate decline in quality/faithfulness; context-dependent impact.<br/>\u8d28\u91cf/\u5fe0\u5b9e\u5ea6\u9002\u5ea6\u4e0b\u964d\uff1b\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Robust Assembly Progress Estimation via Deep Metric Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [metric learning], [assembly progress estimation, deep metric learning, quadruplet loss, anomaly detection, small-scale dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kazuma Miura, Sarthak Pathak, Kazunori Umeda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chuo University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00422",children:"https://arxiv.org/pdf/2601.00422"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Assembly Progress Estimation via Deep Metric Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u88c5\u914d\u4efb\u52a1\u4e2d\u89c6\u89c9\u53d8\u5316\u7ec6\u5fae\u5bfc\u81f4\u8bef\u5206\u7c7b/Misclassification due to subtle visual changes in assembly tasks]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u56db\u5143\u7ec4\u635f\u5931\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u7f51\u7edc/Anomaly Quadruplet-Net with Quadruplet Loss & custom data loader]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728PC\u88c5\u914d\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53471.3%\uff0c\u76f8\u90bb\u4efb\u52a1\u8bef\u5206\u7c7b\u51cf\u5c111.9%/1.3% accuracy improvement & 1.9% reduction in adjacent task misclassification on PC dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging reconstruction], [photoacoustic imaging, point cloud, iterative reconstruction, irregular array, hierarchical optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Peking University, Nanjing University, Pohang University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00551",children:"https://arxiv.org/pdf/2601.00551"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ShuangLiPKU/SlingBAG-Pro",children:"https://github.com/ShuangLiPKU/SlingBAG-Pro"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SingBAG Pro: 3D\u5149\u58f0\u6210\u50cf/SlingBAG Pro: 3D Photoacoustic Imaging] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e0d\u89c4\u5219\u9635\u5217\u91cd\u5efa\u6162/Irregular Array Reconstruction is Slow]\n    C --\x3e C1[\u70b9\u4e91\u8fed\u4ee3\u4e0e\u5206\u5c42\u4f18\u5316/Point Cloud Iteration & Hierarchical Optimization]\n    D --\x3e D1[\u901f\u5ea6\u63d0\u53472.2\u500d/Speedup of 2.2x]\n    D --\x3e D2[\u4eff\u771f\u4e0e\u6d3b\u4f53\u9a8c\u8bc1/Simulation & In Vivo Validation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00705",children:"https://arxiv.org/pdf/2601.00705"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GS-SLAM\u7684\u6b8b\u5dee\u9a71\u52a8\u81f4\u5bc6\u5316\u6548\u7387\u4f4e/Inefficient residual-driven densification in GS-SLAM]\n    C --\x3e C1[\u4e00\u6b21\u6027\u5bc6\u96c6\u521d\u59cb\u5316/One-shot dense initialization]\n    C1 --\x3e C2[\u4f7f\u7528DINOv3\u7279\u5f81\u4e0e\u7f6e\u4fe1\u5185\u70b9\u5206\u7c7b\u5668/Using DINOv3 features & confidence-aware inlier classifier]\n    C2 --\x3e C3[\u591a\u89c6\u89d2\u4e09\u89d2\u5316\u751f\u6210\u9ad8\u65af\u5148\u9a8c/Multi-view triangulation for Gaussian prior]\n    D --\x3e D1[\u6536\u655b\u52a0\u901f~20%/~20% faster convergence]\n    D --\x3e D2[\u66f4\u9ad8\u6e32\u67d3\u4fdd\u771f\u5ea6/Higher rendering fidelity]\n    D --\x3e D3[\u5b9e\u65f6\u6027\u80fd\u8fbe925 FPS/Real-time performance up to 925 FPS]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);