"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8310],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},50638:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CL/20251229-20260104","title":"20251229-20260104 (cs.CL)","description":"2025-12-29","source":"@site/docs/daily/cs_CL/20251229-20260104.md","sourceDirName":"daily/cs_CL","slug":"/daily/cscl/20251229-20260104","permalink":"/ai_toutiao/daily/cscl/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767243055000,"frontMatter":{"slug":"/daily/cscl/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.CL)","permalink":"/ai_toutiao/daily/cscl/20251222-20251228"},"next":{"title":"cs.CR","permalink":"/ai_toutiao/category/cscr"}}');var a=i(74848),r=i(28453);const t={slug:"/daily/cscl/20251229-20260104"},o="20251229-20260104 (cs.CL)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2},{value:"2026-01-01",id:"2026-01-01",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251229-20260104-cscl",children:"20251229-20260104 (cs.CL)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [db], [text-to-SQL], [unanswerable question detection, few-shot prompting, biomedical databases]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zurich University of Applied Sciences, University of Zurich"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21345",children:"https://arxiv.org/pdf/2512.21345"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem") --\x3e P1("Text-to-SQL\u5bf9\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u751f\u6210\u53ef\u6267\u884cSQL/Text-to-SQL generates executable SQL for unanswerable queries")\n    P1 --\x3e P2("\u751f\u7269\u533b\u5b66\u9886\u57df\u98ce\u9669\u9ad8/High risk in biomedical contexts")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method") --\x3e M1("Query Carefully \u7ba1\u9053/Query Carefully pipeline")\n    M1 --\x3e M2("LLM (llama3.3:70b) + \u6a21\u5f0f\u611f\u77e5\u63d0\u793a + \u5c11\u6837\u672c/LLM (llama3.3:70b) + schema-aware prompts + few-shot")\n    M2 --\x3e M3("\u5305\u542b\u53ef\u56de\u7b54\u4e0e\u4e0d\u53ef\u56de\u7b54\u793a\u4f8b/Includes answerable and unanswerable examples")\n    Results("\u5173\u952e\u7ed3\u679c/Results") --\x3e R1("\u6784\u5efaOncoMX-NAQ\u57fa\u51c6/Built OncoMX-NAQ benchmark")\n    R1 --\x3e R2("\u4e0d\u53ef\u56de\u7b54\u68c0\u6d4b\u51c6\u786e\u73870.8/Unanswerable-detection accuracy 0.8")\n    R2 --\x3e R3("\u7ed3\u6784\u6027\u95ee\u9898\u68c0\u6d4b\u597d\uff0c\u8bed\u4e49\u6a21\u7cca\u6311\u6218\u5927/Good for structural, challenging for semantic ambiguity")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Teaching People LLM's Errors and Getting it Right"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [human-ai interaction], [overreliance, failure patterns, mental models, user study, meta-labels]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasovi\u0107"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Utah"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21422",children:"https://arxiv.org/pdf/2512.21422"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness\u2014assessing a user's ability to anticipate LLM errors using taught patterns\u2014which showed a positive effect, unlike traditional human-AI team accuracy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Teaching People LLM\u2019s Errors and Getting it Right] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Users overrely on LLMs due to inaccurate mental models]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Analyze failure pattern existence, test discovery methods, propose new evaluation metric]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Patterns exist, discovery methods are mixed, new metric shows teaching is effective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Intelligent Systems and Robotics, Sorbonne University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21439",children:"https://arxiv.org/pdf/2512.21439"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[COMETH: Learning Interpretable Moral Contexts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Moral judgments are context-dependent]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Probabilistic clustering + LLM semantics + Human judgments]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Doubles alignment with human judgments vs. LLM prompting]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Oogiri-Master: Benchmarking Humor Understanding via Oogiri"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [humor understanding], [Oogiri, benchmark, linguistic analysis, incongruity resolution, insight-augmented prompting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21494",children:"https://arxiv.org/pdf/2512.21494"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Oogiri-Master: Benchmarking Humor Understanding via Oogiri] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: What makes Oogiri responses funny to humans?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce Oogiri-Master benchmark and Oogiri-Corpus dataset with diverse responses and independent ratings]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: LLMs approach human performance; insight-augmented prompting improves results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dartmouth College"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21506",children:"https://arxiv.org/pdf/2512.21506"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Ant Group, Zhejiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21515",children:"https://arxiv.org/pdf/2512.21515"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of "perplexity landscapes" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Perplexity-Aware Data Scaling Law<br>\u56f0\u60d1\u5ea6\u611f\u77e5\u6570\u636e\u7f29\u653e\u5b9a\u5f8b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[CPT\u4e2d\u5355\u7eaf\u589e\u52a0\u6570\u636e\u6536\u76ca\u9012\u51cf<br>Diminishing returns from scaling data in CPT]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u666f\u89c2\u7684\u7f29\u653e\u5b9a\u5f8b<br>Propose perplexity-landscape-based scaling law]\n    C1 --\x3e C2[\u5229\u7528\u56f0\u60d1\u5ea6\u91cf\u5316\u77e5\u8bc6\u5dee\u8ddd<br>Use perplexity to quantify knowledge gap]\n    C2 --\x3e C3[\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u4ef7\u503c\u6570\u636e\u5b50\u96c6<br>Adaptively select high-utility data subsets]\n    D --\x3e D1[\u8bc6\u522b\u63a5\u8fd1\u6700\u4f18\u7684\u8bad\u7ec3\u5b50\u96c6<br>Identifies near-optimal training subsets]\n    D1 --\x3e D2[\u5728\u9886\u57df\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd<br>Achieves superior performance on domain benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21551",children:"https://arxiv.org/pdf/2512.21551"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Human-AI Interaction Alignment] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Establishes agenda for reciprocal, responsible human-AI futures]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [external memory, sequential decision-making, value functions, uncertainty estimators, hierarchical storage]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Changzhi Sun, Xiangyu Chen, Jixiang Luo, Dell Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21567",children:"https://arxiv.org/pdf/2512.21567"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/TeleAI-UAGI/telemem",children:"https://github.com/TeleAI-UAGI/telemem"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a decision-theoretic reframing of agent memory management as a sequential decision-making problem under uncertainty, 2. Introduces the DAM framework that decomposes memory operations into immediate access and hierarchical maintenance, 3. Provides a foundation for future research by evaluating operations via value functions and uncertainty estimators for long-term utility and risk"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d30748df565a671b29899dffbfb153dca58fed0398e13cc6871dfe6f450f11a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d30748df565a671b29899dffbfb153dca58fed0398e13cc6871dfe6f450f11a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper argues that current heuristic-based memory management for LLM agents is inadequate due to delayed and uncertain utility. It proposes DAM, a decision-theoretic framework that uses value functions and uncertainty estimators to make memory decisions based on long-term consequences. The main contribution is a principled reframing of the problem to guide future research on uncertainty-aware memory systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u542f\u53d1\u5f0f\u5185\u5b58\u7ba1\u7406\u7f3a\u4e4f\u5bf9\u957f\u671f\u548c\u4e0d\u786e\u5b9a\u540e\u679c\u7684\u6d1e\u5bdf/Heuristic memory management lacks insight into long-term & uncertain consequences]\n    C --\x3e C1[\u63d0\u51faDAM\u6846\u67b6\uff0c\u5c06\u5185\u5b58\u7ba1\u7406\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898/Propose DAM framework, viewing memory as a sequential decision problem]\n    C --\x3e C2[\u4f7f\u7528\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u8bc4\u4f30\u64cd\u4f5c/Evaluate operations via value functions & uncertainty estimators]\n    D --\x3e D1[\u539f\u5219\u6027\u91cd\u6784\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5185\u5b58\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840/Principled reframing, provides foundation for uncertainty-aware memory systems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21577",children:"https://arxiv.org/pdf/2512.21577"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["Hallucination persists in LLMs / \u5e7b\u89c9\u5728LLM\u4e2d\u6301\u7eed\u5b58\u5728"]\n    Method --\x3e M1["Unified definition: inaccurate world modeling / \u7edf\u4e00\u5b9a\u4e49\uff1a\u4e0d\u51c6\u786e\u7684\u4e16\u754c\u5efa\u6a21"]\n    Method --\x3e M2["Framework: reference world & conflict policy / \u6846\u67b6\uff1a\u53c2\u8003\u4e16\u754c\u4e0e\u51b2\u7a81\u7b56\u7565"]\n    Results --\x3e R1["Clarifies evaluation & terminology / \u6f84\u6e05\u8bc4\u4f30\u4e0e\u672f\u8bed"]\n    Results --\x3e R2["Proposes new benchmark plans / \u63d0\u51fa\u65b0\u57fa\u51c6\u8ba1\u5212"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual language modeling], [two-stage pre-training, cross-lingual alignment, English enrichment, cost-efficient training, Russian LLM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexander Podolskiy, Semen Molokov, Timofey Gerasin, Maksim Titov, Alexey Rukhovich, Artem Khrapov, Kirill Morozov, Evgeny Tetin, Constantine Korikov, Pavel Efimov, Polina Lazukova, Yuliya Skripkar, Nikita Okhotnikov, Irina Piontkovskaya, Meng Xiaojun, Zou Xueyi, Zhang Zhenhe"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Gamayun Team"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21580",children:"https://arxiv.org/pdf/2512.21580"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel two-stage pre-training strategy (balanced multilingual training followed by high-quality English enrichment) for efficient cross-lingual knowledge transfer. 2. Presents Gamayun, a 1.5B-parameter multilingual LLM trained from scratch on 2.5T tokens, designed for resource-constrained environments. 3. Demonstrates state-of-the-art performance for its size (1-2B parameters) on Russian benchmarks and competitive results on English and multilingual tasks, despite a significantly smaller training budget than comparable models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b7ebd17e8a0b7536938c0d12aa8812a6376542abde4f40239a4622472c17ea0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b7ebd17e8a0b7536938c0d12aa8812a6376542abde4f40239a4622472c17ea0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents Gamayun, a cost-efficient 1.5B-parameter multilingual language model. It addresses the lack of small non-English-centric LLMs through a novel two-stage pre-training strategy for cross-lingual alignment. The model achieves state-of-the-art results in Russian and outperforms larger models on many tasks, despite being trained on far fewer tokens."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Gamayun's Path to Multilingual Mastery<br/>Gamayun\u7684\u591a\u8bed\u8a00\u7cbe\u901a\u4e4b\u8def] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Lack of small, efficient, non-English-centric LLMs<br/>\u7f3a\u4e4f\u5c0f\u578b\u3001\u9ad8\u6548\u3001\u975e\u82f1\u8bed\u4e2d\u5fc3\u7684LLM]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Two-stage pre-training<br/>\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3<br/>1. Balanced multilingual training<br/>\u5e73\u8861\u591a\u8bed\u8a00\u8bad\u7ec3<br/>2. High-quality English enrichment<br/>\u9ad8\u8d28\u91cf\u82f1\u8bed\u589e\u5f3a]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Outperforms LLaMA3.2-1B & Qwen2.5-1.5B<br/>\u8d85\u8d8aLLaMA3.2-1B\u548cQwen2.5-1.5B<br/>SOTA in Russian (MERA)<br/>\u4fc4\u8bed\u4efb\u52a1\u8fbe\u5230SOTA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [RLVR, sample polarity, advantage shaping, policy optimization, reasoning models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xinyu Tang, Yuliang Zhan, Zhixun Li, Wayne Xin Zhao, Zhenduo Zhang, Zujie Wen, Zhiqiang Zhang, Jun Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Renmin University of China, The Chinese University of Hong Kong, Ant Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21625",children:"https://arxiv.org/pdf/2512.21625"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A systematic investigation into the distinct roles of positive and negative samples (sample polarity) in RLVR training dynamics, showing positive samples sharpen existing patterns while negative samples encourage exploration. 2. An exploration of how adjusting advantage values for different sample polarities at both the sample and token levels affects training. 3. The proposal of A3PO, an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, which precisely allocates advantage signals to key tokens."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66a5d8d013ee86ee35c80048dbd4d2b03bd023a52b180e92f678fb36aa1f6018_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66a5d8d013ee86ee35c80048dbd4d2b03bd023a52b180e92f678fb36aa1f6018_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the distinct roles of positive and negative samples in Reinforcement Learning with Verifiable Rewards (RLVR) for training large reasoning models. It finds positive samples refine correct patterns while negative samples promote exploration, and proposes a new method called A3PO for adaptive, asymmetric token-level advantage shaping. Experiments on five reasoning benchmarks demonstrate the effectiveness of the proposed approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Rethinking Sample Polarity in RLVR] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[RLVR\u4e2d\u6b63\u8d1f\u6837\u672c\u7684\u89d2\u8272?/Roles of +/- samples in RLVR?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5206\u6790\u6837\u672c\u6781\u6027/Analyze Sample Polarity]\n    Method --\x3e M2[\u63d0\u51faA3PO\u65b9\u6cd5/Propose A3PO Method]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6b63\u6837\u672c\u9510\u5316\u6a21\u5f0f/Positive samples sharpen patterns]\n    Results --\x3e R2[\u8d1f\u6837\u672c\u9f13\u52b1\u63a2\u7d22/Negative samples encourage exploration]\n    Results --\x3e R3[A3PO\u6709\u6548/A3PO is effective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination evaluation], [HIC-Bench, Intelligent Hallucinations, Defective Hallucinations, Torrance Tests of Creative Thinking, Dynamic Hallucination Prompt]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chengxu Yang, Jingling Yuan, Siqi Cai, Jiawei Jiang, Chuang Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Wuhan University of Technology, Wuhan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21635",children:"https://arxiv.org/pdf/2512.21635"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/chujiguangniao/HIC-bench",children:"https://github.com/chujiguangniao/HIC-bench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HIC-Bench, a novel evaluation framework that categorizes LLM hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH) for systematic study. 2. Introduces a structured multi-dimensional assessment matrix combining TTCT creativity metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation). 3. Features cross-domain applicability across ten scientific domains and a Dynamic Prompt Optimization technique (DHP) to guide model outputs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb581f42c65746c28519ac82f0996a0d7ab8f6413a85636583ec1e0abecba3c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/beb581f42c65746c28519ac82f0996a0d7ab8f6413a85636583ec1e0abecba3c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of evaluating LLM hallucinations beyond factual errors by proposing HIC-Bench, a framework that distinguishes between creative (Intelligent) and erroneous (Defective) hallucinations using a multi-metric assessment. It demonstrates that creativity and correctness can be jointly optimized, revealing a nonlinear relationship between the two types of hallucinations and positioning intelligent hallucinations as a catalyst for scientific innovation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861LLM\u5e7b\u89c9\u7684\u521b\u9020\u6027\u4e0e\u51c6\u786e\u6027/Existing methods struggle to balance creativity and accuracy in LLM hallucinations]\n    C --\x3e C1[\u63d0\u51faHIC-Bench\u8bc4\u4f30\u6846\u67b6/Propose HIC-Bench evaluation framework]\n    C1 --\x3e C2[\u5206\u7c7b\u667a\u80fd\u4e0e\u7f3a\u9677\u5e7b\u89c9/Categorize IH and DH]\n    C1 --\x3e C3[\u591a\u7ef4\u5ea6\u8bc4\u4f30\u77e9\u9635/Multi-dimensional metric matrix]\n    C1 --\x3e C4[\u52a8\u6001\u63d0\u793a\u4f18\u5316/Dynamic Prompt Optimization]\n    D --\x3e D1[\u521b\u9020\u529b\u4e0e\u6b63\u786e\u6027\u53ef\u5171\u540c\u4f18\u5316/Creativity and correctness can be jointly optimized]\n    D --\x3e D2[\u667a\u80fd\u5e7b\u89c9\u662f\u521b\u9020\u529b\u7684\u50ac\u5316\u5242/IH is a catalyst for creativity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Liuyang Bai, Weiyi Lu, Li Guo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NYU Shanghai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21653",children:"https://arxiv.org/pdf/2512.21653"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [spoken dialogue systems], [Graph-of-Thoughts, full-duplex, speech acts, causal inference, multimodal transformer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, University of California, Berkeley, Carnegie Mellon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21706",children:"https://arxiv.org/pdf/2512.21706"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://got-duplex.github.io/",children:"https://got-duplex.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Enabling Conversational Behavior Reasoning in Full-Duplex Speech<br/>\u5b9e\u73b0\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u884c\u4e3a\u63a8\u7406"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Current systems lack explicit reasoning for conversational behaviors."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Model reasoning as causal inference in a Graph-of-Thoughts (GoT)."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Robust behavior detection and interpretable reasoning chains."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [ai-generated text detection], [transformer, fine-tuning, zero-shot, Bengali, paraphrase detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not specified in provided content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21709",children:"https://arxiv.org/pdf/2512.21709"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Detecting AI-Generated Paraphrases in Bengali] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLM misuse & lack of Bengali detection research]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Compare 5 transformers (Zero-Shot vs. Fine-Tuned)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Fine-tuning needed; XLM-R, mDeBERTa, mBERT achieve ~91% accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Parameter-Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), Mixture-of-Roles (MoR), Agent Tuning, Data Generation Pipeline]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jing Han, Binwei Yan, Tianyu Guo, Zheyuan Bai, Mengyu Zheng, Hanting Chen, Ying Nie"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications, Huawei Noah's Ark Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21708",children:"https://arxiv.org/pdf/2512.21708"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://mor-agent.github.io/",children:"https://mor-agent.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Decomposes agent capabilities into three distinct roles (reasoner, executor, summarizer) based on the Reason+Action paradigm. 2. Proposes the Mixture-of-Roles (MoR) framework, which uses three specialized LoRA groups, each dedicated to a specific role, to collaboratively accomplish agent tasks. 3. Develops a multi-role data generation pipeline for effective fine-tuning, incorporating role-specific content completion and reliability verification."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c303f17ce31b4315bdd80c394b9ba486dc14690a542d268175927115df139559_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c303f17ce31b4315bdd80c394b9ba486dc14690a542d268175927115df139559_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the underexplored area of parameter-efficient fine-tuning (PEFT) for AI agents. It proposes MoRAgent, a framework that decomposes agent tasks into three roles (reasoner, executor, summarizer) and assigns a specialized LoRA module to each, enabling efficient and collaborative task completion. Extensive experiments demonstrate the method's effectiveness in tuning LLMs for agent tasks while maintaining parameter efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: PEFT for agent tasks is largely unexplored] --\x3e P1[\u6311\u6218/Challenges: Full fine-tuning is resource-heavy and harms general capabilities]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Mixture-of-Roles (MoR) Framework] --\x3e M1[\u7b56\u75651/Strategy 1: Decompose agent into three roles]\n    M1 --\x3e M1_1[\u89d2\u8272/Roles: Reasoner, Executor, Summarizer]\n    Method --\x3e M2[\u7b56\u75652/Strategy 2: Three specialized LoRA groups for the three roles]\n    Method --\x3e M3[\u7b56\u75653/Strategy 3: Multi-role data generation pipeline]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Effectiveness demonstrated] --\x3e R1[\u5b9e\u9a8c/Experiments: Extensive tests on various LLMs & benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [interpretability & analysis], [latent tokens, chain-of-thought, model reliability, causal analysis, shortcut learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21711",children:"https://arxiv.org/pdf/2512.21711"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces "Steering Experiments" to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts "Shortcut Experiments" to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a "pseudo-reasoning" mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Latent token mechanisms unclear, reliability concerns] --\x3e B1[\u6f5c\u5728\u4ee4\u724c\u673a\u5236\u4e0d\u660e\u786e/Unclear latent token mechanisms]\n    B --\x3e B2[\u53ef\u9760\u6027\u95ee\u9898/Reliability concerns]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Causal & adversarial analysis] --\x3e C1[\u5f15\u5bfc\u5b9e\u9a8c/Steering experiments]\n    C --\x3e C2[\u6377\u5f84\u5b9e\u9a8c/Shortcut experiments]\n    D[\u5173\u952e\u7ed3\u679c/Results: COCONUT is pseudo-reasoning] --\x3e D1[\u4ee4\u724c\u5bf9\u6270\u52a8\u4e0d\u654f\u611f/Tokens insensitive to perturbation]\n    D --\x3e D2[\u5229\u7528\u6570\u636e\u96c6\u6377\u5f84/Exploits dataset shortcuts]\n    D --\x3e D3[\u6027\u80fd\u63d0\u5347\u4e0d\u57fa\u4e8e\u771f\u5b9e\u63a8\u7406/Performance gains not from true reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [dialogue systems], [theme detection, topic clustering, hierarchical generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21715",children:"https://arxiv.org/pdf/2512.21715"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[CATCH: \u53ef\u63a7\u4e3b\u9898\u68c0\u6d4b\u6846\u67b6 / Controllable Theme Detection Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898 / Problem] --\x3e P1[\u77ed\u8bdd\u8bed\u7a00\u758f\u8bed\u4e49 / Sparse, short utterances]\n    Problem --\x3e P2[\u8de8\u5bf9\u8bdd\u4e3b\u9898\u5bf9\u9f50 / Cross-dialogue theme alignment]\n    Problem --\x3e P3[\u7528\u6237\u504f\u597d\u6574\u5408 / Personalized user preferences]\n    Method[\u4e3b\u8981\u65b9\u6cd5 / Method] --\x3e M1[\u4e0a\u4e0b\u6587\u611f\u77e5\u4e3b\u9898\u8868\u793a / Context-aware topic representation]\n    Method --\x3e M2[\u504f\u597d\u5f15\u5bfc\u4e3b\u9898\u805a\u7c7b / Preference-guided topic clustering]\n    Method --\x3e M3[\u5206\u5c42\u4e3b\u9898\u751f\u6210 / Hierarchical theme generation]\n    Results[\u5173\u952e\u7ed3\u679c / Results] --\x3e R1[\u5728DSTC-12\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548 / Effective on DSTC-12 benchmark]\n    Results --\x3e R2[\u63d0\u5347\u805a\u7c7b\u4e0e\u751f\u6210\u8d28\u91cf / Improved clustering & generation quality with 8B LLM]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] An Information Theoretic Perspective on Agentic System Design"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher R\xe9, Dan Biderman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21720",children:"https://arxiv.org/pdf/2512.21720"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[An Information Theoretic Perspective on Agentic System Design] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1("Agentic\u7cfb\u7edf\u8bbe\u8ba1\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc<br/>Agentic system design lacks theoretical guidance")\n    C --\x3e C1("\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6\u4e0e\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668<br/>Propose information-theoretic framework & mutual information estimator")\n    D --\x3e D1("\u66f4\u5927\u538b\u7f29\u5668\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e<br/>Larger compressors are more efficient and accurate")\n    D --\x3e D2("\u6269\u5c55\u538b\u7f29\u5668\u4f18\u4e8e\u6269\u5c55\u9884\u6d4b\u5668<br/>Scaling compressors outperforms scaling predictors")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [machine translation], [dialectal arabic, modern standard arabic, post-editing evaluation, error taxonomy, human evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abdullah Alabdullah, Lifeng Han, Chenghua Lin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Edinburgh, University of Manchester, Leiden University Medical Center (LUMC) / Leiden University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21787",children:"https://arxiv.org/pdf/2512.21787"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Ara-HOPE, a human-centric post-editing evaluation framework specifically designed for Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation. 2. Proposes a five-category error taxonomy and a decision-tree annotation protocol to systematically identify dialect-specific translation errors. 3. Provides a comparative evaluation of three MT systems (Jais, GPT-3.5, NLLB-200), highlighting persistent challenges like dialect-specific terminology and semantic preservation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81517a8ae79cb17e9997772074cacae30aa2d04d3b344341b5fa0c68a1bc551b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81517a8ae79cb17e9997772074cacae30aa2d04d3b344341b5fa0c68a1bc551b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of evaluating machine translation from Dialectal Arabic (DA) to Modern Standard Arabic (MSA), where existing metrics fail to capture dialect-specific errors. It proposes Ara-HOPE, a human-centric post-editing evaluation framework with a specialized error taxonomy and annotation protocol. The framework's application reveals that dialect-specific terminology and semantic preservation are the most persistent challenges for current MT systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: DA-MSA\u7ffb\u8bd1\u8bc4\u4f30\u56f0\u96be / DA-MSA Translation Evaluation is Difficult]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faAra-HOPE\u6846\u67b6 / Proposes Ara-HOPE Framework]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u65b9\u8a00\u672f\u8bed\u548c\u8bed\u4e49\u4fdd\u7559\u662f\u4e3b\u8981\u6311\u6218 / Dialect Terminology & Semantic Preservation are Key Challenges]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University, Adobe Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21789",children:"https://arxiv.org/pdf/2512.21789"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u79d1\u5b66\u56fe\u8868\u8bf4\u660e\u8d28\u91cf\u5dee/Poor quality of scientific figure captions]\n    Problem --\x3e P2[\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6/Lack of large-scale real-world dataset]\n    Method --\x3e M1[\u6784\u5efaarXiv\u56fe\u8868-\u8bf4\u660e\u5bf9\u6570\u636e\u96c6/Construct arXiv figure-caption dataset]\n    Method --\x3e M2[\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e0e\u8bc4\u4f30/Domain-specific training & evaluation]\n    Method --\x3e M3[\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5174\u8d77/Navigate rise of LLMs]\n    Results --\x3e R1[\u603b\u7ed3\u6280\u672f\u65b9\u6cd5\u7ecf\u9a8c/Summarize technical & methodological lessons]\n    Results --\x3e R2[\u63d0\u51fa\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411/Outline future challenges & directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] On The Conceptualization and Societal Impact of Cross-Cultural Bias"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [bias and fairness], [cultural bias, literature survey, societal impact, harm evaluation, bias mitigation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vitthal Bhandari"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Washington"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21809",children:"https://arxiv.org/pdf/2512.21809"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),"  1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\nRoot("On The Conceptualization and Societal Impact of Cross-Cultural Bias") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.")\nRoot --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.")\nRoot --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [method decoration, large language models, adaptive method generation, IoT intelligence, on-device reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hong Su"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chengdu University of Information Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21817",children:"https://arxiv.org/pdf/2512.21817"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Method Decoration (DeMe) framework, a novel approach that modifies an LLM's method-generation path using explicit, non-hardcoded decorations derived from hidden goals, learned methods, and environmental feedback. 2. Formalizes two major categories of decorations (whole-process and step-level) and mechanisms (pre-decoration, post-decoration, etc.) to enable context-aware and adaptive method reshaping. 3. Demonstrates experimentally that the framework allows IoT devices to generate more appropriate methods in unknown or faulty operating conditions without modifying the underlying LLM's internal weights."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c4e12db053492542eb54d5fe131cf8b2ac404f1af94254b9d2abeed75d055a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c4e12db053492542eb54d5fe131cf8b2ac404f1af94254b9d2abeed75d055a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that LLM-driven IoT devices struggle to adapt to novel situations due to fixed, pre-trained models. It proposes the Method Decoration (DeMe) framework, which augments an LLM's reasoning path with contextual decorations from experience and environment to generate adaptive methods. Experimental results show DeMe enables devices to derive more appropriate methods for unseen or faulty conditions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: LLMs in IoT lack adaptability to unseen situations and rely on fixed logic."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: DeMe framework modifies LLM method-generation using decorations from goals, experience, and feedback."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Enables derivation of more appropriate methods for unknown/faulty conditions."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [knowledge-augmented reasoning], [GraphRAG, Knowledge Graph, Graph Neural Network, LoRA, ChatGLM]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chongqing Jiaotong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21837",children:"https://arxiv.org/pdf/2512.21837"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an LLM-based approach integrating a domain-specific knowledge graph for reasoning in tobacco pest and disease control, built upon the GraphRAG framework. 2. Employs a GNN to learn expressive node representations that capture relational information within the knowledge graph, enhancing the model's reasoning capability. 3. Demonstrates effective parameter-efficient fine-tuning of a ChatGLM backbone using LoRA, achieving superior performance in complex reasoning scenarios like multi-hop and comparative reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85aaa4f0b61b3033af1966c2105126130730ca9d346945b5a0ca02e3f706eb1a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85aaa4f0b61b3033af1966c2105126130730ca9d346945b5a0ca02e3f706eb1a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a method that enhances large language models for agricultural knowledge reasoning by integrating graph-structured information. It constructs a tobacco pest and disease knowledge graph, uses a GNN to learn node representations, and fine-tunes a ChatGLM model with LoRA. The approach outperforms baselines, significantly improving reasoning accuracy and depth, especially in complex scenarios."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Knowledge Reasoning of LLMs Integrating Graph Information for Tobacco Pest Control<br>LLM\u96c6\u6210\u56fe\u4fe1\u606f\u7684\u70df\u8349\u75c5\u866b\u5bb3\u77e5\u8bc6\u63a8\u7406] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u6548\u7387\u4f4e\u3001\u9519\u8bef\u7387\u9ad8<br>Traditional methods rely on expert experience, low efficiency & high error]\nC --\x3e C1[\u57fa\u4e8eGraphRAG\u6846\u67b6\uff0c\u6784\u5efa\u70df\u8349\u75c5\u866b\u5bb3\u77e5\u8bc6\u56fe\u8c31<br>Built on GraphRAG, construct tobacco pest/disease KG]\nC --\x3e C2[\u4f7f\u7528GNN\u5b66\u4e60\u56fe\u8c31\u8282\u70b9\u8868\u793a\uff0cChatGLM+LoRA\u5fae\u8c03<br>Use GNN for node representations, fine-tune ChatGLM with LoRA]\nD --\x3e D1[\u5728\u591a\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5<br>Outperforms baselines across multiple metrics]\nD --\x3e D2[\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\uff08\u591a\u8df3\u3001\u6bd4\u8f83\uff09\u7684\u51c6\u786e\u6027\u548c\u6df1\u5ea6<br>Significantly improves accuracy & depth in complex reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [machine translation], [sentence alignment, parallel corpora, Arabic-English, legal texts, large language models (LLMs)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Baorong Huang, Ali Asiri"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huaihua University, Umm al-Qura University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21842",children:"https://arxiv.org/pdf/2512.21842"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/XXX",children:"https://github.com/XXX"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposed AlignAR, a generative sentence alignment method for Arabic-English parallel corpora. 2. Introduced a new dataset of complex legal and literary texts, featuring a "Hard" subset with reduced one-to-one mappings to better evaluate alignment methods. 3. Developed a hybrid LLM-plus-human-validation workflow and a bilingual annotation tool for creating gold-standard alignments.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f00c37d022ef71644588277bad96472223331ff4180059a6a3d353133f3a205_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f00c37d022ef71644588277bad96472223331ff4180059a6a3d353133f3a205_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the scarcity of high-quality Arabic-English parallel corpora by proposing AlignAR, a generative sentence alignment method. The method, along with a new dataset of complex legal and literary texts, demonstrates that LLM-based approaches are more robust than traditional methods, achieving an 85.5% F1-score and a 9% improvement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Arabic-English parallel corpora are scarce and lack complex mappings]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes AlignAR, a generative sentence alignment method using LLMs and a new dataset]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: LLM-based methods show superior robustness, achieving 85.5% F1-score, a 9% improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [evaluation], [anthropomorphic intelligence, benchmark, psychological counseling, rubric-based evaluation, reasoning-before-scoring]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Ant Group, Xiamen University, Beijing Normal University, Zhejiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21849",children:"https://arxiv.org/pdf/2512.21849"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/inclusionAI/HeartBench",children:"https://github.com/inclusionAI/HeartBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based "reasoning-before-scoring" evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLMs\u7f3a\u4e4f\u62df\u4eba\u5316\u667a\u80fd / LLMs lack anthropomorphic intelligence]\n    B --\x3e B2[\u4e2d\u6587\u8bed\u5883\u7f3a\u4e4f\u8bc4\u4f30\u6846\u67b6 / Lack of evaluation frameworks in Chinese context]\n    C --\x3e C1[\u57fa\u4e8e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u7684\u57fa\u51c6 / Benchmark based on psychological counseling scenarios]\n    C --\x3e C2[\u7406\u8bba\u9a71\u52a8\u7684\u5206\u7c7b\u6cd5 / Theory-driven taxonomy]\n    C --\x3e C3[\u57fa\u4e8e\u91cf\u89c4\u7684\u63a8\u7406\u8bc4\u5206\u6cd5 / Rubric-based reasoning-before-scoring]\n    D --\x3e D1[\u6a21\u578b\u6027\u80fd\u5b58\u5728\u4e0a\u9650 / Performance ceiling in models]\n    D --\x3e D2[\u590d\u6742\u573a\u666f\u8868\u73b0\u663e\u8457\u4e0b\u964d / Significant decay in complex scenarios]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] TimeBill: Time-Budgeted Inference for Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [time-budgeted inference, KV cache eviction, response length prediction, execution time estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Fan, An Zou, Yehan Ma"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21859",children:"https://arxiv.org/pdf/2512.21859"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a fine-grained response length predictor (RLP) and an execution time estimator (ETE) for accurate end-to-end LLM inference time modeling. 2. Developed a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on predicted execution time and a given time budget. 3. Demonstrated through experiments that TimeBill improves task completion rate and maintains response performance under various time constraints."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a21a4204d1665c895b35788196ab3a0e5b32216d06abc37bfaaa9aefac4cb2f5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a21a4204d1665c895b35788196ab3a0e5b32216d06abc37bfaaa9aefac4cb2f5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes TimeBill, a framework for performing LLM inference within a strict time budget. It uses predictors to estimate response length and execution time, then dynamically adjusts the KV cache eviction ratio to meet deadlines while preserving output quality. Experiments show it improves task completion rates and maintains performance compared to fixed strategies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TimeBill: Time-Budgeted Inference for Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u63a8\u7406\u65f6\u95f4\u4e0d\u786e\u5b9a/Uncertain LLM Inference Time]\n    B --\x3e B2[\u56fa\u5b9aKV\u7f13\u5b58\u7b56\u7565\u4e0d\u7075\u6d3b/Fixed KV Cache Strategy Inflexible]\n    C --\x3e C1[\u54cd\u5e94\u957f\u5ea6\u9884\u6d4b\u5668 (RLP)/Response Length Predictor (RLP)]\n    C --\x3e C2[\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u5668 (ETE)/Execution Time Estimator (ETE)]\n    C --\x3e C3[\u81ea\u9002\u5e94KV\u7f13\u5b58\u9a71\u9010/Adaptive KV Cache Eviction]\n    D --\x3e D1[\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387/Improves Task Completion Rate]\n    D --\x3e D2[\u4fdd\u6301\u54cd\u5e94\u6027\u80fd/Maintains Response Performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [copyright compliance, vision-language models, tool-augmented defense, benchmark dataset, multimodal query]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, University of California, Los Angeles, Palo Alto Networks"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21871",children:"https://arxiv.org/pdf/2512.21871"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/bluedream02/CopyGuard",children:"https://github.com/bluedream02/CopyGuard"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LVLMs may infringe copyright when processing visual inputs"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Benchmark dataset & Tool-augmented defense framework"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Current LVLMs are deficient; Proposed framework reduces risk"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text-to-sql], [benchmark, multilingual, domain-specific, large language models, sports analytics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science (BITS), Pilani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21877",children:"https://arxiv.org/pdf/2512.21877"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a "Gold Standard" dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u5728\u4e13\u4e1a\u9886\u57dfText-to-SQL\u80fd\u529b\u672a\u5145\u5206\u63a2\u7d22/LLMs\' Text-to-SQL capability in specialized domains is under-explored]\n    B --\x3e B2[\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u591a\u8bed\u8a00\u548c\u4f53\u80b2\u5206\u6790\u7279\u6027/Existing benchmarks lack multilingual and sports analytics features]\n    C --\x3e C1[\u6784\u5efa\u677f\u7403\u9886\u57df\u4e13\u4e1a\u591a\u8bed\u8a00\u57fa\u51c6/Build a specialized multilingual benchmark for cricket]\n    C --\x3e C2[\u4e0e\u4e13\u5bb6\u5408\u4f5c\u521b\u5efa"\u9ec4\u91d1\u6807\u51c6"\u67e5\u8be2/Collaborate with experts to create "Gold Standard" queries]\n    C --\x3e C3[\u8bc4\u4f30\u516d\u4e2a\u6700\u5148\u8fdb\u7684LLMs/Evaluate six state-of-the-art LLMs]\n    D --\x3e D1[\u4e13\u4e1a\u9886\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d/Significant performance drop in specialized domain]\n    D --\x3e D2[DeepSeek R1\u8868\u73b0\u6700\u4f73/DeepSeek R1 achieves SOTA]\n    D --\x3e D3[\u5370\u5730\u8bed\u67e5\u8be2\u51c6\u786e\u7387\u53ef\u6bd4\u6216\u66f4\u9ad8/Hindi queries yield parity or higher accuracy]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Explainable Statute Prediction via Attention-based Model and LLM Prompting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [legal text processing], [statute prediction, explainable AI, attention mechanism, large language models, chain-of-thought prompting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sachin Pawar, Girish Keshav Palshikar, Anindita Sinha Banerjee, Nitin Ramrakhiyani, Basit Ali"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TCS Research, Tata Consultancy Services Limited"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21902",children:"https://arxiv.org/pdf/2512.21902"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AoS, an attention-based supervised model using sentence transformers for explainable statute prediction. 2. Proposes LLMPrompt, a zero-shot method using large language models with standard and Chain-of-Thought prompting for prediction and explanation. 3. Evaluates both prediction performance and explanation quality across two datasets using automated and human evaluation methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0a88ac95c85beb5da693179a57fced56221862f49b2b3f82a7923e814deb844_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0a88ac95c85beb5da693179a57fced56221862f49b2b3f82a7923e814deb844_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of automatically predicting relevant legal statutes from case descriptions and providing human-understandable explanations. It proposes two methods: a supervised attention-based model (AoS) and a zero-shot LLM prompting approach (LLMPrompt). The study compares their prediction performance against baselines and evaluates the quality of the generated explanations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Explainable Statute Prediction via Attention-based Model and LLM Prompting<br>\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u578b\u548cLLM\u63d0\u793a\u7684\u53ef\u89e3\u91ca\u6cd5\u89c4\u9884\u6d4b"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Automatic prediction of relevant statutes from case descriptions with explanations<br>\u4ece\u6848\u4f8b\u63cf\u8ff0\u4e2d\u81ea\u52a8\u9884\u6d4b\u76f8\u5173\u6cd5\u89c4\u5e76\u63d0\u4f9b\u89e3\u91ca"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Two proposed techniques: AoS (supervised attention) and LLMPrompt (zero-shot LLM prompting)<br>\u4e24\u79cd\u65b9\u6cd5: AoS(\u76d1\u7763\u6ce8\u610f\u529b)\u548cLLMPrompt(\u96f6\u6837\u672cLLM\u63d0\u793a)"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Comparison of prediction performance and evaluation of explanation quality<br>\u6bd4\u8f83\u9884\u6d4b\u6027\u80fd\u5e76\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Accelerate Speculative Decoding with Sparse Computation in Verification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, sparse computation, verification stage, mixture-of-experts (MoE), efficiency-accuracy trade-off]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jikai Wang, Jianchao Tan, Yuxuan Hu, Jiayu Qin, Yerui Sun, Yuchen Xie, Xunliang Cai, Juntao Li, Min Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Soochow University, Meituan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21911",children:"https://arxiv.org/pdf/2512.21911"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Systematically analyzes and identifies structured computational redundancy across attention, FFN, and MoE components during the verification stage of speculative decoding. 2. Proposes a sparse verification framework that jointly sparsifies these components to reduce the dominant computation cost. 3. Introduces an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without additional training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0540488d64a1ff85171c147d2e74adf0477a0d2787eadf86a76357c955ab86be_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0540488d64a1ff85171c147d2e74adf0477a0d2787eadf86a76357c955ab86be_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck in the verification stage of speculative decoding for LLMs, especially for long-context and MoE models. It proposes a framework that applies sparse computation techniques to the verification stage and employs a retrieval reuse strategy to reduce redundant calculations. Experiments show the method achieves a favorable efficiency-accuracy trade-off while maintaining stable acceptance length."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Accelerate Speculative Decoding with Sparse Computation in Verification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9a8c\u8bc1\u9636\u6bb5\u6210\u4e3a\u74f6\u9888/Verification stage is bottleneck]\n    B1 --\x3e B2[\u957f\u4e0a\u4e0b\u6587\u4e0eMoE\u6a21\u578b/Long-context & MoE models]\n    C --\x3e C1[\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6/Sparse Verification Framework]\n    C1 --\x3e C2[\u8054\u5408\u7a00\u758f\u5316\u6ce8\u610f\u529b\u3001FFN\u3001MoE/Jointly sparsifies Attention, FFN, MoE]\n    C1 --\x3e C3[\u68c0\u7d22\u91cd\u7528\u7b56\u7565/Retrieval Reuse Strategy]\n    D --\x3e D1[\u6709\u5229\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861/Favorable efficiency-accuracy trade-off]\n    D --\x3e D2[\u7a33\u5b9a\u7684\u63a5\u53d7\u957f\u5ea6/Stable acceptance length]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] SWE-RM: Execution-free Feedback For Software Engineering Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [se], [software engineering agents], [reward model, test-time scaling, reinforcement learning, mixture-of-experts, SWE-Bench]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," KaShun Shum, Binyuan Hui, Jiawei Chen, Lei Zhang, X. W., Jiaxi Yang, Yuzhen Huang, Junyang Lin, Junxian He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong University of Science and Technology, Alibaba Group (Qwen Team)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21919",children:"https://arxiv.org/pdf/2512.21919"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified that high TTS performance does not guarantee effective RL training, and introduced classification accuracy and calibration as crucial metrics for robust reward models. 2. Conducted comprehensive experiments to analyze factors (data scale, policy mixtures, data source) impacting reward model training for SWE agents. 3. Proposed SWE-RM, a large-scale mixture-of-experts reward model that significantly improves agent performance on both TTS and RL, achieving new SOTA on SWE-Bench Verified."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcb1e3e885f771ebf5ee27ef70da96ceb4c030de77fdee247afd5d854761a72f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcb1e3e885f771ebf5ee27ef70da96ceb4c030de77fdee247afd5d854761a72f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitations of execution-based feedback for software engineering agents by proposing an execution-free reward model. It introduces SWE-RM, a robust reward model trained with insights from controlled experiments, which substantially improves agent performance on both test-time scaling and reinforcement learning, setting a new state-of-the-art on the SWE-Bench benchmark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["SWE-RM: Execution-free Feedback For Software Engineering Agents"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u6267\u884c\u53cd\u9988\u7684\u5c40\u9650\u6027/Limitations of Execution-based Feedback"]\n    Problem --\x3e P2["\u65e0\u6267\u884c\u53cd\u9988\u672a\u88ab\u5145\u5206\u63a2\u7d22/Execution-free Feedback Underexplored"]\n    Method --\x3e M1["\u8bc6\u522bRL\u5173\u952e\u6307\u6807/Identify Key RL Metrics (Accuracy, Calibration)"]\n    Method --\x3e M2["\u53ef\u63a7\u5b9e\u9a8c\u5206\u6790/Controlled Experiments on Training Factors"]\n    Method --\x3e M3["\u63d0\u51faSWE-RM\u6a21\u578b/Propose SWE-RM (MoE Reward Model)"]\n    Results --\x3e R1["\u63d0\u5347TTS\u6027\u80fd/Improves TTS Performance (e.g., Qwen3-Coder-Max to 74.6%)"]\n    Results --\x3e R2["\u63d0\u5347RL\u6027\u80fd/Improves RL Performance (+3 points)"]\n    Results --\x3e R3["\u5f00\u6e90\u6a21\u578bSOTA/New SOTA Among Open-Source Models"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [tokenization], [tokenization penalty, large language models, byte-pair encoding, vocabulary size, natural word splitting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sachin Pawar, Manoj Apte, Kshitij Jadhav, Girish Keshav Palshikar, Nitin Ramrakhiyani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TCS Research, Tata Consultancy Services Limited"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21933",children:"https://arxiv.org/pdf/2512.21933"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the hypothesis that breaking natural words into multiple tokens negatively impacts LLM performance on NLP tasks. 2. Introduces a set of penalty functions to quantify the "badness" of tokenization for a given text and LLM. 3. Establishes the statistical significance of the hypothesis across multiple NLP tasks and different LLMs.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/015399929e56260633eb709a56e41948acd324ce4065b0fcb286daa6d5ea6e33_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/015399929e56260633eb709a56e41948acd324ce4065b0fcb286daa6d5ea6e33_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how tokenization, specifically the splitting of natural words into multiple sub-tokens due to limited vocabulary, affects the performance of Large Language Models (LLMs). The authors propose penalty functions to measure this tokenization effect and demonstrate its statistically significant negative impact on various NLP tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u5206\u8bcd\u5c06\u81ea\u7136\u8bcd\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u8bcd/LLM tokenization splits natural words into multiple sub-tokens]\n    B --\x3e B2[\u5047\u8bbe\u8fd9\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd/Hypothesized to hurt model performance]\n    C --\x3e C1[\u63d0\u51fa\u91cf\u5316\u5206\u8bcd\u5f71\u54cd\u7684\u60e9\u7f5a\u51fd\u6570/Propose penalty functions to quantify tokenization effect]\n    D --\x3e D1[\u5728\u591a\u4efb\u52a1\u548c\u591a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5047\u8bbe\u7684\u663e\u8457\u6027/Validate hypothesis significance on multiple tasks & models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Self-attention vector output similarities reveal how machines pay attention"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [attention mechanisms], [self-attention, BERT, attention heads, vector similarity, token representation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tal Halevi, Yarden Tzach, Ronit D. Gross, Shalom Rosner, Ido Kanter"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bar-Ilan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21956",children:"https://arxiv.org/pdf/2512.21956"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a new method for quantifying information processing within the self-attention mechanism using a context similarity matrix derived from token vectors. 2. Revealed that different attention heads specialize in distinct linguistic features, such as identifying token repetitions or common contextual tokens. 3. Demonstrated a progression from long-range to short-range token similarities across layers, culminating in a focus on intra-sentence relationships and unique token-centric similarity patterns in final layers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ef3aef5be4139745b138137e89a3f21de053bd91a9fedbe345ad6f90900a98b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ef3aef5be4139745b138137e89a3f21de053bd91a9fedbe345ad6f90900a98b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel approach to analyze the self-attention mechanism in transformer models by examining vector output similarities. The analysis on BERT-12 shows that attention heads specialize in different linguistic features and that similarity patterns evolve from long-range to short-range, focusing on sentence-level structures in deeper layers."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Self-attention vector output similarities reveal how machines pay attention<br/>\u81ea\u6ce8\u610f\u529b\u5411\u91cf\u8f93\u51fa\u76f8\u4f3c\u6027\u63ed\u793a\u673a\u5668\u5982\u4f55\u5173\u6ce8"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Quantitative characterization of self-attention learning process<br/>\u81ea\u6ce8\u610f\u529b\u5b66\u4e60\u8fc7\u7a0b\u7684\u5b9a\u91cf\u8868\u5f81"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Context similarity matrix from self-attention head vectors<br/>\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u5934\u5411\u91cf\u7684\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u77e9\u9635"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Heads specialize linguistically; similarity shifts from long to short range<br/>\u5934\u90e8\u8bed\u8a00\u4e13\u4e1a\u5316;\u76f8\u4f3c\u6027\u4ece\u957f\u7a0b\u8f6c\u5411\u77ed\u7a0b"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [AI Governance & Compliance], [lifecycle management, bias detection, differential privacy, federated learning, terminology drift]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sunil Arora, John Hastings"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dakota State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22060",children:"https://arxiv.org/pdf/2512.22060"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy)."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Context as a Tool: Context Management for Long-Horizon SWE-Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [context management, long-horizon reasoning, SWE-agents, trajectory compression, structured workspace]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, Bryan Dai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, University of Manchester, Ubiquant"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22087",children:"https://arxiv.org/pdf/2512.22087"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CAT, a new paradigm that treats context management as an integrated, callable tool for agents, enabling proactive control. 2. Introduces a structured context workspace with stable semantics, condensed long-term memory, and high-fidelity short-term interactions. 3. Presents CAT-GENERATOR, a trajectory-level supervision framework for training the SWE-Compressor model, which achieves state-of-the-art performance on SWE-Bench-Verified."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554c080cfd26463c6d73be16144f677075ea893401e3c8ae26ee7321c48b2be8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554c080cfd26463c6d73be16144f677075ea893401e3c8ae26ee7321c48b2be8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of context explosion and semantic drift in long-horizon software engineering agents by proposing CAT, a paradigm that integrates proactive context management as a tool. It introduces a structured workspace and a training framework to produce the SWE-Compressor model. Experiments show this model significantly outperforms existing baselines on a software engineering benchmark while maintaining stable reasoning under a bounded context budget."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Context as a Tool: Context Management for Long-Horizon SWE-Agents] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u4ee3\u7406\u4e0a\u4e0b\u6587\u7206\u70b8\u3001\u8bed\u4e49\u6f02\u79fb/Existing agents suffer context explosion & semantic drift]\n    C --\x3e C1[CAT: \u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177/CAT: Context management as a callable tool]\n    C --\x3e C2[\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u4f5c\u533a/Structured context workspace]\n    C --\x3e C3[CAT-GENERATOR \u8bad\u7ec3\u6846\u67b6/CAT-GENERATOR training framework]\n    D --\x3e D1[SWE-Compressor \u8fbe\u5230 57.6% \u89e3\u51b3\u7387/SWE-Compressor achieves 57.6% solved rate]\n    D --\x3e D2[\u663e\u8457\u4f18\u4e8e\u57fa\u51c6/Significantly outperforms baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chiwun Yang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22088",children:"https://arxiv.org/pdf/2512.22088"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of \u0398(C^{-1/6}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Scaling Law\u7406\u8bba\u539f\u7406\u4e0d\u6e05 / Poorly understood theoretical underpinnings of scaling laws]\n    C --\x3e C1[\u5f62\u5f0f\u5316\u5b66\u4e60\u52a8\u6001\u4e3aODE\u7cfb\u7edf / Formalize learning dynamics as ODE system]\n    C --\x3e C2[\u8fd1\u4f3c\u4e3a\u6838\u884c\u4e3a / Approximate to kernel behaviors]\n    C --\x3e C3[\u5206\u6790SGD\u8bad\u7ec3\u771f\u5b9eTransformer / Analyze SGD training for real transformers]\n    D --\x3e D1[\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\u4e0e\u76f8\u53d8 / Upper bound on excess risk with phase transition]\n    D --\x3e D2[\u4f18\u5316\u76f8:\u6307\u6570\u8870\u51cf / Optimization phase: Exponential decay]\n    D --\x3e D3[\u7edf\u8ba1\u76f8:\u5e42\u5f8b\u8870\u51cf \u0398(C^{-1/6}) / Statistical phase: Power-law decay \u0398(C^{-1/6})]\n    D --\x3e D4[\u5206\u79bb\u7684\u89c4\u6a21\u5b9a\u5f8b / Isolated scaling laws for model size, time, data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [benchmark construction], [Turkish NLU benchmark, semi-automated annotation, sentiment analysis dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Duygu Altinok"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Independent Researcher"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22100",children:"https://arxiv.org/pdf/2512.22100"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Introducing TrGLUE and SentiTurca] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u571f\u8033\u5176\u8bed\u7efc\u5408\u57fa\u51c6/Lack of Turkish NLU Benchmark]\n    C --\x3e C1[\u534a\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b/Semi-automated Pipeline]\n    C1 --\x3e C2[LLM\u6807\u6ce8 + \u4ea4\u53c9\u9a8c\u8bc1 + \u4eba\u5de5\u6821\u9a8c/LLM Annotation + Cross-check + Human Validation]\n    D --\x3e D1[\u53d1\u5e03TrGLUE & SentiTurca/Release TrGLUE & SentiTurca]\n    D --\x3e D2[\u63d0\u4f9b\u4ee3\u7801\u4e0e\u8d44\u6e90/Provide Code & Resources]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Minnesota"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22101",children:"https://arxiv.org/pdf/2512.22101"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[A2P-Vis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u74f6\u9888/Gaps in automating data science]\n    B1 --\x3e B2[\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u53ef\u89c6\u5316/Generating insightful visual evidence]\n    B1 --\x3e B3[\u7ec4\u88c5\u6210\u4e13\u4e1a\u62a5\u544a/Assembling coherent professional report]\n    C --\x3e C1[\u4e24\u90e8\u5206\u591a\u667a\u80fd\u4f53\u7ba1\u9053/Two-part multi-agent pipeline]\n    C1 --\x3e C2[\u6570\u636e\u5206\u6790\u5668/Data Analyzer]\n    C2 --\x3e C3[\u751f\u6210\u5e76\u8bc4\u4f30\u56fe\u8868\u4e0e\u6d1e\u5bdf/Generates & evaluates charts & insights]\n    C1 --\x3e C4[\u62a5\u544a\u5448\u73b0\u5668/Presenter]\n    C4 --\x3e C5[\u7f16\u6392\u4e3b\u9898\u5e76\u64b0\u5199\u53d9\u8ff0/Orders topics & composes narrative]\n    D --\x3e D1[\u7aef\u5230\u7aef\u534f\u540c\u5206\u6790/End-to-end co-analysis]\n    D1 --\x3e D2[\u63d0\u9ad8\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\u7684\u5b9e\u7528\u6027/Improves usefulness of automated analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal reasoning], [vision-language models, spurious correlations, information bottleneck, reinforcement learning, modular reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22183",children:"https://arxiv.org/pdf/2512.22183"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Unbiased Visual Reasoning with Controlled Visual Inputs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[VLMs exploit spurious correlations/VLMs\u5229\u7528\u865a\u5047\u5173\u8054]\n    C --\x3e C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: \u89e3\u8026\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u6a21\u5757\u5316\u6846\u67b6]\n    C1 --\x3e C2[Frozen VLM sensor + LLM reasoner/\u51bb\u7ed3VLM\u611f\u77e5\u5668 + LLM\u63a8\u7406\u5668]\n    C2 --\x3e C3[Train with RL (GRPO)/\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60(GRPO)\u8bad\u7ec3]\n    D --\x3e D1[Improved robustness on SpuriVerse/\u5728SpuriVerse\u4e0a\u9c81\u68d2\u6027\u63d0\u5347]\n    D --\x3e D2[Competitive on MMVP & SeedBench/\u5728MMVP & SeedBench\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b]\n    D --\x3e D3[Transferable & interpretable/\u53ef\u8fc1\u79fb\u4e14\u53ef\u89e3\u91ca]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Convolutional Neural Network, SHAP, LIME, Saliency Maps, Malaria Diagnosis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md. Ismiel Hossen Abir, Awolad Hossain"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Department of Computer Science & Engineering, International Standard University, Dhaka, Bangladesh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22205",children:"https://arxiv.org/pdf/2512.22205"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a custom CNN model for automated malaria diagnosis from blood cell images, achieving high accuracy (96%). 2. Compares the performance of the custom CNN with established deep learning architectures like ResNet50 and VGG16. 3. Enhances model interpretability for clinical trust by applying Explainable AI techniques, including SHAP, LIME, and Saliency Maps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a deep learning-based system using a custom Convolutional Neural Network (CNN) to automatically diagnose malaria from blood cell images, achieving high accuracy. It compares this model against several established architectures and applies Explainable AI (XAI) techniques like SHAP and LIME to make the model's decisions interpretable. The study concludes that this approach can provide a quick, accurate, and understandable diagnostic tool, particularly valuable in resource-limited settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional microscopic diagnosis is slow, subjective, and resource-intensive."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: A custom CNN model for classification, compared with other architectures, enhanced with SHAP, LIME, and Saliency Maps for explainability."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves 96% accuracy, high precision/recall, providing a fast and interpretable diagnostic tool."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22208",children:"https://arxiv.org/pdf/2512.22208"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/moxin-org/Moxin-LLM",children:"https://github.com/moxin-org/Moxin-LLM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Open-Source Multimodal Moxin Models<br/>\u5f00\u6e90\u591a\u6a21\u6001Moxin\u6a21\u578b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Proprietary vs. Open-Source LLMs<br/>\u95ed\u6e90\u4e0e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b]\n    B --\x3e B2[Need for transparent, capable open models<br/>\u9700\u8981\u900f\u660e\u3001\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b]\n    C --\x3e C1[Develop Moxin 7B under Model Openness Framework<br/>\u57fa\u4e8e\u6a21\u578b\u5f00\u653e\u6846\u67b6\u5f00\u53d1Moxin 7B]\n    C --\x3e C2[Create variants: VLM, VLA, Chinese<br/>\u521b\u5efa\u53d8\u4f53: VLM, VLA, \u4e2d\u6587\u6a21\u578b]\n    D --\x3e D1[Superior performance in evaluations<br/>\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02]\n    D --\x3e D2[Full release of models, code, data<br/>\u5b8c\u6574\u53d1\u5e03\u6a21\u578b\u3001\u4ee3\u7801\u3001\u6570\u636e]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Imperial College London, UnlikelyAI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22213",children:"https://arxiv.org/pdf/2512.22213"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and characterizes a new class of "secondary attention sinks" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink\'s direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper identifies a new phenomenon called "secondary attention sinks" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["On the Existence and Behaviour of Secondary Attention Sinks<br/>\u4e8c\u6b21\u6ce8\u610f\u529b\u6c47\u7684\u5b58\u5728\u4e0e\u884c\u4e3a"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>\u5148\u524d\u7814\u7a76\u4ec5\u5173\u6ce8\u6301\u4e45\u7684\u4e3b\u6c47\uff08\u5982BOS\uff09"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Extensive experiments across 11 model families<br/>\u5bf911\u4e2a\u6a21\u578b\u7cfb\u5217\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>\u6b21\u7ea7\u6c47\u901a\u8fc7\u4e2d\u95f4\u5c42MLP\u5f62\u6210<br/>2. L2-norm determines sink score & lifetime<br/>L2\u8303\u6570\u51b3\u5b9a\u6c47\u5206\u6570\u4e0e\u5bff\u547d<br/>3. Sink levels are deterministic in large models<br/>\u5927\u6a21\u578b\u4e2d\u6c47\u5c42\u7ea7\u66f4\u786e\u5b9a"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Ant Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22226",children:"https://arxiv.org/pdf/2512.22226"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zheng980629/VideoScaffold",children:"https://github.com/zheng980629/VideoScaffold"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [representation analysis], [sentence embeddings, probing, hierarchical geometry, transformer models, cognitive states]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sophie Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22227",children:"https://arxiv.org/pdf/2512.22227"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed a novel dataset of 480 sentences annotated with continuous energy scores and discrete tier labels for seven ordered cognitive categories. 2. Demonstrated that both continuous scores and discrete tier labels are reliably decodable from fixed transformer sentence embeddings using linear and nonlinear probes, with nonlinear probes providing consistent gains. 3. Provided statistical and qualitative evidence (via permutation tests, UMAP visualizations, and confusion matrices) that the embedding space exhibits a hierarchical geometric organization aligned with human-defined cognitive attributes, beyond surface word statistics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether transformer-based sentence embeddings encode a hierarchical structure aligned with cognitive states. The authors construct an annotated dataset and use linear and nonlinear probes to decode continuous scores and discrete labels from embeddings, finding reliable recoverability and a structured geometric gradient. The results show that transformer embedding spaces exhibit a systematic organization corresponding to interpretable cognitive attributes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Do sentence embeddings encode hierarchical cognitive structure?")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Probe analysis on annotated dataset with linear/nonlinear classifiers")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Reliable decoding, hierarchical geometry aligned with cognitive attributes")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language model safety], [sparse autoencoder, feature orthogonalization, stealth slip, pragmatic interpretation, statistical co-occurrence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tsogt-Ochir Enkhbayar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Mongol-AI (inferred from email domain)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22293",children:"https://arxiv.org/pdf/2512.22293"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Empirically demonstrates that warning-framed training data fails to teach language models to avoid warned-against behaviors, showing generation rates similar to direct exposure. 2. Provides a mechanistic interpretation using sparse autoencoders, identifying a failure of feature orthogonalization where "describing" and "performing" an action activate overlapping latent features. 3. Identifies and names the "stealth slip" phenomenon, where conversational preambles can rotate activations into subspaces undetectable by linear probes, and shows that training-time feature ablation, not prompting, is required to address the issue.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates why language models trained on warning-framed examples (e.g., "DO NOT USE") still learn to generate the warned-against content. Through behavioral experiments and sparse autoencoder analysis, it finds that models learn statistical co-occurrences rather than pragmatic intent, due to overlapping latent features for description and action. The core conclusion is that current architectures prioritize pattern completion over understanding speaker intent, requiring training-time interventions like feature ablation for correction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Do warning-framed examples teach models to avoid bad behavior?")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Behavioral analysis & Sparse Autoencoder mechanistic interpretability")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: No. Models learn statistical co-occurrence, not pragmatic intent.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Tencent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22322",children:"https://arxiv.org/pdf/2512.22322"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://huggingface.co/collections/yolay/smartsnap",children:"https://huggingface.co/collections/yolay/smartsnap"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n    D[\u5173\u952e\u7ed3\u679c/Results: Performance gains up to 26.08%; competitive with larger models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [evaluation & benchmarking], [scientific intelligence, multimodal reasoning, benchmarking toolkit]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Artificial Intelligence Laboratory"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22334",children:"https://arxiv.org/pdf/2512.22334"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/InternScience/SciEvalKit",children:"https://github.com/InternScience/SciEvalKit"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces SciEvalKit, a unified, open-source toolkit for evaluating AI models across a broad range of scientific disciplines and core scientific intelligence capabilities. 2. Provides a flexible and extensible evaluation pipeline supporting batch evaluation, custom model/dataset integration, and ensuring transparent, reproducible results. 3. Curates expert-grade scientific benchmarks from real-world, domain-specific datasets to reflect authentic scientific challenges across six major domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across multiple disciplines and core competencies like multimodal reasoning and code generation. It provides a flexible, extensible pipeline for reproducible evaluation and is built on expert-grade, real-world scientific benchmarks. The toolkit is open-sourced to foster community-driven development in AI for science."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Lack of specialized evaluation for scientific AI across diverse disciplines and capabilities]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified benchmarking toolkit with flexible pipeline, real-world benchmarks, and support for six scientific domains]\n    D[\u5173\u952e\u7ed3\u679c/Results: Open-source toolkit enabling standardized, reproducible evaluation of scientific foundation models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [symbolic world models, multi-agent feedback, PDDL, adaptive testing, supervised fine-tuning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengkang Hu, Bowei Xia, Yuran Wu, Ailing Yu, Yude Zou, Qiguang Chen, Shijian Wang, Jiarui Jin, Kexin Li, Wenxiang Jiao, Yuan Lu, Ping Luo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Xiaohongshu Inc., UESTC, Harbin Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22336",children:"https://arxiv.org/pdf/2512.22336"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," agent2world.github.io"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Agent2World, a tool-augmented multi-agent framework for generating symbolic world models via adaptive multi-agent feedback. 2. Introduced a three-stage pipeline with specialized agents (Deep Researcher, Model Developer, Testing Team) for knowledge synthesis, implementation, and behavior-aware validation. 3. Demonstrated that the framework not only achieves state-of-the-art inference-time performance but also serves as a data engine for supervised fine-tuning, leading to substantial model improvement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating correct symbolic world models (like PDDL domains) from natural language by proposing Agent2World, a multi-agent framework that uses adaptive feedback for validation and repair. The method outperforms existing approaches on benchmarks and the feedback collected also enables effective supervised fine-tuning, significantly improving model performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Lack of verifiable supervision for training LLMs to generate behaviorally correct symbolic world models]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Tool-augmented multi-agent framework with three-stage pipeline: Deep Researcher, Model Developer, and Testing Team for adaptive feedback]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA inference-time performance; Framework serves as data engine for fine-tuning, yielding ~31% average relative gain]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [generative syntax], [qulk-clauses, Minimalist Program, biclausal structures, bipartite negation, Morphological Merger]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zubaida Mohammed Albadani, Mohammed Q. Shormani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Qalam University, Ibb University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22376",children:"https://arxiv.org/pdf/2512.22376"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes that qulk-clauses in Yemeni Ibbi Arabic are biclausal structures where 'qulk' functions as a clause-embedding predicate selecting a full CP complement. 2. Provides a syntactic derivation using core Minimalist operations (Merge, Move, Agree, Spell-out) and post-syntactic processes like Morphological Merger. 3. Accounts for dialect-specific syntactic phenomena such as bipartite negation and cliticization within the Minimalist framework."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac0d347b0a0aa0409fd5577dddb1a9b52a8488a6784d65d923c9b923a8a1edb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac0d347b0a0aa0409fd5577dddb1a9b52a8488a6784d65d923c9b923a8a1edb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the syntax of 'qulk'-clauses in Yemeni Ibbi Arabic using the Minimalist Program, proposing they are biclausal structures derived through operations like Merge and Move. It concludes that the analysis accounts for dialect-specific features and supports the theoretical universality of minimalist syntax."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5206\u6790qulk-\u4ece\u53e5\u7684\u53e5\u6cd5\u7ed3\u6784/Analyze syntax of qulk-clauses]\n    C --\x3e C1[\u4f7f\u7528\u6700\u7b80\u65b9\u6848\u4e0e\u64cd\u4f5c/Use Minimalist Program & operations]\n    C --\x3e C2[\u53cc\u5c42\u7ed3\u6784\u5206\u6790/Biclausal structure analysis]\n    D --\x3e D1[\u89e3\u91ca\u65b9\u8a00\u7279\u5f81/Account for dialect features]\n    D --\x3e D2[\u652f\u6301\u6700\u7b80\u65b9\u6848\u666e\u9002\u6027/Support universality of minimalism]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Towards Efficient Post-Training via Fourier-Driven Adapter Architectures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [parameter-efficient fine-tuning], [Fourier-Activated Adapter, random Fourier features, frequency-aware activation, parameter-efficient fine-tuning, spectral sparsity]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Donggyun Bae, Jongil Park"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Konkuk University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22378",children:"https://arxiv.org/pdf/2512.22378"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Fourier-Activated Adapter (FAA), a novel PEFT framework that integrates random Fourier features to decompose representations into frequency components. 2. Introduces a dynamic, frequency-aware activation mechanism to selectively modulate semantic information across different frequency bands. 3. Demonstrates through extensive experiments that FAA achieves competitive or superior performance on multiple benchmarks while maintaining low computational overhead."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes the Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method for large language models that uses random Fourier features to enable frequency-aware modulation of semantic representations. Experiments on GLUE and other benchmarks show that FAA achieves strong performance with low computational cost, highlighting the effectiveness of its frequency-based approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Efficient Post-Training via Fourier-Driven Adapter Architectures] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709PEFT\u65b9\u6cd5\u96be\u4ee5\u6355\u83b7\u9ad8\u9891\u8bed\u4e49\u4fe1\u606f / Existing PEFT methods struggle to capture high-frequency semantic information]\n    C --\x3e C1[\u63d0\u51fa\u5085\u91cc\u53f6\u6fc0\u6d3b\u9002\u914d\u5668(FAA) / Propose Fourier-Activated Adapter (FAA)]\n    C1 --\x3e C2[\u96c6\u6210\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u5206\u89e3\u8868\u793a / Integrate random Fourier features to decompose representations]\n    C2 --\x3e C3[\u4f7f\u7528\u9891\u7387\u611f\u77e5\u673a\u5236\u9009\u62e9\u6027\u8c03\u5236 / Use frequency-aware mechanism for selective modulation]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c / Achieves competitive results on multiple benchmarks]\n    D --\x3e D2[\u4fdd\u6301\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500 / Maintains low computational and memory overhead]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [few-shot learning], [exemplar selection, large language model, human activity recognition, facility-location optimization, PageRank]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Elsen Ronando, Sozo Inoue"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22385",children:"https://arxiv.org/pdf/2512.22385"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LLM-Guided Exemplar Selection for Few-Shot HAR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u4e0e\u51e0\u4f55\u9009\u62e9 / Reliance on large datasets & geometric selection]\n    B --\x3e B2[\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u6d3b\u52a8 / Hard to distinguish similar activities]\n    C --\x3e C1[LLM\u751f\u6210\u8bed\u4e49\u5148\u9a8c / LLM-generated semantic priors]\n    C --\x3e C2[\u7ed3\u5408\u591a\u7ebf\u7d22\u4f18\u5316 / Combine multiple cues for optimization]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf / Outperforms baselines (88.78% F1)]\n    D --\x3e D2[\u8bed\u4e49\u5148\u9a8c\u6709\u6548 / Semantic priors are effective]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Hallucination Detection and Evaluation of Large Language Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [HHEM, KnowHalu, segment-based retrieval, factual consistency, CDF analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chenggong Zhang, Haopeng Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22416",children:"https://arxiv.org/pdf/2512.22416"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Integrated the lightweight Hughes Hallucination Evaluation Model (HHEM) to significantly reduce computational cost and time for hallucination detection compared to multi-stage methods like KnowHalu. 2. Introduced a segment-based retrieval technique to improve the detection of localized hallucinations in summarization tasks, addressing a key limitation of HHEM. 3. Conducted a comparative CDF analysis revealing that larger LLMs (7B-9B parameters) exhibit fewer hallucinations, while intermediate-sized models show higher instability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a208bbdfd47e46de589a2306cd9e02976448bce48e5e81a002adcb6f30ec224d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a208bbdfd47e46de589a2306cd9e02976448bce48e5e81a002adcb6f30ec224d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of efficiently detecting hallucinations in Large Language Models. It proposes integrating the lightweight HHEM framework and a segment-based retrieval method, which together reduce evaluation time dramatically while maintaining high accuracy. The study concludes that larger models are generally more reliable and highlights the need for efficient yet robust evaluation frameworks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Hallucination Detection and Evaluation of Large Language Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u5e7b\u89c9\u635f\u5bb3\u53ef\u4fe1\u5ea6/LLM Hallucinations Undermine Trust]\n    C --\x3e C1[\u96c6\u6210\u8f7b\u91cf\u7ea7HHEM\u6846\u67b6/Integrate Lightweight HHEM Framework]\n    C --\x3e C2[\u5f15\u5165\u5206\u6bb5\u68c0\u7d22\u6280\u672f/Introduce Segment-based Retrieval]\n    D --\x3e D1[\u6548\u7387\u63d0\u5347: 8\u5c0f\u65f6->10\u5206\u949f/Efficiency Gain: 8hrs->10mins]\n    D --\x3e D2[\u6700\u9ad8\u51c6\u786e\u7387: 82.2%/Best Accuracy: 82.2%]\n    D --\x3e D3[\u5927\u6a21\u578b\u5e7b\u89c9\u66f4\u5c11/Larger Models Hallucinate Less]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Monadic Context Engineering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Monadic Context Engineering, Monad Transformers, Meta-Agents, computational contexts, algebraic structures]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Mengdi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22431",children:"https://arxiv.org/pdf/2512.22431"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/yifanzhang-pro/monadic-context-engineering",children:"https://github.com/yifanzhang-pro/monadic-context-engineering"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Monadic Context Engineering (MCE), a novel architectural paradigm using Functors, Applicatives, and Monads to provide a formal foundation for AI agent design. 2. Demonstrates how Monads and Applicatives manage sequential composition and parallel execution, and how Monad Transformers enable systematic composition of capabilities like state and error handling. 3. Extends the MCE framework to describe Meta-Agents for generative orchestration, dynamically creating and managing sub-agent workflows via metaprogramming."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the brittleness and complexity in current AI agent architectures by introducing Monadic Context Engineering (MCE), a paradigm that leverages algebraic structures like Monads to formally manage state, errors, and concurrency within agent workflows. The proposed method enables the construction of complex, resilient agents from simple, verifiable components and is extended to support generative orchestration via Meta-Agents. The work concludes that MCE provides a principled foundation for building robust and scalable autonomous agent systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Monadic Context Engineering"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u5f53\u524d\u4ee3\u7406\u67b6\u6784\u8106\u5f31/Current agent architectures are brittle"]\n    Problem --\x3e P2["\u72b6\u6001\u3001\u9519\u8bef\u3001\u5e76\u53d1\u7ba1\u7406\u56f0\u96be/Difficulties in state, error, concurrency management"]\n    Method --\x3e M1["\u5f15\u5165\u5355\u5b50\u4e0a\u4e0b\u6587\u5de5\u7a0b/Introduce Monadic Context Engineering (MCE)"]\n    Method --\x3e M2["\u5229\u7528\u51fd\u5b50\u3001\u5e94\u7528\u51fd\u5b50\u3001\u5355\u5b50/Leverage Functors, Applicatives, Monads"]\n    Method --\x3e M3["\u4f7f\u7528\u5355\u5b50\u53d8\u6362\u5668\u7ec4\u5408\u80fd\u529b/Use Monad Transformers to compose capabilities"]\n    Results --\x3e R1["\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840/Provides a formal foundation"]\n    Results --\x3e R2["\u652f\u6301\u6784\u5efa\u590d\u6742\u3001\u9c81\u68d2\u7684\u4ee3\u7406/Enables building complex, resilient agents"]\n    Results --\x3e R3["\u6269\u5c55\u81f3\u5143\u4ee3\u7406\u8fdb\u884c\u751f\u6210\u5f0f\u7f16\u6392/Extends to Meta-Agents for generative orchestration"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cattalyya Nuengsigkapian"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Google"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22442",children:"https://arxiv.org/pdf/2512.22442"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HiFi-RAG] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5f00\u653e\u57dfRAG\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u4e0e\u610f\u56fe\u5bf9\u9f50/Open-domain RAG faces irrelevant info & intent misalignment]\n    C --\x3e C1[\u5206\u5c42\u8fc7\u6ee4\u4e0e\u4e24\u9636\u6bb5\u751f\u6210/Hierarchical Filtering & Two-Pass Generation]\n    C1 --\x3e C2[\u4f7f\u7528Gemini Flash\u8fdb\u884c\u8fc7\u6ee4/Use Gemini Flash for filtering]\n    C1 --\x3e C3[\u4f7f\u7528Gemini Pro\u8fdb\u884c\u751f\u6210/Use Gemini Pro for generation]\n    D --\x3e D1[\u5728MMU-RAGent\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baseline on MMU-RAGent]\n    D --\x3e D2[\u5728\u81ea\u5b9a\u4e49\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u63d0\u5347/Substantial gains on custom test set]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [domain-specific reasoning], [vertical-domain reasoning, accounting reasoning, prompt engineering, GLM-series, evaluation criteria]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jie Zhou, Xin Chen, Jie Zhang, Zhe Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," School of Computer Engineering, Jiangsu Ocean University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22443",children:"https://arxiv.org/pdf/2512.22443"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces the concept of "vertical-domain accounting reasoning" and establishes corresponding evaluation criteria based on analyzing training data characteristics of GLM-series models. 2. Proposes a framework to evaluate the accounting reasoning capabilities of several representative LLMs (GLM-6B, GLM-130B, GLM-4, GPT-4) using different prompt engineering strategies. 3. Provides benchmarks and foundational insights for improving LLM performance in professional accounting scenarios, identifying GPT-4\'s superior capability and the gap to real-world enterprise application requirements.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b34441f407f7adb435b84980dce10bceeed5c1737baae9c574e35ada53430c91_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b34441f407f7adb435b84980dce10bceeed5c1737baae9c574e35ada53430c91_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This study investigates the domain-specific reasoning capabilities of Large Language Models (LLMs) in the accounting field. It establishes evaluation criteria for "vertical-domain accounting reasoning" and tests models like GLM-series and GPT-4 on accounting tasks using prompt engineering. The results show GPT-4 performs best, but all models still require further optimization to meet real-world enterprise accounting needs.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLMs\u4e0e\u4e13\u4e1a\u9886\u57df\u878d\u5408\u7684\u6311\u6218/Challenge of integrating LLMs with professional domains)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u5efa\u7acb\u5782\u76f4\u9886\u57df\u4f1a\u8ba1\u63a8\u7406\u8bc4\u4f30\u6846\u67b6/Establish vertical-domain accounting reasoning evaluation framework)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: GPT-4\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u9700\u4f18\u5316/GPT-4 performs best but requires further optimization)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [LoRA, Parameter-Efficient Fine-Tuning, Activation Function Annealing, Non-linear Adaptation, Model Merging]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meituan, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22455",children:"https://arxiv.org/pdf/2512.22455"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AFA-LoRA, a novel training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability., 2. Introduces an annealed activation function that transitions from non-linear to linear during training, enabling strong initial learning and final linear integration., 3. Demonstrates the method's effectiveness across multiple tasks, including supervised fine-tuning, reinforcement learning, and speculative decoding, reducing the performance gap with full-parameter training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limited expressive power of linear Low-Rank Adaptation (LoRA) by proposing AFA-LoRA, a method that uses an annealed activation function to enable non-linear training while ensuring the final adapter remains mergeable. This approach narrows the performance gap between LoRA and full-parameter fine-tuning across various tasks, offering a more powerful and practical parameter-efficient adaptation paradigm."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LoRA\u7ebf\u6027\u9002\u914d\u7684\u8868\u8fbe\u80fd\u529b\u6709\u9650<br>LoRA's linear adaptation limits expressive power]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f15\u5165\u9000\u706b\u6fc0\u6d3b\u51fd\u6570<br>Introduce annealed activation function]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u7f29\u5c0fLoRA\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u7684\u5dee\u8ddd<br>Reduces gap between LoRA and full-parameter training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Constituency Structure over Eojeol in Korean Treebanks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [syntactic parsing], [constituency parsing, treebank annotation, eojeol, Korean syntax, morphological segmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jungyeul Park, Chulwoo Park"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST, Anyang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22487",children:"https://arxiv.org/pdf/2512.22487"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),"  1. Argues for an eojeol-based constituency representation in Korean treebanks, separating morphological information into a non-constituent layer. 2. Shows that the Sejong and Penn Korean treebanks are representationally equivalent at the eojeol-based constituency level under explicit normalization. 3. Outlines an eojeol-based annotation scheme that supports cross-treebank comparison and constituency-dependency conversion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d48ef7fceed98f1ac78d435bdf24a8049dd0338b9040587dd433cea8b3ff74a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d48ef7fceed98f1ac78d435bdf24a8049dd0338b9040587dd433cea8b3ff74a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the representational issue of terminal units in Korean constituency treebanks. It proposes using eojeol (spacing units) as the constituency terminals while encoding morphology separately, and demonstrates that two major treebanks are equivalent under this scheme, enabling better comparison and conversion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Constituency Structure over Eojeol in Korean Treebanks<br>\u97e9\u8bed\u6811\u5e93\u4e2d\u57fa\u4e8eEojeol\u7684\u9009\u533a\u7ed3\u6784"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Korean treebank terminal unit choice<br>\u97e9\u8bed\u6811\u5e93\u7ec8\u7aef\u5355\u5143\u9009\u62e9"] --\x3e P1["\u5f62\u6001\u4e0e\u53e5\u6cd5\u6df7\u6dc6<br>Morphology-syntax conflation"]\n    Problem --\x3e P2["\u4e0e\u4f9d\u5b58\u8d44\u6e90\u4e0d\u5339\u914d<br>Mismatch with dependency resources"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Eojeol-based constituency<br>\u57fa\u4e8eEojeol\u7684\u9009\u533a\u8868\u793a"] --\x3e M1["\u4ee5Eojeol\u4e3a\u7ec8\u7aef<br>Eojeol as terminals"]\n    Method --\x3e M2["\u5f62\u6001\u4fe1\u606f\u5355\u72ec\u7f16\u7801<br>Morphology in separate layer"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Treebank equivalence & scheme<br>\u6811\u5e93\u7b49\u4ef7\u6027\u4e0e\u65b9\u6848"] --\x3e R1["Sejong\u4e0ePenn\u6811\u5e93\u7b49\u4ef7<br>Sejong & Penn equivalence"]\n    Results --\x3e R2["\u652f\u6301\u8de8\u5e93\u6bd4\u8f83\u4e0e\u8f6c\u6362<br>Supports comparison & conversion"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [speech synthesis], [flow matching, hierarchical attention, low-resource TTS, agglutinative language, non-autoregressive generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Suhua Wang, Zifan Wang, Xiaoxin Sun, D. J. Wang, Zhanbo Liu, Xin Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northeast Normal University, Changchun Humanities and Sciences College, Zhejiang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22491",children:"https://arxiv.org/pdf/2512.22491"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel hierarchical text representation and cross-modal attention mechanism to handle Manchu's agglutinative phonology. 2. Introduces an end-to-end speech synthesis model integrating deep convolutional networks with a flow-matching Transformer for efficient, non-autoregressive generation. 3. Constructs the first public Manchu TTS dataset and employs data augmentation to address severe data scarcity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ManchuTTS, a novel text-to-speech system designed for the endangered and agglutinative Manchu language. The method uses a three-tier text representation and a flow-matching Transformer with hierarchical guidance to tackle data scarcity and complex phonology. Experiments show it achieves a high MOS score of 4.52 and significantly improves pronunciation accuracy and prosodic naturalness compared to baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ManchuTTS: \u6ee1\u8bed\u8bed\u97f3\u5408\u6210] --\x3e B1(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e B2(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e B3(\u5173\u952e\u7ed3\u679c/Results)\n    B1 --\x3e C1[\u6570\u636e\u7a00\u7f3a/Data Scarcity]\n    B1 --\x3e C2[\u7c98\u7740\u8bed\u8bed\u97f3\u5b66/Agglutinative Phonology]\n    B2 --\x3e D1[\u4e09\u5c42\u6587\u672c\u8868\u793a/Three-tier Text Representation]\n    B2 --\x3e D2[\u6d41\u5339\u914dTransformer/Flow-matching Transformer]\n    B2 --\x3e D3[\u5206\u5c42\u5bf9\u6bd4\u635f\u5931/Hierarchical Contrastive Loss]\n    B3 --\x3e E1[MOS\u5f97\u52064.52/MOS Score 4.52]\n    B3 --\x3e E2[AWPA\u63d0\u534731%/AWPA +31%]\n    B3 --\x3e E3[\u97f5\u5f8b\u81ea\u7136\u5ea6\u63d0\u534727%/Prosodic Naturalness +27%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Learning When Not to Attend Globally"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [All-or-Here Attention, sliding window attention, conditional computation, binary router, context dependency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xuan Luo, Kailai Zhang, Xifeng Yan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UC Santa Barbara"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22562",children:"https://arxiv.org/pdf/2512.22562"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes All-or-Here Attention (AHA), a novel attention mechanism that dynamically toggles between full and local sliding window attention using a binary router per head. 2. Demonstrates empirically that full attention is largely redundant, showing up to 93% of full attention operations can be replaced with local attention without performance loss. 3. Identifies a long-tail distribution in context dependency, revealing that the need for global context decays rapidly as the local window expands."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational inefficiency of full self-attention in LLMs by proposing All-or-Here Attention (AHA), which learns to dynamically switch between full and local sliding window attention for each token. The results show that most full attention operations are unnecessary, and efficient inference can be achieved with on-demand global context access."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Learning When Not to Attend Globally] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Quadratic complexity of full self-attention is inefficient]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: All-or-Here Attention (AHA) with binary router to toggle between full and local sliding window attention]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Up to 93% full attention replaced without loss; reveals long-tail context dependency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [aspect-based sentiment analysis], [structured prompting, llm ensembling, multimodal conversation, sentiment flipping, sentiment sextuple]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiqiang Gao, Shihao Gao, Zixing Zhang, Yihao Guo, Hongyu Chen, Jing Han"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hunan University, University of Cambridge"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22603",children:"https://arxiv.org/pdf/2512.22603"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A structured prompting pipeline for LLMs to sequentially extract complex sentiment sextuples (holder, target, aspect, opinion, sentiment, rationale) from multimodal dialogues. 2. An ensemble strategy leveraging three LLMs to robustly identify sentiment flipping (dynamic sentiment shifts) and their triggers. 3. Demonstrating the effectiveness of step-wise refinement and model ensembling for rich, multimodal sentiment analysis tasks, achieving competitive results on the MCABSA challenge."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d8e53db2395e8a03a0b4b7ac36c2aa4287b60b1a961f7cc83bcd52a3ef4fae4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d8e53db2395e8a03a0b4b7ac36c2aa4287b60b1a961f7cc83bcd52a3ef4fae4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA). It proposes a structured prompting pipeline for extracting sentiment sextuples and an LLM ensembling method for detecting sentiment flipping. The approach achieved strong results, demonstrating the effectiveness of these strategies for complex multimodal sentiment understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u7406\u89e3\u591a\u6a21\u6001\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f<br>Understanding Sentiment in Multimodal Conversations]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u7ed3\u6784\u5316\u63d0\u793a\u4e0eLLM\u96c6\u6210<br>Structured Prompting & LLM Ensembling]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Subtask-I: 47.38%<br>Subtask-II: 74.12% F1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [diffusion language model, vision-language-action, parallel generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Huawei Technologies"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22615",children:"https://arxiv.org/pdf/2512.22615"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Dream-VL, a state-of-the-art open diffusion-based Vision-Language Model (dVLM) that matches top AR-based VLMs on benchmarks and excels at visual planning. 2. Introduces Dream-VLA, a diffusion-based Vision-Language-Action model built upon Dream-VL, leveraging the bidirectional nature of diffusion for superior action chunking and faster fine-tuning convergence. 3. Demonstrates that diffusion-based VLMs/VLAs outperform autoregressive baselines on downstream tasks, achieving top-tier performance on robotic benchmarks like LIBERO, SimplerEnv-Bridge, and SimplerEnv-Fractal."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes building Vision-Language and Vision-Language-Action models on diffusion-based language models to overcome the limitations of autoregressive models in complex planning and control. The introduced models, Dream-VL and Dream-VLA, leverage the bidirectional, parallel generation nature of diffusion for superior performance in visual planning and robotic tasks, achieving state-of-the-art results on key benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Dream-VL & Dream-VLA<br>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[AR\u6a21\u578b\u5728\u89c6\u89c9\u89c4\u5212\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u5c40\u9650<br>\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6784\u5efaVLM\u548cVLA\u6a21\u578b<br>\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd<br>\u5173\u952e\u7ed3\u679c/Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Chain-of-thought Reviewing and Correction for Time Series Question Answering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [time series question answering], [chain-of-thought, multi-step reasoning, self-correction, LLM fine-tuning, time series analysis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chen Su, Yuanhe Tian, Yan Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Zhongguancun Institute of Artificial Intelligence"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22627",children:"https://arxiv.org/pdf/2512.22627"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/synlp/T3LLM",children:"https://github.com/synlp/T3LLM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes T3LLM, a novel framework that integrates a multi-agent LLM system (worker, reviewer, student) for chain-of-thought reasoning with an explicit correction mechanism for time series QA. 2. Leverages the inherent verifiability of time series data to enable consistency checking between reasoning steps and original input, allowing the reviewer to identify and correct errors. 3. Fine-tunes a student model using the collaboratively generated corrected reasoning chains, internalizing both multi-step reasoning and self-correction capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2399e517488f19982d1f0bdffd324837014399b83d0c64b4314235e4143c5a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2399e517488f19982d1f0bdffd324837014399b83d0c64b4314235e4143c5a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of reasoning errors in LLM-based time series question answering. It proposes T3LLM, a framework that uses three LLMs (worker, reviewer, student) to generate, review/correct, and learn from multi-step reasoning chains. Experiments show that T3LLM achieves state-of-the-art performance on multiple TSQA benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Chain-of-thought Reviewing and Correction for Time Series Question Answering] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs prone to reasoning errors on complex numerical time series data]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: T3LLM framework with worker, reviewer, student LLMs for CoT generation, review/correction, and fine-tuning]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art performance on real-world TSQA benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [se], [code generation evaluation], [multi-granularity evaluation, multilingual code generation, Group Relative Policy Optimization, contamination-controlled benchmark, fine-grained diagnosis]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fanglin Xu, Wei Zhang, Jian Yang, Guo Chen, Aishan Liu, Zhoujun Li, Xianglong Liu, Bryan Dai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Hunan University, Ubiquant"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22628",children:"https://arxiv.org/pdf/2512.22628"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced M2G-Eval, a novel multi-granularity and multilingual benchmark for evaluating code LLMs across four structural levels (Class, Function, Block, Line) and 18 programming languages. 2. Developed M2G-Eval-Coder models by fine-tuning Qwen3-8B with supervised fine-tuning and a novel Group Relative Policy Optimization method. 3. Conducted a comprehensive evaluation of 30 models, revealing key insights such as a difficulty hierarchy across granularities and evidence of transferable programming concepts across languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c666b8c3b59bf1276ef94dab61d81839bb62d98333e1c6d0052ca3f61fdebad_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c666b8c3b59bf1276ef94dab61d81839bb62d98333e1c6d0052ca3f61fdebad_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces M2G-Eval, a framework for evaluating code generation in large language models across multiple structural granularities and programming languages. The authors also develop enhanced models using the benchmark and conduct an extensive evaluation, finding a clear difficulty hierarchy among tasks and evidence that models learn transferable programming concepts across different languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u57fa\u51c6\u8bc4\u4f30\u5355\u4e00\u7c92\u5ea6\u4e0e\u6709\u9650\u8bed\u8a00 / Existing benchmarks assess single granularity & limited languages]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51fa\u591a\u7c92\u5ea6\u591a\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6 / Propose multi-granularity multilingual evaluation framework]\n    Method --\x3e M2[\u8bad\u7ec3M2G-Eval-Coder\u6a21\u578b / Train M2G-Eval-Coder models]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u96be\u5ea6\u5c42\u6b21: \u884c\u7ea7\u6700\u7b80\u5355, \u7c7b\u7ea7\u6700\u96be / Difficulty hierarchy: Line easiest, Class hardest]\n    Results --\x3e R2[\u6027\u80fd\u5dee\u8ddd\u968f\u590d\u6742\u5ea6\u589e\u52a0\u800c\u6269\u5927 / Performance gap widens with complexity]\n    Results --\x3e R3[\u8de8\u8bed\u8a00\u5f3a\u76f8\u5173, \u8868\u660e\u53ef\u8fc1\u79fb\u6982\u5ff5 / Strong cross-language correlation suggests transferable concepts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning and explainability], [Chain-of-Thought, Faithfulness, GRPO, DPO, LLM Alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hadi Mohammadi, Tamas Kozak, Anastasia Giachanou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Utrecht University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22631",children:"https://arxiv.org/pdf/2512.22631"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Evaluates and compares two optimization methods, GRPO and DPO, for improving the faithfulness of Chain-of-Thought reasoning in LLMs. 2. Demonstrates that GRPO outperforms DPO in larger models, with Qwen2.5-14B-Instruct achieving the best results. 3. Shows a positive correlation between model size and performance for both methods, with GRPO showing greater potential for improving faithfulness metrics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/873da11748c86e67ee5cce9b0d9ee43cc3075f93ef1cc7775d7d9b6683adb946_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/873da11748c86e67ee5cce9b0d9ee43cc3075f93ef1cc7775d7d9b6683adb946_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the problem of unfaithful Chain-of-Thought (CoT) reasoning in large language models, where generated rationales may not reflect the model's actual reasoning process. It evaluates two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), for improving CoT faithfulness. The main conclusion is that GRPO achieves higher performance than DPO, especially in larger models, suggesting it is a promising direction for developing more transparent and trustworthy reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: CoT\u89e3\u91ca\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u8bef\u5bfc\u4e14\u4e0d\u53ef\u9760]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u8bc4\u4f30GRPO\u548cDPO\u4f18\u5316\u65b9\u6cd5\u4ee5\u63d0\u5347\u5fe0\u5b9e\u6027]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: GRPO\u5728\u5927\u578b\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u4e8eDPO\uff0c\u662f\u63d0\u5347\u63a8\u7406\u900f\u660e\u5ea6\u7684\u6709\u524d\u666f\u65b9\u5411]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] On the Role of Discreteness in Diffusion LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [diffusion language models], [diffusion language models, parallel decoding, iterative refinement, continuous diffusion, discrete diffusion]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ziqi Jin, Bin Wang, Xiang Lin, Lidong Bing, Aixin Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," MiroMind AI, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22630",children:"https://arxiv.org/pdf/2512.22630"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Revisits diffusion language modeling and outlines five essential properties that separate diffusion mechanics from language-specific requirements. 2. Categorizes existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens, showing each only partially satisfies the essential properties. 3. Identifies two central issues in recent large diffusion language models: uniform corruption not respecting information distribution and token-wise marginal training failing to capture multi-token dependencies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf752083457bd2f08f3e62c6bd3acca194cf8603c88b7225f7783b764b30535_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf752083457bd2f08f3e62c6bd3acca194cf8603c88b7225f7783b764b30535_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the application of diffusion models to language generation, highlighting the challenges posed by the discrete nature of text. It categorizes existing approaches and identifies key structural trade-offs and issues, such as uniform corruption and token-wise training limitations. The findings motivate the development of diffusion processes better aligned with text structure for more coherent generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[On the Role of Discreteness in Diffusion LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u79bb\u6563\u4e0e\u7ed3\u6784\u5316\u6587\u672c\u6311\u6218\u6269\u6563\u6a21\u578b\u5e94\u7528<br/>Discrete & Structured Text Challenges Diffusion]\n    C --\x3e C1[\u5206\u7c7b\u8fde\u7eed\u4e0e\u79bb\u6563\u6269\u6563\u65b9\u6cd5<br/>Categorize Continuous & Discrete Diffusion]\n    C --\x3e C2[\u5206\u6790\u4e94\u5927\u7279\u6027\u4e0e\u6743\u8861<br/>Analyze Five Properties & Trade-offs]\n    D --\x3e D1[\u8bc6\u522b\u4e24\u5927\u6838\u5fc3\u95ee\u9898<br/>Identify Two Central Issues]\n    D --\x3e D2[\u6fc0\u52b1\u9762\u5411\u6587\u672c\u7ed3\u6784\u7684\u6269\u6563\u8fc7\u7a0b<br/>Motivate Text-Structured Diffusion Processes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Minnesota"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22650",children:"https://arxiv.org/pdf/2512.22650"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://minnesotanlp.github.io/insight-scaling-webpage",children:"https://minnesotanlp.github.io/insight-scaling-webpage"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u591a\u9636\u6bb5\u4efb\u52a1\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u5956\u52b1/Multi-stage tasks lack verifiable rewards]\n    B --\x3e B2[\u57fa\u4e8e\u8bc4\u5224\u7684\u4f18\u5316\u6613\u7d2f\u79ef\u8bef\u5dee/Judge-based refinement prone to error accumulation]\n    C --\x3e C1[\u9009\u62e9\u6027\u6d4b\u8bd5\u65f6\u6269\u5c55/Selective Test-Time Scaling]\n    C --\x3e C2[\u8de8\u9636\u6bb5\u5206\u914d\u8ba1\u7b97\u8d44\u6e90/Distribute compute across stages]\n    C --\x3e C3[\u65e9\u671f\u526a\u679d\u4f4e\u8d28\u91cf\u5206\u652f/Prune low-quality branches early]\n    D --\x3e D1[\u63d0\u5347\u5e73\u5747\u5206\u6570/Increased mean scores (61.64 to 65.86)]\n    D --\x3e D2[\u964d\u4f4e\u65b9\u5dee/Reduced variance]\n    D --\x3e D3[\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u9f50\u7684\u8bc4\u5224\u6a21\u578b/Judge model aligned with human experts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Pere Martra"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Universidad Internacional Men\xe9ndez Pelayo (UIMP)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22671",children:"https://arxiv.org/pdf/2512.22671"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: How does structured width pruning affect different LLM capabilities?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]\n    D[\u5173\u952e\u7ed3\u679c/Results: Knowledge \u2193, Instruction-following \u2191, Truthfulness \u2191, Efficiency trade-offs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [uncertainty quantification], [conformal prediction, adaptive prediction sets, vocabulary-aware, coverage-efficiency tradeoff, marginal coverage]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yoshith Roy Kotla, Varshith Roy Kotla"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The ICFAI Foundation for Higher Education"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22682",children:"https://arxiv.org/pdf/2512.22682"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified and formally characterized the coverage-efficiency tradeoff unique to applying conformal prediction to next-token prediction in LLMs with large vocabularies. 2. Proposed Vocabulary-Aware Conformal Prediction (VACP), a framework using semantic masking and temperature-adjusted scoring to reduce the effective prediction space. 3. Provided a theoretical analysis of when vocabulary reduction preserves conformal validity and demonstrated a 197x improvement in prediction set efficiency on benchmarks while maintaining coverage guarantees."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that naive conformal prediction for LLM next-token prediction produces uninformatively large prediction sets due to large vocabularies. It proposes Vocabulary-Aware Conformal Prediction (VACP), which uses semantic masking and hierarchical conformalization to drastically reduce set size. The method achieves near-target coverage while improving set efficiency by 197x, making conformal prediction practical for LLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Conformal Prediction Sets for LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u6807\u51c6\u7f6e\u4fe1\u9884\u6d4b\u5728\u5927\u578b\u8bcd\u6c47\u8868\u4e2d\u4ea7\u751f\u5de8\u5927\u4e14\u65e0\u7528\u7684\u9884\u6d4b\u96c6]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u8bcd\u6c47\u611f\u77e5\u7f6e\u4fe1\u9884\u6d4b(VACP), \u4f7f\u7528\u8bed\u4e49\u63a9\u7801\u548c\u5206\u5c42\u6821\u51c6]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4fdd\u630190%\u8986\u76d6\u7387\u7684\u540c\u65f6, \u5c06\u5e73\u5747\u9884\u6d4b\u96c6\u5927\u5c0f\u4ece847\u4e2a\u8bcd\u5143\u51cf\u5c11\u52304.3\u4e2a]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ahmed Abdullah, Sana Fatima, Haroon Mahmood"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," FAST-National University, Al Ain University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22705",children:"https://arxiv.org/pdf/2512.22705"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[GHaLIB: \u591a\u8bed\u8a00\u5e0c\u671b\u8bed\u97f3\u68c0\u6d4b\u6846\u67b6 / GHaLIB: Multilingual Hope Speech Detection Framework] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results]\n    B --\x3e B1[\u5e0c\u671b\u8bed\u97f3\u5728NLP\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3 / Hope speech underrepresented in NLP]\n    B --\x3e B2[\u4f4e\u8d44\u6e90\u8bed\u8a00(\u5982\u4e4c\u5c14\u90fd\u8bed)\u7f3a\u4e4f\u8d44\u6e90 / Lack of resources for low-resource languages (e.g., Urdu)]\n    C --\x3e C1[\u4f7f\u7528\u9884\u8bad\u7ec3\u591a\u8bed\u8a00Transformer\u6a21\u578b / Use pretrained multilingual Transformer models]\n    C --\x3e C2[\u7b80\u5355\u9884\u5904\u7406\u4e0e\u5206\u7c7b\u5668\u8bad\u7ec3 / Simple preprocessing & classifier training]\n    D --\x3e D1[\u4e4c\u5c14\u90fd\u8bed\u4e8c\u5143\u5206\u7c7bF1: 95.2% / Urdu binary F1: 95.2%]\n    D --\x3e D2[\u4e4c\u5c14\u90fd\u8bed\u591a\u7c7b\u5206\u7c7bF1: 65.2% / Urdu multi-class F1: 65.2%]\n    D --\x3e D3[\u591a\u8bed\u8a00\u6a21\u578b\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883 / Multilingual models viable for low-resource settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multilingual reasoning evaluation], [reasoning-answer misalignment, chain-of-thought prompting, crosslingual evaluation, error taxonomy, GlobalMMLU]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Anaelia Ovalle, Candace Ross, Sebastian Ruder, Adina Williams, Karen Ullrich, Mark Ibrahim, Levent Sagun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meta Superintelligence Labs"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22712",children:"https://arxiv.org/pdf/2512.22712"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a human-validated framework to evaluate the logical alignment between model-generated reasoning traces and their conclusions across languages. 2. Conducted a large-scale analysis revealing that reasoning traces in non-Latin scripts exhibit at least twice as much misalignment as those in Latin scripts, despite high task accuracy. 3. Developed an error taxonomy through human annotation, identifying evidential errors (e.g., unsupported claims) and illogical reasoning steps as primary failure modes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd9c868e57697e675d2ed0bdd5294f15cfad25e4c549988b95bcc1aca9c8bb57_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd9c868e57697e675d2ed0bdd5294f15cfad25e4c549988b95bcc1aca9c8bb57_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether the reasoning quality of large language models transfers across languages. The authors propose a framework to evaluate if model-generated reasoning traces logically support their conclusions, analyzing 65k traces across 6 languages and 6 models. They find a critical misalignment, especially in non-Latin scripts, showing that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages<br>\u8bba\u6587\u6807\u9898] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Does reasoning quality transfer across languages in LLMs?<br>LLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u5426\u8de8\u8bed\u8a00\u8fc1\u79fb\uff1f]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Human-validated framework to evaluate reasoning-answer alignment<br>\u4eba\u5de5\u9a8c\u8bc1\u7684\u6846\u67b6\u8bc4\u4f30\u63a8\u7406-\u7b54\u6848\u5bf9\u9f50]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High misalignment in non-Latin scripts, error taxonomy developed<br>\u975e\u62c9\u4e01\u6587\u5b57\u811a\u672c\u4e2d\u5bf9\u9f50\u5ea6\u5dee\uff0c\u5efa\u7acb\u4e86\u9519\u8bef\u5206\u7c7b]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Mitigating Social Desirability Bias in Random Silicon Sampling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [silicon sampling, social desirability bias, prompt engineering, jensen-shannon divergence, american national election study]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sashank Chapala, Maksym Mironov, Songgaojun Deng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Eindhoven University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22725",children:"https://arxiv.org/pdf/2512.22725"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Replicates and confirms the presence of persistent Social Desirability Bias (SDB) in LLM-based silicon sampling. 2. Proposes and systematically evaluates four psychologically grounded prompt-based methods (reformulated, reverse-coded, priming, preamble) for mitigating SDB. 3. Demonstrates that reformulated prompts (neutral, third-person phrasing) are the most effective method for improving alignment between silicon and human survey response distributions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e514b34cb5fd24baebc22116c45f73b1898e7c713905a13d372408df0900782b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e514b34cb5fd24baebc22116c45f73b1898e7c713905a13d372408df0900782b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how to reduce Social Desirability Bias in LLM-generated survey responses (silicon sampling). It tests four prompt-based mitigation methods and finds that reformulating questions into neutral, third-person phrasing most effectively aligns the LLM outputs with real human data from the American National Election Study."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Mitigating Social Desirability Bias in Random Silicon Sampling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLM\u7845\u91c7\u6837\u5b58\u5728\u793e\u4f1a\u671f\u671b\u504f\u5dee/Social Desirability Bias in Silicon Sampling]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6d4b\u8bd5\u56db\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u7f13\u89e3\u65b9\u6cd5/Test Four Prompt-based Mitigation Methods]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u91cd\u6784\u63d0\u793a\u6700\u6709\u6548\uff0c\u6539\u5584\u4e0e\u4eba\u7c7b\u6570\u636e\u5bf9\u9f50/Reformulated Prompts Most Effective, Improve Alignment]\n    C --\x3e C1[\u91cd\u6784/Reformulated]\n    C --\x3e C2[\u53cd\u5411\u7f16\u7801/Reverse-coded]\n    C --\x3e C3[\u542f\u52a8/Priming]\n    C --\x3e C4[\u5e8f\u8a00/Preamble]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text classification], [data augmentation, imbalanced dataset, social media analysis, natural language processing, pregnancy outcome]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Md Badsha Biswas"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," George Mason University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22732",children:"https://arxiv.org/pdf/2512.22732"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel approach to use public social media data (e.g., Twitter) as an adjunctive resource for studying negative pregnancy outcomes, addressing data scarcity in traditional epidemiological research. 2. Constructs an NLP pipeline to automatically identify and classify pregnancy experiences from unstructured, noisy social media text, distinguishing between positive and negative outcomes. 3. Investigates and evaluates various data augmentation techniques specifically to address the severe class imbalance inherent in social media data for this sensitive health domain."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of classifying negative pregnancy outcomes from imbalanced social media data. It proposes an NLP pipeline to extract and categorize pregnancy experiences from Twitter and investigates data augmentation techniques to balance the dataset. The research demonstrates the viability of social media data as a supplementary resource for epidemiological studies on pregnancy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5a74\u513f\u6b7b\u4ea1\u7387\u9ad8\uff0c\u8d1f\u9762\u598a\u5a20\u7ed3\u5c40\u6570\u636e\u7a00\u7f3a / High infant mortality, scarce data on negative pregnancy outcomes]\n    B --\x3e B2[\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e0d\u5e73\u8861\u3001\u6709\u566a\u58f0 / Social media data is imbalanced and noisy]\n    C --\x3e C1[\u6784\u5efaNLP\u6d41\u6c34\u7ebf\u5206\u7c7b\u598a\u5a20\u7ed3\u5c40 / Build NLP pipeline to classify pregnancy outcomes]\n    C --\x3e C2[\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e / Use data augmentation for imbalanced data]\n    D --\x3e D1[\u9a8c\u8bc1\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u8d44\u6e90\u7684\u53ef\u884c\u6027 / Validate social media data as an adjunctive resource]\n    D --\x3e D2[\u4e3a\u672a\u6765\u5065\u5eb7\u7814\u7a76\u63d0\u4f9b\u6846\u67b6 / Provide a framework for future health studies]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Harnessing Large Language Models for Biomedical Named Entity Recognition"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [named entity recognition], [instruction tuning, data filtering, weak-to-strong learning, biomedical named entity recognition, json generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jian Chen, Leilei Su, Cong Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hainan University, Weill Cornell Medicine"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22738",children:"https://arxiv.org/pdf/2512.22738"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes BioSelectTune, a data-centric framework for fine-tuning LLMs for BioNER that prioritizes data quality. 2. Introduces a Hybrid Superfiltering strategy, a weak-to-strong data curation method to distill a high-impact training dataset. 3. Reformulates BioNER as a structured JSON generation task to leverage LLMs' instruction-following capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of adapting general-domain LLMs to Biomedical Named Entity Recognition (BioNER) by proposing BioSelectTune, a framework that uses a novel Hybrid Superfiltering data curation strategy and formulates BioNER as a JSON generation task. The method achieves state-of-the-art performance on multiple benchmarks, outperforming specialized models even when trained on only 50% of the curated data."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Harnessing LLMs for BioNER] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs lack domain knowledge for BioNER / LLMs\u7f3a\u4e4f\u751f\u7269\u533b\u5b66\u9886\u57df\u77e5\u8bc6]\n    B --\x3e B2[Low-quality data degrades performance / \u4f4e\u8d28\u91cf\u6570\u636e\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d]\n    C --\x3e C1[BioSelectTune Framework / BioSelectTune\u6846\u67b6]\n    C1 --\x3e C2[Reformulate as JSON generation / \u91cd\u6784\u4e3aJSON\u751f\u6210\u4efb\u52a1]\n    C1 --\x3e C3[Hybrid Superfiltering / \u6df7\u5408\u8d85\u7ea7\u8fc7\u6ee4\u7b56\u7565]\n    D --\x3e D1[SOTA on benchmarks / \u57fa\u51c6\u6d4b\u8bd5\u8fbe\u5230SOTA]\n    D --\x3e D2[Outperforms BioMedBERT / \u8d85\u8d8aBioMedBERT]\n    D --\x3e D3[50% data surpasses baseline / 50%\u6570\u636e\u8d85\u8d8a\u5168\u91cf\u57fa\u7ebf]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [diffusion language models, causal attention, prefix KV caching, topological reordering, streaming decoding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Aiwei Liu, Minghua He, Shaoxun Zeng, Sijun Zhang, Linhao Zhang, Chuhan Wu, Wei Jia, Yuan Liu, Xiao Zhou, Jie Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent (WeChat AI), Peking University, Tsinghua University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22737",children:"https://arxiv.org/pdf/2512.22737"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/tencent/WeDLM",children:"https://github.com/tencent/WeDLM"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes WeDLM, a diffusion decoding framework built entirely on standard causal attention to be compatible with prefix KV caching. 2. Introduces Topological Reordering to allow masked positions to condition on all observed tokens while maintaining a strict causal mask. 3. Designs a streaming decoding procedure that commits confident tokens continuously to avoid stop-and-wait behavior and maintain fixed parallel workload."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad23d9024363d110ea78bc7ec9d949004a721e40ff6ee0dc4259437ba273af5e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad23d9024363d110ea78bc7ec9d949004a721e40ff6ee0dc4259437ba273af5e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the inefficiency of diffusion language models (DLLMs) at inference time, which often fail to achieve speedups over autoregressive models due to their reliance on bidirectional attention that breaks prefix KV caching. It proposes WeDLM, a framework that uses causal attention and a novel streaming decoding procedure with topological reordering to enable efficient parallel generation. Experiments show WeDLM preserves model quality while achieving up to 3x-10x speedup over optimized AR engines like vLLM."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Diffusion LLMs use bidirectional attention, breaking prefix KV caching and hurting inference speed.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>WeDLM uses causal attention with Topological Reordering and streaming decoding for prefix-cache friendly parallel generation.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Preserves AR model quality, achieves up to 3x-10x speedup vs. vLLM-served AR baselines.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multi-modal sentiment analysis], [multi-modal sentiment analysis, mixture-of-experts, temporal alignment, multi-modal large language model, cross-attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dongning Rao, Yunbiao Zeng, Zhihua Jiang, Jujian Lv"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Guangdong University of Technology, Guangdong Polytechnic Normal University, Jinan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22741",children:"https://arxiv.org/pdf/2512.22741"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/fip-lab/TEXT",children:"https://github.com/fip-lab/TEXT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel text-routed sparse mixture-of-experts model with gate fusion for multi-modal sentiment analysis. 2. Introduces a temporal alignment block that merges the benefits of Mamba and temporal cross-attention to align audio and video representations. 3. Augments explanations for MSA using Multi-modal Large Language Models (MLLMs) and aligns different modalities with these explanations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcf1bd04e7c087db4d11cc1d5f3e99f88fc80cf401c5c782a8b3003074eef9b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcf1bd04e7c087db4d11cc1d5f3e99f88fc80cf401c5c782a8b3003074eef9b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TEXT, a model for multi-modal sentiment analysis that uses MLLM-generated explanations and a novel temporal alignment block to better fuse text, audio, and video modalities. The method achieves state-of-the-art performance across four datasets, significantly reducing error metrics compared to recent approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: The power of explanations and temporal alignments in MSA is underexplored.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: TEXT uses MLLM explanations, temporal alignment (Mamba & cross-attention), and text-routed sparse mixture-of-experts.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves best performance on four datasets, e.g., 13.5% MAE decrement on CH-SIMS.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [fake news detection], [domain adaptation, XLM-RoBERTa, mBERT, low-resource language, Urdu]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Zain Ali, Bernhard Pfahringer, Tony Smith"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Waikato"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22778",children:"https://arxiv.org/pdf/2512.22778"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zainali93/DomainAdaptation",children:"https://github.com/zainali93/DomainAdaptation"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Investigates domain adaptation before fine-tuning for fake news classification in Urdu, a low-resource language. 2. Evaluates and compares the effectiveness of this staged training approach on two multilingual models (XLM-R and mBERT). 3. Demonstrates that domain-adapted XLM-R consistently outperforms its vanilla counterpart across four Urdu fake news datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34b44093cc6cdaad3233e04caa61c5300da4dbe090a12e70cdd829572c5a7c29_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34b44093cc6cdaad3233e04caa61c5300da4dbe090a12e70cdd829572c5a7c29_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses fake news classification in Urdu, a low-resource language, by proposing a domain adaptation approach before fine-tuning multilingual language models. The method involves domain-adaptive pretraining on an Urdu news corpus, applied to XLM-RoBERTa and mBERT. Results show that domain-adapted XLM-R consistently improves performance, while mBERT shows mixed results."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f4e\u8d44\u6e90\u8bed\u8a00\u5047\u65b0\u95fb\u68c0\u6d4b/Low-Resource Language Fake News Detection]\n    C --\x3e C1[\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3/Domain-Adaptive Pretraining]\n    C --\x3e C2[\u5fae\u8c03\u591a\u8bed\u8a00\u6a21\u578b/Fine-tuning Multilingual Models]\n    D --\x3e D1[\u9886\u57df\u81ea\u9002\u5e94XLM-R\u6027\u80fd\u63d0\u5347/Domain-Adapted XLM-R Improves]\n    D --\x3e D2[mBERT\u7ed3\u679c\u4e0d\u4e00/mBERT Shows Mixed Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22795",children:"https://arxiv.org/pdf/2512.22795"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[CNSight: \u4e34\u5e8a\u7b14\u8bb0\u5206\u5272\u5de5\u5177\u8bc4\u4f30 / CNSight: Evaluation of Clinical Note Segmentation Tools]\n    Root --\x3e Problem[\u4e34\u5e8a\u7b14\u8bb0\u975e\u7ed3\u6784\u5316 / Clinical Notes Unstructured]\n    Root --\x3e Method[\u8bc4\u4f30\u89c4\u5219/\u53d8\u6362\u5668/\u5927\u8bed\u8a00\u6a21\u578b / Evaluate Rule-based/Transformer/LLMs]\n    Root --\x3e Results[\u5927\u6a21\u578b\u6027\u80fd\u6700\u4f73 / Large Models Best Performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [emotion and sentiment analysis], [multi-label classification, transformer models, topic modelling, temporal analysis, linguistic insights]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sameer Sitoula, Tej Bahadur Shahi, Laxmi Prasad Bhatt, Anisha Pokhrel, Arjun Neupane"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tribhuvan University, Queensland University of Technology, Advanced College of Engineering and Management, Central Queensland University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22823",children:"https://arxiv.org/pdf/2512.22823"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduction of NepEMO, a novel manually annotated dataset for multi-label emotion and sentiment analysis on Nepali Reddit posts in multiple scripts. 2. A detailed linguistic and temporal analysis of the dataset, including emotion trends, co-occurrence, and topic modeling. 3. A comprehensive benchmark comparing traditional ML, deep learning, and transformer models, demonstrating the superiority of transformers for the tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbfaa02abfed829cc20f377fe4ff49e093d956ae5d2c81018f8575d9b08ab30b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbfaa02abfed829cc20f377fe4ff49e093d956ae5d2c81018f8575d9b08ab30b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces NepEMO, a new dataset for multi-label emotion and sentiment analysis on posts from the Nepali subreddit. The authors perform linguistic and temporal analysis on the data and benchmark various machine learning models. The results show that transformer models outperform traditional machine learning and deep learning models for both classification tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[NepEMO: \u591a\u6807\u7b7e\u60c5\u611f\u4e0e\u60c5\u7eea\u5206\u6790 / Multi-Label Emotion and Sentiment Analysis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7f3a\u4e4f\u5c3c\u6cca\u5c14\u8bed\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6 / Lack of Nepali social media dataset]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u6bd4\u8f83 / Build annotated dataset & model comparison]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u53d8\u538b\u5668\u6a21\u578b\u6027\u80fd\u6700\u4f18 / Transformer models perform best]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [automated environment synthesis, environment-level RL, agentic reinforcement learning, simulated user, policy optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22857",children:"https://arxiv.org/pdf/2512.22857"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified, automated pipeline for synthesizing scalable simulated environments with high-difficulty, easily verifiable tasks. 2. An Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and performs advantage estimation at the environment level. 3. Comprehensive validation on agentic benchmarks demonstrating effectiveness and out-of-domain generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AutoForge, a framework to automate the synthesis of challenging simulated environments for training language-based agents via reinforcement learning. It introduces an environment-level RL algorithm to improve training stability and efficiency by handling simulated user instability and heterogeneous environments. Evaluations show the method is effective and generalizes well to out-of-domain tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AutoForge] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73af\u5883\u5408\u6210\u534a\u81ea\u52a8/Semi-automated Environment Synthesis]\n    B --\x3e B2[\u4efb\u52a1\u96be\u5ea6\u4e0d\u8db3/Insufficient Task Difficulty]\n    B --\x3e B3[\u6a21\u62df\u7528\u6237\u4e0d\u7a33\u5b9a/Simulated User Instability]\n    C --\x3e C1[\u81ea\u52a8\u5316\u73af\u5883\u5408\u6210\u7ba1\u9053/Automated Environment Synthesis Pipeline]\n    C --\x3e C2[\u73af\u5883\u7ea7RL\u7b97\u6cd5/Environment-level RL Algorithm (ERPO)]\n    D --\x3e D1[\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548/Effective on Benchmarks (\u03c4-bench, etc.)]\n    D --\x3e D2[\u57df\u5916\u6cdb\u5316\u5f3a/Strong Out-of-domain Generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Debugging Tabular Log as Dynamic Graphs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [dynamic graph, graph neural network, tabular log, log debugging, heterogeneous nodes]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Amazon"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22903",children:"https://arxiv.org/pdf/2512.22903"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GraphLogDebugger, a novel framework that models tabular log data as dynamic graphs with heterogeneous nodes for objects and events, 2. Demonstrates that a simple dynamic GNN can outperform large language models (LLMs) in debugging tasks using this graph representation, 3. Validates the approach on real-world datasets from computer systems and academic papers, showing improved flexibility and scalability over LLM-based methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GraphLogDebugger, a framework that converts tabular log data into dynamic graphs to detect inconsistencies in real-world systems. By representing logs as evolving graphs with object and event nodes, a lightweight dynamic Graph Neural Network effectively debugs logs, outperforming larger LLM-based models in experiments on system and academic log datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[DEBUGGING TABULAR LOG AS DYNAMIC GRAPHS] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs and heavy models lack flexibility and scalability for tabular log debugging]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: GraphLogDebugger models logs as dynamic graphs with heterogeneous nodes and edges]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Simple dynamic GNN outperforms LLMs in debugging on real-world datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multimodal Fact-Checking: An Agent-based Approach"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal fact-checking], [multimodal misinformation, agent-based reasoning, explainable dataset, vision-language models, evidence retrieval]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Danni Xu, Shaojing Fan, Xuanang Cheng, Mohan Kankanhalli"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore (NUS)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22933",children:"https://arxiv.org/pdf/2512.22933"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RW-Post, a high-quality, explainable dataset for real-world multimodal fact-checking that aligns claims with original social media posts and provides detailed reasoning and evidence. 2. Proposes AgentFact, a novel agent-based multimodal fact-checking framework that emulates the human verification workflow through five specialized, collaboratively working agents. 3. Demonstrates that the synergy between the new dataset and the agent framework substantially improves both the accuracy and interpretability of multimodal fact-checking."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of automated multimodal fact-checking by introducing a new dataset (RW-Post) and an agent-based framework (AgentFact). The dataset provides real-world misinformation instances with reasoning and evidence, while the framework uses specialized agents to collaboratively perform verification tasks. The combined approach is shown to significantly enhance the accuracy and explainability of fact-checking systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multimodal Fact-Checking: An Agent-based Approach] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u4e0e\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u548c\u8bc1\u636e\u5229\u7528\u4e0a\u7684\u4e0d\u8db3 / The spread of multimodal misinformation and the limitations of existing methods in reasoning and evidence utilization]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faRW-Post\u6570\u636e\u96c6\u548cAgentFact\u667a\u80fd\u4f53\u6846\u67b6 / Proposes the RW-Post dataset and the AgentFact agent-based framework]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027 / Substantially improves the accuracy and interpretability of multimodal fact-checking]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Diversity or Precision? A Deep Dive into Next Token Prediction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [policy gradient, reward shaping, next-token prediction, exploration space, cross-entropy loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haoyuan Wu, Hai Wang, Jiajia Wu, Jinxiang Ou, Keyao Wang, Weile Chen, Zihao Zheng, Bei Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent, The Chinese University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22955",children:"https://arxiv.org/pdf/2512.22955"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Reinterprets standard cross-entropy loss as a specific instance of policy gradient optimization in a single-step episode, bridging supervised learning and RL. 2. Proposes a generalized pre-training objective using on-policy RL principles and a novel reward-shaping strategy to balance diversity and precision in the token-output distribution. 3. Empirically finds that a precision-oriented prior, rather than a high-entropy one, creates a more favorable exploration space for subsequent RL, enhancing reasoning performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e03f108685a26b73d2c14c07a3e7234e09a5b32e612d72a9751508fcbb93ec32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e03f108685a26b73d2c14c07a3e7234e09a5b32e612d72a9751508fcbb93ec32_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how the token-output distribution from pre-training shapes the exploration space for subsequent reinforcement learning (RL) in language models. It proposes a new pre-training method that frames next-token prediction as an RL problem, using a reward-shaping strategy to control distribution precision. The key finding is that a precision-focused prior, contrary to intuition, provides a better exploration foundation for RL than a high-entropy one."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Diversity or Precision? A Deep Dive into Next Token Prediction] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u9884\u8bad\u7ec3\u5206\u5e03\u5982\u4f55\u5f71\u54cd\u540e\u7eedRL\u7684\u63a2\u7d22\u7a7a\u95f4\uff1f/How does the pre-trained distribution affect the RL exploration space?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5c06\u4ea4\u53c9\u71b5\u635f\u5931\u91cd\u65b0\u89e3\u91ca\u4e3a\u7b56\u7565\u68af\u5ea6/Reinterpret cross-entropy as policy gradient]\n    Method --\x3e M2[\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u5851\u5f62\u7684\u5e7f\u4e49\u9884\u8bad\u7ec3\u76ee\u6807/Propose a generalized pre-training objective with reward shaping]\n    M2 --\x3e M2_1[\u6b63\u5956\u52b1\u7f29\u653e\u56e0\u5b50/Positive reward scaling factor]\n    M2 --\x3e M2_2[\u6392\u540d\u611f\u77e5\u7684\u8d1f\u4ee4\u724c\u5904\u7406/Rank-aware negative token treatment]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u7cbe\u5ea6\u5bfc\u5411\u7684\u5148\u9a8c\u4f18\u4e8e\u9ad8\u71b5\u5148\u9a8c/Precision-oriented prior yields superior exploration space]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [large language models for healthcare], [prompt engineering, clinical decision support, few-shot learning, model evaluation, temperature setting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengdi Chai, Ali R. Zomorrodi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harvard School of Public Health, Massachusetts General Hospital, Harvard Medical School, Broad Institute of MIT and Harvard"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22966",children:"https://arxiv.org/pdf/2512.22966"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Comprehensive evaluation of three state-of-the-art LLMs across the full clinical reasoning workflow, revealing high task-dependent performance variability. 2. Demonstration that prompt engineering (specifically MedPrompt variations) is not universally beneficial, improving performance only on the task with the lowest baseline accuracy. 3. Finding that targeted dynamic few-shot prompting does not consistently outperform random selection, suggesting a trade-off between example relevance and contextual diversity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30c995933cd74a3ca7c2af152821552a5355b6d092f084b174049b87f299a2bc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30c995933cd74a3ca7c2af152821552a5355b6d092f084b174049b87f299a2bc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study evaluated the performance of LLMs like ChatGPT-4o on clinical decision-making tasks and tested if prompt engineering could improve it. The results show that prompt engineering is not a universal solution; it helps only on specific tasks and targeted few-shot learning is not always better than random selection. The impact of prompt engineering is highly dependent on the model and the specific clinical task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Prompt engineering does not universally improve LLM performance across clinical decision-making tasks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u5728\u771f\u5b9e\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5b9e\u7528\u6027\u672a\u5145\u5206\u63a2\u7d22/LLMs' practical utility in real-world clinical decision-making is underexplored]\n    C --\x3e C1[\u8bc4\u4f30\u4e09\u4e2aLLM\u5728\u4e94\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u7684\u5f00\u7bb1\u5373\u7528\u6027\u80fd/Evaluate three LLMs' out-of-the-box performance on five clinical tasks]\n    C --\x3e C2[\u5e94\u7528MedPrompt\u6846\u67b6\u7684\u53d8\u4f53\u8fdb\u884c\u63d0\u793a\u5de5\u7a0b/Apply variations of the MedPrompt framework for prompt engineering]\n    D --\x3e D1[\u63d0\u793a\u5de5\u7a0b\u5e76\u975e\u4e07\u80fd\u65b9\u6848\uff0c\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02/Prompt engineering is not a one-size-fits-all solution, effect varies by task]\n    D --\x3e D2[\u9488\u5bf9\u6027\u5c11\u6837\u672c\u63d0\u793a\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u968f\u673a\u9009\u62e9/Targeted few-shot prompting does not consistently outperform random selection]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [structured pruning, post-training pruning, function-aware grouping, calibration bias, sparsity allocation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tao Yu, Yongqi An, Kuan Zhu, Guibo Zhu, Ming Tang, Jinqiao Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Wuhan AI Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23014",children:"https://arxiv.org/pdf/2512.23014"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Function-Aware Neuron Grouping (FANG), a pruning framework that groups neurons based on the semantic context types they process to mitigate calibration bias. 2. Introduces a weighted importance estimation within each group that prioritizes tokens strongly correlated with the group's functional role, and preserves cross-context neurons. 3. Develops an adaptive sparsity allocation strategy per model block based on its functional complexity to better balance sparsity and performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a53f8a0eafa56a813a03f94cde349a2ee5aa7da51f314f7ec42da88585b4c3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a53f8a0eafa56a813a03f94cde349a2ee5aa7da51f314f7ec42da88585b4c3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limited generalization of existing post-training structured pruning methods for LLMs when calibration data is biased. It proposes FANG, a method that groups neurons by function, weights importance estimation accordingly, and adaptively allocates sparsity. Experiments show FANG improves downstream task accuracy and achieves state-of-the-art results when combined with existing pruning methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u6821\u51c6\u96c6\u504f\u5dee\u5bfc\u81f4\u526a\u679d\u540e\u6cdb\u5316\u80fd\u529b\u5dee/Calibration bias leads to poor generalization after pruning]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u529f\u80fd\u611f\u77e5\u795e\u7ecf\u5143\u5206\u7ec4\u4e0e\u81ea\u9002\u5e94\u526a\u679d/Function-Aware Neuron Grouping & Adaptive Pruning]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u8fbe\u5230SOTA/Improves downstream accuracy, achieves SOTA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [multimodal language models], [multimodal sensing, time-series encoding, ecological momentary assessment (EMA)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenxuan Xu, Arvind Pillai, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dartmouth College, University of Virginia, Massachusetts General Hospital, Harvard Medical School"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23025",children:"https://arxiv.org/pdf/2512.23025"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental health narratives. 2. Constructs a large-scale dataset of over 100,000 sensor-text QA pairs by transforming Ecological Momentary Assessment (EMA) responses. 3. Trains a patch-level encoder to project raw sensor time-series signals directly into an LLM's representation space for native integration."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of translating long-duration, multimodal sensor data into interpretable natural language for mental health assessment. It proposes the LENS framework, which creates a large sensor-text dataset and trains a specialized encoder to align sensor signals with an LLM, enabling the generation of clinically meaningful narratives. The results show LENS outperforms baselines on NLP and clinical metrics, and is validated by mental health professionals."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[LENS: LLM-Enabled Narrative Synthesis] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u4f20\u611f\u5668\u6570\u636e\u96be\u4ee5\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00/Sensor data hard to translate to text]\n    Problem --\x3e P2[\u7f3a\u4e4f\u914d\u5bf9\u6570\u636e\u96c6/Lack of paired sensor-text datasets]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6784\u5efa\u5927\u89c4\u6a21\u4f20\u611f\u5668-\u6587\u672cQA\u6570\u636e\u96c6/Build large-scale sensor-text QA dataset]\n    Method --\x3e M2[\u8bad\u7ec3\u8865\u4e01\u7ea7\u7f16\u7801\u5668\u5bf9\u9f50LLM/Train patch-level encoder to align with LLM]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5728NLP\u548c\u75c7\u72b6\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baselines on NLP & symptom metrics]\n    Results --\x3e R2[\u4e34\u5e8a\u533b\u751f\u8ba4\u4e3a\u53d9\u8ff0\u5168\u9762\u6709\u610f\u4e49/Clinicians find narratives comprehensive & meaningful]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kerem Zaman, Shashank Srivastava"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UNC Chapel Hill"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23032",children:"https://arxiv.org/pdf/2512.23032"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Biasing Features \u6307\u6807\u5c06\u4e0d\u5b8c\u6574\u6027\u8bef\u5224\u4e3a\u4e0d\u5fe0\u5b9e\u6027<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]\n    C --\x3e C1[\u63d0\u51fa faithful@k \u6307\u6807\u5e76\u589e\u52a0\u63a8\u7406\u4ee4\u724c\u9884\u7b97<br/>Propose faithful@k metric & increase inference token budget]\n    C --\x3e C2[\u4f7f\u7528\u56e0\u679c\u4e2d\u4ecb\u5206\u6790<br/>Use Causal Mediation Analysis]\n    D --\x3e D1[\u8bb8\u591a\u88ab\u6807\u8bb0\u4e3a\u4e0d\u5fe0\u5b9e\u7684 CoT \u88ab\u5176\u4ed6\u6307\u6807\u5224\u5b9a\u4e3a\u5fe0\u5b9e<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]\n    D --\x3e D2[\u66f4\u5927\u7684\u4ee4\u724c\u9884\u7b97\u663e\u8457\u63d0\u9ad8\u63d0\u793a\u8bcd\u663e\u5316\u7387<br/>Larger token budgets greatly increase hint verbalization]\n    D --\x3e D3[\u672a\u663e\u5316\u7684\u63d0\u793a\u8bcd\u4ecd\u53ef\u901a\u8fc7 CoT \u56e0\u679c\u4e2d\u4ecb\u9884\u6d4b<br/>Non-verbalized hints can causally mediate predictions through CoT]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Accelerating Language Model Workflows with Prompt Choreography"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [KV cache, multi-agent workflows, prompt caching, fine-tuning, parallel decoding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," TJ Bai, Jason Eisner"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Johns Hopkins University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23049",children:"https://arxiv.org/pdf/2512.23049"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Prompt Choreography, a framework for efficient LLM workflow execution using a dynamic, global KV cache that allows arbitrary, reordered attention to cached messages. 2. Enables parallel LLM calls and overcomes limitations of static caching by allowing runtime reuse of dynamically generated messages. 3. Demonstrates that fine-tuning the LLM to work with the cache can mitigate performance differences, achieving significant latency reduction (2.0\u20136.2\xd7 faster time-to-first-token) and end-to-end speedups (>2.2\xd7)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7be6fd2c8ed387f8c6d6cfba8ffad178e5ac937348602af7f9d1951150964c8c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7be6fd2c8ed387f8c6d6cfba8ffad178e5ac937348602af7f9d1951150964c8c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the inefficiency of redundant computation in multi-agent LLM workflows. It proposes Prompt Choreography, a framework that uses a dynamic, global KV cache to allow LLM calls to attend to arbitrary subsets of previously encoded messages, supporting parallel execution. The method, combined with fine-tuning, significantly reduces latency and achieves substantial speedups by reusing cached encodings instead of re-encoding from scratch."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Accelerating Language Model Workflows with Prompt Choreography") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5197\u4f59\u8ba1\u7b97/Multi-agent workflow redundant computation")\n    Method --\x3e M1("\u52a8\u6001\u5168\u5c40KV\u7f13\u5b58/Dynamic global KV cache")\n    Method --\x3e M2("\u652f\u6301\u4efb\u610f\u6d88\u606f\u5b50\u96c6\u4e0e\u91cd\u6392\u5e8f/Support arbitrary message subset & reordering")\n    Method --\x3e M3("\u652f\u6301\u5e76\u884c\u8c03\u7528\u4e0e\u5fae\u8c03/Support parallel calls & fine-tuning")\n    Results --\x3e R1("\u964d\u4f4e\u6bcf\u6d88\u606f\u5ef6\u8fdf/Reduce per-message latency")\n    Results --\x3e R2("\u5b9e\u73b0\u7aef\u5230\u7aef\u52a0\u901f/Achieve end-to-end speedup")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [ModernBERT, Rotary Positional Embeddings, FlashAttention, monolingual Turkish, encoder-only transformer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Melik\u015fah T\xfcrker, A. Ebrar K\u0131z\u0131lo\u011flu, Onur G\xfcng\xf6r, Susan \xdcsk\xfcdarl\u0131"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bo\u011fazi\xe7i University, VNGRS-AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23065",children:"https://arxiv.org/pdf/2512.23065"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces TabiBERT, the first monolingual Turkish encoder trained from scratch using the modern ModernBERT architecture (RoPE, FlashAttention). 2. Presents TabiBench, a unified benchmarking framework with 28 datasets across 8 tasks for standardized evaluation of Turkish NLP models. 3. Demonstrates state-of-the-art performance on TabiBench, outperforming prior models like BERTurk and showing strong cross-domain generalization."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc5c874163783e3d6e86937eb817ae8080c269ccc3607c50deebbe47ac22673_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc5c874163783e3d6e86937eb817ae8080c269ccc3607c50deebbe47ac22673_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces TabiBERT, a large-scale monolingual Turkish encoder based on the ModernBERT architecture, trained from scratch on a diverse corpus. It also presents TabiBench, a comprehensive benchmarking framework. TabiBERT achieves state-of-the-art results on multiple Turkish NLP tasks, demonstrating superior performance and generalization compared to existing models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Turkish NLP lacks a modern monolingual encoder/\u571f\u8033\u5176NLP\u7f3a\u4e4f\u73b0\u4ee3\u5355\u8bed\u7f16\u7801\u5668]\n    C --\x3e C1[Train TabiBERT from scratch with ModernBERT architecture/\u4f7f\u7528ModernBERT\u67b6\u6784\u4ece\u5934\u8bad\u7ec3TabiBERT]\n    C --\x3e C2[Create TabiBench benchmark/\u521b\u5efaTabiBench\u57fa\u51c6]\n    D --\x3e D1[SOTA on TabiBench (77.58), beats BERTurk/\u5728TabiBench\u4e0a\u8fbe\u5230SOTA\uff0c\u8d85\u8d8aBERTurk]\n    D --\x3e D2[Strong gains in QA, code retrieval/\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u68c0\u7d22\u4efb\u52a1\u4e0a\u63d0\u5347\u663e\u8457]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yingru Li, Ziniu Li, Jiacai Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23097",children:"https://arxiv.org/pdf/2512.23097"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Hybrid Online RL and IL for LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Train-inference distribution mismatch in LLM fine-tuning]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified framework combining Imitation Learning and Reinforcement Learning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Reservoir Computing inspired Matrix Multiplication-free Language Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [MatMul-free LM, reservoir computing, weight sharing, ternary quantization, MLGRU]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Takumi Shiratsuchi, Yuichiro Tanaka, Hakaru Tamukoh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kyushu Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23145",children:"https://arxiv.org/pdf/2512.23145"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel language model architecture that integrates reservoir computing principles into a MatMul-free LM to reduce training costs. 2. Introduces techniques of partially fixing/sharing weights and inserting reservoir layers to obtain dynamic representations without extra training overhead. 3. Combines operations to reduce memory accesses, achieving reductions in parameters, training time, and inference time while maintaining performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high computational cost of large language models by proposing a matrix multiplication-free model enhanced with reservoir computing. The method fixes/shared weights in selected layers and inserts reservoir layers to reduce training overhead and memory accesses. Experiments show the approach reduces parameters by up to 19%, training time by 9.9%, and inference time by 8.0% while maintaining comparable performance to the baseline."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Reservoir Computing inspired Matrix Multiplication-free Language Model] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs\u8ba1\u7b97\u6210\u672c\u9ad8/High computational cost of LLMs]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7ed3\u5408\u50a8\u5c42\u8ba1\u7b97\u4e0e\u65e0\u77e9\u9635\u4e58\u6cd5\u6a21\u578b/Combine RC with MatMul-free LM, \u56fa\u5b9a\u5171\u4eab\u6743\u91cd/Fix & share weights, \u51cf\u5c11\u5185\u5b58\u8bbf\u95ee/Reduce memory access]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u53c2\u6570\u51cf\u5c1119%/Params reduced by 19%, \u8bad\u7ec3\u65f6\u95f4\u51cf\u5c119.9%/Training time reduced by 9.9%, \u63a8\u7406\u65f6\u95f4\u51cf\u5c118.0%/Inference time reduced by 8.0%, \u6027\u80fd\u76f8\u5f53/Performance maintained]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Not too long do read: Evaluating LLM-generated extreme scientific summaries"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text summarization], [extreme summarization, TLDR, abstractive summarization, extractive summarization, dataset creation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhuoqi Lyu, Qing Ke"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23206",children:"https://arxiv.org/pdf/2512.23206"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/netknowledge/LLM_summarization",children:"https://github.com/netknowledge/LLM_summarization"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BiomedTLDR, a novel high-quality dataset of researcher-authored scientific TLDRs, curated from author annotations in bibliographies. 2. Evaluates the performance of popular open-weight LLMs in generating scientific TLDRs from paper abstracts. 3. Provides an analysis revealing that LLM-generated summaries tend to be more extractive (closer to the source text's lexicon and structure) compared to more abstractive human-written summaries."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of high-quality datasets for evaluating LLMs in generating scientific extreme summaries (TLDRs) by introducing BiomedTLDR, a dataset of human-authored summaries. It then evaluates open-weight LLMs on this task and finds that, while some can produce human-like summaries, LLMs generally tend to be more extractive and less abstractive than human experts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Not too long do read: Evaluating LLM-generated extreme scientific summaries<br>\u8bba\u6587\u6807\u9898"]\n    A --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem<br>Lack of high-quality scientific TLDR dataset hinders LLM evaluation"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose BiomedTLDR dataset & test LLMs on TLDR generation"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results<br>LLMs are more extractive; humans are more abstractive"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Anka: A Domain-Specific Language for Reliable LLM Code Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Domain-Specific Language, Constrained Syntax, Code Generation, Data Transformation Pipeline, In-Context Learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Saif Khalfan Saif Al Mazrouei"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23214",children:"https://arxiv.org/pdf/2512.23214"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Anka: A Domain-Specific Language for Reliable LLM Code Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs make systematic errors in complex multi-step code generation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Design Anka, a constrained DSL for data transformation pipelines]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High parse success & task accuracy; Anka outperforms Python on multi-step tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [LLM Ensemble, LLM-as-a-Judge, Peer-Review, Unsupervised Selection, Truth Inference]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhijun Chen, Zeyu Ji, Qianren Mao, Junhang Cheng, Bangjie Qin, Hao Wu, Zhuoran Li, Jingzheng Li, Kai Sun, Zizhe Wang, Yikun Ban, Zhu Sun, Xiangyang Ji, Hailong Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Zhongguancun Laboratory, Xi'an Jiaotong University, Hong Kong University of Science and Technology, Tsinghua University, Singapore University of Technology and Design"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23213",children:"https://arxiv.org/pdf/2512.23213"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LLM-PeerReview, a novel, peer-review-inspired, and interpretable framework for unsupervised LLM ensemble selection. 2. Introduces a three-stage process (scoring via LLM-as-a-Judge, reasoning via aggregation, and selection) that leverages multiple LLMs to evaluate each other's responses. 3. Demonstrates strong empirical performance, with two variants significantly outperforming a recent advanced baseline (Smoothie-Global) on multiple datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes LLM-PeerReview, an unsupervised ensemble method that selects the best response from multiple LLM candidates. The method uses a peer-review process where LLMs score each other's outputs, then aggregates these scores to make a final selection. The approach is shown to be simple and powerful, outperforming a strong baseline by a significant margin across several datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[LLM-PeerReview: Ensembling LLMs via Peer-Review] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Single LLM limitations & diverse model strengths]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unsupervised 3-stage peer-review framework]\n    C --\x3e C1[\u8bc4\u5206/Scoring: LLM-as-a-Judge]\n    C --\x3e C2[\u63a8\u7406/Reasoning: Score aggregation (graphical model or averaging)]\n    C --\x3e C3[\u9009\u62e9/Selection: Pick highest-scoring response]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms Smoothie-Global by ~7% points]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23260",children:"https://arxiv.org/pdf/2512.23260"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("LoRA\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/LoRA lacks interpretability")\n    Problem --\x3e P2("\u5b50\u7a7a\u95f4\u5b66\u4e60\u662f\u9ed1\u76d2\u7684/Subspace learning is black-box")\n    Method --\x3e M1("\u5229\u7528\u9884\u8bad\u7ec3SAE/Use pre-trained SAEs")\n    Method --\x3e M2("\u6784\u5efa\u663e\u5f0f\u4f4e\u79e9\u5b50\u7a7a\u95f4/Construct explicit low-rank subspace")\n    Results --\x3e R1("\u9ad8\u5b89\u5168\u738799.6%/High safety rate 99.6%")\n    Results --\x3e R2("\u53c2\u6570\u9ad8\u65480.19%/Parameter-efficient 0.19%")\n    Results --\x3e R3("\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027/Provides interpretability")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Chinese Morph Resolution in E-commerce Live Streaming Scenarios"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [morph resolution], [morph resolution, text-to-text generation, large language models, automatic speech recognition, e-commerce live streaming]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiahao Zhu, Jipeng Qiang, Ran Bai, Chenyu Liu, Xiaoye Ouyang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yangzhou University, China Academy of Electronic and Information Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23280",children:"https://arxiv.org/pdf/2512.23280"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced the novel Live Auditory Morph Resolution (LiveAMR) task for detecting pronunciation-based evasion in e-commerce live streams, distinct from prior text-based morph research. 2. Constructed the first large-scale LiveAMR dataset containing 86,790 samples from health and medical live streaming scenarios. 3. Proposed a method that transforms the morph resolution task into a text-to-text generation problem and leveraged LLMs for data augmentation to improve model performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5833e430257262b6c7e7229b982e2421b63671f2902d24282bd2f2167c1934c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5833e430257262b6c7e7229b982e2421b63671f2902d24282bd2f2167c1934c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the LiveAMR task to detect pronunciation-based morphs used by hosts in Chinese e-commerce live streams to evade voice censorship. The authors built a new dataset and framed the problem as a text-to-text generation task, using LLMs for data augmentation to boost performance. The study concludes that resolving these morphs significantly enhances the effectiveness of live streaming content regulation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Chinese Morph Resolution in E-commerce Live Streaming Scenarios] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Hosts use pronunciation-based morphs for false advertising in live streams]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Transform task to text-to-text generation, use LLMs for data augmentation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Built first LiveAMR dataset, method improves performance, aids regulation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent collaboration, large language models (LLMs), speech synthesis, audiobook interpretation, Chinese NLP]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Minjiang Huang, Jipeng Qiang, Yi Zhu, Chaowei Zhang, Xiangyu Zhao, Kui Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yangzhou University, City University of Hong Kong, Hefei University of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23300",children:"https://arxiv.org/pdf/2512.23300"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/9624219/AI4reading",children:"https://github.com/9624219/AI4reading"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed AI4Reading, a novel multi-agent collaboration system for automatically generating podcast-like Chinese audiobook interpretations. 2. Designed a framework with 11 specialized agents (e.g., topic analysts, case analysts, editors) to achieve accurate content preservation, enhanced comprehensibility, and logical narrative structure. 3. Demonstrated through comparison with expert interpretations that the system generates simpler and more accurate interpretative scripts, though speech quality has room for improvement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554ec6976f875a67d0faba12d389415bd8119634b07160c7f87e65bff826fc84_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554ec6976f875a67d0faba12d389415bd8119634b07160c7f87e65bff826fc84_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AI4Reading, a system that uses multi-agent collaboration with LLMs and speech synthesis to automatically generate Chinese audiobook interpretations. The system employs 11 specialized agents to process content for accuracy, comprehensibility, and narrative structure. Evaluation shows the generated scripts are simpler and more accurate than expert ones, though speech generation quality still lags behind."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AI4Reading: Chinese Audiobook Interpretation System] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Manual audiobook interpretation creation is time-consuming and resource-intensive.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Multi-agent collaboration framework with 11 specialized agents using LLMs and speech synthesis.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Generated scripts are simpler and more accurate, but speech generation quality has a gap.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [agent evaluation], [spatial reasoning, long-horizon planning, partial observability, mental simulation, diagnostic benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23328",children:"https://arxiv.org/pdf/2512.23328"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLM\u667a\u80fd\u4f53\u7f3a\u4e4f\u7269\u7406\u4e16\u754c\u90e8\u7f72\u6240\u9700\u7684\u7a33\u5065\u7a7a\u95f4\u5fc3\u667a\u6a21\u578b/LLM agents lack robust spatial mental models for physical-world deployment]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u9b54\u65b9\u7684\u4e09\u5c42\u8bca\u65ad\u57fa\u51c6/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]\n    C --\x3e C2[\u4ece\u5b8c\u6574\u7b26\u53f7\u72b6\u6001\u5230\u90e8\u5206\u89c6\u89c9\u72b6\u6001\u9010\u6b65\u8bc4\u4f30/Progressive evaluation from full symbolic to partial visual state]\n    D --\x3e D1[\u9886\u5148LLM\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u901a\u8fc7\u7387\u4e3a0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]\n    D --\x3e D2[\u63ed\u793a\u4e86\u957f\u671f\u89c4\u5212\u548c\u4e3b\u52a8\u63a2\u7d22\u7684\u6839\u672c\u6027\u5931\u8d25/Exposes fundamental failure in long-term planning and active exploration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [memory systems, cognitive neuroscience, LLM-driven agents, memory security, multimodal memory]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23343",children:"https://arxiv.org/pdf/2512.23343"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AgentMemory/Huaman-Agent-Memory",children:"https://github.com/AgentMemory/Huaman-Agent-Memory"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[AI Meets Brain: Memory Systems / AI\u4e0e\u5927\u8111\uff1a\u8bb0\u5fc6\u7cfb\u7edf] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[Interdisciplinary Gap / \u8de8\u5b66\u79d1\u9e3f\u6c9f]\n    P1 --\x3e P2[Existing works struggle to assimilate human memory essence / \u73b0\u6709\u5de5\u4f5c\u96be\u4ee5\u5438\u6536\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u7cbe\u9ad3]\n\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Systematic Synthesis / \u7cfb\u7edf\u7efc\u8ff0]\n    M1 --\x3e M2[Comparative Analysis / \u5bf9\u6bd4\u5206\u6790]\n    M2 --\x3e M3[Review Benchmarks & Security / \u56de\u987e\u57fa\u51c6\u4e0e\u5b89\u5168]\n\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Unified Memory Framework / \u7edf\u4e00\u7684\u8bb0\u5fc6\u6846\u67b6]\n    R1 --\x3e R2[Future Directions / \u672a\u6765\u65b9\u5411]\n    R2 --\x3e R3[Multimodal Memory & Skill Acquisition / \u591a\u6a21\u6001\u8bb0\u5fc6\u4e0e\u6280\u80fd\u83b7\u53d6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [knowledge-augmented reasoning], [external knowledge graph, subgraph generation, stepwise reasoning, large language models, structured reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xin Zhang, Yang Cao, Baoxing Wu, Xinyi Chen, Kai Song, Siying Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Chongqing Jiaotong University, Chongqing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23356",children:"https://arxiv.org/pdf/2512.23356"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel stepwise reasoning enhancement framework (SGR) that dynamically constructs query-relevant subgraphs from external knowledge bases to guide LLM reasoning. 2. Introduces a method to perform multi-step reasoning grounded in the structured subgraph, reducing the influence of noisy information and improving logical consistency. 3. Demonstrates the framework's effectiveness through experiments on multiple benchmark datasets, showing consistent performance improvements over strong baselines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3999b4a75409aeb08ff19dfef01f6296ec2c9184a598991b29d9ce92c1f8e0a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3999b4a75409aeb08ff19dfef01f6296ec2c9184a598991b29d9ce92c1f8e0a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of LLMs in complex reasoning tasks by proposing SGR, a framework that enhances reasoning through dynamic external subgraph generation and step-by-step inference over the structured knowledge. Experimental results show that SGR improves reasoning accuracy and outperforms baseline methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u57fa\u4e8e\u5916\u90e8\u5b50\u56fe\u751f\u6210\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9010\u6b65\u589e\u5f3a\u63a8\u7406\u6846\u67b6<br>A Stepwise-Enhanced Reasoning Framework for LLMs Based on External Subgraph Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLMs\u5728\u6df1\u5ea6\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218<br>LLMs face challenges in deep reasoning tasks]\n    B1 --\x3e B2[\u751f\u6210\u8fc7\u7a0b\u53ef\u80fd\u5305\u542b\u566a\u58f0\u4fe1\u606f<br>Generation may incorporate noisy information]\n    C --\x3e C1[\u52a8\u6001\u6784\u5efa\u67e5\u8be2\u76f8\u5173\u5916\u90e8\u5b50\u56fe<br>Dynamically constructs query-relevant external subgraphs]\n    C1 --\x3e C2[\u57fa\u4e8e\u5b50\u56fe\u8fdb\u884c\u591a\u6b65\u63a8\u7406<br>Performs multi-step reasoning grounded in the subgraph]\n    C2 --\x3e C3[\u96c6\u6210\u591a\u6761\u63a8\u7406\u8def\u5f84<br>Integrates multiple reasoning paths]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02<br>Outperforms baselines on multiple benchmark datasets]\n    D1 --\x3e D2[\u6709\u6548\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b<br>Effectively enhances LLM reasoning capabilities]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23407",children:"https://arxiv.org/pdf/2512.23407"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Theoretically and empirically extends the neural scaling law to the "familial models" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (\u03b3\u22480.041), validating the "train once, deploy many" paradigm without compromising compute-optimality.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Theoretical Foundations of Scaling Law in Familial Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u7f29\u653e\u5b9a\u5f8b\u5ffd\u7565\u591a\u6a21\u578b\u8303\u5f0f/Traditional scaling laws overlook the multi-model paradigm]\n    C --\x3e C1[\u5f15\u5165\u7c92\u5ea6\u4f5c\u4e3a\u65b0\u53d8\u91cf/Introduce Granularity (G) as a new variable]\n    C --\x3e C2[\u7edf\u4e00\u51fd\u6570\u5f62\u5f0f L(N, D, G)/Unified functional form L(N, D, G)]\n    C --\x3e C3[\u91c7\u7528IsoFLOP\u5b9e\u9a8c\u8bbe\u8ba1/Employ rigorous IsoFLOP experimental design]\n    D --\x3e D1[\u7c92\u5ea6\u60e9\u7f5a\u9075\u5faa\u5e42\u5f8b/Granularity penalty follows a power law]\n    D --\x3e D2[\u6307\u6570\u6781\u5c0f (\u03b3\u22480.041)/Exponent is extremely small]\n    D --\x3e D3[\u9a8c\u8bc1"\u4e00\u6b21\u8bad\u7ec3\uff0c\u591a\u6b21\u90e8\u7f72"/Validates "train once, deploy many"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language model training], [token dropout, entropy-guided regularization, multi-epoch training degradation, autoregressive models, data-constrained adaptation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiapeng Wang, Yiwen Hu, Yanzipeng Gao, Haoyu Wang, Shuo Wang, Hongyu Lu, Jiaxin Mao, Wayne Xin Zhao, Junyi Li, Xiao Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Renmin University of China, Tsinghua University, Tencent (WeChat), City University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23422",children:"https://arxiv.org/pdf/2512.23422"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the root cause of multi-epoch training degradation in autoregressive LLMs as an imbalance in learning dynamics between low-entropy and high-entropy tokens. 2. Proposes EntroDrop, a novel entropy-guided token dropout method that acts as structured data regularization by selectively masking low-entropy tokens during training. 3. Demonstrates through experiments on models from 0.6B to 8B parameters that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac5e3a92b434a83b0821bec18501089160580e913a30fb1e39f43b38943b7ef0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac5e3a92b434a83b0821bec18501089160580e913a30fb1e39f43b38943b7ef0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of performance degradation in autoregressive language models during multi-epoch training on limited domain data. It proposes EntroDrop, an entropy-guided token dropout method that selectively masks predictable, low-entropy tokens to regularize training and uses a curriculum schedule to adjust regularization strength. Experiments show EntroDrop effectively mitigates overfitting and maintains robust model performance across different scales, offering a promising approach for adapting LLMs in data-constrained domains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6570\u636e\u7a00\u7f3a\u4e0e\u591a\u8f6e\u8bad\u7ec3\u9000\u5316 / Data Scarcity & Multi-epoch Training Degradation]\n    C --\x3e C1[\u71b5\u5f15\u5bfc\u7684\u4ee4\u724c\u4e22\u5f03 / Entropy-Guided Token Dropout (EntroDrop)]\n    C1 --\x3e C2[\u7ed3\u6784\u5316\u6570\u636e\u6b63\u5219\u5316 / Structured Data Regularization]\n    C1 --\x3e C3[\u8bfe\u7a0b\u5b66\u4e60\u8ba1\u5212 / Curriculum Schedule]\n    D --\x3e D1[\u4f18\u4e8e\u57fa\u7ebf\u6b63\u5219\u5316\u65b9\u6cd5 / Outperforms Standard Regularization Baselines]\n    D --\x3e D2[\u4fdd\u6301\u591a\u8f6e\u8bad\u7ec3\u9c81\u68d2\u6027 / Maintains Robust Performance in Multi-epoch Training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [bias mitigation], [Causal-Contrastive Preference Optimization, spurious feature correlation, fairness-sensitive preference update, counterfactual signals, composite bias]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xuan Feng, Bo An, Tianlong Gu, Liang Chang, Fengrui Hao, Peipeng Yu, Shuai Zhao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Jinan University, Nanyang Technological University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23430",children:"https://arxiv.org/pdf/2512.23430"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework that simultaneously discovers and suppresses latent spurious feature correlations to mitigate both stereotypical and structural biases in LLMs. 2. Proposes a method that leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths during optimization. 3. Designs a fairness-sensitive preference update mechanism that dynamically evaluates logit-level contributions to suppress shortcut features while preserving general reasoning capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26a9c5a41f0184f3fdb110be083a06995183a94e4255ffb7b2066647ec2306c9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26a9c5a41f0184f3fdb110be083a06995183a94e4255ffb7b2066647ec2306c9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the composite bias problem in LLMs, where mitigating one type of bias (e.g., stereotypical) often worsens another (e.g., structural). It proposes Causal-Contrastive Preference Optimization (C2PO), a unified framework that uses causal counterfactual signals and a fairness-sensitive update to suppress spurious feature correlations. Experiments show C2PO effectively reduces both bias types while maintaining strong general reasoning performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Composite Bias in LLMs<br>\u523b\u677f\u5370\u8c61\u4e0e\u7ed3\u6784\u504f\u89c1\u5e76\u5b58] --\x3e B1[\u523b\u677f\u5370\u8c61\u504f\u89c1/Stereotypical Bias<br>e.g., gender, race]\n    B --\x3e B2[\u7ed3\u6784\u504f\u89c1/Structural Bias<br>e.g., lexical overlap, position]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Causal-Contrastive Preference Optimization (C2PO)] --\x3e C1[\u56e0\u679c\u53cd\u4e8b\u5b9e\u4fe1\u53f7/Causal Counterfactual Signals<br>\u9694\u79bb\u504f\u89c1\u7279\u5f81]\n    C --\x3e C2[\u516c\u5e73\u654f\u611f\u504f\u597d\u66f4\u65b0/Fairness-Sensitive Preference Update<br>\u52a8\u6001\u6291\u5236\u6377\u5f84\u7279\u5f81]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Extensive Experiments] --\x3e D1[\u51cf\u8f7b\u504f\u89c1/Mitigates Biases<br>BBQ, Unqover, MNLI, HANS]\n    D --\x3e D2[\u4fdd\u6301\u901a\u7528\u80fd\u529b/Preserves General Utility<br>MMLU, GSM8K]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [scientometrics], [gender diversity, team roles, author contribution statements, threshold regression, citation impact]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yi Zhao, Yongjun Zhu, Donghun Kim, Yuzhuo Wang, Heng Zhang, Chao Lu, Chengzhi Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Anhui University, Yonsei University, Nanjing University, Central China Normal University, Hohai University, Nanjing University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23429",children:"https://arxiv.org/pdf/2512.23429"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a team roles perspective by classifying authors into leadership and support roles using contribution statements, moving beyond aggregate diversity measures. 2. Discovered a non-linear (inverted U-shape) relationship between gender diversity and team impact for both leadership and support groups. 3. Revealed the moderating effect of team size, showing that the impact of leadership-group gender diversity shifts from negative to positive as team size increases, while support-group diversity remains consistently positive."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8e0d6494840d098b4c65bf9f6eb6b4b27a82bd62024c95c0f60db9e5b8fa31f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8e0d6494840d098b4c65bf9f6eb6b4b27a82bd62024c95c0f60db9e5b8fa31f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how gender diversity within specific team roles (leadership vs. support) affects scientific team impact, measured by citations. By analyzing over 130,000 PLOS papers and using contribution statements to define roles, the authors employed multivariable and threshold regression. They found the relationship is an inverted U-shape, identified high-impact team compositions, and showed that team size significantly moderates the effect of leadership diversity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Effect of Gender Diversity on Scientific Team Impact<br>\u6027\u522b\u591a\u6837\u6027\u5bf9\u79d1\u7814\u56e2\u961f\u5f71\u54cd\u529b\u7684\u5f71\u54cd] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Inconsistent findings on gender diversity's effect,<br>lack of role-differentiated analysis.<br>\u6027\u522b\u591a\u6837\u6027\u5f71\u54cd\u7ed3\u8bba\u4e0d\u4e00\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u56e2\u961f\u89d2\u8272\u7684\u5206\u6790]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Analyzed 130k+ PLOS papers, used contribution<br>statements for role classification (leadership/support),<br>applied multivariable & threshold regression.<br>\u5206\u679013\u4e07+PLOS\u8bba\u6587\uff0c\u5229\u7528\u8d21\u732e\u58f0\u660e\u8fdb\u884c\u89d2\u8272\u5206\u7c7b\uff0c\u5e94\u7528\u591a\u5143\u53ca\u9608\u503c\u56de\u5f52]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>1. Inverted U-shape relationship.<br>2. All-female leadership + all-male support yields high impact.<br>3. Team size moderates leadership diversity effect.<br>1. \u5012U\u578b\u5173\u7cfb\u3002<br>2. \u5168\u5973\u6027\u9886\u5bfc+\u5168\u7537\u6027\u652f\u6301\u7684\u56e2\u961f\u5f71\u54cd\u529b\u66f4\u9ad8\u3002<br>3. \u56e2\u961f\u89c4\u6a21\u8c03\u8282\u9886\u5bfc\u7ec4\u591a\u6837\u6027\u6548\u5e94\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [evaluation & benchmarking], [clinical reasoning, dynamic evaluation, diagnostic dialogue, knowledge graph, multi-turn interaction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuqi Tang, Jing Yu, Zichang Su, Kehua Feng, Zhihui Zhu, Libin Wang, Lei Liang, Qiang Zhang, Keyan Ding, Huajun Chen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, AntGroup"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23440",children:"https://arxiv.org/pdf/2512.23440"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ClinDEF, a dynamic evaluation framework for LLMs that simulates multi-turn diagnostic dialogues grounded in a disease knowledge graph to better reflect real-world clinical reasoning. 2. Introduces a granular evaluation protocol that goes beyond diagnostic accuracy to include efficiency analysis and rubric-based assessment of diagnostic quality. 3. Demonstrates that the framework effectively exposes critical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63b6e11110703cffd0afa796f1abd60fa8928363db33520ff03e2f7a54abe3b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63b6e11110703cffd0afa796f1abd60fa8928363db33520ff03e2f7a54abe3b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap in evaluating LLMs for clinical reasoning, which is a dynamic, interactive process poorly captured by static benchmarks. It proposes ClinDEF, a framework that uses a disease knowledge graph to dynamically generate patient cases and simulate diagnostic dialogues between an LLM doctor and an automated patient. Experiments show that ClinDEF effectively reveals critical reasoning deficiencies in advanced LLMs, providing a more realistic and detailed assessment tool."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[ClinDEF: A Dynamic Evaluation Framework for LLMs in Clinical Reasoning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u52a8\u6001\u4e34\u5e8a\u63a8\u7406/Static benchmarks fail to evaluate dynamic clinical reasoning]\n    C --\x3e C1[\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u52a8\u6001\u75c5\u4f8b\u751f\u6210/Dynamic case generation via disease knowledge graph]\n    C --\x3e C2[\u6a21\u62df\u591a\u8f6e\u8bca\u65ad\u5bf9\u8bdd/Simulated multi-turn diagnostic dialogue]\n    C --\x3e C3[\u7ec6\u7c92\u5ea6\u591a\u5c42\u7ea7\u8bc4\u4f30\u534f\u8bae/Fine-grained, multi-level evaluation protocol]\n    D --\x3e D1[\u6709\u6548\u66b4\u9732LLM\u7684\u4e34\u5e8a\u63a8\u7406\u7f3a\u9677/Effectively exposes LLMs' clinical reasoning gaps]\n    D --\x3e D2[\u63d0\u4f9b\u66f4\u7ec6\u81f4\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u8bc4\u4f30/Offers more nuanced, clinically meaningful evaluation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Mixture-of-Experts, Router-Expert Coupling, Auxiliary Loss, Expert Specialization, Efficient Training]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance, Renmin University of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23447",children:"https://arxiv.org/pdf/2512.23447"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel lightweight auxiliary loss (ERC loss) to explicitly couple router decisions with expert capabilities in MoE models. 2. Introduces a computationally efficient method that scales with the square of the number of experts (n^2), independent of batch size, unlike prior token-dependent methods. 3. Enables flexible control and quantitative tracking of expert specialization levels during training, providing new insights into MoE model dynamics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the misalignment between router decisions and expert capabilities in Mixture-of-Experts (MoE) models. It proposes an Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that enforces constraints via perturbed router embeddings to ensure each expert specializes in its routed tokens and each router embedding faithfully represents its expert. The method is shown to be effective and computationally efficient, enabling better control and analysis of expert specialization during large-scale pre-training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[\u8026\u5408\u4e13\u5bb6\u4e0e\u8def\u7531\u5668<br/>Coupling Experts and Routers in Mixture-of-Experts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u8def\u7531\u5668\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u5339\u914d<br/>Router decisions misaligned with expert capabilities]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931 (ERC Loss)<br/>Propose Expert-Router Coupling (ERC) Loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9ad8\u6548\u8ba1\u7b97\uff0c\u53ef\u91cf\u5316\u8ffd\u8e2a\u4e13\u5bb6\u4e13\u4e1a\u5316<br/>Improved performance, efficient computation, quantifiable tracking of specialization]\n    C --\x3e E[\u65b9\u6cd5\u539f\u7406/Mechanism: \u4f7f\u7528\u6270\u52a8\u8def\u7531\u5668\u5d4c\u5165\u4f5c\u4e3a\u4ee3\u7406\u4ee4\u724c<br/>Use perturbed router embeddings as proxy tokens]\n    E --\x3e F[\u7ea6\u675f/Constraints: \u4e13\u5bb6\u5bf9\u81ea\u8eab\u4ee3\u7406\u4ee4\u724c\u6fc0\u6d3b\u6700\u9ad8\uff1b\u4ee3\u7406\u4ee4\u724c\u5f15\u53d1\u5bf9\u5e94\u4e13\u5bb6\u6700\u5f3a\u6fc0\u6d3b<br/>Expert highest activation for own proxy; Proxy elicits strongest activation from corresponding expert]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23457",children:"https://arxiv.org/pdf/2512.23457"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/zhangkc97/HiR",children:"https://github.com/zhangkc97/HiR"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u7a00\u758f/\u4e0d\u53ef\u533a\u5206\u7684\u5956\u52b1\u963b\u788d\u5b66\u4e60<br>Sparse/Indistinguishable Rewards Impede Learning]\n    Method[\u540e\u89c1\u6307\u4ee4\u91cd\u653e (HiR)<br>Hindsight instruction Replay (HiR)]\n    Results[\u8de8\u4efb\u52a1\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548<br>Effective Across Tasks & Computationally Efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text clustering], [hierarchical clustering, density-based clustering, semantic embeddings, large language models, topic modeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Thomas Haschka, Joseph Bakarji"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Wien, American University of Beirut"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23471",children:"https://arxiv.org/pdf/2512.23471"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel nested density clustering method to construct hierarchical semantic trees from text embeddings. 2. Demonstrates the method's application for data-driven discovery of research areas and subfields without predefined categories. 3. Validates the approach's robustness and general applicability across diverse domains using benchmark datasets like 20 Newsgroups and IMDB reviews."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of uncovering the global hierarchical semantic structure in text corpora, which remains opaque when using LLM embeddings only for similarity search. It proposes a method that applies nested density clustering on LLM embeddings, gradually relaxing a density criterion to merge clusters into a hierarchical tree. This approach enables the data-driven discovery of semantic relationships and topic hierarchies without predefined categories, as demonstrated on scientific abstracts and benchmark datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Semantic Tree Inference on Text Corpa / \u6587\u672c\u8bed\u6599\u5e93\u7684\u8bed\u4e49\u6811\u63a8\u65ad"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Opaque global semantic structure in text corpora / \u6587\u672c\u8bed\u6599\u5e93\u4e2d\u4e0d\u900f\u660e\u7684\u5168\u5c40\u8bed\u4e49\u7ed3\u6784"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Nested density clustering on LLM embeddings / \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u5d4c\u5957\u5bc6\u5ea6\u805a\u7c7b"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Data-driven hierarchical semantic tree discovery / \u6570\u636e\u9a71\u52a8\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u6811\u53d1\u73b0"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Automatic Detection of Complex Quotation Patterns in Aggadic Literature"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text reuse detection], [morphology-aware alignment, context-sensitive enrichment, quotation pattern classification, Hebrew text]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hadar Miller, Tsvi Kuflik, Moshe Lavee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Haifa"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23504",children:"https://arxiv.org/pdf/2512.23504"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes ACT, a novel three-stage algorithm combining morphology-aware alignment and context-sensitive enrichment for detecting complex biblical quotations in Rabbinic literature. 2. Introduces the ability to classify complex stylistic citation patterns such as "Wave" and "Echo" quotations, which existing frameworks struggle with. 3. Demonstrates superior performance (F1=0.91) over leading baselines, effectively bridging the gap between machine-based detection and human editorial judgment in digital humanities.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2678dc7eaa177b1fdea4a65073a64104dc415c64285803a4059dad8d21355551_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2678dc7eaa177b1fdea4a65073a64104dc415c64285803a4059dad8d21355551_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces ACT, a three-stage algorithm designed to automatically detect complex biblical quotations in Rabbinic literature by using morphology-aware alignment and context-sensitive enrichment. The method outperforms existing systems, achieving an F1 score of 0.91, and successfully identifies intricate citation patterns like "Wave" and "Echo" quotations. This work advances computational philology by improving the detection of short, paraphrased, and embedded text reuse in morphologically rich languages like Hebrew.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Automatic Detection of Complex Quotation Patterns in Aggadic Literature<br/>\u300a\u963f\u52a0\u8fbe\u6587\u5b66\u4e2d\u590d\u6742\u5f15\u7528\u6a21\u5f0f\u7684\u81ea\u52a8\u68c0\u6d4b\u300b"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Existing frameworks struggle with short, paraphrased, embedded quotations<br/>\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u5904\u7406\u7b80\u77ed\u3001\u8f6c\u8ff0\u3001\u7ed3\u6784\u5d4c\u5165\u7684\u5f15\u7528"] --\x3e P1["\u6311\u6218/Challenges<br/>Morphologically rich language (Hebrew)<br/>\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff08\u5e0c\u4f2f\u6765\u8bed\uff09"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>ACT (Allocate Connections between Texts) algorithm<br/>ACT\uff08\u6587\u672c\u95f4\u8fde\u63a5\u5206\u914d\uff09\u7b97\u6cd5"] --\x3e M1["\u9636\u6bb51/Stage 1<br/>Morphology-aware alignment<br/>\u5f62\u6001\u611f\u77e5\u5bf9\u9f50"]\n    Method --\x3e M2["\u9636\u6bb52/Stage 2<br/>Context-sensitive enrichment<br/>\u4e0a\u4e0b\u6587\u654f\u611f\u589e\u5f3a"]\n    Method --\x3e M3["\u76ee\u6807/Goal<br/>Detect \'Wave\' & \'Echo\' patterns<br/>\u68c0\u6d4b\'\u6ce2\u6d6a\'\u4e0e\'\u56de\u58f0\'\u5f15\u7528\u6a21\u5f0f"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Outperforms baselines (Dicta, Passim, Text-Matcher)<br/>\u8d85\u8d8a\u57fa\u7ebf\u7cfb\u7edf"] --\x3e R1["\u6027\u80fd/Performance<br/>F1=0.91, Recall=0.89, Precision=0.94<br/>F1\u5206\u65700.91\uff0c\u53ec\u56de\u73870.89\uff0c\u7cbe\u786e\u73870.94"]\n    Results --\x3e R2["\u5e94\u7528/Application<br/>Opens avenues for genre classification & intertextual analysis<br/>\u4e3a\u4f53\u88c1\u5206\u7c7b\u4e0e\u4e92\u6587\u5206\u6790\u5f00\u8f9f\u65b0\u9014\u5f84"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm robustness & bias], [confirmation bias, latent concept experts, inference-time intervention, multi-agent debate, compositional language]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hazel Kim, Philip Torr"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Oxford"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23518",children:"https://arxiv.org/pdf/2512.23518"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MoLaCE, a lightweight inference-time framework that mitigates LLM confirmation bias by mixing experts instantiated as different activation strengths over latent concepts. 2. Provides the key insight that no single fixed intervention works universally because prompts reweight latent concepts in prompt-specific ways. 3. Demonstrates that the method enables a single LLM to emulate debate benefits efficiently and can be integrated into multi-agent frameworks to reduce correlated errors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/293e04fd5f7eeb86a4cdafce819a7b7ac1c4ab97756b2f0a2b0305fec15d2fe8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/293e04fd5f7eeb86a4cdafce819a7b7ac1c4ab97756b2f0a2b0305fec15d2fe8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of confirmation bias in LLMs, where models reinforce a preferred answer implied by a prompt. It proposes MoLaCE, a framework that mixes latent concept experts at inference time to diversify perspectives internally. The method reduces bias, improves robustness, and matches multi-agent debate performance with significantly less computation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MoLaCE: Mixture of Latent Concept Experts <br/> \u6f5c\u5728\u6982\u5ff5\u4e13\u5bb6\u6df7\u5408] --\x3e B[Problem: LLM Confirmation Bias <br/> \u95ee\u9898: LLM\u786e\u8ba4\u504f\u8bef]\n    A --\x3e C[Method: Mix Experts via Latent Concept Activations <br/> \u65b9\u6cd5: \u901a\u8fc7\u6f5c\u5728\u6982\u5ff5\u6fc0\u6d3b\u6df7\u5408\u4e13\u5bb6]\n    A --\x3e D[Results: Reduces Bias, Efficient, Matches Multi-Agent Debate <br/> \u7ed3\u679c: \u51cf\u5c11\u504f\u8bef, \u9ad8\u6548, \u5339\u654c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [vision-language model, unified model, semantic generation, autoregression, data scaling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meituan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23512",children:"https://arxiv.org/pdf/2512.23512"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that generation enhances understanding in large-scale VLM training only when operating at the semantic level (e.g., autoregressing high-level visual representations), not at the pixel level. 2. Shows that unified generation-understanding models exhibit superior data scaling trends and higher data utilization efficiency compared to understanding-only models. 3. Proposes that autoregression on input embeddings is an effective and modality-independent method for capturing visual details, enabling pixel-level generation from learned semantics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether visual generation tasks can enhance understanding in large-scale vision-language models. Through large-scale pretraining (>200M samples) with a model called UniHetero, the authors find that semantic-level generation (not pixel-level) improves understanding, reveals better data scaling, and that autoregression on input embeddings effectively captures visual details."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Does visual generation enhance understanding at large scale?);\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Large-scale pretraining of unified model UniHetero (>200M samples));\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results);\n    B --\x3e D;\n    C --\x3e D;\n    D --\x3e E(\u7ed3\u679c1/Result 1: Generation helps, but Only if you generate Semantics, Not Pixels);\n    D --\x3e F(\u7ed3\u679c2/Result 2: Superior Data Scaling trend and higher Data Utilization);\n    D --\x3e G(\u7ed3\u679c3/Result 3: Autoregression on Input Embedding is effective);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [knowledge graphs, self-detection, structured verification, GPT-4o, Gemini-2.5-Flash]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sahil Kale, Antonio Luca Alfeo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Knowledge Verse AI, eCampus University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23547",children:"https://arxiv.org/pdf/2512.23547"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/knowledge-verse-ai/kg-hallu-eval",children:"https://github.com/knowledge-verse-ai/kg-hallu-eval"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel hallucination self-detection method that converts LLM responses into knowledge graphs for structured analysis., 2. Introduces a manually curated and enhanced hallucination detection dataset to support more reliable future benchmarking., 3. Demonstrates significant performance improvements (up to 16% accuracy, 20% F1) over standard self-detection and a state-of-the-art baseline (SelfCheckGPT)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of hallucinations in LLMs by proposing a self-detection method that converts model responses into knowledge graphs to better analyze atomic facts and estimate hallucination likelihood. The method, evaluated on GPT-4o and Gemini-2.5-Flash, shows substantial improvements in accuracy and F1-score over existing approaches. The work concludes that structuring facts as knowledge graphs enables more robust hallucination detection, offering a low-cost, model-agnostic path toward safer language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A["Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: LLM Hallucinations hinder safe deployment"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Convert responses to Knowledge Graphs for structured self-verification"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Up to 16% accuracy & 20% F1 improvement over baselines"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23562",children:"https://arxiv.org/pdf/2512.23562"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/K1nght/VL-RouterBench",children:"https://github.com/K1nght/VL-RouterBench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u590d\u73b0\u7684<br>VLM\u8def\u7531\u8bc4\u4f30\u57fa\u51c6<br>Lack of systematic, reproducible<br>benchmark for VLM routing]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8e\u539f\u59cb\u63a8\u7406\u65e5\u5fd7\u6784\u5efa<br>\u8d28\u91cf\u4e0e\u6210\u672c\u77e9\u9635<br>Construct quality & cost matrices<br>from raw inference logs]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u89c2\u5bdf\u5230\u663e\u8457\u7684\u8def\u7531\u589e\u76ca<br>\u4f46\u4e0e\u7406\u60f3\u6027\u80fd\u4ecd\u6709\u5dee\u8ddd<br>Observe significant routability gain<br>but clear gap to ideal Oracle]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Instruction-Following Evaluation of Large Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [instruction-following evaluation], [large vision-language models, visual instruction tuning, output format specification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Daiki Shiono, Shumpei Miyawaki, Ryota Tanaka, Jun Suzuki"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tohoku University, NTT Corporation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23572",children:"https://arxiv.org/pdf/2512.23572"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Quantitatively demonstrates the decline in instruction-following ability of LVLMs after visual instruction fine-tuning. 2. Constructs new training datasets that highlight whether the output format is specified. 3. Shows that explicitly indicating the output format during fine-tuning helps LVLMs follow instructions more accurately."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies and quantifies a problem where Large Vision-Language Models (LVLMs) lose their instruction-following ability after visual instruction tuning. The authors propose constructing datasets that explicitly specify the output format and find that training with such data mitigates the performance decline. The main conclusion is that including instructions on output format during fine-tuning can help preserve LVLMs' instruction-following capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Instruction-Following Evaluation of Large Vision-Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LVLMs lose instruction-following ability after fine-tuning)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Construct datasets highlighting output format specification)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Explicit output format instructions improve instruction-following)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [spoken language understanding], [spoken language models, style amnesia, multi-turn conversation, paralinguistic features, instruction following]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yu-Xiang Lin, Cheng-Han Chiang, Hung-yi Lee"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Taiwan University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23578",children:"https://arxiv.org/pdf/2512.23578"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and defines the "style amnesia" problem where spoken language models fail to maintain a user-specified speaking style across multiple conversation turns. 2. Provides a comprehensive evaluation across multiple proprietary and open-source SLMs, demonstrating the pervasiveness of the issue across different emotion, accent, volume, and speed styles. 3. Investigates mitigation strategies, finding that explicit recall prompts can partially alleviate the problem and revealing a counter-intuitive weakness when style instructions are placed in system messages.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the problem of "style amnesia" in spoken language models (SLMs), where models instructed to adopt a specific speaking style fail to maintain it over a multi-turn conversation. The authors evaluate several SLMs and find that explicitly prompting the model to recall the style instruction can partially mitigate the issue. The study concludes that current SLMs struggle with long-term style consistency, a critical challenge for natural spoken interactions.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("SLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u65e0\u6cd5\u7ef4\u6301\u6307\u5b9a\u7684\u526f\u8bed\u8a00\u98ce\u683c/SLMs cannot maintain specified paralinguistic style in multi-turn conversation")\n    Method --\x3e M1("\u5728\u591a\u8f6e\u5bf9\u8bdd\u5f00\u59cb\u65f6\u6307\u5b9a\u98ce\u683c\u5e76\u8bc4\u4f30/Instruct style at conversation start and evaluate")\n    Method --\x3e M2("\u4f7f\u7528\u81ea\u52a8\u8bc4\u4f30\u5668\u6d4b\u91cf\u6307\u4ee4\u9075\u5faa\u7387/Use automatic judges to measure instruction-following rate")\n    Method --\x3e M3("\u6d4b\u8bd5\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565/Test different prompting strategies")\n    Results --\x3e R1("\u53d1\u73b0\u98ce\u683c\u9057\u5fd8\u73b0\u8c61\uff0c\u6307\u4ee4\u9075\u5faa\u7387\u968f\u8f6e\u6b21\u4e0b\u964d/Style amnesia found, IF rate degrades over turns")\n    Results --\x3e R2("\u663e\u5f0f\u56de\u5fc6\u6307\u4ee4\u53ef\u90e8\u5206\u7f13\u89e3\u95ee\u9898/Explicit recall can partially mitigate")\n    Results --\x3e R3("\u7cfb\u7edf\u63d0\u793a\u4e2d\u7684\u6307\u4ee4\u6548\u679c\u4e0d\u4f73/Instructions in system prompts perform poorly")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent synthesis, tool calling, Group Relative Policy Optimization (GRPO), closed-loop training, synthetic data generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuwen Li, Wei Zhang, Zelong Huang, Mason Yang, Jiajun Wu, Shawn Guo, Huahao Hu, Lingyi Sun, Jian Yang, Mingjie Tang, Byran Dai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sichuan University, Beihang University, IQuest Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23611",children:"https://arxiv.org/pdf/2512.23611"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces InfTool, a fully autonomous framework that uses a multi-agent role-playing system (User Simulator, Tool-Calling Assistant, MCP Server) to generate high-quality, verified tool-use trajectories from raw API specifications without human annotation. 2. Establishes a closed-loop system where synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, and the improved model then generates higher-quality data, iteratively targeting capability gaps. 3. Demonstrates state-of-the-art performance, transforming a base 32B model's accuracy on the Berkeley Function-Calling Leaderboard from 19.8% to 70.9% using only synthetic data, surpassing much larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e1a538efae439be7bacc740036b08c6466db773784245f942d9dee539143f92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e1a538efae439be7bacc740036b08c6466db773784245f942d9dee539143f92_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of enabling LLMs to reliably use external tools by proposing InfTool, a fully autonomous framework that uses multi-agent role-playing to synthesize and iteratively improve tool-use data in a closed loop. The method eliminates the need for human annotation and significantly boosts model performance, as shown by a 258% accuracy improvement on a standard benchmark, rivaling top proprietary models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLMs\u53ef\u9760\u8c03\u7528\u5916\u90e8\u5de5\u5177\u5b58\u5728\u74f6\u9888<br>1. \u6570\u636e\u7a00\u7f3a\u4e0e\u6807\u6ce8\u6602\u8d35<br>2. \u6cdb\u5316\u80fd\u529b\u5dee<br>3. \u5355\u6a21\u578b\u5408\u6210\u5b58\u5728\u8d28\u91cf\u4e0a\u9650]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>InfTool: \u81ea\u6f14\u5316\u7684\u591a\u667a\u80fd\u4f53\u5408\u6210\u6846\u67b6<br>1. \u4e09\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u751f\u6210\u8f68\u8ff9<br>2. \u95ed\u73af\u8bad\u7ec3: GRPO\u4e0e\u95e8\u63a7\u5956\u52b1<br>3. \u4ece\u539f\u59cbAPI\u89c4\u8303\u5408\u6210\u6570\u636e]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5728BFCL\u4e0a\uff0c32B\u57fa\u7840\u6a21\u578b\u51c6\u786e\u7387\u4ece19.8%\u63d0\u5347\u81f370.9%<br>\u8d85\u8d8a10\u500d\u5927\u7684\u6a21\u578b\uff0c\u5ab2\u7f8eClaude-Opus<br>\u5b8c\u5168\u4f7f\u7528\u5408\u6210\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] A Dataset and Benchmark for Consumer Healthcare Question Summarization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text summarization], [consumer health questions, abstractive summarization, dataset creation, benchmark evaluation, domain-expert annotation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abhishek Basu, Deepak Gupta, Dina Demner-Fushman, Shweta Yadav"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois at Chicago, U.S. National Library of Medicine (National Institutes of Health)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23637",children:"https://arxiv.org/pdf/2512.23637"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces CHQ-Summ, a new dataset of 1507 domain-expert annotated consumer health questions and summaries. 2. Addresses the lack of domain-expert annotated data for the specific task of consumer healthcare question summarization. 3. Provides a benchmark evaluation of the dataset using multiple state-of-the-art summarization models to demonstrate its utility."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0dbdf4700510ae2858b673b1f5b293ecd42ac6dcf6f757d42fb65d3706d4b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0dbdf4700510ae2858b673b1f5b293ecd42ac6dcf6f757d42fb65d3706d4b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of summarizing verbose consumer health questions by introducing CHQ-Summ, a new expert-annotated dataset. It benchmarks this dataset on modern summarization models to validate its effectiveness for developing better healthcare question understanding systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CHQ-Summ Dataset and Benchmark] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Lack of expert-annotated data for summarizing consumer health questions]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Create CHQ-Summ dataset with 1507 expert-annotated Q&A pairs]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Benchmark shows dataset's effectiveness for model training and evaluation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Nested Browser-Use Learning for Agentic Information Seeking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [information-seeking agents, browser interaction, ReAct-style agents, nested framework]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tongyi Lab, Alibaba Group"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23647",children:"https://arxiv.org/pdf/2512.23647"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Alibaba-NLP/DeepResearch",children:"https://github.com/Alibaba-NLP/DeepResearch"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a minimal and complete browser-action framework for agents, 2. Introduces a nested structure to decouple interaction control from page exploration, 3. Demonstrates improved performance on deep information-seeking benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of current information-seeking agents, which rely on simple API calls and cannot perform real browsing. It proposes NestBrowse, a framework that uses a nested structure to enable fine-grained browser control for agents, simplifying reasoning and improving performance on deep search tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Nested Browser-Use Learning for Agentic Information Seeking<br>\u9762\u5411\u667a\u80fd\u4fe1\u606f\u641c\u7d22\u7684\u5d4c\u5957\u6d4f\u89c8\u5668\u4f7f\u7528\u5b66\u4e60"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Agents lack real browsing, limited to APIs."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>NestBrowse: nested browser-action framework."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Better performance on deep IS benchmarks."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Less is more: Probabilistic reduction is best explained by small-scale predictability measures"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [computational psycholinguistics], [probabilistic reduction, n-gram, language model, cognitive planning, articulatory dynamics]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cassandra L. Jacobs, Andr\xe9s Bux\xf3-Lugo, Anna K. Taylor, Marie Leopold-Hooke"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University at Buffalo, EURECOM"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23659",children:"https://arxiv.org/pdf/2512.23659"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that local, small-scale predictability measures (n-grams) are sufficient to explain probabilistic reduction in speech, challenging the necessity of large-context LLMs for this cognitive phenomenon. 2. Provides evidence that n-gram representations can serve as effective cognitive units of planning in language production. 3. Argues for the cognitive plausibility and computational efficiency of incremental, phrase-level planning over whole-utterance planning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5de081f03ed5643721a634ff064418b253cb9682ea7ef7e28d08b32112cd4028_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5de081f03ed5643721a634ff064418b253cb9682ea7ef7e28d08b32112cd4028_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates how much context is needed to model the relationship between word predictability and articulatory reduction (probabilistic reduction). It finds that local n-gram probabilities are sufficient to explain this effect, suggesting that language production planning operates on a small, incremental scale rather than relying on large-context language models. The main conclusion is that "less is more"\u2014small-scale predictability measures best explain the cognitive phenomenon of probabilistic reduction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Less is more: Probabilistic reduction is best explained by small-scale predictability measures<br>\u8bba\u6587\u6807\u9898"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>How much context is needed to link language model probabilities to cognitive phenomena?"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Compare n-gram vs. whole-utterance (LLM) context for modeling probabilistic reduction"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>N-gram representations suffice; small-scale predictability is best"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial attacks], [prompt injection, large language models, academic peer review, multilingual, adversarial robustness]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," International School of Athens, Idiap Research Institute"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23684",children:"https://arxiv.org/pdf/2512.23684"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Constructed a dataset of ~500 real ICML papers to empirically evaluate hidden prompt injection attacks in a realistic academic reviewing context. 2. Demonstrated that embedding semantically equivalent adversarial instructions in multiple languages (English, Japanese, Chinese, Arabic) can significantly alter LLM-generated review scores and decisions. 3. Revealed notable cross-lingual differences in attack effectiveness, with Arabic injections having minimal impact compared to others."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the vulnerability of LLM-based academic peer review systems to hidden prompt injection attacks. By injecting adversarial instructions in four languages into a dataset of real papers and having an LLM review them, the authors found that such attacks can substantially change review outcomes for English, Japanese, and Chinese, but not Arabic. The results highlight a critical security risk and language-dependent susceptibility in automated reviewing pipelines."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLM-based reviewing systems are vulnerable to hidden prompt injection attacks.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Inject semantically equivalent adversarial prompts in 4 languages into ~500 real papers and review with an LLM.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>English, Japanese, Chinese injections change scores/decisions; Arabic injections have little effect.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Eliciting Behaviors in Multi-Turn Conversations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [behavior elicitation, multi-turn conversation, online methods, dynamic benchmarks, test case generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Google DeepMind, Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23701",children:"https://arxiv.org/pdf/2512.23701"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an analytical framework categorizing behavior elicitation methods into three families based on their interaction with the target model (prior knowledge, offline, online). 2. Introduces a generalized multi-turn formulation for online behavior elicitation methods, unifying single-turn and multi-turn settings. 3. Demonstrates the superior efficiency of online methods in discovering failure cases in multi-turn conversations compared to static benchmarks, advocating for a shift to dynamic evaluation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the problem of efficiently eliciting specific behaviors from large language models in multi-turn conversational settings. It introduces a framework for categorizing existing elicitation methods and proposes a generalized online method for multi-turn interactions. The key finding is that online methods can discover many more failure cases with few queries than static benchmarks, highlighting the need for dynamic evaluation approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Eliciting Behaviors in Multi-Turn Conversations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How to efficiently elicit specific behaviors from LLMs in multi-turn conversations?)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Categorizes methods into three families; Proposes a generalized multi-turn online formulation.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Online methods achieve high success rates with few queries, outperforming static benchmarks.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [human feedback for alignment], [fine-grained feedback, preference optimization, feedback-driven improvement chains, direct alignment, Lamarckian evolution]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sky CH-Wang, Justin Svegliato, Helen Appel, Jason Eisner"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University, Microsoft, Johns Hopkins University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23693",children:"https://arxiv.org/pdf/2512.23693"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces a novel dataset with fine-grained human feedback where annotators mark and explain "liked" and "disliked" spans in model responses. 2. Proposes a method to use this feedback to generate "feedback-driven improvement chains," where the base model incrementally rewrites disliked spans to create a sequence of improved responses. 3. Demonstrates that constructing preference pairs from adjacent steps in these chains for direct alignment outperforms standard A/B preference ranking or full contrastive rewrites, leading to more efficient and effective preference tuning.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3cdf920bd014e6557ee3cfbe026bd31e5dfb982b5040c20b2797bf29ee9574_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3cdf920bd014e6557ee3cfbe026bd31e5dfb982b5040c20b2797bf29ee9574_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a method for fine-tuning LLMs using fine-grained human feedback on specific text spans. The core idea is to have annotators mark liked/disliked spans, then use the base model to iteratively rewrite the disliked spans, creating a chain of improvements. The key finding is that using these incremental revisions to create preference pairs for direct alignment leads to better model performance than standard preference-based methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6807\u51c6A/B\u504f\u597d\u5bf9\u566a\u58f0\u5927\u4e14\u96be\u4ee5\u5224\u65ad/Standard A/B preference pairs are noisy and hard to judge]\n    C --\x3e C1[\u6536\u96c6\u7ec6\u7c92\u5ea6\u8de8\u5ea6\u7ea7\u53cd\u9988/Collect fine-grained span-level feedback]\n    C --\x3e C2[\u6784\u5efa\u53cd\u9988\u9a71\u52a8\u7684\u6539\u8fdb\u94fe/Build feedback-driven improvement chains]\n    C --\x3e C3[\u4ece\u94fe\u4e2d\u76f8\u90bb\u6b65\u9aa4\u521b\u5efa\u504f\u597d\u5bf9/Create preference pairs from adjacent chain steps]\n    D --\x3e D1[\u57fa\u4e8e\u4fee\u8ba2\u7684\u76d1\u7763\u66f4\u6709\u6548/Revision-based supervision is more effective]\n    D --\x3e D2[\u8d85\u8d8a\u6807\u51c6\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5/Outperforms standard direct alignment methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Web World Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [world model, language agent, web framework, structured latent state, deterministic generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles, University of Pennsylvania"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23676",children:"https://arxiv.org/pdf/2512.23676"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://princeton-ai2-lab.github.io/Web-World-Models/",children:"https://princeton-ai2-lab.github.io/Web-World-Models/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world "physics" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Web World Models] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models)."]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top."]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [speech recognition], [context-conditioned ASR, entity-aware evaluation, professional speech]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Deepak Babu Piskala"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: gmail.com)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23686",children:"https://arxiv.org/pdf/2512.23686"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/prdeepakbabu/ProfASR-Bench",children:"https://github.com/prdeepakbabu/ProfASR-Bench"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces ProfASR-Bench, a benchmark for evaluating context-conditioned ASR in high-stakes professional domains (finance, medicine, legal, technology). 2. Identifies and defines the "context-utilization gap" (CUG), showing current promptable models underuse textual context for improving recognition. 3. Provides a standardized evaluation framework with a context ladder, entity/slice-aware reporting, and a reproducible testbed for comparing fusion strategies.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces ProfASR-Bench, a benchmark for evaluating Automatic Speech Recognition (ASR) in high-stakes professional settings. It tests models like Whisper and Qwen-Omni with various contextual prompts and finds a "context-utilization gap," where current systems fail to effectively use available side information to improve accuracy, despite being promptable. The benchmark provides tools for entity-aware and slice-wise evaluation to advance context-conditioned ASR.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PROFASR-BENCH: A Benchmark for Context-Conditioned ASR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u4f4e\u4f30\u4e13\u4e1a\u573a\u666f\u6311\u6218 / Existing benchmarks underplay professional challenges]\n    B1 --\x3e B2[\u5bc6\u96c6\u672f\u8bed, \u6b63\u5f0f\u8bed\u4f53, \u5173\u952e\u5b9e\u4f53\u96f6\u5bb9\u5fcd / Dense terminology, formal register, zero tolerance for entity errors]\n    C --\x3e C1[\u6784\u5efa\u4e13\u4e1a\u8bed\u97f3\u8bc4\u4f30\u5957\u4ef6 / Build professional-talk evaluation suite]\n    C1 --\x3e C2[\u914d\u5bf9\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e0e\u76ee\u6807\u8bdd\u8bed / Pair natural-language prompts with target utterances]\n    C2 --\x3e C3[\u652f\u6301\u5b9e\u4f53\u611f\u77e5\u548c\u5206\u7247\u62a5\u544a / Support entity-aware and slice-wise reporting]\n    D --\x3e D1[\u53d1\u73b0\u4e0a\u4e0b\u6587\u5229\u7528\u5dee\u8ddd(CUG) / Uncover context-utilization gap (CUG)]\n    D1 --\x3e D2[\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\u5bf9WER\u6539\u5584\u751a\u5fae / Lightweight textual context yields little WER change]\n    D2 --\x3e D3[\u5bf9\u6297\u6027\u63d0\u793a\u4e0d\u4f1a\u53ef\u9760\u964d\u4f4e\u6027\u80fd / Adversarial prompts do not reliably degrade performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] Training AI Co-Scientists Using Rubric Rewards"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [research plan generation, self-grading, rubric rewards]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meta Superintelligence Labs, ELLIS Institute T\xfcbingen, Max Planck Institute for Intelligent Systems, University of Oxford, University of Cambridge"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23707",children:"https://arxiv.org/pdf/2512.23707"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A scalable method to automatically extract research goals and goal-specific grading rubrics from existing papers to build a training corpus. 2. A reinforcement learning framework with self-grading, where a frozen initial model acts as the grader using rubrics, enabling unsupervised improvement. 3. Demonstration of significant performance gains (12-22% relative improvement) and cross-domain generalization (e.g., to medical research) validated by human experts and frontier model juries."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of training language models to generate high-quality, constraint-following research plans. The proposed method uses reinforcement learning with self-grading, where rubrics automatically extracted from research papers provide reward signals. The approach shows significant improvements in plan quality and generalizes across domains like machine learning and medicine, validated by human expert preference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Training AI Co-Scientists Using Rubric Rewards"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LMs struggle to generate research plans that follow all constraints."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: RL with self-grading using automatically extracted rubrics."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Human experts prefer finetuned model\'s plans; method generalizes across domains."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251230] The Big Three in Marriage Talk: LLM-Assisted Analysis of Moral Ethics and Sentiment on Weibo and Xiaohongshu"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [sentiment analysis], [large language model, moral ethics, content analysis, sentiment classification, Shweder's Big Three]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Frank Tian-Fang Ye, Xiaozi Gao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The HKU SPACE Community College, The University of Hong Kong; Education University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23609",children:"https://arxiv.org/pdf/2512.23609"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Applied Shweder's Big Three moral ethics framework (Autonomy, Community, Divinity) to analyze public discourse on marriage in China via social media. 2. Demonstrated the utility of LLM-assisted content analysis for scaling qualitative analysis of large-scale social media data. 3. Revealed platform-specific sentiment patterns and associations between moral framing and sentiment, linking negative marriage attitudes to concerns over personal autonomy and communal obligations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7156acf600b8fd25e2938afd6f0f65ccd5503ace7d9b3afb68c9bb152d1a0fee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7156acf600b8fd25e2938afd6f0f65ccd5503ace7d9b3afb68c9bb152d1a0fee_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study uses large language models to analyze 219,358 marriage-related posts from Weibo and Xiaohongshu, coding them for sentiment and moral dimensions based on Shweder's Big Three framework. It finds platform differences in sentiment and that posts invoking Autonomy and Community ethics are predominantly negative, while Divinity-framed posts are more neutral or positive. The research demonstrates LLMs' utility for scaling qualitative analysis and provides insights into the moral reasoning behind declining marriage rates in China."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Big Three in Marriage Talk: LLM-Assisted Analysis of Moral Ethics and Sentiment on Weibo and Xiaohongshu] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e2d\u56fd\u5a5a\u59fb\u767b\u8bb0\u6570\u4e0b\u964d<br/>Declining Marriage Registrations in China]\n    B --\x3e B2[\u516c\u4f17\u5bf9\u5a5a\u59fb\u7684\u6001\u5ea6<br/>Public Attitudes Toward Marriage]\n    C --\x3e C1[LLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790<br/>LLM-Assisted Content Analysis]\n    C --\x3e C2[\u57fa\u4e8eShweder\u9053\u5fb7\u4e09\u539f\u5219\u7f16\u7801<br/>Coding with Shweder's Big Three Ethics]\n    C --\x3e C3[\u5206\u6790219,358\u6761\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50<br/>Analyzing 219,358 Social Media Posts]\n    D --\x3e D1[\u5e73\u53f0\u5dee\u5f02: \u5fae\u535a\u504f\u6b63\u9762, \u5c0f\u7ea2\u4e66\u504f\u4e2d\u6027<br/>Platform Differences: Weibo Positive, Xiaohongshu Neutral]\n    D --\x3e D2[\u591a\u6570\u5e16\u5b50\u65e0\u660e\u786e\u9053\u5fb7\u6846\u67b6<br/>Most Posts Lack Explicit Moral Framing]\n    D --\x3e D3[\u81ea\u4e3b\u6027\u4e0e\u793e\u7fa4\u6027\u9053\u5fb7\u5173\u8054\u8d1f\u9762\u60c5\u7eea<br/>Autonomy & Community Ethics Linked to Negative Sentiment]\n    D --\x3e D4[\u795e\u6027\u9053\u5fb7\u5173\u8054\u4e2d\u6027/\u6b63\u9762\u60c5\u7eea<br/>Divinity Ethics Linked to Neutral/Positive Sentiment]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2026-01-01",children:"2026-01-01"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [information extraction], [OCR, LLM, record linkage, digital humanities, data harmonization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zahra Abedi, Richard M.K. van Dijk, Gijs Wijnholds, Tessa Verhoef"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Leiden University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23710",children:"https://arxiv.org/pdf/2512.23710"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Designed an automated pipeline integrating OCR, LLM-based interpretation, and database linking for historical document digitization. 2. Demonstrated that generative AI can partially correct low OCR performance during structured data extraction. 3. Developed a record linkage algorithm achieving high accuracy (94% on annotated data, 81% on OCR-derived data) for integrating extracted data with existing databases."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d89795562de4ae19fec35c7e5d2890c5c08f327549b9a9887347777baf5b0c5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d89795562de4ae19fec35c7e5d2890c5c08f327549b9a9887347777baf5b0c5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an automated pipeline using OCR and generative AI to extract and structure biographical data from historical documents, then links this data to existing database records. The method achieved high OCR accuracy and demonstrated that LLMs can correct some OCR errors, with the final linkage algorithm performing well. The study contributes a practical tool for digital humanities research by addressing challenges like layout variability and terminology differences."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>How to automate the integration of historical document data with existing databases?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>OCR + LLM-based interpretation + Database linkage]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>High OCR accuracy, LLM corrects OCR errors, Effective record linkage]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [safety alignment], [multi-agent debate, stealthy harmful queries, safety training data, query transformation, adversarial prompting]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shenzhe Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Toronto"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23717",children:"https://arxiv.org/pdf/2512.23717"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces HarmTransform, the first multi-agent debate framework designed for transforming harmful queries into stealthier forms while preserving intent. 2. Designs a comprehensive evaluation protocol and provides an in-depth analysis of debate dynamics, identifying its benefits and drawbacks. 3. Demonstrates the framework's potential for generating data to enhance LLM safety alignment, highlighting both the promise and limitations of the approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9bfc9b669283c35e277363d47c398576a9ae941e3c10bc7ea3296632fc0184f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9bfc9b669283c35e277363d47c398576a9ae941e3c10bc7ea3296632fc0184f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces HarmTransform, a multi-agent debate framework that iteratively refines harmful queries into stealthier forms to expose gaps in LLM safety mechanisms. Experiments show it outperforms baselines in generating effective transformations. The analysis reveals that while debate improves stealth, it can also introduce topic shifts and complexity, highlighting its dual nature for safety data generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLM\u5b89\u5168\u673a\u5236\u5ffd\u7565\u9690\u853d\u6709\u5bb3\u67e5\u8be2/LLM safety overlooks covert harmful queries]\n    C --\x3e C1[\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8fed\u4ee3\u4f18\u5316\u67e5\u8be2/Multi-agent debate iteratively refines queries]\n    D --\x3e D1[\u8fa9\u8bba\u63d0\u5347\u9690\u853d\u6027\u4f46\u53ef\u80fd\u5f15\u5165\u590d\u6742\u6027/Debate improves stealth but may add complexity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [evaluation & metrics], [STED, consistency scoring, structured output, JSON, semantic equivalence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Guanghui Wang, Jinze Yu, Xing Zhang, Dayuan Jiang, Yin Song, Tomal Deb, Xuefeng Liu, Peiyang He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," AWS Generative AI Innovation Center, AWS WWSO SA Field Initiatives"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23712",children:"https://arxiv.org/pdf/2512.23712"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes STED (Semantic Tree Edit Distance), a novel similarity metric for comparing JSON outputs that balances semantic flexibility with structural strictness. 2. Introduces a comprehensive consistency scoring framework that aggregates multiple STED measurements across repeated generations to quantify LLM output reliability. 3. Provides a systematic benchmark of six LLMs using the proposed framework, revealing significant variations in model consistency and enabling practical applications like model selection and prompt refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2a404e6faa604b1e48c4c20d60e0b88bb889cfe94b29f60a234af8a87b5d05b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2a404e6faa604b1e48c4c20d60e0b88bb889cfe94b29f60a234af8a87b5d05b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of evaluating the consistency of LLM-generated structured outputs (like JSON). It proposes a framework combining a new similarity metric (STED) and a consistency scoring method. The framework effectively benchmarks LLMs, showing Claude-3.7-Sonnet to be highly consistent, and provides tools for improving reliability in production systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: LLM\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u53ef\u9760\u6027\u8bc4\u4f30/Evaluating Reliability of LLM Structured Outputs]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: STED\u5ea6\u91cf\u4e0e\u4e00\u81f4\u6027\u8bc4\u5206\u6846\u67b6/STED Metric & Consistency Scoring Framework]\n    D[\u5173\u952e\u7ed3\u679c/Results: STED\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0cClaude-3.7-Sonnet\u4e00\u81f4\u6027\u6700\u4f73/STED Outperforms Baselines, Claude-3.7-Sonnet Most Consistent]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [code generation], [agent-based framework, iterative self-correction, multilingual LLM, Thought-Code-Observation loop, zero-shot]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jahidul Islam, Md Ataullha, Saiful Azad"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Green University of Bangladesh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23713",children:"https://arxiv.org/pdf/2512.23713"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," github.com/jahidulzaid/PyBanglaCodeActAgent"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced BanglaCodeAct, an agent-based framework for Bangla-to-Python code generation. 2. Leveraged a multilingual LLM in a zero-shot setting without task-specific fine-tuning. 3. Demonstrated the effectiveness of an iterative Thought-Code-Observation loop for dynamic code testing and refinement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6751bddc58c50ee4806ff6af3cccc66715b2e854b912a102cbc99edee59d5c86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6751bddc58c50ee4806ff6af3cccc66715b2e854b912a102cbc99edee59d5c86_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating Python code from Bangla natural language instructions, a low-resource language. It proposes BanglaCodeAct, an agent-based framework that uses a multilingual LLM within an iterative Thought-Code-Observation loop for zero-shot code generation and self-correction. The method, tested with Qwen3-8B, achieves high accuracy on the mHumanEval dataset, setting a new benchmark for Bangla NL-to-Code."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PyBangla at BLP-2025 Task 2<br>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[LLMs excel in English but not low-resource languages<br>\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[Introduce BanglaCodeAct with multi-agent & iterative self-correction<br>\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[Qwen3-8B achieves 94.0% (dev) and 71.6% (test) pass@1<br>\u5173\u952e\u7ed3\u679c/Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [consistency-accuracy relation, minimum-consistency accuracy, consistency-oriented robustness estimate, controlled input variations, multiple-choice benchmarks]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Paulo Cavalin, Cassia Sanctos, Marcelo Grave, Claudio Pinhanez, Yago Primerano"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," IBM Research Brazil"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23711",children:"https://arxiv.org/pdf/2512.23711"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the CAT framework for evaluating and visualizing the interplay between accuracy and response consistency in LLMs under controlled input variations. 2. Proposes Consistency-Accuracy Relation (CAR) curves and the Minimum-Consistency Accuracy (MCA) metric to visualize the trade-off. 3. Defines the Consistency-Oriented Robustness Estimate (CORE) index as a global metric to quantify the accuracy-consistency trade-off from the CAR curve."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e09f948d2d6005bc5c42feadc24f0b59efdcf1128e65b7ae2e76dbdcde54307_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e09f948d2d6005bc5c42feadc24f0b59efdcf1128e65b7ae2e76dbdcde54307_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces CAT, a framework for analyzing how the accuracy of Large Language Models relates to their response consistency when inputs are varied. The core of the method involves CAR curves and the CORE index to visualize and quantify this trade-off. The framework is demonstrated on multiple-choice benchmarks and is designed to be extensible to other evaluation formats."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CAT: \u4e00\u81f4\u6027-\u51c6\u786e\u6027\u5206\u6790\u6846\u67b6] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u8bc4\u4f30\u5ffd\u89c6\u4e00\u81f4\u6027-\u51c6\u786e\u6027\u5173\u7cfb/Current evaluations overlook accuracy-consistency interplay]\n    C --\x3e C1[\u63d0\u51faCAR\u66f2\u7ebf\u4e0eCORE\u6307\u6570/Proposes CAR curves & CORE index]\n    D --\x3e D1[\u6846\u67b6\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u4e0a\u9a8c\u8bc1/Framework validated on MC benchmarks]\n    D --\x3e D2[\u53ef\u6269\u5c55\u81f3\u5f00\u653e\u5f0f\u8bc4\u4f30/Extensible to open-ended evaluations]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Noise-Driven Persona Formation in Reflexive Neural Language Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language generation], [noise-driven generation, persona emergence, reflexive generation, entropy dynamics, long-range coherence]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Toshiyuki Shigemura"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Independent Researcher"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23716",children:"https://arxiv.org/pdf/2512.23716"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Luca-Noise Reflex Protocol (LN-RP), a novel computational framework for studying persona emergence in LLMs. 2. Demonstrates that injecting stochastic noise seeds can induce nonlinear phase transitions and stable persona modes with distinct entropy signatures. 3. Provides a reproducible method for analyzing reflexive generation dynamics and long-range linguistic coherence."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/494e2777d739d8f177739adabd9937930e8c3fbdf053845eab849c66fabde7bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/494e2777d739d8f177739adabd9937930e8c3fbdf053845eab849c66fabde7bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes the Luca-Noise Reflex Protocol (LN-RP) to study how personas emerge in large language models by injecting noise into the initial generation state. The results show that this method reliably induces phase transitions, leading to three distinct, stable persona modes with measurable entropy differences. The protocol offers a reproducible framework for investigating reflexive and emergent linguistic behaviors in LLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Noise-Driven Persona Formation<br>\u566a\u58f0\u9a71\u52a8\u7684\u4eba\u683c\u5f62\u6210] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How does linguistic identity/persona emerge from minimal initialization in LLMs?<br>LLM\u4e2d\u8bed\u8a00\u8eab\u4efd/\u4eba\u683c\u5982\u4f55\u4ece\u6700\u5c0f\u521d\u59cb\u5316\u6761\u4ef6\u4e2d\u6d8c\u73b0\uff1f)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Luca-Noise Reflex Protocol (LN-RP)<br>\u6ce8\u5165\u968f\u673a\u566a\u58f0\u79cd\u5b50\uff0c\u5206\u6790152\u4e2a\u751f\u6210\u5468\u671f<br>Inject stochastic noise seeds, analyze 152 generation cycles)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Three stable persona modes with distinct entropy signatures<br>\u566a\u58f0\u53ef\u8bf1\u5bfc\u76f8\u53d8\uff0c\u4eba\u683c\u4fdd\u6301\u7a33\u5b9a<br>Three stable persona modes, noise-induced phase transitions, consistent persona retention)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [document understanding], [entity-centric evaluation, reading-order regularization, pharmaceutical documents]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tingwei Xie, Tianyi Zhou, Yonghong Song"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," School of Software Engineering, Xi\u2019an Jiaotong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23714",children:"https://arxiv.org/pdf/2512.23714"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/KevinYuLei/PharmaShip",children:"https://github.com/KevinYuLei/PharmaShip"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PharmaShip, a new real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test models under noisy OCR and heterogeneous templates. 2. Proposes an entity-centric evaluation protocol for three complementary tasks (SER, RE, ROP) to minimize architectural confounds. 3. Demonstrates through benchmarking that injecting reading-order-oriented regularization improves model robustness and highlights sequence-aware constraints as a transferable bias for structure modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c53ca47e769ad4299f2003c7674f1a5bc588ef0b3b2bf086373539b4f80b21bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c53ca47e769ad4299f2003c7674f1a5bc588ef0b3b2bf086373539b4f80b21bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces PharmaShip, a benchmark dataset for Chinese pharmaceutical shipping documents to test document understanding models. It benchmarks several layout-aware models and finds that combining pixel and geometric information with reading-order regularization yields the most robust performance. The work establishes a controlled benchmark for safety-critical document understanding and highlights the importance of sequence-aware modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u9488\u5bf9\u533b\u836f\u8fd0\u8f93\u6587\u6863\u7684\u516c\u5f00\u6570\u636e\u96c6 / Lack of public dataset for pharmaceutical shipping docs]\n    C --\x3e C1[\u63d0\u51fa\u5b9e\u4f53\u4e2d\u5fc3\u5316\u8bc4\u4f30\u534f\u8bae / Propose entity-centric evaluation protocol]\n    C --\x3e C2[\u57fa\u51c6\u6d4b\u8bd5\u50cf\u7d20\u611f\u77e5\u4e0e\u51e0\u4f55\u611f\u77e5\u6a21\u578b / Benchmark pixel-aware & geometry-aware models]\n    C --\x3e C3[\u6ce8\u5165\u9605\u8bfb\u987a\u5e8f\u6b63\u5219\u5316 / Inject reading-order regularization]\n    D --\x3e D1[\u50cf\u7d20\u4e0e\u51e0\u4f55\u63d0\u4f9b\u4e92\u8865\u504f\u5dee / Pixels & geometry provide complementary biases]\n    D --\x3e D2[\u9605\u8bfb\u987a\u5e8f\u6b63\u5219\u5316\u63d0\u5347\u9c81\u68d2\u6027 / Reading-order regularization improves robustness]\n    D --\x3e D3[\u5efa\u7acb\u53ef\u63a7\u53ef\u590d\u73b0\u7684\u57fa\u51c6 / Establishes controlled, reproducible benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hate speech detection], [collaborative expert judgment, confidence-based routing, class-balanced focal loss]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Anwar Alajmi, Gabriele Pergola"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Warwick, Public Authority of Applied Education and Training (Kuwait)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23732",children:"https://arxiv.org/pdf/2512.23732"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A two-stage framework combining targeted training for noisy, imbalanced data with selective, reasoning-based inference for ambiguous cases. 2. A novel Collaborative Expert Judgment (CEJ) module that uses multiple LLM personas in a structured debate to resolve uncertain predictions. 3. A dynamic routing mechanism at inference time that directly classifies high-confidence cases and escalates low-confidence ones to the CEJ module."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d2baab37215bb5672da3307e81be99d99c0b9c3a16715fc3d5d0430dfd9273_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d2baab37215bb5672da3307e81be99d99c0b9c3a16715fc3d5d0430dfd9273_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenges of detecting subtle, context-dependent sexist content online, which suffers from data noise, class imbalance, and conceptual ambiguity. It proposes a framework that uses robust training techniques and a novel inference module where uncertain cases are routed to a multi-persona LLM debate for judgment. This approach achieves state-of-the-art performance on benchmark sexism detection tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Subtle, ambiguous sexist content<br>Data noise, imbalance, ambiguity]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two-stage framework<br>Robust training & CEJ routing]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>SOTA on benchmarks<br>+2.72% F1 on EXIST]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Emergent World Beliefs: Exploring Transformers in Stochastic Games"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [mechanistic interpretability], [world models, POMDP, nonlinear probes, belief states, poker]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Adam Kamel, Tanish Rastogi, Michael Ma, Kailash Ranganathan, Kevin Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Waterloo, University of California, Berkeley, Algoverse AI Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23722",children:"https://arxiv.org/pdf/2512.23722"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Extends the study of emergent world models in LLMs from perfect-information games to the incomplete-information domain using poker as a canonical POMDP. 2. Demonstrates that a GPT-style model pretrained on poker hand histories learns both deterministic (e.g., hand ranks) and stochastic (e.g., equity) features without explicit instruction. 3. Shows that the model's internal representations, decoded primarily via nonlinear probes, correlate with theoretical belief states, suggesting it learns its own representation of a stochastic environment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0b3ac8d82fbf09f5fe3d9ad22b6223250d752757d0f69d998f59ef234c0fd9e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0b3ac8d82fbf09f5fe3d9ad22b6223250d752757d0f69d998f59ef234c0fd9e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether transformer-based LLMs develop internal world models in stochastic, partially observable environments. The authors pretrain a GPT model on poker hand history data and probe its activations, finding it learns key game features and belief states. The results suggest LLMs can form their own representations of uncertainty in complex decision-making tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Emergent World Beliefs: Exploring Transformers in Stochastic Games] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLMs\u80fd\u5426\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u5f62\u6210\u4e16\u754c\u6a21\u578b\uff1f/Can LLMs form world models in games of incomplete information?]\n    C --\x3e C1[\u5728\u6251\u514b\u624b\u724c\u5386\u53f2\u6570\u636e\u4e0a\u9884\u8bad\u7ec3GPT\u6a21\u578b/Pretrain GPT on Poker Hand History data]\n    C --\x3e C2[\u4f7f\u7528\u975e\u7ebf\u6027\u63a2\u9488\u5206\u6790\u5185\u90e8\u6fc0\u6d3b/Probe internal activations with nonlinear probes]\n    D --\x3e D1[\u6a21\u578b\u5b66\u4e60\u4e86\u786e\u5b9a\u6027\u4e0e\u968f\u673a\u6027\u7279\u5f81/Model learns deterministic and stochastic features]\n    D --\x3e D2[\u5185\u90e8\u8868\u5f81\u4e0e\u7406\u8bba\u4fe1\u5ff5\u72b6\u6001\u76f8\u5173/Internal representations correlate with theoretical belief states]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Break Out the Silverware -- Semantic Understanding of Stored Household Items"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [commonsense reasoning], [benchmark dataset, vision-language model, hybrid agent pipeline, storage location prediction, semantic understanding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Michaela Levi-Richter, Reuth Mirsky, Oren Glickman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bar Ilan University, Tufts University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23739",children:"https://arxiv.org/pdf/2512.23739"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] State-of-the-art Small Language Coder Model: Mify-Coder"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [compute-optimal training, CPT-SFT, synthetic data generation, model quantization, quality filtering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Abhinav Parmar, Abhisek Panigrahi, Abhishek Kumar Dwivedi, Abhishek Bhattacharya, Adarsh Ramachandra, Aditya Choudhary, Aditya Garg, Aditya Raj, Alankrit Bhatt, Alpesh Yadav, Anant Vishnu, Ananthu Pillai, Ankush Kumar, Aryan Patnaik, Aswatha Narayanan S, Avanish Raj Singh, Bhavya Shree Gadda, Brijesh Pankajbhai Kachhadiya, Buggala Jahnavi, Chidurala Nithin Krishna, Chintan Shah, Chunduru Akshaya, Debarshi Banerjee, Debrup Dey, Deepa R., Deepika B G, Faiz ur Rahman, Gagan Gayari, Gudhi Jagadeesh Kumar Naidu, Gursimar Singh, Harshal Tyagi, Harshini K, James Mani Vathalloor, Jayarama Nettar, Jayashree Gajjam, Joe Walter Sugil George, Kamalakara Sri Krishna Tadepalli, Kamalkumar Rathinasamy, Karan Chaurasia, Karthikeyan S, Kashish Arora, Kaushal Desai, Khushboo Buwade, Kiran Manjrekar, Malikireddy Venkata Sai Likhitha, Manjunath A, Mitali Mahavir Bedmutha, Mohammed Rafee Tarafdar, Nikhil Tiwari, Nikitha K Gigi, Pavan Ravikumar, Pendyala Swarnanjali, Piyush Anand, Prakash Chandrasekar, Prasanna Bhalchandra Gawade, Prasanth Sivan, Preeti Khurana, Priyanshi Babbar, Rajab Ali Mondal, Rajesh Kumar Vissapragada, Rajeshwari Ganesan, Rajeswari Koppisetti, Ramjee R., Ramkumar Thiruppathisamy, Rani G. S., S Reka, Samarth Gupta, Sandeep Reddy Kothakota, Sarathy K, Sathyanarayana Sampath Kumar, Saurabh Kumar, Shashank Khasare, Shenbaga Devi Venkatesh Kumar, Shiva Rama Krishna Parvatham, Shoeb Shaikh, Shrishanmathi A, Shubham Pathak, Sree Samhita Koppaka, Sreenivasa Raghavan K S, Sreeram Venkatasubramanian, Suprabha Desai Bojja, Swetha R, Syed Ahmed, Chinmai Harshitha Thota, Tushar Yadav, Veeravelly Kusumitha, V V S S Prasanth Patnaik, Vidya Sri Sesetti, Vijayakeerthi K, Vikram Raj Bakshi, Vinay K K, Vinoth Kumar Loganathan, Vipin Tiwari, Vivek Kumar Shrivastav, V Venkata Sri Datta Charan, Wasim Akhtar Khan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Infosys AI Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23747",children:"https://arxiv.org/pdf/2512.23747"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced Mify-Coder, a 2.5B-parameter code model trained with a compute-optimal strategy on 4.2T tokens, demonstrating that compact models can match frontier-grade performance. 2. Developed a training pipeline combining high-quality curated data with agentically generated synthetic data, refined using enterprise-grade evaluations and LLM-based quality filtering for high data density. 3. Showed that disciplined exploration of training objectives and data mixtures within a single continuous trajectory enables competitive accuracy, efficiency, and safety, with quantized variants enabling deployment on standard hardware."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c8af9d7668bbc4ac7ae9ad0ed379feb93f982a5fecdc8491169bd4d6c33d30_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c8af9d7668bbc4ac7ae9ad0ed379feb93f982a5fecdc8491169bd4d6c33d30_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost of large code models by proposing Mify-Coder, a compact 2.5B-parameter model trained using a compute-optimal strategy that integrates curated and synthetic data with quality filtering. It demonstrates that this approach allows a small model to achieve performance comparable to much larger models on coding benchmarks while maintaining safety and enabling efficient desktop deployment."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Mify-Coder] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5927\u6a21\u578b\u6210\u672c\u9ad8/High cost of large code models]\n    C --\x3e C1[\u8ba1\u7b97\u6700\u4f18\u8bad\u7ec3/Compute-optimal training]\n    C --\x3e C2[\u5408\u6210\u6570\u636e\u751f\u6210/Synthetic data generation]\n    C --\x3e C3[\u8d28\u91cf\u8fc7\u6ee4/Quality filtering]\n    D --\x3e D1[\u6027\u80fd\u53ef\u6bd4/Competitive performance]\n    D --\x3e D2[\u9ad8\u6548\u90e8\u7f72/Efficient deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, entropy penalty, training-free, reasoning acceleration, draft-model verification]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tiancheng Su, Meicong Zhang, Guoxiu He"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," East China Normal University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23765",children:"https://arxiv.org/pdf/2512.23765"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Entropy-Aware Speculative Decoding (EASD), a training-free method that introduces a dynamic entropy-based penalty to reject low-confidence draft tokens, 2. Enables speculative decoding to potentially surpass the target model's performance by incorporating draft-model verification and preventing error propagation, 3. Demonstrates that EASD maintains efficiency comparable to standard speculative decoding while improving reasoning accuracy across multiple benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/326c86ae03e220a7cb48737a0a6fe149bd4384ccc08f3113a051c3548bc2d30e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/326c86ae03e220a7cb48737a0a6fe149bd4384ccc08f3113a051c3548bc2d30e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of speculative decoding being constrained by the target model's performance. It proposes Entropy-Aware Speculative Decoding (EASD), which uses entropy to quantify uncertainty and reject low-confidence draft tokens. Experiments show EASD outperforms existing methods and can surpass the target LLM's performance while maintaining comparable efficiency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Entropy-Aware Speculative Decoding] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[SD\u6027\u80fd\u53d7\u9650\u4e8e\u76ee\u6807\u6a21\u578b/SD performance capped by target model]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5f15\u5165\u52a8\u6001\u71b5\u60e9\u7f5a/Introduce dynamic entropy penalty]\n    Method --\x3e M2[\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u62d2\u7edd\u4f4e\u7f6e\u4fe1\u5ea6\u4ee4\u724c/Reject low-confidence tokens based on uncertainty]\n    Method --\x3e M3[\u76ee\u6807\u6a21\u578b\u91cd\u91c7\u6837/Target model re-sampling]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u8d85\u8d8a\u73b0\u6709SD\u65b9\u6cd5/Outperforms existing SD methods]\n    Results --\x3e R2[\u5e38\u8d85\u8d8a\u76ee\u6807LLM\u672c\u8eab/Often surpasses target LLM]\n    Results --\x3e R3[\u6548\u7387\u4e0eSD\u76f8\u5f53/Efficiency comparable to SD]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] MiMo-Audio: Audio Language Models are Few-Shot Learners"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [audio language model, few-shot learning, instruction tuning, scaling pretraining, speech continuation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiaomi LLM-Core Team, Dong Zhang, Gang Wang, Jinlong Xue, Kai Fang, Liang Zhao, Rui Ma, Shuhuai Ren, Shuo Liu, Tao Guo, Weiji Zhuang, Xin Zhang, Xingchen Song, Yihan Yan, Yongzhe He, Cici, Bowen Shen, Chengxuan Zhu, Chong Ma, Chun Chen, Heyu Chen, Jiawei Li, Lei Li, Menghang Zhu, Peidian Li, Qiying Wang, Sirui Deng, Weimin Xiong, Wenshan Huang, Wenyu Yang, Yilin Jiang, Yixin Yang, Yuanyuan Tian, Yue Ma, Yue Yu, Zihan Zhang, Zihao Yue, Bangjun Xiao, Bingquan Xia, Bofei Gao, Bowen Ye, Can Cai, Chang Liu, Chenhong He, Chunan Li, Dawei Zhu, Duo Zhang, Fengyuan Shi, Guoan Wang, Hailin Zhang, Hanglong Lv, Hanyu Li, Hao Tian, Heng Qu, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianguang Zuo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Linghao Zhang, Meng Chen, Nuo Chen, Peng Zhang, Qianli Chen, Qiantong Wang, Rang Li, Shaohui Liu, Shengfan Wang, Shicheng Li, Shihua Yu, Shijie Cao, Shimao Chen, Shuhao Gu, Weikun Wang, Wenhan Ma, Xiangwei Deng, Xing Yong, Xing Zhang, Xu Wang, Yifan Song, Yihao Zhao, Yingbo Zhao, Yizhao Gao, Yu Cheng, Yu Tu, Yudong Wang, Zhaojun Huang, Zhengju Tang, Zhenru Lin, Zhichao Song, Zhipeng Xu, Zhixian Zheng, Zihan Jiang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xiaomi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23808",children:"https://arxiv.org/pdf/2512.23808"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/XiaomiMiMo/MiMo-Audio",children:"https://github.com/XiaomiMiMo/MiMo-Audio"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Scaled audio language model pretraining to over 100 million hours, demonstrating emergent few-shot learning capabilities across diverse audio tasks. 2. Introduced a systematic evaluation framework and showed that the base model achieves SOTA performance among open-source models on speech intelligence and audio understanding benchmarks, with generalization to unseen tasks. 3. Developed an instruction-tuned variant with a curated corpus and thinking mechanisms, achieving open-source SOTA on multiple audio understanding, spoken dialogue, and TTS benchmarks, rivaling closed-source models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb9eb77f0c6bfe175c709a98be8909fd963e33e44e830db2cd023ecf616b62_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb9eb77f0c6bfe175c709a98be8909fd963e33e44e830db2cd023ecf616b62_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes MiMo-Audio, a large-scale audio language model pretrained on over 100 million hours of data, which demonstrates emergent few-shot learning capabilities for various audio tasks without task-specific fine-tuning. The instruction-tuned variant further achieves state-of-the-art performance on multiple benchmarks, showing strong generalization in audio understanding and generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["MiMo-Audio: Audio Language Models are Few-Shot Learners"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Existing audio models require task-specific fine-tuning, lacking human-like generalization."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Scale next-token prediction pretraining on 100M+ hours of audio; Use instruction-tuning with thinking mechanisms."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Emergent few-shot learning; SOTA open-source performance; Generalizes to unseen tasks like voice conversion."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [mental health text classification], [transfer learning, continual training, RoBERTa, cross-condition, stress detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amal Alqahtani, Efsun Kayi, Mona Diab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The George Washington University, King Saud University, Johns Hopkins University Applied Physics Laboratory, Carnegie Mellon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23813",children:"https://arxiv.org/pdf/2512.23813"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes StressRoBERTa, a cross-condition transfer learning approach for detecting self-reported chronic stress in tweets. 2. Demonstrates that continual training on a focused set of clinically related mental health conditions (depression, anxiety, PTSD) improves stress detection over general models. 3. Shows effective transfer from clinical mental health contexts to situational stress discussions via evaluation on the Dreaddit dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a57224b94917298d3e6f94d2c9255d2d054abcab5c5051070f928ff8ddd8bc6b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a57224b94917298d3e6f94d2c9255d2d054abcab5c5051070f928ff8ddd8bc6b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces StressRoBERTa, a method that continually trains a RoBERTa model on social media text from users with depression, anxiety, and PTSD before fine-tuning it for chronic stress detection. The approach outperforms the previous best system on the SMM4H 2022 shared task by 3% F1-score, showing that focused cross-condition transfer learning from related disorders provides stronger representations for stress detection."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[StressRoBERTa: Cross-Condition Transfer Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u6162\u6027\u538b\u529b/Detect chronic stress on social media)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u4ece\u76f8\u5173\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u8fdb\u884c\u8de8\u6761\u4ef6\u8fc1\u79fb\u5b66\u4e60/Cross-condition transfer learning from related mental health conditions)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u8d85\u8d8a\u6700\u4f73\u5171\u4eab\u4efb\u52a1\u7cfb\u7edf\uff0cF1\u5206\u6570\u8fbe82%/Outperforms best shared task system with 82% F1)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [adversarial robustness], [mechanistic interpretability, attention layers, adversarial examples, LLM evaluation, token substitution]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kaustubh Dhole"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Emory University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23837",children:"https://arxiv.org/pdf/2512.23837"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel adversarial example generation method that exploits intermediate attention layer token distributions, contrasting with prompt-based or gradient-based attacks. 2. Introduces two specific attention-based generation techniques: attention-based token substitution and attention-based conditional generation. 3. Empirically demonstrates that such adversarial examples can degrade performance on an evaluation task (argument quality assessment) while maintaining semantic similarity, highlighting both the promise and limitations (e.g., grammatical degradation) of the approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new method to generate adversarial examples by extracting token predictions from the intermediate attention layers of LLMs, leveraging their iterative refinement property. The approach is used to stress-test LLM-based evaluation pipelines, showing it can cause performance drops on an argument quality task while preserving semantics, though grammatical issues can arise. The findings illustrate the potential and current constraints of using internal model representations for adversarial testing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Can intermediate attention layers be used to generate adversarial examples for LLM evaluation?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Leverage attention-layer token distributions for token substitution/conditional generation]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Adversarial examples cause performance drop but may introduce grammatical issues]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [bias detection], [SHAP, transformer, interpretability, false positives, domain adaptation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Himel Ghosh"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Sapienza University of Rome"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23835",children:"https://arxiv.org/pdf/2512.23835"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comparative interpretability study of two transformer-based bias detection models using SHAP to analyze their decision mechanisms. 2. Revealed that a standard bias detector model exhibits a misalignment between attribution strength and prediction correctness, leading to systematic over-flagging, while a domain-adapted model produces significantly fewer false positives. 3. Demonstrated that model errors, particularly false positives, arise from discourse-level ambiguity rather than explicit bias cues, highlighting distinct linguistic failure modes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cbe893cabdbb4c14e3f0b2a14a91011067d2ee0ee4225b0a232eb5591a1b743_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cbe893cabdbb4c14e3f0b2a14a91011067d2ee0ee4225b0a232eb5591a1b743_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares how two transformer models detect bias in news text using SHAP-based explanations. It finds that while both models focus on similar evaluative language, a domain-adapted model integrates these signals more reliably, producing far fewer false positives than a standard bias detector. The study concludes that interpretability analysis is crucial for evaluating bias detection systems and that architectural choices critically impact their reliability for journalistic use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Explaining News Bias Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[How do bias detection models make decisions?]\n    C --\x3e C1[Comparative SHAP analysis of two transformer models]\n    D --\x3e D1[Domain-adapted model has better alignment and fewer false positives]\n    D --\x3e D2[False positives driven by discourse ambiguity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [adaptive prompting, context window, open-domain QA, retrieval-augmented generation, LLM ignorance]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dingmin Wang, Ji Ma, Shankar Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Google Research, University of Oxford"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23836",children:"https://arxiv.org/pdf/2512.23836"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an adaptive prompting strategy for RAG that splits retrieved information into smaller chunks for sequential processing, mitigating the noise from irrelevant information in long contexts. 2. Demonstrates experimentally that this strategy matches or outperforms standard prompting on open-domain QA datasets while using fewer tokens. 3. Identifies and analyzes a key failure mode where LLMs generate incorrect answers instead of declining when information is insufficient, highlighting a critical area for future research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that longer context windows in Retrieval-Augmented Generation (RAG) introduce irrelevant information, degrading LLM performance. It proposes an adaptive prompting strategy that processes retrieved text in smaller, sequential chunks, achieving comparable accuracy with lower token usage. The study concludes that a major source of error is the LLM's tendency to generate wrong answers rather than admit ignorance, pointing to the need for improved refusal capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u957f\u4e0a\u4e0b\u6587\u5f15\u5165\u65e0\u5173\u4fe1\u606f\uff0c\u964d\u4f4eLLM\u6027\u80fd/Long contexts introduce irrelevant info, degrading LLM performance]\n    C --\x3e C1[\u81ea\u9002\u5e94\u63d0\u793a\u7b56\u7565\uff1a\u5206\u5757\u987a\u5e8f\u5904\u7406/Adaptive prompting: sequential chunk processing]\n    D --\x3e D1[\u6027\u80fd\u76f8\u5f53\uff0c\u4f7f\u7528\u66f4\u5c11token/Matches performance, uses fewer tokens]\n    D --\x3e D2[LLM\u5e38\u751f\u6210\u9519\u8bef\u7b54\u6848\u800c\u975e\u62d2\u7edd/LLM often generates wrong answers instead of declining]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [question answering], [Retrieval-Augmented Generation (RAG), SecBERT, financial numerical reasoning, multi-retriever, few-shot learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yukun Zhang, Stefan Elbl Droguett, Samyak Jain"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23848",children:"https://arxiv.org/pdf/2512.23848"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a multi-retriever RAG system that retrieves both external domain knowledge (e.g., financial definitions) and internal question contexts to improve financial QA. 2. Demonstrated that domain-specific training with the SecBERT encoder significantly boosts performance, allowing a neural symbolic model to surpass a strong baseline. 3. Showed that a prompt-based LLM generator achieves state-of-the-art performance with a >7% improvement, highlighting the enhanced few-shot numerical reasoning of latest LLMs and the trade-off between hallucination and external knowledge gains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de021f75daa09442db831a9d2d84064bf5381a47f4cb0fc792e2cfd3bfbd128b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de021f75daa09442db831a9d2d84064bf5381a47f4cb0fc792e2cfd3bfbd128b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses errors in financial numerical QA by proposing a multi-retriever RAG system that retrieves external financial knowledge and internal context. The best model, using domain-specific training and a prompt-based LLM, achieves state-of-the-art results, though still below human expert performance, and reveals a trade-off between hallucination and knowledge gains."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Integrating Domain Knowledge for Financial QA<br>\u91d1\u878dQA\u9886\u57df\u77e5\u8bc6\u96c6\u6210] --\x3e B[Problem: Errors in financial numerical QA due to lack of domain knowledge<br>\u6838\u5fc3\u95ee\u9898: \u91d1\u878d\u6570\u503cQA\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u51fa\u9519]\n    A --\x3e C[Method: Multi-retriever RAG system with external/internal retrieval<br>\u4e3b\u8981\u65b9\u6cd5: \u591a\u68c0\u7d22\u5668RAG\u7cfb\u7edf]\n    A --\x3e D[Results: SOTA performance, domain-specific training effective, trade-off analyzed<br>\u5173\u952e\u7ed3\u679c: SOTA\u6027\u80fd, \u9886\u57df\u8bad\u7ec3\u6709\u6548, \u6743\u8861\u5206\u6790]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language model evaluation], [epistemic robustness, semantic compression, adversarial fabrication, two-system cognitive model, comprehension integrity]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Baxi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: alumni.cmu.edu, Carnegie Mellon University)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23850",children:"https://arxiv.org/pdf/2512.23850"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Drill-Down and Fabricate Test (DDFT), a novel protocol for measuring epistemic robustness in language models under stress from semantic compression and adversarial fabrication. 2. Proposes a two-system cognitive model (Semantic System and Epistemic Verifier) to explain and analyze LLM behavior. 3. Provides empirical evidence that epistemic robustness is orthogonal to model scale and architecture, identifying error detection as the critical bottleneck."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a gap in current language model evaluations, which fail to measure how robustly models maintain factual knowledge under stress. It introduces the Drill-Down and Fabricate Test (DDFT) to measure epistemic robustness by applying semantic compression and adversarial fabrication. The key finding is that epistemic robustness is not predicted by model size or architecture but by a model's internal verification mechanisms, challenging assumptions about scaling and reliability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[The Drill-Down and Fabricate Test (DDFT)] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u8bc4\u4f30\u65e0\u6cd5\u8861\u91cf\u77e5\u8bc6\u9c81\u68d2\u6027/Current evaluations fail to measure knowledge robustness]\n    C --\x3e C1[DDFT\u534f\u8bae: \u8bed\u4e49\u538b\u7f29\u4e0e\u5bf9\u6297\u4f2a\u9020/DDFT Protocol: Semantic Compression & Adversarial Fabrication]\n    C --\x3e C2[\u53cc\u7cfb\u7edf\u8ba4\u77e5\u6a21\u578b/Two-System Cognitive Model]\n    D --\x3e D1[\u9c81\u68d2\u6027\u4e0e\u6a21\u578b\u89c4\u6a21/\u67b6\u6784\u65e0\u5173/Robustness orthogonal to model size/architecture]\n    D --\x3e D2[\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u662f\u5173\u952e\u74f6\u9888/Error detection is the critical bottleneck]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Trellis: Learning to Compress Key-Value Memory in Attention Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [KV cache compression, bounded memory, online gradient descent, recurrent compression, long-context]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mahdi Karami, Ali Behrouz, Praneeth Kacham, Vahab Mirrokni"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Google Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23852",children:"https://arxiv.org/pdf/2512.23852"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Trellis, a novel Transformer architecture that replaces the standard unbounded KV cache with a fixed-size memory, enabling bounded memory usage. 2. Proposes a trainable two-pass recurrent compression mechanism that dynamically compresses new key-value pairs into the fixed memory at test time. 3. Leverages an online gradient descent procedure with a forget gate to recursively update the compressed memory, learning to retain important contextual information from long sequences."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/889c9150f49160406df5713209e4165753466bc69bac6ffa78b28c214caa5c7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/889c9150f49160406df5713209e4165753466bc69bac6ffa78b28c214caa5c7d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic complexity and unbounded memory growth of the KV cache in Transformers by proposing Trellis, an architecture with a fixed-size memory and a learned recurrent compression mechanism. The method uses online gradient descent with a forget gate to dynamically update the compressed memory during inference. Experiments show Trellis outperforms baselines, with increasing gains on longer sequences, demonstrating its potential for efficient long-context modeling."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Trellis: Learning to Compress Key-Value Memory"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Transformer KV cache leads to quadratic complexity and unbounded memory"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Fixed-size memory + Two-pass recurrent compression with online gradient descent & forget gate"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines; Gains increase with sequence length for long-context"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Infini-attention, compressive memory, small language models (SLMs), long-context extrapolation, pretraining]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huawei Technologies Canada Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23862",children:"https://arxiv.org/pdf/2512.23862"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/RRaAy-H/nanotron-infini",children:"https://github.com/RRaAy-H/nanotron-infini"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Replaced standard attention in a 300M-parameter LLaMA model with Infini-attention to study compressive memory behavior under short-sequence pretraining. 2. Analyzed the training dynamics of SLMs with Infini-attention, revealing characteristics like loss fluctuations, gradient volatility, and early-layer memory concentration. 3. Demonstrated that Infini-attention improves long-context extrapolation over a baseline model, with supervised fine-tuning further boosting performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether the Infini-attention mechanism, which combines local attention with compressive memory, can enhance long-context capabilities in Small Language Models (SLMs) during small-scale pretraining. The authors empirically study a 300M-parameter LLaMA model equipped with Infini-attention and find it improves long-context retrieval accuracy over a baseline, despite some degradation over very long sequences. The conclusion is that architectural memory like Infini-attention is beneficial for achieving robust long-context performance in SLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Enhancing long-context extrapolation for Small Language Models (SLMs)]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Using Infini-attention (compressive memory + local attention) in small-scale pretraining]\n    D[\u5173\u952e\u7ed3\u679c/Results: Improves long-context retrieval; Identifies balance factor importance; Shows performance degradation over very long sequences but still outperforms baseline]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Disentangling Learning from Judgment: Representation Learning for Open Response Analytics"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [educational data mining], [sentence embeddings, rater effects, residualization, teacher priors, interpretability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Conrad Borchers, Manit Patel, Seiyon M. Lee, Anthony F. Botelho"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, University of Florida"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23941",children:"https://arxiv.org/pdf/2512.23941"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An analytics-first framework that disentangles student response content from teacher grading tendencies, making rater effects visible and auditable. 2. A modeling pipeline using dynamic teacher priors and residualized sentence embeddings to mitigate prompt and rater confounds, validated temporally. 3. A projection method to surface disagreements between content and rater signals for qualitative inspection, transforming embeddings into reflective learning analytics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/969e6dbddbe7eb87582932064e8e7d3a0853ca2850b5329fafc1020e7228f365_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/969e6dbddbe7eb87582932064e8e7d3a0853ca2850b5329fafc1020e7228f365_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of conflating student response content with teacher grading bias in automated scoring. The proposed method uses dynamic teacher priors and residualized sentence embeddings to separate these signals, with linear models quantifying their contributions. The main conclusion is that teacher priors heavily influence predictions, and adjusting for them sharpens the content representation, enabling better analysis of student understanding versus grading practices."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Disentangling Learning from Judgment<br>\u5b66\u4e60\u4e0e\u8bc4\u5224\u7684\u89e3\u8026] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Automated scoring conflates content with rater bias<br>\u81ea\u52a8\u8bc4\u5206\u6df7\u6dc6\u4e86\u5185\u5bb9\u4e0e\u8bc4\u5206\u8005\u504f\u5dee]\n    C --\x3e C1[Framework separates content signals & teacher priors<br>\u6846\u67b6\u5206\u79bb\u5185\u5bb9\u4fe1\u53f7\u4e0e\u6559\u5e08\u5148\u9a8c]\n    C --\x3e C2[Uses centering & residualization on embeddings<br>\u5bf9\u5d4c\u5165\u4f7f\u7528\u4e2d\u5fc3\u5316\u4e0e\u6b8b\u5dee\u5316]\n    C --\x3e C3[Temporal validation & projection for inspection<br>\u65f6\u95f4\u9a8c\u8bc1\u4e0e\u6295\u5f71\u4ee5\u4f9b\u68c0\u67e5]\n    D --\x3e D1[Teacher priors heavily influence grades<br>\u6559\u5e08\u5148\u9a8c\u4e25\u91cd\u5f71\u54cd\u8bc4\u5206]\n    D --\x3e D2[Combined model achieves best AUC (~0.815)<br>\u7ec4\u5408\u6a21\u578b\u53d6\u5f97\u6700\u4f73AUC]\n    D --\x3e D3[Residual content reveals student understanding<br>\u6b8b\u5dee\u5185\u5bb9\u63ed\u793a\u5b66\u751f\u7406\u89e3]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [hypergraph memory, multi-step reasoning, global sense-making, long-context modeling, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, WeChat AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23959",children:"https://arxiv.org/pdf/2512.23959"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Encyclomen/HGMem",children:"https://github.com/Encyclomen/HGMem"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HGMem, a novel hypergraph-based memory mechanism that models memory as a dynamic structure with higher-order interactions, moving beyond passive storage. 2. Addresses the limitation of existing multi-step RAG memory in capturing complex relational structures and providing strong guidance for subsequent reasoning steps. 3. Demonstrates through extensive experiments that the method consistently improves multi-step RAG performance and substantially outperforms strong baselines on global sense-making tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of static, passive memory in multi-step RAG systems, which leads to fragmented reasoning in long-context tasks. It proposes HGMem, a dynamic hypergraph-based memory mechanism that captures high-order correlations among facts to form an integrated knowledge structure for stronger reasoning guidance. The method is shown to consistently and substantially outperform baseline systems across diverse global sense-making tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Improving Multi-step RAG with Hypergraph-based Memory<br>\u6539\u8fdb\u591a\u6b65RAG\u7684\u8d85\u56fe\u8bb0\u5fc6] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u8bb0\u5fc6\u6a21\u5757\u662f\u88ab\u52a8\u7684\u9759\u6001\u5b58\u50a8<br>Existing memory is passive static storage]\n    B1 --\x3e B2[\u5ffd\u7565\u4e86\u9ad8\u9636\u5173\u8054\uff0c\u5bfc\u81f4\u788e\u7247\u5316\u63a8\u7406<br>Ignores high-order correlations, causing fragmented reasoning]\n    C --\x3e C1[\u63d0\u51fa\u8d85\u56fe\u8bb0\u5fc6\u673a\u5236 HGMem<br>Propose hypergraph memory mechanism HGMem]\n    C1 --\x3e C2[\u5c06\u8bb0\u5fc6\u8868\u793a\u4e3a\u52a8\u6001\u8d85\u56fe<br>Represent memory as a dynamic hypergraph]\n    C2 --\x3e C3[\u8d85\u8fb9\u5f62\u6210\u9ad8\u9636\u4ea4\u4e92\uff0c\u6784\u5efa\u96c6\u6210\u77e5\u8bc6\u7ed3\u6784<br>Hyperedges form high-order interactions, building integrated knowledge]\n    D --\x3e D1[\u5728\u591a\u6b65RAG\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb<br>Achieves consistent improvement on multi-step RAG]\n    D1 --\x3e D2[\u5728\u5168\u5c40\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf<br>Substantially outperforms baselines on global sense-making tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Efficient Context Scaling with LongCat ZigZag Attention"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [sparse attention, long-context, mid-training, ZigZag Attention]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chen Zhang, Yang Bai, Jiahuan Li, Anchun Gui, Keheng Wang, Feifan Liu, Guanyu Wu, Yuwei Jiang, Defei Bu, Li Wei, Haihang Jing, Hongyin Tang, Xin Chen, Xiangzhou Huang, Fengcun Li, Rongxiang Weng, Yulei Qian, Yifan Lu, Yerui Sun, Jingang Wang, Yuchen Xie, Xunliang Cai"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meituan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23966",children:"https://arxiv.org/pdf/2512.23966"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LongCat ZigZag Attention (LoZA), a sparse attention scheme to convert full-attention models into sparse versions with limited compute. 2. Demonstrates LoZA's effectiveness for speed-up in both prefill-intensive (e.g., RAG) and decode-intensive (e.g., tool use) long-context scenarios. 3. Applies LoZA to create LongCat-Flash-Exp, a foundation model capable of efficiently processing up to 1 million tokens."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46b353da1c2cbb89962a2e909144fbcc8c92d821f35157049a111e1855b4242c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46b353da1c2cbb89962a2e909144fbcc8c92d821f35157049a111e1855b4242c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces LongCat ZigZag Attention (LoZA), a sparse attention method designed to efficiently transform standard full-attention language models into sparse models suitable for long-context tasks. This approach enables significant speed improvements for both prefill and decoding phases. The resulting model, LongCat-Flash-Exp, can process up to 1 million tokens, facilitating efficient long-term reasoning and agentic capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Efficient Context Scaling with LongCat ZigZag Attention] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5168\u6ce8\u610f\u529b\u8ba1\u7b97\u5f00\u9500\u5927/High computational cost of full attention in long-context scenarios]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faLoZA\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848/Propose LoZA sparse attention scheme]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u652f\u6301\u767e\u4e07token\u9ad8\u6548\u5904\u7406/Achieve significant speed-up, enable efficient processing of 1M tokens]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [spelling correction], [reinforcement learning, zero-supervision, Chinese spelling correction, PPO, cluster-consensus reward]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhiming Lin, Kai Zhao, Sophie Zhang, Peilai Yu, Canran Xiao"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nankai University, Western Sydney University, Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23971",children:"https://arxiv.org/pdf/2512.23971"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CEC-Zero, a zero-supervision RL framework for Chinese spelling correction that enables LLMs to self-correct without labeled data. 2. Introduces a cluster-consensus reward mechanism based on semantic similarity and candidate agreement to guide policy optimization. 3. Demonstrates superior performance over supervised and fine-tuned LLM baselines across multiple benchmarks, establishing a label-free paradigm."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f497dc978d283e6c34450b390462e1f0ee33c507d41a14f70226f631c7d28168_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f497dc978d283e6c34450b390462e1f0ee33c507d41a14f70226f631c7d28168_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of Chinese spelling correction (CSC) by introducing CEC-Zero, a zero-supervision reinforcement learning framework. The method synthesizes errors from clean text and uses a novel cluster-consensus reward to optimize an LLM policy with PPO, eliminating the need for costly annotations. It significantly outperforms existing supervised and fine-tuned methods on multiple benchmarks, offering a robust and scalable solution for real-world noisy text processing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5927\u89c4\u6a21\u4e2d\u6587\u62fc\u5199\u7ea0\u9519\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u65b0\u578b\u9519\u8bef\u4e0d\u9c81\u68d2/Large-scale Chinese spelling correction relies on costly annotations; existing methods lack robustness to novel errors.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u96f6\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5408\u6210\u9519\u8bef\u8f93\u5165\uff0c\u4f7f\u7528\u805a\u7c7b\u5171\u8bc6\u5956\u52b1\u548cPPO\u4f18\u5316/Proposes a zero-supervision RL framework, synthesizes errorful inputs, uses cluster-consensus rewards and PPO for optimization.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d85\u8d8a\u76d1\u7763\u57fa\u7ebf10-13 F1\u70b9\uff0c\u8d85\u8d8aLLM\u5fae\u8c035-8\u70b9/Outperforms supervised baselines by 10-13 F1 points and strong LLM fine-tunes by 5-8 points across 9 benchmarks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [mechanistic interpretability], [sparse auto-encoder, reasoning vectors, chain-of-thought]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Google DeepMind, The University of Texas at Austin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23988",children:"https://arxiv.org/pdf/2512.23988"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes RISE, an unsupervised framework using sparse auto-encoders (SAEs) to discover "reasoning vectors" that encode distinct reasoning behaviors from step-level LLM activations. 2. Demonstrates that these discovered vectors correspond to interpretable behaviors (e.g., reflection, backtracking) and can be used for targeted intervention to controllably steer the reasoning process without retraining. 3. Shows SAEs can uncover novel, human-undefined reasoning behaviors and structural properties, such as controlling response confidence, highlighting the potential of unsupervised latent discovery.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of interpreting the internal reasoning process of large language models (LLMs). It proposes RISE, an unsupervised framework that uses sparse auto-encoders to discover disentangled "reasoning vectors" from chain-of-thought activations. The method enables the identification, visualization, and controllable intervention of specific reasoning behaviors, revealing novel insights beyond supervised analysis.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u63a8\u7406\u5185\u90e8\u673a\u5236\u4e0d\u660e\u786e<br>Supervised methods are limited"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>RISE\u6846\u67b6: \u65e0\u76d1\u7763\u7a00\u758f\u81ea\u7f16\u7801\u5668<br>Unsupervised SAEs on step-level activations"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u53d1\u73b0\u53ef\u89e3\u91ca\u63a8\u7406\u884c\u4e3a\u5411\u91cf<br>\u53ef\u63a7\u5e72\u9884\u63a8\u7406\u8f68\u8ff9<br>Discover novel behaviors"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] WISE: Web Information Satire and Fakeness Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text classification], [transformer models, fake news detection, satire classification, misinformation, model benchmarking]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Gaurab Chhetri, Subasish Das, Tausif Islam Chowdhury"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Texas State University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24000",children:"https://arxiv.org/pdf/2512.24000"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed the WISE framework for benchmarking lightweight transformer models to distinguish fake news from satire. 2. Conducted a comprehensive evaluation of eight lightweight and two baseline models using a balanced dataset and multiple performance metrics. 3. Demonstrated that lightweight models like MiniLM and DistilBERT can achieve high performance, offering practical solutions for resource-constrained misinformation detection systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9f60db2d4dc284973a6b6a30b146469da5c5cbcb175e674896cd55dd4f66c42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9f60db2d4dc284973a6b6a30b146469da5c5cbcb175e674896cd55dd4f66c42_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of distinguishing fake news from satire by proposing the WISE framework. It benchmarks several lightweight transformer models on a dataset from Fakeddit and finds that models like MiniLM and DistilBERT achieve high accuracy and efficiency. The conclusion is that lightweight models are viable for real-world misinformation detection in resource-limited settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[WISE: Web Information Satire and Fakeness Evaluation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Distinguishing fake news from satire due to overlapping features)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Benchmark lightweight transformers using stratified cross-validation)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: MiniLM highest accuracy, RoBERTa highest ROC-AUC, lightweight models effective)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning], [latent planning, implicit cognition, vector-quantized autoencoder, chain-of-thought]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sijia Chen, Di Niu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology (Guangzhou), University of Alberta"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24014",children:"https://arxiv.org/pdf/2512.24014"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/AgenticFinLab/latent-planning",children:"https://github.com/AgenticFinLab/latent-planning"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes iCLP, a novel framework inspired by human Implicit Cognition to enable LLMs to generate and use compact latent plans for reasoning. 2. Introduces a method to distill explicit plans from reasoning trajectories and learn their discrete representations via a vector-quantized autoencoder. 3. Demonstrates that fine-tuning LLMs on latent plans improves reasoning accuracy, efficiency, and cross-domain generalization while preserving interpretability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dce54c76575e0ccf630d7de1d6cf89260c30415f1817167f368dcb00b0d64bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dce54c76575e0ccf630d7de1d6cf89260c30415f1817167f368dcb00b0d64bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of LLMs generating unreliable explicit textual plans for reasoning. It proposes the iCLP framework, which enables LLMs to learn and use compact latent plans, inspired by human subconscious cognition. Experiments show this approach improves reasoning performance and generalization on mathematical and coding tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[iCLP: \u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u9690\u5f0f\u8ba4\u77e5\u6f5c\u5728\u89c4\u5212<br>iCLP: LLM Reasoning with Implicit Cognition Latent Planning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u663e\u5f0f\u6587\u672c\u89c4\u5212\u751f\u6210\u56f0\u96be<br>Challenges in generating explicit textual plans]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5b66\u4e60\u5e76\u4f7f\u7528\u6f5c\u5728\u89c4\u5212<br>Learn and use latent plans via VQ-VAE and fine-tuning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u51c6\u786e\u7387\u3001\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b<br>Improves accuracy, efficiency, and generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial attacks], [jailbreaking, content safety filters, LLM safety alignment, input/output filtering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," CISPA Helmholtz Center for Information Security, Max Planck Institute for Intelligent Systems, Southern University of Science and Technology"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24044",children:"https://arxiv.org/pdf/2512.24044"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. First systematic evaluation of jailbreak attacks across the full LLM inference pipeline including input and output safety filters, 2. Demonstration that nearly all jailbreak techniques can be detected by at least one safety filter, challenging prior overestimations of attack success, 3. Identification of gaps in balancing recall and precision for optimizing protection and user experience in safety systems"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e40bd85a824936f762c47f0b98464b3b30e7733690440363f10ee46695920a8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e40bd85a824936f762c47f0b98464b3b30e7733690440363f10ee46695920a8_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap in evaluating jailbreak attacks by systematically testing them against both LLM safety alignment and external content filters in the full deployment pipeline. The study finds that most jailbreaks can be detected by safety filters, suggesting prior success rates were overestimated, and highlights the need for better precision-recall balance in filter design."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\nA[Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Jailbreak attacks bypass LLM safety alignment, prior evaluations neglect full deployment pipeline with content filters]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: First systematic evaluation of jailbreak attacks across full inference pipeline including input/output filtering stages]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Most jailbreaks detectable by safety filters, prior success overestimated; need better recall-precision balance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [reliability, calibration, robustness, uncertainty quantification, composite score]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rohit Kumar Salla, Manoj Saravanan, Shrikar Reddy Kota"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Virginia Tech"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24058",children:"https://arxiv.org/pdf/2512.24058"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/rohitsalla/CRS.git",children:"https://github.com/rohitsalla/CRS.git"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A unified reliability metric (CRS) integrating calibration, robustness, and uncertainty. 2. A large-scale evaluation of ten open-source LLMs on five QA datasets. 3. The demonstration that CRS provides stable model rankings and uncovers hidden failure modes."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the fragmented evaluation of Large Language Model (LLM) reliability by proposing the Composite Reliability Score (CRS), a unified metric that integrates calibration, robustness, and uncertainty quantification. Through experiments on ten open-source LLMs, the authors show that CRS provides consistent model rankings and reveals trade-offs between reliability dimensions. The main conclusion is that the most dependable LLM systems balance accuracy, robustness, and calibrated uncertainty."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond Hallucinations: A Composite Score for Measuring Reliability] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLM\u53ef\u9760\u6027\u8bc4\u4f30\u788e\u7247\u5316/Fragmented LLM Reliability Evaluation]\n    C --\x3e C1[\u63d0\u51faCRS\u590d\u5408\u5206\u6570/Propose Composite Reliability Score (CRS)]\n    D --\x3e D1[CRS\u63d0\u4f9b\u7a33\u5b9a\u6a21\u578b\u6392\u540d/CRS Delivers Stable Model Rankings]\n    D --\x3e D2[\u63ed\u793a\u9690\u85cf\u7684\u5931\u8d25\u6a21\u5f0f/Uncovers Hidden Failure Modes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal reasoning], [audio-language models, hallucination mitigation, counterfactual hard negatives, preference alignment, temporal reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24052",children:"https://arxiv.org/pdf/2512.24052"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/LLM-VLM-GSL/AHA",children:"https://github.com/LLM-VLM-GSL/AHA"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[Large Audio-Language Models (LALMs) suffer from hallucinations / \u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Propose AHA framework with counterfactual hard negative mining / \u63d0\u51faAHA\u6846\u67b6\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\u786c\u8d1f\u4f8b\u6316\u6398]\n    Method --\x3e M2[Construct preference dataset for alignment / \u6784\u5efa\u7528\u4e8e\u5bf9\u9f50\u7684\u504f\u597d\u6570\u636e\u96c6]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[13.7% improvement on AHA-Eval benchmark / \u5728AHA-Eval\u57fa\u51c6\u4e0a\u63d0\u534713.7%]\n    Results --\x3e R2[Gains on public benchmarks (MMAU-Test, MMAR) / \u5728\u516c\u5f00\u57fa\u51c6(MMAU-Test, MMAR)\u4e0a\u53d6\u5f97\u63d0\u5347]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Training a Huggingface Model on AWS Sagemaker (Without Tears)"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [AWS SageMaker, Hugging Face, MLOps, cloud computing, Jupyter as a Service]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Liling Tan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," (Institution not explicitly stated in provided content. Based on author name and context, likely independent researcher or affiliation not listed on first page.)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24098",children:"https://arxiv.org/pdf/2512.24098"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Provides a centralized, comprehensive guide to train a Hugging Face model on AWS SageMaker, addressing fragmented documentation. 2. Bridges the knowledge gap between local Jupyter Notebook development and cloud-based training on SageMaker. 3. Aims to democratize cloud adoption for researchers lacking on-premise computing resources by lowering the platform's learning curve."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9d3b4e38e89877a6b18e1df5bad45a785b69e26e421db2b8a658f22bdff539_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9d3b4e38e89877a6b18e1df5bad45a785b69e26e421db2b8a658f22bdff539_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This demo paper addresses the steep learning curve and fragmented documentation that hinder researchers from using AWS SageMaker to train Hugging Face models. It proposes a centralized guide to bridge the gap between local and cloud-based development workflows. The main conclusion is that this approach can democratize cloud adoption, enabling more researchers to train models without extensive on-premise resources."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Training a Huggingface Model on AWS Sagemaker (Without Tears)] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Barriers to Cloud Adoption<br>\u8d44\u6e90\u58c1\u5792\u4e0e\u5b66\u4e60\u66f2\u7ebf]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Centralized Guide<br>\u63d0\u4f9b\u96c6\u4e2d\u5316\u6307\u5357]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Democratized Cloud Training<br>\u5b9e\u73b0\u4e91\u8bad\u7ec3\u7684\u6c11\u4e3b\u5316]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Factorized Learning for Temporally Grounded Video-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [video-language models], [temporal grounding, factorized learning, preference optimization, evidence tokens, video understanding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24097",children:"https://arxiv.org/pdf/2512.24097"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/nusnlp/d2vlm",children:"https://github.com/nusnlp/d2vlm"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a "grounding then answering with evidence referencing" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Factorized Learning for Temporally Grounded Video-Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Existing models struggle with accurate temporal grounding for event-level perception. \u73b0\u6709\u6a21\u578b\u5728\u4e8b\u4ef6\u7ea7\u611f\u77e5\u7684\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u4e0a\u5b58\u5728\u56f0\u96be\u3002]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). \u63d0\u51faD2VLM\u6846\u67b6\u548c\u56e0\u5b50\u5316\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u3002]\n    D[\u5173\u952e\u7ed3\u679c/Results: Demonstrates clear advantage on various tasks. \u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] HY-MT1.5 Technical Report"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [machine translation], [holistic training framework, on-policy distillation, parameter efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mao Zheng, Zheng Li, Tao Chen, Mingyang Song, Di Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent Hunyuan Team"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24092",children:"https://arxiv.org/pdf/2512.24092"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Tencent-Hunyuan/HY-MT",children:"https://github.com/Tencent-Hunyuan/HY-MT"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of high-performance machine translation models. 2. Proposes a holistic multi-stage training framework integrating general/MT pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. 3. Demonstrates state-of-the-art performance, with the 1.8B model achieving remarkable parameter efficiency and the 7B model surpassing ultra-large proprietary models on challenging benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef782a5a818ae655e93a8b8755b15f73eb04f3d46c041e15eb5c4a26d7b05b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef782a5a818ae655e93a8b8755b15f73eb04f3d46c041e15eb5c4a26d7b05b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This report introduces the HY-MT1.5 series of machine translation models, developed using a holistic multi-stage training pipeline. The models, particularly the 1.8B parameter version, show exceptional parameter efficiency, outperforming much larger models and commercial APIs, while the 7B model sets a new state-of-the-art for its size class."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HY-MT1.5 Technical Report] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u9ad8\u6027\u80fd\u673a\u5668\u7ffb\u8bd1/High-performance Machine Translation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6574\u4f53\u8bad\u7ec3\u6846\u67b6/Holistic Training Framework]\n    C --\x3e C1[\u591a\u9636\u6bb5\u7ba1\u9053/Multi-stage Pipeline]\n    C1 --\x3e C1a[\u9884\u8bad\u7ec3/Pre-training]\n    C1 --\x3e C1b[\u76d1\u7763\u5fae\u8c03/Supervised Fine-tuning]\n    C1 --\x3e C1c[\u7b56\u7565\u84b8\u998f/On-policy Distillation]\n    C1 --\x3e C1d[\u5f3a\u5316\u5b66\u4e60/Reinforcement Learning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5353\u8d8a\u6027\u80fd\u4e0e\u6548\u7387/Outstanding Performance & Efficiency]\n    D --\x3e D1[HY-MT1.5-1.8B: \u53c2\u6570\u9ad8\u6548/Parameter Efficient]\n    D --\x3e D2[HY-MT1.5-7B: \u65b0SOTA/New SOTA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [post-training quantization, weight outliers, rotation, GPTQ, data-free]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Advait Gadhikar, Riccardo Grazzi, James Hensman"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," CISPA Helmholtz Center for Information Security, Microsoft Research"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24124",children:"https://arxiv.org/pdf/2512.24124"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes OptRot, a data-free method that learns fusible rotations by minimizing a principled, cheap proxy objective (element-wise fourth power of weights) to reduce weight outliers for quantization. 2. Demonstrates that OptRot outperforms existing rotation methods (Hadamard, SpinQuant, OSTQuant) for weight quantization and improves W4A8 activation quantization. 3. Introduces OptRot+, a data-dependent variant that incorporates activation covariance information for further performance gains, while highlighting a trade-off between weight and activation quantization in the W4A4 setting."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of quantizing Large Language Models (LLMs) by mitigating weight outliers. It proposes OptRot, a data-free method that learns efficient rotations to minimize a proxy for weight quantization error, and shows it outperforms existing techniques for weight and W4A8 activation quantization. The work also introduces an enhanced data-dependent variant and reveals a performance trade-off in more aggressive quantization settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLM\u6743\u91cd\u548c\u6fc0\u6d3b\u4e2d\u7684\u5f02\u5e38\u503c\u4f7f\u91cf\u5316\u56f0\u96be/Outliers in LLM weights & activations make quantization difficult]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u901a\u8fc7\u6700\u5c0f\u5316\u65cb\u8f6c\u540e\u6743\u91cd\u7684\u56db\u9636\u77e9\u5b66\u4e60\u53ef\u878d\u5408\u7684\u65cb\u8f6c/Learn fusible rotations by minimizing element-wise fourth power of rotated weights (OptRot)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: OptRot\u5728\u6743\u91cd\u91cf\u5316\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6539\u8fdbW4A8\u6fc0\u6d3b\u91cf\u5316\uff0cW4A4\u4e0b\u5b58\u5728\u6743\u8861/OptRot outperforms existing methods for weight quant., improves W4A8 activation quant., trade-off in W4A4 setting]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Activation Steering for Masked Diffusion Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [controllable text generation], [masked diffusion language models, activation steering, inference-time control, contrastive examples, denoising process]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Adi Shnaidman, Erin Feiglin, Osher Yaari, Efrat Mentel, Amit Levi, Raz Lapid"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Deepkeep, Technion"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24143",children:"https://arxiv.org/pdf/2512.24143"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an activation-steering framework for Masked Diffusion Language Models (MDLMs) to achieve inference-time control. 2. Introduces a method to compute layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. 3. Demonstrates effective steering for modulating high-level attributes (e.g., compliance, language) through experiments and ablations on LLaDA-8B-Instruct."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd08d3e6e7b873796f6e3e5e5022c66a9ad714f8a6e71a0a17f32e0087c5706_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd08d3e6e7b873796f6e3e5e5022c66a9ad714f8a6e71a0a17f32e0087c5706_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of inference-time control mechanisms for Masked Diffusion Language Models (MDLMs). It proposes an efficient activation-steering framework that computes steering vectors from contrastive examples and applies them during the reverse-diffusion process. The method successfully modulates attributes like compliance and language in text generation, as validated on the LLaDA-8B-Instruct model."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Activation Steering for Masked Diffusion Language Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Lack of inference-time control in MDLMs")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Activation steering via contrastive vectors")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Reliable attribute modulation demonstrated")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Large Emotional World Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [world models], [emotion-aware reasoning, theory of mind, causal relationship modeling, social behavior prediction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Changhao Song, Yazhou Zhang, Hui Gao, Chang Yang, Peng Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tianjin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24149",children:"https://arxiv.org/pdf/2512.24149"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates the importance of emotional information for world understanding by showing that removing it degrades reasoning performance. 2. Constructs the Emotion-Why-How (EWH) dataset, which integrates emotion into causal chains for reasoning about actions and future states. 3. Proposes the Large Emotional World Model (LEWM) that explicitly models emotional states alongside observations and actions to predict future states and emotional transitions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3bcd78e33e89c57127b0a9b1c8aea7e8a64af3fc78b1daecaa947e6e7bf3f59_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3bcd78e33e89c57127b0a9b1c8aea7e8a64af3fc78b1daecaa947e6e7bf3f59_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a gap in existing world models, which focus on physical regularities but neglect emotional factors crucial for predicting human behavior. To address this, the authors propose the Large Emotional World Model (LEWM), trained on a novel Emotion-Why-How dataset, which explicitly models and predicts emotional states and transitions. Experiments show that LEWM more accurately predicts emotion-driven social behaviors while maintaining performance on basic world modeling tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Large Emotional World Model] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u60c5\u611f\u5efa\u6a21 / Existing World Models Lack Emotion Modeling]\n    C --\x3e C1[\u6784\u5efaEWH\u6570\u636e\u96c6 / Construct EWH Dataset]\n    C --\x3e C2[\u63d0\u51faLEWM\u6a21\u578b / Propose LEWM]\n    D --\x3e D1[\u66f4\u51c6\u786e\u9884\u6d4b\u60c5\u611f\u9a71\u52a8\u884c\u4e3a / More Accurately Predicts Emotion-Driven Behavior]\n    D --\x3e D2[\u57fa\u7840\u4efb\u52a1\u6027\u80fd\u76f8\u5f53 / Comparable Performance on Basic Tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Training Report of TeleChat3-MoE"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Mixture-of-Experts (MoE), Ascend NPU, distributed parallelism, performance optimization, cluster-level optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xinzhang Liu, Chao Wang, Zhihao Yang, Zhuo Jiang, Xuncheng Zhao, Haoran Wang, Lei Li, Dongdong He, Luobin Liu, Kaizhe Yuan, Han Gao, Zihan Wang, Yitong Yao, Sishi Xiong, Wenmin Deng, Haowei He, Kaidong Yu, Yu Zhao, Ruiyu Fang, Yuhao Jiang, Yingyan Li, Xiaohui Hu, Xi Yu, Jingqi Li, Yanwei Liu, Qingli Li, Xinyu Shi, Junhao Niu, Chengnuo Huang, Yao Xiao, Ruiwen Wang, Fengkai Li, Luwen Pu, Kaipeng Jia, Fubei Yao, Yuyao Huang, Xuewei He, Zhuoru Jiang, Ruiting Song, Rui Xue, Qiyi Xie, Jie Zhang, Zilu Huang, Zhaoxi Zhang, Zhilong Lu, Yanhan Zhang, Yin Zhang, Yanlei Xue, Zhu Yuan, Teng Su, Xin Jiang, Shuangyong Song, Yongxiang Li, Xuelong Li"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom Corp Ltd; Huawei"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24157",children:"https://arxiv.org/pdf/2512.24157"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/Tele-AI/TeleChat3",children:"https://github.com/Tele-AI/TeleChat3"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Systematic methodologies for operator-level and end-to-end numerical accuracy verification to ensure consistency across hardware and distributed strategies. 2. A suite of performance optimizations including interleaved pipeline scheduling, attention-aware data scheduling, hierarchical/overlapped communication for expert parallelism, and DVM-based operator fusion. 3. A systematic parallelization framework using analytical estimation and integer linear programming to optimize multi-dimensional parallelism configurations, along with cluster-level optimizations for host- and device-bound bottlenecks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7397c7d9494485b235d8038c6e7686b695b8b88031d87dd613f41eafdfca9fe1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7397c7d9494485b235d8038c6e7686b695b8b88031d87dd613f41eafdfca9fe1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This technical report presents the training infrastructure for the TeleChat3-MoE large language model series, which features a Mixture-of-Experts architecture. The core contributions are systematic methods for numerical verification, performance optimizations for efficient distributed training, and a parallelization framework for optimal configuration, resulting in significant throughput improvements and near-linear scaling on large clusters."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[TeleChat3-MoE Training Report] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u53ef\u9760\u9ad8\u6548\u5730\u6269\u5c55\u81f3\u524d\u6cbf\u6a21\u578b\u89c4\u6a21/Reliably and efficiently scaling to frontier model sizes]\n    C --\x3e C1[\u7cfb\u7edf\u5316\u6570\u503c\u7cbe\u5ea6\u9a8c\u8bc1/Systematic numerical accuracy verification]\n    C --\x3e C2[\u6027\u80fd\u4f18\u5316\u5957\u4ef6/Performance optimization suite]\n    C --\x3e C3[\u7cfb\u7edf\u5316\u5e76\u884c\u5316\u6846\u67b6/Systematic parallelization framework]\n    C2 --\x3e C2_1[\u4ea4\u9519\u6d41\u6c34\u7ebf\u8c03\u5ea6/Interleaved pipeline scheduling]\n    C2 --\x3e C2_2[\u6ce8\u610f\u529b\u611f\u77e5\u6570\u636e\u8c03\u5ea6/Attention-aware data scheduling]\n    C2 --\x3e C2_3[\u5206\u5c42\u91cd\u53e0\u901a\u4fe1/Hierarchical and overlapped communication]\n    C3 --\x3e C3_1[\u5206\u6790\u4f30\u8ba1\u4e0e\u6574\u6570\u7ebf\u6027\u89c4\u5212/Analytical estimation and integer linear programming]\n    D --\x3e D1[\u663e\u8457\u541e\u5410\u91cf\u63d0\u5347/Significant throughput improvements]\n    D --\x3e D2[\u6570\u5343\u8bbe\u5907\u8fd1\u7ebf\u6027\u6269\u5c55/Near-linear scaling on thousands of devices]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [clinical dialogue systems], [medical knowledge graph, information gain, OSCE-format state]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qipeng Wang, Rui Sheng, Yafei Li, Huamin Qu, Yushi Sun, Min Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sichuan University, HKUST"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24181",children:"https://arxiv.org/pdf/2512.24181"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MedKGI, a diagnostic framework that integrates a medical knowledge graph to ground reasoning in validated ontologies and prevent hallucinations. 2. Introduces an information-gain-based question selection strategy to ask discriminative questions and improve diagnostic efficiency. 3. Adopts an OSCE-format structured state to maintain coherent evidence tracking and consistency across multi-turn diagnostic dialogues."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b46d6dca11e4b1d6269ffe806fbde3df70705f709f863770c57d2ee3313df423_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b46d6dca11e4b1d6269ffe806fbde3df70705f709f863770c57d2ee3313df423_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes MedKGI, a framework that uses a medical knowledge graph and information-guided inquiry to improve iterative clinical diagnosis with LLMs. It addresses issues like hallucinations and inefficient questioning by grounding reasoning in verified knowledge and selecting questions based on information gain. Experiments show MedKGI outperforms LLM baselines in accuracy and improves dialogue efficiency by 30%."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027/Limitations of LLMs in Clinical Diagnosis]\n    B1 --\x3e B2[\u5e7b\u89c9\u3001\u4f4e\u6548\u63d0\u95ee\u3001\u5bf9\u8bdd\u4e0d\u4e00\u81f4/Hallucinations, Inefficient Questioning, Inconsistent Dialogue]\n    C --\x3e C1[\u6574\u5408\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31/Integrate Medical Knowledge Graph]\n    C --\x3e C2[\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u63d0\u95ee/Information-Gain-Based Questioning]\n    C --\x3e C3[\u7ed3\u6784\u5316OSCE\u72b6\u6001\u8ddf\u8e2a/Structured OSCE State Tracking]\n    D --\x3e D1[\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u5347/Improved Diagnostic Accuracy]\n    D --\x3e D2[\u5bf9\u8bdd\u6548\u7387\u63d0\u534730%/30% Improved Dialogue Efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [automated essay scoring], [Arabic NLP, dataset, trait-based annotation, benchmark models, cross-prompt evaluation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," May Bashendy, Walid Massoud, Sohaila Eltanbouly, Salam Albatarni, Marwan Sayed, Abrar Abir, Houda Bouamor, Tamer Elsayed"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Qatar University, Carnegie Mellon University in Qatar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24235",children:"https://arxiv.org/pdf/2512.24235"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced LAILA, the largest publicly available Arabic AES dataset with 7,859 essays. 2. Provided detailed holistic and trait-specific annotations across seven writing dimensions. 3. Established benchmark results using state-of-the-art models in both prompt-specific and cross-prompt settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae05a0c997e3fe72bc7d8f192412113731426447fc7b73d020117ab725e12165_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae05a0c997e3fe72bc7d8f192412113731426447fc7b73d020117ab725e12165_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of public datasets for Arabic Automated Essay Scoring (AES) by introducing LAILA, a large-scale dataset with 7,859 essays annotated for holistic and seven trait-specific scores. It details the dataset's collection and annotation process and provides benchmark results using modern Arabic and English models. The dataset is presented as a critical resource to support the development of robust Arabic AES systems."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u963f\u62c9\u4f2f\u8bedAES\u7814\u7a76\u6709\u9650/Limited Arabic AES Research]\n    Problem --\x3e P2[\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6/Lack of Public Datasets]\n    Method --\x3e M1[\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6/Build Large-Scale Dataset]\n    Method --\x3e M2[\u63d0\u4f9b\u591a\u7ef4\u6807\u6ce8/Provide Multi-Dimensional Annotations]\n    Results --\x3e R1[\u53d1\u5e03LAILA\u6570\u636e\u96c6/Release LAILA Dataset]\n    Results --\x3e R2[\u5efa\u7acb\u57fa\u51c6\u7ed3\u679c/Establish Benchmark Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Tracing the Flow of Knowledge From Science to Technology Using Deep Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [semantic similarity], [Pat-SPECTER, SPECTER2, patent-paper citation, semantic similarity, duty of candor]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Michael E. Rose, Mainak Ghosh, Sebastian Erhardt, Cheng Li, Erik Buunk, Dietmar Harhoff"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Max Planck Institute for Innovation and Competition"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24259",children:"https://arxiv.org/pdf/2512.24259"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Developed Pat-SPECTER, a language similarity model fine-tuned on patents for joint analysis of patents and scientific publications. 2. Conducted a comparative evaluation of eight models, demonstrating Pat-SPECTER's superior performance in predicting credible patent-paper citations. 3. Applied the model to test the hypothesis that US patents cite semantically less similar papers due to the duty of candor, providing empirical evidence for this legal influence on knowledge flow."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf4d52b11ccdc3180438f19688e00f41d2edb954de210ba3f59b428918311f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf4d52b11ccdc3180438f19688e00f41d2edb954de210ba3f59b428918311f6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops Pat-SPECTER, a language model fine-tuned on patents, to measure semantic similarity between patents and scientific papers. The model outperforms others in predicting patent-paper citations and is used to show that US patents tend to cite less semantically similar papers, potentially due to legal disclosure requirements."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Tracing Knowledge Flow<br>\u8ffd\u8e2a\u77e5\u8bc6\u6d41"] --\x3e Problem["Problem: Need model for patent-paper similarity<br>\u95ee\u9898\uff1a\u9700\u8981\u4e13\u5229-\u8bba\u6587\u76f8\u4f3c\u6027\u6a21\u578b"]\n    Root --\x3e Method["Method: Develop & fine-tune Pat-SPECTER<br>\u65b9\u6cd5\uff1a\u5f00\u53d1\u5e76\u5fae\u8c03Pat-SPECTER"]\n    Root --\x3e Results["Results: Best performance; US patents cite less similar papers<br>\u7ed3\u679c\uff1a\u6027\u80fd\u6700\u4f73\uff1b\u7f8e\u56fd\u4e13\u5229\u5f15\u7528\u76f8\u4f3c\u6027\u8f83\u4f4e\u7684\u8bba\u6587"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [data selection, policy gradient, mask learning, quality-diversity trade-off, FineWeb]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ziqing Fan, Yuqiao Xian, Yan Sun, Li Shen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ByteDance Seed, Shanghai Jiao Tong University, University of Sydney, Sun Yat-sen University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24265",children:"https://arxiv.org/pdf/2512.24265"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/ByteDance-Seed/DATAMASK",children:"https://github.com/ByteDance-Seed/DATAMASK"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DATAMASK, a novel joint learning framework for large-scale pre-training data selection that simultaneously optimizes quality and diversity metrics. 2. Formulates data selection as a mask learning problem and solves it efficiently using policy gradient-based optimization with acceleration enhancements, reducing selection time by 98.9% compared to greedy algorithms. 3. Creates and releases FineWeb-Mask, a high-quality and diverse 10% subset of the 15-trillion-token FineWeb dataset, which significantly improves model performance (e.g., +3.2% on a 1.5B model) across diverse tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fd504349b8c0bb1934304d821a648ed4ca490b2f41e136f9b7a6220f39d5d2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02fd504349b8c0bb1934304d821a648ed4ca490b2f41e136f9b7a6220f39d5d2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of efficiently selecting high-quality and diverse data for large-scale LLM pre-training, where traditional methods are costly and suboptimal. It proposes DATAMASK, a policy gradient-based framework that learns optimal data masks to jointly optimize quality and diversity, drastically speeding up selection. The resulting curated dataset, FineWeb-Mask, leads to significant performance gains in pre-trained models, demonstrating the framework's effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\uff0c\u8054\u5408\u8003\u8651\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u6307\u6807\u8fdb\u884c\u6837\u672c\u9009\u62e9\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faDATAMASK\u6846\u67b6\uff0c\u5c06\u9009\u62e9\u8fc7\u7a0b\u89c6\u4e3a\u63a9\u7801\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u8fdb\u884c\u4f18\u5316]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u9009\u62e9\u65f6\u95f4\u51cf\u5c1198.9%\uff0c\u4eceFineWeb\u4e2d\u9009\u51fa\u7684\u5b50\u96c6\u663e\u8457\u63d0\u5347\u591a\u79cd\u6a21\u578b\u6027\u80fd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [information extraction], [large language models, EU Taxonomy, key performance indicators, sustainability reporting, benchmark dataset]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jonathan Schmoll, Adam Jatowt"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Innsbruck"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24289",children:"https://arxiv.org/pdf/2512.24289"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel, structured benchmark dataset from 190 corporate reports for EU Taxonomy analysis, containing ground-truth economic activities and quantitative KPIs. 2. Conducts the first systematic evaluation of LLMs on the core compliance workflow, revealing a performance gap between qualitative and quantitative tasks. 3. Proposes a multi-step agentic framework that modestly enhances precision for the qualitative task of identifying economic activities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8227132ec6a07313687e383d60d2cb42bc66f5e99544d0d728a348620db4f75d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8227132ec6a07313687e383d60d2cb42bc66f5e99544d0d728a348620db4f75d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of automating EU Taxonomy compliance by evaluating Large Language Models (LLMs) for extracting and predicting sustainability-related information from corporate reports. It introduces a new benchmark dataset and finds that while LLMs show moderate success in qualitative tasks like activity identification, they fail at quantitative KPI prediction in a zero-shot setting. The authors conclude that LLMs are not yet ready for full automation but can serve as effective assistive tools for human experts."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Automated Analysis of Sustainability Reports] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Manual EU Taxonomy compliance is resource-intensive] --\x3e ProblemDetail[\u7f3a\u4e4f\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6/Lack of public benchmark datasets]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce structured dataset & evaluate LLMs] --\x3e MethodDetail[\u4f7f\u7528LLM\u8fdb\u884c\u5b9a\u6027\u4e0e\u5b9a\u91cf\u4efb\u52a1\u8bc4\u4f30/Evaluate LLMs on qualitative & quantitative tasks]\n    Results[\u5173\u952e\u7ed3\u679c/Results: LLMs moderate on qualitative, fail on quantitative] --\x3e Conclusion[\u7ed3\u8bba/Conclusion: LLMs as assistive tools, not full automation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal reasoning], [visual thinking, reinforcement learning, chain-of-thought, geometric reasoning, multimodal integration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Meiqi Chen, Fandong Meng, Jie Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tencent Inc (WeChat AI)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24297",children:"https://arxiv.org/pdf/2512.24297"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FIGR, a novel method that integrates active visual thinking into multi-step reasoning via end-to-end reinforcement learning. 2. Proposes a mechanism to adaptively regulate when and how to invoke visual reasoning, externalizing structural hypotheses by constructing visual representations. 3. Demonstrates significant performance improvements on challenging mathematical reasoning benchmarks, enhancing the stability and reliability of complex reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87f7f4c5ac39e44125cddcd070468c932e7b1c2ea80095ffe3322857f26d40ed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87f7f4c5ac39e44125cddcd070468c932e7b1c2ea80095ffe3322857f26d40ed_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FIGR, a method that enhances complex reasoning by integrating active visual thinking through reinforcement learning, allowing models to construct visual diagrams during problem-solving. It adaptively decides when to use visual reasoning to better capture spatial and structural relationships. Experiments show FIGR outperforms text-only baselines on mathematical reasoning benchmarks, improving stability and reliability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Text-based reasoning struggles with implicit spatial and structural relationships.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: FIGR integrates active visual thinking via RL to construct visual representations adaptively.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Outperforms text-only baselines, improves stability and reliability on math benchmarks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [domain-specific language models], [continual pre-training, supervised fine-tuning, reinforcement learning, financial reasoning, agentic capabilities]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shupeng Li, Weipeng Lu, Linyun Liu, Chen Lin, Shaofei Li, Zhendong Tan, Hanjun Zhong, Yucheng Zeng, Chenghao Zhu, Mengyue Liu, Daxiang Dong, Jianmin Wu, Yunting Xiao, Annan Li, Danyu Liu, Jingnan Zhang, Licen Liu, Dawei Yin, Dou Shen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Baidu AI Cloud"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24314",children:"https://arxiv.org/pdf/2512.24314"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel, generalizable multi-stage training paradigm for enhancing industrial LLMs, moving beyond simple knowledge injection to include reasoning and agentic capabilities. 2. Introduces QianfanHuijin, a financial domain LLM developed using this paradigm, which includes stages of Financial SFT, Finance Reasoning RL, and Finance Agentic RL. 3. Empirically validates the paradigm, showing superior performance on financial benchmarks and confirming the specific gains from the targeted Reasoning and Agentic RL stages through ablation studies."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4582f947672a6fc36e2da1b1d6c10ce7940df34a52b02ba28048ca5cea0e2715_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4582f947672a6fc36e2da1b1d6c10ce7940df34a52b02ba28048ca5cea0e2715_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the need for financial LLMs with robust reasoning and agentic capabilities beyond just domain knowledge. It proposes a multi-stage training paradigm involving continual pre-training followed by a fine-grained post-training pipeline (SFT, Reasoning RL, Agentic RL, General RL) and introduces the QianfanHuijin model. The results show the model achieves state-of-the-art performance on financial benchmarks, validating the effectiveness of the progressive training approach."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[QianfanHuijin Technical Report<br>\u6280\u672f\u62a5\u544a] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u91d1\u878dLLM\u9700\u63a8\u7406\u4e0e\u667a\u80fd\u4f53\u80fd\u529b<br>Financial LLMs need reasoning & agentic capabilities]\n    C --\x3e C1[\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f<br>Multi-Stage Training Paradigm]\n    C1 --\x3e C2[\u6301\u7eed\u9884\u8bad\u7ec3 CPT<br>Continual Pre-training]\n    C1 --\x3e C3[\u6e10\u8fdb\u5f0f\u540e\u8bad\u7ec3<br>Progressive Post-training]\n    C3 --\x3e C4[\u91d1\u878dSFT<br>Financial SFT]\n    C3 --\x3e C5[\u91d1\u878d\u63a8\u7406RL<br>Finance Reasoning RL]\n    C3 --\x3e C6[\u91d1\u878d\u667a\u80fd\u4f53RL<br>Finance Agentic RL]\n    C3 --\x3e C7[\u901a\u7528RL<br>General RL]\n    D --\x3e D1[\u6743\u5a01\u57fa\u51c6\u6027\u80fd\u4f18\u8d8a<br>Superior on authoritative benchmarks]\n    D --\x3e D2[\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u9636\u6bb5\u6709\u6548\u6027<br>Ablation validates stage gains]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] World model inspired sarcasm reasoning with large language model agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [sarcasm detection], [world model, large language model agents, interpretability, semantic inconsistency, intention reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Keito Inoshita, Shinnosuke Mizuno"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Kansai University, Shiga University, The University of Tokyo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24329",children:"https://arxiv.org/pdf/2512.24329"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Reformulates sarcasm understanding as a world model inspired reasoning process, decomposing the task into components like literal meaning and normative expectation. 2. Proposes WM-SAR, a framework using specialized LLM-based agents to generate explicit, quantifiable scores for semantic inconsistency and intention. 3. Integrates these interpretable scores via a lightweight Logistic Regression model, achieving strong performance and high interpretability on sarcasm detection benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea54c90970e87e4614b497dfa13507774353c0c9ec5d9c020257c78898239831_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea54c90970e87e4614b497dfa13507774353c0c9ec5d9c020257c78898239831_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes WM-SAR, a method that reformulates sarcasm detection as a world model inspired reasoning process using multiple LLM-based agents to explicitly quantify semantic inconsistency and speaker intention. These scores are then integrated by a simple classifier to predict sarcasm. Experiments show the method outperforms existing approaches and provides an interpretable decision structure."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[World model inspired sarcasm reasoning with large language model agents] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Sarcasm understanding requires capturing discrepancy between literal meaning and speaker's intention/context, and existing black-box models lack interpretability.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes WM-SAR, using specialized LLM agents to decompose and score literal meaning, normative expectation, and intention, then integrates scores with Logistic Regression.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: WM-SAR outperforms existing methods on benchmarks, and ablation studies show the importance of semantic inconsistency and intention reasoning for performance and interpretability.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [visual question answering, lesion segmentation, multimodal models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Microsoft, University of Washington, Stanford University, Hospital Clinic of Barcelona"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24340",children:"https://arxiv.org/pdf/2512.24340"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://osf.io/72rp3",children:"https://osf.io/72rp3"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduction of the Dermatology Assessment Schema (DAS), a novel expert-developed framework for structured dermatological feature assessment. 2. Release of DermaVQA-DAS, an extended dataset supporting closed-ended question answering and lesion segmentation on patient-generated images. 3. Comprehensive benchmarking of state-of-the-art multimodal models on the new tasks, analyzing the impact of prompt design on segmentation performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of patient-centered benchmarks in dermatology by introducing DermaVQA-DAS, a dataset extension built upon a novel expert-developed assessment schema (DAS) for structured feature annotation. It supports two tasks\u2014closed-ended visual question answering and lesion segmentation\u2014on patient-generated images and queries. The study benchmarks modern multimodal models, finding strong QA performance and demonstrating that prompt design significantly impacts segmentation results."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[DermaVQA-DAS] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u60a3\u8005\u89c6\u89d2/Existing datasets lack patient perspective]\n    P1 --\x3e P2[\u9650\u5236\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u5e94\u7528/Limits patient-centered care applications]\n\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51fa\u76ae\u80a4\u75c5\u8bc4\u4f30\u6846\u67b6(DAS)/Propose Dermatology Assessment Schema (DAS)]\n    M1 --\x3e M2[\u6269\u5c55DermaVQA\u6570\u636e\u96c6/Extend DermaVQA dataset]\n    M2 --\x3e M3[\u652f\u6301\u4e24\u9879\u4efb\u52a1:\u5c01\u95ed\u5f0f\u95ee\u7b54\u4e0e\u5206\u5272/Support two tasks: closed QA & segmentation]\n\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u5206\u5272\u6027\u80fd/Prompt design impacts segmentation performance]\n    R1 --\x3e R2[\u6a21\u578b\u5728QA\u4e0a\u8868\u73b0\u5f3a\u52b2/Models perform strongly on QA]\n    R2 --\x3e R3[\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u534f\u8bae/Publicly release dataset & evaluation protocols]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Skim-Aware Contrastive Learning for Efficient Document Representation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [document representation], [contrastive learning, natural language inference, long document, self-supervised learning, hierarchical transformer]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Waheed Ahmed Abro, Zied Bouraoui"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Univ Artois, CNRS"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24373",children:"https://arxiv.org/pdf/2512.24373"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel self-supervised contrastive learning framework inspired by human skimming behavior for long document representation. 2. A method that uses random section masking and an NLI-based contrastive objective to align relevant parts and distance unrelated ones. 3. Demonstrated significant improvements in both accuracy and efficiency on legal and biomedical document tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae38f3776c3982e71d1fd2fda48c20229eb63f1918544341ca35ab3cb48a02af_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae38f3776c3982e71d1fd2fda48c20229eb63f1918544341ca35ab3cb48a02af_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of efficiently representing long documents like legal and medical texts. It proposes a self-supervised contrastive learning method that mimics human skimming by masking sections and using an NLI objective to relate document parts. Experiments show the method achieves better accuracy and computational efficiency compared to existing approaches."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Skim-Aware Contrastive Learning for Efficient Document Representation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u957f\u6587\u6863\u8868\u793a\u56f0\u96be/Inefficient long document representation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8eNLI\u7684\u5bf9\u6bd4\u5b66\u4e60/NLI-based contrastive learning with section masking]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347/Significant gains in accuracy and efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Comparing Approaches to Automatic Summarization in Less-Resourced Languages"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text summarization], [less-resourced languages, multilingual transfer, data augmentation, LLM prompting, mT5]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chester Palen-Michel, Constantine Lignos"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Brandeis University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24410",children:"https://arxiv.org/pdf/2512.24410"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A comprehensive comparative study of multiple summarization approaches for less-resourced languages, including zero-shot LLMs, fine-tuned mT5, and a translation pipeline. 2. Exploration and evaluation of three data augmentation methods using Wikipedia to generate synthetic training data for low-resource settings. 3. An analysis showing that a fine-tuned multilingual mT5 baseline often outperforms zero-shot LLMs and that LLM-as-judge evaluation may be unreliable for less-resourced languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14f0d3397d12fd7c93ad9814a09fa7bd5c030b47c51091b59d65cc5392446ba_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14f0d3397d12fd7c93ad9814a09fa7bd5c030b47c51091b59d65cc5392446ba_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares various methods for automatic text summarization in less-resourced languages, including prompting large language models (LLMs), fine-tuning multilingual models like mT5 with data augmentation, and a translation pipeline. The evaluation across multiple metrics finds that a fine-tuned multilingual mT5 model generally outperforms zero-shot LLMs, and highlights potential issues with using LLMs as evaluators for these languages."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[Comparing Approaches to Automatic Summarization in Less-Resourced Languages] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u9ad8\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u597d\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u4e0d\u8db3 / High performance in high-resourced languages, less attention to less-resourced languages"]\n    C --\x3e C1["\u6bd4\u8f83\u591a\u79cd\u65b9\u6cd5 / Compare various approaches"]\n    C1 --\x3e C1_1["\u96f6\u6837\u672c\u63d0\u793aLLM / Zero-shot prompting LLMs"]\n    C1 --\x3e C1_2["\u5fae\u8c03mT5\uff08\u542b\u6570\u636e\u589e\u5f3a\uff09 / Fine-tuning mT5 (with data augmentation)"]\n    C1 --\x3e C1_3["\u7ffb\u8bd1-\u603b\u7ed3-\u7ffb\u8bd1\u6d41\u7a0b / Translate-summarize-translate pipeline"]\n    D --\x3e D1["\u5fae\u8c03mT5\u4f18\u4e8e\u5927\u591a\u6570\u65b9\u6cd5 / Fine-tuned mT5 outperforms most approaches"]\n    D --\x3e D2["LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u53ef\u80fd\u4e0d\u53ef\u9760 / LLM as judge may be less reliable"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Cleaning English Abstracts of Scientific Publications"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text preprocessing], [scientific abstracts, text cleaning, document similarity, language model, embeddings]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Michael E. Rose, Nils A. Herrmann, Sebastian Erhardt"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Max Planck Institute for Innovation and Competition, Technical University Munich"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24459",children:"https://arxiv.org/pdf/2512.24459"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an open-source, easy-to-integrate language model specifically designed to clean English-language scientific abstracts by removing extraneous clutter (e.g., copyrights, metadata). 2. Demonstrates that the model is conservative and precise in its cleaning, effectively altering document similarity rankings to be more content-focused. 3. Shows that cleaning abstracts improves the information content of standard-length textual embeddings, enhancing downstream NLP analyses."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4321a5aec5e6660d2de29896a2c0e5ac6c2c0e3366797584c85ba08542e17aa2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4321a5aec5e6660d2de29896a2c0e5ac6c2c0e3366797584c85ba08542e17aa2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of extraneous, non-content text (e.g., copyrights, section headings) polluting scientific abstracts, which distorts analyses like document similarity. It proposes a dedicated language model to automatically clean such abstracts. The results show the model effectively removes clutter, alters similarity rankings, and improves the quality of text embeddings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[Cleaning English Abstracts of Scientific Publications] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[Abstracts contain clutter/\u6458\u8981\u5305\u542b\u65e0\u5173\u4fe1\u606f]\n    P1 --\x3e P2[Distorts similarity & embeddings/\u626d\u66f2\u76f8\u4f3c\u6027\u4e0e\u5d4c\u5165]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Open-source LM/\u5f00\u6e90\u8bed\u8a00\u6a21\u578b]\n    M1 --\x3e M2[Automatic cleaning/\u81ea\u52a8\u6e05\u7406]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Conservative & precise/\u4fdd\u5b88\u4e14\u7cbe\u786e]\n    R1 --\x3e R2[Alters similarity rankings/\u6539\u53d8\u76f8\u4f3c\u6027\u6392\u540d]\n    R2 --\x3e R3[Improves embeddings/\u63d0\u5347\u5d4c\u5165\u8d28\u91cf]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [automated essay scoring], [DistilBERT, regression head, Design-Based Research (DBR), adaptive feedback, transformer model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Titas Ramancauskas, Kotryna Ramancauske"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," (Institution not explicitly stated in provided content; inferred from author names as potentially independent researchers)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24460",children:"https://arxiv.org/pdf/2512.24460"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Development of an IELTS writing revision platform with a dedicated UI that separates conversational guidance from the writing interface to reduce cognitive load. 2. Implementation of an Automated Essay Scoring (AES) system using a DistilBERT transformer model with a regression head, achieving improved scoring accuracy (MAE 0.66, positive R\xb2) over rule-based methods. 3. Design and evaluation of adaptive feedback tailored to the IELTS rubric, which demonstrated statistically significant score improvements (mean +0.060 bands) and identified conservative surface-level corrections as more reliable than aggressive structural interventions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cd49db97d083a1a08542b5c9894656f76ab161b46611cfa6c1b0426b21442e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cd49db97d083a1a08542b5c9894656f76ab161b46611cfa6c1b0426b21442e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of personalized feedback in IELTS writing preparation by developing a revision platform featuring an Automated Essay Scoring system and adaptive feedback. The core method involves iterative Design-Based Research, transitioning from rule-based scoring to a more accurate DistilBERT transformer model with a regression head. The main conclusion is that such automated feedback is best used as a supplement to human instruction, with surface-level corrections proving more effective for IELTS contexts than deep structural interventions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u4e2a\u6027\u5316\u53cd\u9988/Traditional methods lack personalized feedback]\n    C --\x3e C1[\u57fa\u4e8e\u8bbe\u8ba1\u7684\u7814\u7a76\u8fed\u4ee3/Iterative Design-Based Research (DBR)]\n    C1 --\x3e C2[\u4ece\u89c4\u5219\u5230Transformer/From rule-based to transformer-based (DistilBERT)]\n    C2 --\x3e C3[\u5e26\u56de\u5f52\u5934\u7684\u8bc4\u5206\u6a21\u578b/Scoring model with regression head]\n    C --\x3e C4[\u81ea\u9002\u5e94\u53cd\u9988\u7cfb\u7edf/Adaptive feedback system]\n    D --\x3e D1[\u8bc4\u5206\u51c6\u786e\u7387\u63d0\u5347/Improved scoring accuracy (MAE 0.66, positive R\xb2)]\n    D --\x3e D2[\u5206\u6570\u663e\u8457\u63d0\u9ad8/Statistically significant score improvement (mean +0.060 bands)]\n    D --\x3e D3[\u7ed3\u8bba: \u81ea\u52a8\u5316\u53cd\u9988\u662f\u4eba\u5de5\u6559\u5b66\u7684\u8865\u5145/Conclusion: Automated feedback is a supplement to human instruction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [text segmentation], [paragraph segmentation, constrained decoding, hierarchical segmentation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fabian Retkowski, Alexander Waibel"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Karlsruhe Institute of Technology, Carnegie Mellon University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24517",children:"https://arxiv.org/pdf/2512.24517"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces TEDPara and YTSegPara, the first benchmarks for paragraph segmentation in speech. 2. Proposes a constrained-decoding formulation for LLMs to insert paragraph breaks while preserving the original transcript. 3. Presents MiniSeg, a compact model achieving state-of-the-art accuracy and capable of hierarchical chapter/paragraph prediction."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91854c0eca4ed53c1e5121a89b5235513f5b721bd7a6b98dd3e52d1c104f23ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91854c0eca4ed53c1e5121a89b5235513f5b721bd7a6b98dd3e52d1c104f23ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unstructured speech transcripts by establishing paragraph segmentation as a standard task. It introduces new benchmarks, a constrained-decoding method for LLMs, and a compact model called MiniSeg that achieves state-of-the-art performance. The work successfully establishes paragraph segmentation as a practical and standardized step in speech processing."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Unstructured speech transcripts lack readability"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: New benchmarks, constrained decoding, compact model"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Establishes a standardized, practical task"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [spatial reasoning, LoRA, GRPO, supervised fine-tuning, reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Purdue University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24532",children:"https://arxiv.org/pdf/2512.24532"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A two-stage approach for multi-step spatial reasoning that first fine-tunes an LLM on atomic spatial transformations and then trains lightweight LoRA adapters via RL to compose these blocks for planning. 2. The creation of a synthetic ASCII-art dataset and a corresponding ASCII-based RL environment to support training and evaluation. 3. Demonstration that the proposed method outperforms baselines in both dynamic and static environments, with faster convergence and more stable training than end-to-end RL."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19427bf120d4b89a11879461b589b7313caa689fdec51f99f516c6485aef495_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19427bf120d4b89a11879461b589b7313caa689fdec51f99f516c6485aef495_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of multi-step spatial reasoning in LLMs by proposing a two-stage method: first, supervised fine-tuning on basic spatial transformations to build physics awareness, and then training LoRA adapters with reinforcement learning (GRPO) to learn planning policies. The approach is evaluated using a custom ASCII-art environment and is shown to outperform various baselines, converging faster and more stably than training from scratch with RL."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LLMs struggle with spatial transformations and multi-step planning]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two-stage: SFT on spatial blocks, then RL (GRPO) with LoRA for planning]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Outperforms baselines, faster convergence, stable training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [extreme quantization, double binary factorization, low-bit LLM, post-training quantization, binary matrix multiplication]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24545",children:"https://arxiv.org/pdf/2512.24545"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Multi-Envelope Double Binary Factorization (MDBF), which replaces the single magnitude envelope in DBF with a rank-l envelope to enhance magnitude expressiveness while maintaining a shared binary sign carrier. 2. Introduced a closed-form initialization and an alternating refinement method to effectively optimize the MDBF parameters. 3. Demonstrated that MDBF improves perplexity and zero-shot accuracy over prior binary formats on LLaMA and Qwen models at matched bit budgets while preserving the same efficient inference primitive."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the performance saturation of Double Binary Factorization (DBF) in extreme low-bit quantization of LLMs, where a single magnitude envelope limits expressiveness. It proposes Multi-Envelope DBF (MDBF), which uses multiple envelope components to allocate more expressivity to magnitudes while keeping binary sign matrices shared. Experiments on LLaMA and Qwen families show MDBF outperforms previous binary formats in accuracy and perplexity at the same bit rate without changing the inference primitive."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>DBF scaling too restrictive,<br>single envelope causes<br>performance saturation"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose MDBF: shared 1-bit sign bases,<br>replace single envelope with<br>rank-l envelope"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Better perplexity & accuracy<br>over previous binary formats,<br>same inference primitive"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [ai safety & alignment], [multilingual safety, temporal reasoning, adversarial evaluation, complex interference, invariant alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Abdullahi Said, Muhammad Sammani Sani"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," African Institute for Mathematical Science, University of Vienna"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24556",children:"https://arxiv.org/pdf/2512.24556"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/mohdasaid/HausaSafety_Audit",children:"https://github.com/mohdasaid/HausaSafety_Audit"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduced HausaSafety, a novel adversarial dataset for safety auditing grounded in West African threat scenarios. 2. Identified a mechanism of Complex Interference, showing safety is determined by the intersection of language and temporal variables, not a simple degradation. 3. Discovered a profound Temporal Asymmetry where past-tense framing bypasses safety defenses while future-tense triggers hyper-conservative refusals."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cf2ca379a60fe7b5c54e48b05602cc104b0bbc975a7e3c93cc9bfd7355e306_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cf2ca379a60fe7b5c54e48b05602cc104b0bbc975a7e3c93cc9bfd7355e306_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper systematically audits the safety of LLMs across languages and temporal contexts, finding that safety is not fixed but context-dependent, with models relying on superficial heuristics. The authors propose a paradigm shift towards Invariant Alignment to ensure safety stability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u591a\u8bed\u8a00\u5b89\u5168\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u76f2\u70b9/Blind spot in zero-shot multilingual safety transfer]\n    C --\x3e C1[\u4f7f\u7528HausaSafety\u8fdb\u884c\u7cfb\u7edf\u5ba1\u8ba1/Systematic audit using HausaSafety]\n    C --\x3e C2[2x4\u56e0\u5b50\u8bbe\u8ba1\u8bc4\u4f30\u8bed\u8a00\u4e0e\u65f6\u95f4/2x4 factorial design evaluating language & time]\n    D --\x3e D1[\u590d\u6742\u5e72\u6270\u673a\u5236/Complex Interference mechanism]\n    D --\x3e D2[\u53cd\u5411\u8bed\u8a00\u5b89\u5168/Reverse Linguistic Safety]\n    D --\x3e D3[\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027/Temporal Asymmetry]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [uncertainty modeling, token-level signals, multi-granular fusion, lightweight framework, one-pass detection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chaodong Tong, Qi Zhang, Jiayang Gao, Lei Jiang, Yanbing Liu, Nannan Sun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Information Engineering, Chinese Academy of Sciences; University of Chinese Academy of Sciences; China Industrial Control Systems Cyber Emergency Response Team; Beijing Institute of Computer Technology and Application"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24562",children:"https://arxiv.org/pdf/2512.24562"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HaluNet, a lightweight neural framework that integrates multi-granular token-level uncertainties (semantic embeddings, probabilistic confidence, distributional uncertainty). 2. Introduces a multi-branch architecture that adaptively fuses model knowledge with output uncertainty for efficient one-pass hallucination detection. 3. Demonstrates strong detection performance and computational efficiency on multiple QA benchmarks (SQuAD, TriviaQA, Natural Questions), with or without access to context."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483dc55be4bf85e380bc716c1ed587cc46c0c417e77e0a9ba4c6ab2e2afb2949_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483dc55be4bf85e380bc716c1ed587cc46c0c417e77e0a9ba4c6ab2e2afb2949_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of hallucination detection in LLM question answering. It proposes HaluNet, a lightweight framework that combines multiple token-level uncertainty signals for efficient one-pass detection. Experiments show it achieves strong performance and favorable computational efficiency, highlighting its potential for real-time use."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[HaluNet: Multi-Granular Uncertainty Modeling<br>for Efficient Hallucination Detection in LLM QA] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs in QA often generate hallucinations (factual errors/fabricated content)]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: HaluNet, a lightweight neural framework integrating multi-granular token-level uncertainties (semantic + probabilistic) via multi-branch fusion]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Strong hallucination detection performance and favorable computational efficiency on SQuAD, TriviaQA, Natural Questions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [legal reasoning evaluation], [legal benchmark, knowledge-independent evaluation, precedent-aligned questions, automated rubric evaluation, Korean legal domain]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hongseok Oh, Wonseok Hwang, Kyoung-Woon On"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Seoul, LBOX"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24572",children:"https://arxiv.org/pdf/2512.24572"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/lbox-kr/kcl",children:"https://github.com/lbox-kr/kcl"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Korean Canonical Legal Benchmark (KCL), designed to assess legal reasoning capabilities independently of domain-specific knowledge by providing question-level supporting precedents. 2. Presents a comprehensive benchmark with two components: KCL-MCQA (multiple-choice) and KCL-Essay (open-ended generation), including instance-level rubrics for automated evaluation. 3. Conducts systematic evaluation of 30+ models, revealing significant performance gaps and demonstrating that reasoning-specialized models consistently outperform general-purpose models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/defb63442086b6432fc3b38da8a4cfae8a2c23c830816fe13d1440a09c526554_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/defb63442086b6432fc3b38da8a4cfae8a2c23c830816fe13d1440a09c526554_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the Korean Canonical Legal Benchmark (KCL) to evaluate language models' legal reasoning ability without relying on memorized domain knowledge, by providing aligned precedents for each question. The benchmark includes multiple-choice and essay components with automated rubrics. Evaluation shows large performance gaps, especially in essay tasks, and that reasoning-specialized models outperform general ones."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs\' Legal Reasoning Capabilities"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Assessing legal reasoning independently of domain knowledge is challenging"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Introduce KCL benchmark with precedent-aligned questions and automated rubrics"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Large performance gaps exist; reasoning-specialized models outperform general ones"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [chain-of-thought reasoning, attention heads, test-time intervention, computational efficiency, reasoning steering]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin, Together AI, University of Sydney"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24574",children:"https://arxiv.org/pdf/2512.24574"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/togethercomputer/CREST",children:"https://github.com/togethercomputer/CREST"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identified specialized attention heads in LLMs that correlate with distinct cognitive reasoning behaviors (e.g., verification, backtracking). 2. Proposed CREST, a training-free method for Cognitive REasoning Steering at Test-time, which involves offline calibration to find steering vectors and inference-time rotation to suppress unproductive reasoning. 3. Demonstrated that CREST improves reasoning accuracy and reduces token usage across diverse benchmarks, offering a pathway to faster and more reliable LLM inference."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency and instability of long chain-of-thought reasoning in LLMs, which leads to high latency and alternating underthinking/overthinking. The authors propose CREST, a training-free method that identifies and steers specific attention heads at test-time to suppress unproductive cognitive behaviors. The method improves accuracy by up to 17.5% and reduces token usage by 37.6%, enabling faster and more reliable reasoning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[LLM\u63a8\u7406\u8f68\u8ff9\u4f4e\u6548\u4e14\u4e0d\u7a33\u5b9a/Inefficient & Unstable LLM Reasoning Trajectories]\n    B1 --\x3e B2[\u8fc7\u5ea6\u601d\u8003\u4e0e\u601d\u8003\u4e0d\u8db3/Overthinking & Underthinking]\n    B1 --\x3e B3[\u9ad8\u5ef6\u8fdf\u4e0e\u9ad8\u4ee4\u724c\u6d88\u8017/High Latency & Token Usage]\n    C --\x3e C1[\u8bc6\u522b\u4e0e\u8ba4\u77e5\u884c\u4e3a\u76f8\u5173\u7684\u6ce8\u610f\u529b\u5934/Identify Cognitive Attention Heads]\n    C --\x3e C2[\u63d0\u51faCREST\u65b9\u6cd5: \u6d4b\u8bd5\u65f6\u8ba4\u77e5\u63a8\u7406\u5f15\u5bfc/Propose CREST: Test-time Cognitive REasoning Steering]\n    C2 --\x3e C3[\u79bb\u7ebf\u6821\u51c6\u83b7\u53d6\u5f15\u5bfc\u5411\u91cf/Offline Calibration for Steering Vectors]\n    C2 --\x3e C4[\u63a8\u7406\u65f6\u65cb\u8f6c\u9690\u85cf\u8868\u793a/Inference-time Representation Rotation]\n    D --\x3e D1[\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347/Accuracy Improved Up to 17.5%]\n    D --\x3e D2[\u4ee4\u724c\u4f7f\u7528\u5927\u5e45\u51cf\u5c11/Token Usage Reduced by 37.6%]\n    D --\x3e D3[\u5b9e\u73b0\u66f4\u5feb\u66f4\u53ef\u9760\u7684\u63a8\u7406/Enables Faster, More Reliable Reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Recursive Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [recursive language models, long-context processing, inference-time scaling, context condensation, out-of-core algorithms]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alex L. Zhang, Tim Kraska, Omar Khattab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," MIT CSAIL"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24601",children:"https://arxiv.org/pdf/2512.24601"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Recursive Language Models (RLMs), a general inference strategy that allows LLMs to programmatically examine, decompose, and recursively call themselves over long prompts. 2. Demonstrates that RLMs can handle inputs up to two orders of magnitude beyond standard model context windows. 3. Shows RLMs outperform base LLMs and existing long-context methods across diverse tasks while maintaining comparable or lower cost per query."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b76e3870cfbc09a08f87a31e423d89e08fb4d1e100fa580e4e6b58754309af5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b76e3870cfbc09a08f87a31e423d89e08fb4d1e100fa580e4e6b58754309af5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLMs struggling with arbitrarily long prompts due to limited context windows and context rot. It introduces Recursive Language Models (RLMs), an inference-time method that treats long prompts as an external environment, enabling recursive decomposition and processing. The results show RLMs effectively scale to inputs far beyond standard context limits and outperform baseline approaches in quality and cost."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Recursive Language Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs have limited context lengths and suffer from context rot with long prompts]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Recursive Language Models (RLMs) treat long prompts as an external environment, allowing programmatic examination and recursive self-calls]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: RLMs handle inputs up to 100x beyond context windows, outperform base LLMs and long-context scaffolds, with comparable or cheaper cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [Multi-Latent Attention (MLA), STEM-oriented vocabulary, Commonsense-STEM-Agent curriculum, agentic mid-training, long-context reasoning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, Yunsheng Wu, Yinsong Liu, Shuangyin Liu, Mingkong Tang, Haodong Lin, Jiayi Kuang, Fanxu Meng, Xiaojuan Tang, Yunjia Xi, Junjie Huang, Haotong Yang, Zhenyi Shen, Yangning Li, Qianwen Zhang, Yifei Yu, Siyu An, Junnan Dong, Qiufeng Wang, Jie Wang, Keyu Chen, Wei Wen, Taian Guo, Zhifeng Shen, Daohai Yu, Jiahao Li, Ke Li, Zongyi Li, Xiaoyu Tan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' Tencent (inferred from "Youtu-LLM Team", code repository URL, and Hugging Face collection)']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24618",children:"https://arxiv.org/pdf/2512.24618"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/TencentCloudADP/youtu-tip/youtu-llm",children:"https://github.com/TencentCloudADP/youtu-tip/youtu-llm"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces a compact Multi-Latent Attention (MLA) architecture with a STEM-oriented vocabulary to support 128k context windows for efficient long-context reasoning. 2. Proposes a principled "Commonsense-STEM-Agent" multi-stage pre-training curriculum using an 11T token corpus to cultivate deep cognitive abilities. 3. Develops scalable agentic mid-training with diverse synthetic trajectories for math, coding, and tool-use to internalize planning and reflection behaviors.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f514e6cca418a1474a2cb6b11d79c745aa9a14b34387f03f4838426b14afaf3f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f514e6cca418a1474a2cb6b11d79c745aa9a14b34387f03f4838426b14afaf3f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Youtu-LLM, a 1.96B parameter lightweight language model pre-trained from scratch with a novel architecture and a staged curriculum focusing on commonsense, STEM, and agentic data. This approach enables the model to achieve state-of-the-art performance for sub-2B models, demonstrating strong native agentic capabilities and competitive general performance against larger models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8f7b\u91cf\u6a21\u578b\u7f3a\u4e4f\u539f\u751f\u667a\u80fd\u4f53\u80fd\u529b/Lightweight models lack native agentic capabilities]\n    C --\x3e C1[\u7d27\u51d1\u67b6\u6784\u4e0e\u957f\u4e0a\u4e0b\u6587\u652f\u6301/Compact Architecture with Long-Context Support]\n    C --\x3e C2[\u5206\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b/Principled Multi-Stage Curriculum]\n    C --\x3e C3[\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u4e2d\u671f\u8bad\u7ec3/Scalable Agentic Mid-training]\n    C1 --\x3e C1a[\u591a\u6f5c\u5728\u6ce8\u610f\u529b (MLA) / Multi-Latent Attention (MLA)]\n    C1 --\x3e C1b[STEM\u5bfc\u5411\u8bcd\u8868/STEM-oriented Vocabulary]\n    C2 --\x3e C2a[\u5e38\u8bc6-STEM-\u667a\u80fd\u4f53\u8bfe\u7a0b/Commonsense-STEM-Agent Curriculum]\n    C2 --\x3e C2b[11T \u4ee4\u724c\u6570\u636e/11T Token Corpus]\n    C3 --\x3e C3a[\u591a\u6837\u5316\u8f68\u8ff9\u5408\u6210/Diverse Trajectory Synthesis]\n    C3 --\x3e C3b[\u6570\u5b66\u3001\u7f16\u7801\u3001\u5de5\u5177\u4f7f\u7528/Math, Coding, Tool-use]\n    D --\x3e D1[\u5728 sub-2B \u6a21\u578b\u4e2d\u8fbe\u5230 SOTA/Achieves SOTA for sub-2B LLMs]\n    D --\x3e D2[\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf/Significantly surpasses baselines on agent tasks]\n    D --\x3e D3[\u5c55\u793a\u8f7b\u91cf\u6a21\u578b\u7684\u5f3a\u5185\u5728\u80fd\u529b/Demonstrates strong intrinsic agentic capabilities in lightweight models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Do Large Language Models Know What They Are Capable Of?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [in-advance confidence, overconfidence, capability awareness, decision-making, agentic tasks]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Casey O. Barkan, Sid Black, Oliver Sourbut"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," RAND Corporation, UK AI Security Institute, The Future of Life Foundation"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24661",children:"https://arxiv.org/pdf/2512.24661"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Evaluates LLMs' in-advance confidence and its impact on decision-making in costly-failure scenarios, a less studied area compared to after-the-fact calibration. 2. Investigates how LLMs' confidence and overconfidence evolve during multi-step agentic tasks and with in-context failure experiences. 3. Demonstrates that while LLMs' decisions are rational given their self-estimates, their systematic overconfidence leads to poor task pursuit decisions, highlighting a lack of capability awareness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c758274cef27c9ad79258c27a063b8420cef801a9f25ff7610a7c4c12509a926_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c758274cef27c9ad79258c27a063b8420cef801a9f25ff7610a7c4c12509a926_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether large language models (LLMs) can accurately predict their own success on tasks, especially when failure is costly. It evaluates their in-advance confidence, how it changes during multi-step tasks and with in-context failure, and its impact on decision-making. The main finding is that current LLMs are generally overconfident, which impairs their decision-making despite rational behavior based on their flawed self-assessments, indicating a lack of self-awareness of their capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Do Large Language Models Know What They Are Capable Of?] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u80fd\u5426\u9884\u6d4b\u81ea\u8eab\u4efb\u52a1\u6210\u529f\u7387?<br/>Can LLMs predict their task success?]\n    B --\x3e B2[\u5931\u8d25\u6210\u672c\u9ad8\u65f6\u5982\u4f55\u51b3\u7b56?<br/>How to decide when failure is costly?]\n    C --\x3e C1[\u8bc4\u4f30\u4e8b\u524d\u7f6e\u4fe1\u5ea6<br/>Evaluate in-advance confidence]\n    C --\x3e C2[\u5206\u6790\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u4fe1\u5fc3\u53d8\u5316<br/>Analyze confidence change in multi-step tasks]\n    C --\x3e C3[\u7814\u7a76\u4e0a\u4e0b\u6587\u5931\u8d25\u7ecf\u9a8c\u7684\u5f71\u54cd<br/>Study impact of in-context failure]\n    D --\x3e D1[LLMs\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1<br/>LLMs are generally overconfident]\n    D --\x3e D2[\u65b0/\u5927\u6a21\u578b\u5224\u522b\u529b\u672a\u663e\u8457\u63d0\u5347<br/>Newer/larger models don't have greater discriminatory power]\n    D --\x3e D3[\u90e8\u5206\u6a21\u578b\u80fd\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60<br/>Some models learn from failure]\n    D --\x3e D4[\u51b3\u7b56\u7406\u6027\u4f46\u4f30\u8ba1\u8fc7\u4e8e\u4e50\u89c2<br/>Decisions rational but estimates overly optimistic]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [computational argumentation], [retrieval-augmented generation, argumentative memory, multi-turn debate, agentic framework, rhetorical grounding]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Auckland, Wuhan College of Communication"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24684",children:"https://arxiv.org/pdf/2512.24684"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://anonymous.4open.science/r/R-debater-E87F/",children:"https://anonymous.4open.science/r/R-debater-E87F/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes R-Debater, a novel agentic framework for multi-turn debate generation grounded in the concept of "argumentative memory" from rhetoric and memory studies. 2. Integrates a debate knowledge base for retrieving evidence and prior arguments with a role-based agent to ensure stance consistency and coherent multi-turn composition. 3. Demonstrates superior performance over strong LLM baselines in both single-turn and multi-turn debate tasks through automated metrics (InspireScore, Debatrix) and human evaluation with experienced debaters.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d8c7f9f1e27feb6f7eff24aacab0f5dd1eb3b83c76e27f254c97c97b4c0b3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d8c7f9f1e27feb6f7eff24aacab0f5dd1eb3b83c76e27f254c97c97b4c0b3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper presents R-Debater, a framework that generates multi-turn debates by retrieving and adapting arguments from a knowledge base ("argumentative memory") using a role-based agent. Evaluated on ORCHID debates, it outperforms LLM baselines in producing more consistent, evidence-grounded, and coherent debates across turns.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[R-Debater: Retrieval-Augmented Debate Generation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs generate fluent but shallow, ungrounded debates with weak stance fidelity]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Agentic framework with argumentative memory for retrieval & role-based utterance composition]\n    D[\u5173\u952e\u7ed3\u679c/Results: Higher scores than LLM baselines; more faithful, stance-aligned, coherent debates]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [reward modeling], [multi-turn reward model, data augmentation, instruction contrast, preference learning, conversational AI]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenzhe Li, Shujian Zhang, Wenxuan Zhou, John Lambert, Chi Jin, Andrew Hard, Rajiv Mathews, Lun Wang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, Google DeepMind"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24693",children:"https://arxiv.org/pdf/2512.24693"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Identifies a key limitation in standard preference datasets for training multi-turn reward models, noting their focus on single-turn contrasts is insufficient. 2. Proposes MUSIC, an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs with differences spanning multiple turns. 3. Demonstrates empirically that a reward model trained with MUSIC-augmented data outperforms baselines on multi-turn evaluation and maintains performance on single-turn benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e585f0fe2b96110399073cec36c18e1459439f39fdd0ff201f3bbb2378ee4ed7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e585f0fe2b96110399073cec36c18e1459439f39fdd0ff201f3bbb2378ee4ed7_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of evaluating multi-turn conversations by proposing MUSIC, an unsupervised data augmentation method for training multi-turn reward models. MUSIC creates contrastive examples with differences across multiple turns, which provides a stronger training signal. The resulting reward model shows improved alignment with advanced LLM judges on multi-turn dialogues without sacrificing single-turn performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Evaluating multi-turn conversations is challenging and costly; standard single-turn contrast datasets are insufficient."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Proposes MUSIC, an unsupervised data augmentation strategy to synthesize multi-turn contrastive pairs."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: MUSIC-augmented RM outperforms baselines on multi-turn evaluation and maintains single-turn performance."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [biomedical text mining], [multi-omics, pathway enrichment, large language models, benchmark, biomolecular interaction]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sibo Wei, Peng Chen, Lifeng Dong, Yin Luo, Lei Wang, Peng Zhang, Wenpeng Lu, Jianbin Guo, Hongjun Yang, Dajun Zeng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Wenge Technology Co., Ltd, China Academy of Chinese Medical Sciences, Institute of Automation, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24733",children:"https://arxiv.org/pdf/2512.24733"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/DYJG-research/BIOME-Bench/",children:"https://github.com/DYJG-research/BIOME-Bench/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BIOME-Bench, a novel benchmark for evaluating LLMs on biomolecular interaction inference and multi-omics pathway mechanism elucidation., 2. Proposes a rigorous four-stage workflow for constructing the benchmark and develops specific evaluation protocols for the two core tasks., 3. Conducts comprehensive experiments showing current LLMs' substantial deficiencies in fine-grained relation distinction and faithful mechanistic explanation generation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfe5bd2d9fd13eb61c8aa2f4136960e60693b10732d1462bb1b83a8304eb2997_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfe5bd2d9fd13eb61c8aa2f4136960e60693b10732d1462bb1b83a8304eb2997_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces BIOME-Bench, a benchmark designed to evaluate large language models on two key tasks in multi-omics analysis: inferring biomolecular interactions and elucidating pathway mechanisms from scientific literature. The benchmark is constructed via a rigorous workflow and includes specific evaluation protocols. The authors' experiments reveal that current state-of-the-art models still struggle significantly with these tasks, highlighting a need for improvement."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation]\n    Root --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30/Lack of standardized benchmark]\n    Problem --\x3e P2[\u8bc4\u4f30\u5c40\u9650\u4e8e\u5c0f\u6570\u636e\u96c6/Evaluation confined to small datasets]\n    Method --\x3e M1[\u6784\u5efaBIOME-Bench/Build BIOME-Bench]\n    Method --\x3e M2[\u56db\u9636\u6bb5\u5de5\u4f5c\u6d41/Four-stage workflow]\n    Method --\x3e M3[\u8bc4\u4f30\u4e24\u4e2a\u6838\u5fc3\u4efb\u52a1/Evaluate two core tasks]\n    Results --\x3e R1[\u6a21\u578b\u5b58\u5728\u663e\u8457\u7f3a\u9677/Models exhibit substantial deficiencies]\n    Results --\x3e R2[\u96be\u4ee5\u533a\u5206\u7ec6\u7c92\u5ea6\u5173\u7cfb/Struggle with fine-grained relations]\n    Results --\x3e R3[\u96be\u4ee5\u751f\u6210\u53ef\u9760\u89e3\u91ca/Struggle to generate faithful explanations]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Pareto frontier, test-time compute, Mixture of Experts (MoE), Chain-of-Thought (CoT), reasoning benchmarks]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," \xc1kos Prucs, M\xe1rton Csutora, M\xe1ty\xe1s Antal, M\xe1rk Marosi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," E-Group Research, Budapest University of Technology and Economics"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24776",children:"https://arxiv.org/pdf/2512.24776"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a test-time-compute aware evaluation of open-source LLMs, mapping their Pareto frontiers on reasoning benchmarks. 2. Identified Mixture of Experts (MoE) architecture as a strong candidate for balancing performance and efficiency. 3. Demonstrated a saturation point for inference-time compute, beyond which accuracy gains diminish."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d5a9f81e306448bd700214cb119ef785bd34ba4a396aa5b35650cc485b74caa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d5a9f81e306448bd700214cb119ef785bd34ba4a396aa5b35650cc485b74caa_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the overlooked computational cost of generating long reasoning sequences in LLMs. It evaluates open-source LLMs by mapping their compute-accuracy Pareto frontiers and finds that Mixture of Experts models offer a good efficiency-performance trade-off, while also identifying a saturation point for inference-time compute."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Overlooked computational burden of long reasoning sequences in LLMs"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Test-time-compute aware evaluation & Pareto frontier mapping"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: MoE balances efficiency; Compute saturation point exists"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Uncertainty-aware Semi-supervised Ensemble Teacher Framework for Multilingual Depression Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [mental health detection], [semi-supervised learning, ensemble learning, pseudo-labeling, multilingual, uncertainty estimation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohammad Zia Ur Rehman, Velpuru Navya, Sanskar, Shuja Uddin Qureshi, Nagendra Kumar"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Indore, Shri Govindram Seksaria Institute of Technology and Science, Indore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24772",children:"https://arxiv.org/pdf/2512.24772"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Semi-SMDNet, a semi-supervised framework combining teacher-student pseudo-labeling, ensemble learning, and data augmentation for multilingual depression detection. 2. Introduced an uncertainty-based threshold to filter low-confidence pseudo-labels and a confidence-weighted training method to focus on reliable samples, improving robustness and stability. 3. Demonstrated the framework's effectiveness across Arabic, Bangla, English, and Spanish datasets, significantly reducing the performance gap between high-resource and low-resource settings."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64615e1e795d40f008be7ac94e30d5a04c99243dcb00282015a4513c137ecd78_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64615e1e795d40f008be7ac94e30d5a04c99243dcb00282015a4513c137ecd78_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of detecting depression from multilingual social media text where annotated data is scarce. It proposes a semi-supervised ensemble teacher framework (Semi-SMDNet) that uses uncertainty-aware pseudo-labeling and confidence-weighted training to improve robustness. Experiments on four language datasets show the method consistently outperforms baselines and is suitable for scalable, cross-language mental health monitoring."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Uncertainty-aware Semi-supervised Ensemble Teacher Framework for Multilingual Depression Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u7684\u6291\u90c1\u75c7\u68c0\u6d4b/Depression detection in multilingual social media text]\n    B --\x3e B2[\u6807\u6ce8\u6570\u636e\u7a00\u7f3a/Lack of annotated data]\n    B --\x3e B3[\u8bed\u8a00\u98ce\u683c\u591a\u6837\u4e0e\u8868\u8fbe\u975e\u6b63\u5f0f/Diverse language styles & informal expression]\n    C --\x3e C1[\u534a\u76d1\u7763\u96c6\u6210\u6559\u5e08\u6846\u67b6/Semi-supervised ensemble teacher framework (Semi-SMDNet)]\n    C --\x3e C2[\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f2a\u6807\u7b7e/Uncertainty-aware pseudo-labeling]\n    C --\x3e C3[\u7f6e\u4fe1\u5ea6\u52a0\u6743\u8bad\u7ec3/Confidence-weighted training]\n    D --\x3e D1[\u5728\u963f\u62c9\u4f2f\u8bed\u3001\u5b5f\u52a0\u62c9\u8bed\u3001\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baselines on Arabic, Bangla, English, Spanish datasets]\n    D --\x3e D2[\u51cf\u5c11\u9ad8\u4f4e\u8d44\u6e90\u573a\u666f\u6027\u80fd\u5dee\u8ddd/Reduces performance gap between high- and low-resource settings]\n    D --\x3e D3[\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u8de8\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b/Suitable for scalable cross-language mental health monitoring]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Practising responsibility: Ethics in NLP as a hands-on course"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [ethics in nlp], [ethics education, active learning, curriculum development, hands-on activities, learning by teaching]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Malvina Nissim, Viviana Patti, Beatrice Savoldi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Groningen, University of Turin, Fondazione Bruno Kessler"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24825",children:"https://arxiv.org/pdf/2512.24825"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduction of a dedicated course "Ethical Aspects in NLP" designed to integrate ethics into NLP education, 2. Development of a pedagogical approach based on active learning, interactive sessions, hands-on activities, and "learning by teaching" methods, 3. Creation and refinement of the course over four years, adapting it across different institutions, educational levels, and interdisciplinary backgrounds, yielding reusable teaching materials and student-made educational products.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad8d866d1d641ab747e97be881ac0eec09fdae4762445938b33c32d5a452c3e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad8d866d1d641ab747e97be881ac0eec09fdae4762445938b33c32d5a452c3e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the challenge of integrating ethical considerations into NLP education by proposing a hands-on course. The method employs active learning through interactive sessions, practical activities, and "learning by teaching". The main conclusion is that this approach successfully fosters critical thinking and produces reusable educational resources, providing a model for educators to incorporate social impact into curricula.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Practising responsibility: Ethics in NLP as a hands-on course") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("NLP\u7cfb\u7edf\u666e\u53ca/NLP systems pervasive")\n    Problem --\x3e P2("\u4f26\u7406\u6559\u80b2\u6311\u6218/Ethics education challenges")\n    Method --\x3e M1("\u4e3b\u52a8\u5b66\u4e60/Active learning")\n    Method --\x3e M2("\u5b9e\u8df5\u4e0e\u4e92\u52a8/Hands-on & interactive")\n    Method --\x3e M3("\u4ee5\u6559\u4fc3\u5b66/Learning by teaching")\n    Results --\x3e R1("\u8bfe\u7a0b\u4f18\u5316\u4e0e\u9002\u5e94/Course refined & adapted")\n    Results --\x3e R2("\u4ea7\u51fa\u53ef\u590d\u7528\u4ea7\u54c1/Reusable products created")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [mechanistic interpretability], [causal abstraction, interchange intervention, automatic circuit discovery, invariance, multilingual models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yanan Long"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," StickFlux Labs, University of Chicago"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24842",children:"https://arxiv.org/pdf/2512.24842"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes triangulation, a formal acceptance rule for mechanistic explanations requiring necessity, sufficiency, and invariance across reference families. 2. Grounds the triangulation rule in causal abstraction theory by framing it as an approximate transformation score over interchange interventions. 3. Introduces a comparative experimental protocol to evaluate mechanistic claims across multiple model families, language pairs, and tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c4f8b0a487ba28d3349ff7d744b0fdbcdbab78c52f6689b7eba6cbeae2812a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c4f8b0a487ba28d3349ff7d744b0fdbcdbab78c52f6689b7eba6cbeae2812a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper proposes "triangulation," a causal standard for evaluating mechanistic explanations in multilingual language models. It requires a proposed circuit to be necessary, sufficient, and invariant across meaning-preserving variations like different languages. This method filters out spurious circuits that pass single-environment tests but fail under cross-lingual scrutiny.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Multilingual models behave unpredictably; need causal, cross-referenced explanations]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Triangulation acceptance rule (necessity, sufficiency, invariance) via circuit discovery & interchange interventions]\n    D[\u5173\u952e\u7ed3\u679c/Results: Provides falsifiable standard; filters spurious circuits failing cross-lingual invariance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sec], [Privacy protections], [PrivacyBench, Retrieval-Augmented Generation (RAG), secret leakage, privacy-aware prompt, privacy-by-design]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Srija Mukhopadhyay, Sathwik Reddy, Shruthi Muthukumar, Jisun An, Ponnurangam Kumaraguru"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," International Institute of Information Technology Hyderabad, Indiana University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24848",children:"https://arxiv.org/pdf/2512.24848"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces PrivacyBench, a novel conversational benchmark with socially grounded datasets containing embedded secrets for evaluating privacy in AI assistants. 2. Provides a multi-turn conversational evaluation framework to measure secret preservation capabilities of personalized AI systems. 3. Empirically demonstrates that current RAG-based assistants leak secrets in up to 26.56% of interactions and identifies a critical architectural flaw where the retrieval mechanism is a single point of failure for privacy."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7685e487e221610623a5fd5b2084368051b6218a6804a6749e97cc93aef26acb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7685e487e221610623a5fd5b2084368051b6218a6804a6749e97cc93aef26acb_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces PrivacyBench, a benchmark to evaluate privacy leakage in personalized AI agents that access a user's digital footprint. Testing shows RAG-based assistants leak secrets frequently, and while privacy-aware prompts help, the fundamental architecture is unsafe, highlighting the need for structural, privacy-by-design solutions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Personalized AI agents risk exposing sensitive user data from their digital footprint.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce PrivacyBench benchmark with datasets containing secrets and multi-turn conversational evaluation.]\n    D[\u5173\u952e\u7ed3\u679c/Results: RAG assistants leak secrets; privacy prompts partially mitigate; need for privacy-by-design safeguards.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Big AI is accelerating the metacrisis: What can we do?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [ethics & society], [metacrisis, language engineers, human flourishing, planetary boundaries, technofeudalism]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Steven Bird"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Charles Darwin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24863",children:"https://arxiv.org/pdf/2512.24863"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and critiques the role of "Big AI" and language engineers in accelerating converging global crises (ecological, meaning, language). 2. Highlights the ethical conflict between professional obligations (e.g., ACL Code of Ethics) and the harms caused by current NLP/AI development practices. 3. Proposes a paradigm shift for NLP, advocating for a future centered on human flourishing and amplifying social networks rather than scaling through large, polluting models.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper argues that the current trajectory of "Big AI," particularly in NLP, is accelerating a global metacrisis. It critiques the field\'s focus on scalability and value-neutral technology development, which benefits powerful interests at the expense of the public good and the planet. The paper concludes by urgently calling for an alternative, life-affirming future for NLP centered on human flourishing.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Big AI is accelerating the metacrisis: What can we do?] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Big AI\u52a0\u901f\u751f\u6001\u3001\u610f\u4e49\u548c\u8bed\u8a00\u5371\u673a/Big AI accelerates ecological, meaning, and language crises]\n    B --\x3e B2[\u8bed\u8a00\u5de5\u7a0b\u5e08\u7684\u4f26\u7406\u56f0\u5883/Ethical dilemma of language engineers]\n    C --\x3e C1[\u6279\u5224\u5f53\u524d\u53ef\u6269\u5c55\u6027\u53d9\u4e8b/Critique current scalability narrative]\n    C --\x3e C2[\u547c\u5401\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848/Call to explore alternatives]\n    D --\x3e D1[\u9700\u8981\u4ee5\u4eba\u7c7b\u7e41\u8363\u4e3a\u4e2d\u5fc3\u7684\u672a\u6765/NLP future must center human flourishing]\n    D --\x3e D2[\u5229\u7528\u96c6\u4f53\u667a\u6167\u8bbe\u8ba1\u751f\u547d\u80af\u5b9a\u7684NLP/Design life-affirming NLP with collective intelligence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [benchmark, knowledge statements, dynamic composition, data contamination, multi-knowledge assessment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiming Liang, Yizhi Li, Yantao Du, Ge Zhang, Jiayi Zhou, Yuchen Wu, Yinzhu Piao, Denghui Cao, Tong Sun, Ziniu Li, Li Du, Bo Lei, Jiaheng Liu, Chenghua Lin, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Chinese Academy of Sciences, Chinese Academy of Sciences, Bytedance, Nanjing University, M-A-P, BAAI, The University of Manchester"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24867",children:"https://arxiv.org/pdf/2512.24867"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://encyclo-k.github.io",children:"https://encyclo-k.github.io"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel statement-based benchmark (Encyclo-K) that uses knowledge statements as the fundamental curation unit instead of pre-defined questions., 2. Introduces a dynamic evaluation method where questions are composed by randomly sampling multiple statements at test time, mitigating data contamination and enabling periodic refresh., 3. Demonstrates a scalable, low-cost annotation process that requires only formatting verification, not domain expertise, while enabling comprehensive multi-knowledge assessment per question."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf5e4785262ae916ad666afc0e0e212833641ae7597f48f69f148b35cfc4807b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf5e4785262ae916ad666afc0e0e212833641ae7597f48f69f148b35cfc4807b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Encyclo-K, a new benchmark for evaluating LLMs that constructs questions by dynamically combining multiple knowledge statements extracted from textbooks at test time. This approach addresses key limitations of existing benchmarks, such as data contamination and single-point assessment. Experiments show it poses a significant challenge to state-of-the-art models, with top accuracy at only 62.07%, validating its effectiveness for assessing comprehensive, multi-statement understanding."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027/Limitations of Existing Benchmarks]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8e\u77e5\u8bc6\u9648\u8ff0\u7684\u52a8\u6001\u7ec4\u5408/Dynamic Composition from Knowledge Statements]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5f3a\u533a\u5206\u6027\uff0c\u6a21\u578b\u8868\u73b0\u68af\u5ea6\u5206\u5e03/Strong Discriminative Power, Gradient Performance]\n    B --\x3e B1[\u6570\u636e\u6c61\u67d3/Data Contamination]\n    B --\x3e B2[\u5355\u77e5\u8bc6\u70b9\u8bc4\u4f30/Single-Knowledge Assessment]\n    B --\x3e B3[\u9ad8\u6807\u6ce8\u6210\u672c/High Annotation Cost]\n    C --\x3e C1[\u4ece\u6743\u5a01\u6559\u6750\u63d0\u53d6\u9648\u8ff0/Extract Statements from Textbooks]\n    C --\x3e C2[\u6d4b\u8bd5\u65f6\u968f\u673a\u7ec4\u5408/Compose Questions at Test Time]\n    D --\x3e D1[GPT-5.1\u51c6\u786e\u738762.07%/GPT-5.1 Accuracy 62.07%]\n    D --\x3e D2[\u63a8\u7406\u6a21\u578b16.04%-62.07%/Reasoning Models 16.04%-62.07%]\n    D --\x3e D3[\u804a\u5929\u6a21\u578b9.71%-50.40%/Chat Models 9.71%-50.40%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Agentic Learning Ecosystem (ALE), Interaction-based Policy Alignment (IPA), Terminal Bench Pro, post-training, trajectory generation]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," ROCK & ROLL & IFLOW & DT Joint Team (Inferred from the author list and likely represents a collaboration, but no specific university or company is named. The domain is unclear from the provided text.)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24873",children:"https://arxiv.org/pdf/2512.24873"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Agentic Learning Ecosystem (ALE), an end-to-end infrastructure for developing agent LLMs, comprising the ROLL post-training framework, the ROCK sandbox manager, and the iFlow CLI agent framework. 2. Proposes a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks to improve long-horizon training stability. 3. Releases the ROME agent model, trained on over one million trajectories, and introduces the Terminal Bench Pro benchmark with improved scale and contamination control for rigorous evaluation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408c7a8de1b2fe7261d28a9d7eb1d3df3afe81fc17a03cc9b2c09e332ac8782_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408c7a8de1b2fe7261d28a9d7eb1d3df3afe81fc17a03cc9b2c09e332ac8782_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of a principled, end-to-end ecosystem for developing agentic LLMs by introducing the Agentic Learning Ecosystem (ALE). ALE streamlines the agent development pipeline, and the authors use it to build and release the ROME agent model, which demonstrates strong performance on benchmarks, validating the effectiveness of their infrastructure."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Let It Flow: Agentic Crafting on Rock and Roll<br/>\u6784\u5efaROME\u6a21\u578b\u4e8e\u5f00\u653e\u667a\u80fd\u4f53\u5b66\u4e60\u751f\u6001\u4e2d] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Open-source community lacks a principled, end-to-end ecosystem for agent LLM development.] --\x3e B1[\u963b\u788d/Block<br/>Hinders practical development and production adoption of agents.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Introduce Agentic Learning Ecosystem (ALE)] --\x3e C1[\u7ec4\u4ef6/Components<br/>ROLL (post-training), ROCK (sandbox), iFlow CLI (agent framework)]\n    C --\x3e C2[\u6a21\u578b/Model<br/>Release ROME agent trained on 1M+ trajectories]\n    C --\x3e C3[\u7b97\u6cd5/Algorithm<br/>Propose Interaction-based Policy Alignment (IPA)]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>ROME achieves strong benchmark performance.] --\x3e D1[\u57fa\u51c6/Benchmarks<br/>24.72% on Terminal-Bench 2.0, 57.40% on SWE-bench Verified]\n    D --\x3e D2[\u65b0\u57fa\u51c6/New Benchmark<br/>Introduce Terminal Bench Pro for rigorous evaluation.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] mHC: Manifold-Constrained Hyper-Connections"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Hyper-Connections, residual connection, identity mapping, manifold constraint, training stability]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," DeepSeek-AI"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24880",children:"https://arxiv.org/pdf/2512.24880"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Manifold-Constrained Hyper-Connections (mHC), a framework that projects the residual connection space onto a specific manifold to restore the identity mapping property compromised by Hyper-Connections (HC). 2. Incorporates rigorous infrastructure optimization to address the memory access overhead and ensure training efficiency. 3. Demonstrates that mHC enables effective large-scale training with tangible performance improvements and superior scalability, offering a flexible and practical extension of HC."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that Hyper-Connections (HC), while improving performance, lose the identity mapping property of standard residual connections, leading to training instability and memory overhead. To solve this, the authors propose Manifold-Constrained Hyper-Connections (mHC), which projects HC's connection space onto a manifold to restore identity mapping and includes infrastructure optimizations. Empirical results show mHC is effective for scalable training, offering better performance and stability."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    A[mHC: Manifold-Constrained Hyper-Connections] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["HC \u7834\u574f\u4e86\u6052\u7b49\u6620\u5c04\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a/HC compromises identity mapping, causing instability"]\n    B --\x3e B2["HC \u5e26\u6765\u5185\u5b58\u5f00\u9500/HC incurs memory overhead"]\n    C --\x3e C1["\u5c06\u6b8b\u5dee\u8fde\u63a5\u7a7a\u95f4\u6295\u5f71\u5230\u7279\u5b9a\u6d41\u5f62/Project residual space onto a manifold"]\n    C --\x3e C2["\u6062\u590d\u6052\u7b49\u6620\u5c04\u5c5e\u6027/Restore identity mapping property"]\n    C --\x3e C3["\u7ed3\u5408\u57fa\u7840\u8bbe\u65bd\u4f18\u5316/Incorporate infrastructure optimization"]\n    D --\x3e D1["\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6709\u6548\u8bad\u7ec3/Enables effective training at scale"]\n    D --\x3e D2["\u63d0\u4f9b\u6027\u80fd\u6539\u8fdb\u548c\u53ef\u6269\u5c55\u6027/Offers performance improvements & scalability"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [strategic dialogue], [belief estimation, probabilistic constraints, dialogue acts, adversarial, alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hengli Li, Zhaoxin Yu, Qi Shen, Chenxi Li, Mengmeng Wang, Tinglang Wu, Yipeng Kang, Yuxuan Wang, Song-Chun Zhu, Zixia Jia, Zilong Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University (PKU), Beijing Institute for General Artificial Intelligence (BIGAI), Chinese Academy of Sciences (CAS), Beijing University of Posts and Telecommunications (BUPT), Tsinghua University (THU)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24885",children:"https://arxiv.org/pdf/2512.24885"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes two core strategic dialogue acts (Adversarial and Alignment) within a game-theoretic framework of beliefs and common knowledge. 2. Proposes a principled mechanism that operationalizes these acts by casting belief estimation as probabilistic constraints on utterance generation. 3. Introduces the BEDA framework, which integrates a world set, a belief estimator, and a conditional generator to select acts and realize utterances consistent with inferred beliefs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f74e9bdf6a51c0f1ab61ad8d67cba11cf929a7b681e1d4c3a1c8682bf34bb3fe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f74e9bdf6a51c0f1ab61ad8d67cba11cf929a7b681e1d4c3a1c8682bf34bb3fe_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap between accurate belief estimation and its principled use in strategic dialogue generation. It proposes BEDA, a framework that formalizes adversarial and alignment dialogue acts and uses belief estimates as probabilistic constraints to guide utterance generation. The method consistently outperforms strong baselines across adversarial, cooperative, and negotiation settings, demonstrating its effectiveness."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("BEDA: Belief Estimation as Probabilistic Constraints") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Prior work lacks principled use of beliefs for generation/\u5148\u524d\u5de5\u4f5c\u7f3a\u4e4f\u4f7f\u7528\u4fe1\u5ff5\u7684\u673a\u5236")\n    Method --\x3e M1("Formalizes Adversarial & Alignment acts/\u5f62\u5f0f\u5316\u5bf9\u6297\u4e0e\u5bf9\u9f50\u884c\u4e3a")\n    Method --\x3e M2("Beliefs as probabilistic constraints/\u4fe1\u5ff5\u4f5c\u4e3a\u6982\u7387\u7ea6\u675f")\n    Method --\x3e M3("BEDA framework: belief estimator + conditional generator/BEDA\u6846\u67b6")\n    Results --\x3e R1("Outperforms baselines on CKBG, MF, CaSiNo/\u5728\u591a\u4e2a\u8bbe\u5b9a\u8d85\u8d8a\u57fa\u7ebf")\n    Results --\x3e R2("Improves success rates significantly/\u663e\u8457\u63d0\u5347\u6210\u529f\u7387")\n    Results --\x3e R3("Provides simple, general mechanism/\u63d0\u4f9b\u7b80\u5355\u901a\u7528\u673a\u5236")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Vibe Coding, Interface Flattening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [se], [human-computer interaction], [large language models, interface flattening, vibe coding, Model Context Protocol, symbolic labour]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hongrui Jin"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Amsterdam"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24939",children:"https://arxiv.org/pdf/2512.24939"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a critical framework for understanding "vibe coding" as "interface flattening," where distinct modalities converge into a conversational surface while the underlying translation chain thickens. 2. Conducts a materialist reconstruction of the vibe-coding stack, analyzing how remote compute, structured outputs, and protocols like MCP relocate control to model providers. 3. Demonstrates how LLM-mediated development redistributes symbolic power, obscures responsibility, and privatizes competencies, offering a critical lens on the political economy of AI-mediated interaction.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d80b187b5915d1a8220a2cb617bff9a959789d5181355291a3d22991715d25_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d80b187b5915d1a8220a2cb617bff9a959789d5181355291a3d22991715d25_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper analyzes the phenomenon of "vibe coding," where software is developed through natural language interaction with LLMs. It conceptualizes this as "interface flattening," arguing that while the user experience appears simplified, the underlying infrastructure and dependencies become more complex and concentrated. The main conclusion is that this apparent democratization of programming creates new dependencies, redistributes power to model providers, and privatizes competencies previously held by the programming community.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Vibe Coding, Interface Flattening] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to understand the shift in programming via LLMs?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Critical analysis using media theory & materialist reconstruction of the stack]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Interface flattening obscures complexity, redistributes power, creates new dependencies]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [prompt optimization, multi-step LLM pipeline, Shapley value, text-gradient estimation, dependency modeling]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huawei Poisson Lab"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24933",children:"https://arxiv.org/pdf/2512.24933"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ADOPT, a framework that explicitly models the dependency between each LLM step and the final task outcome for precise text-gradient estimation. 2. Decouples textual gradient estimation from gradient updates, reducing complex multi-prompt optimization to flexible single-prompt optimization steps. 3. Employs a Shapley-based mechanism to adaptively allocate optimization resources across different pipeline steps."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0120693b7eaf05e640c60779fef913238cda72212cee4971942ac26a248c12d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0120693b7eaf05e640c60779fef913238cda72212cee4971942ac26a248c12d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of jointly optimizing prompts in multi-step LLM pipelines, where missing step-level supervision and inter-step dependencies make optimization difficult. It proposes ADOPT, an Adaptive Dependency-aware Prompt Optimization framework that models step dependencies for precise gradient estimation and uses a Shapley-based resource allocation mechanism. Experiments show ADOPT is effective and robust, consistently outperforming existing prompt optimization methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    Root[ADOPT: Adaptive Dependency-aware Prompt Optimization Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Multi-step LLM pipeline prompt optimization is difficult due to missing supervision and dependencies] --\x3e P1[\u5b50\u95ee\u9898/Sub-problem: Missing step-level supervision]\n    Problem --\x3e P2[\u5b50\u95ee\u9898/Sub-problem: Inter-step dependencies]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: ADOPT Framework] --\x3e M1[\u5173\u952e\u6280\u672f/Key Technique: Explicit dependency modeling for text-gradient estimation]\n    Method --\x3e M2[\u5173\u952e\u6280\u672f/Key Technique: Decouples gradient estimation from updates]\n    Method --\x3e M3[\u5173\u952e\u6280\u672f/Key Technique: Shapley-based adaptive resource allocation]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Effective and robust, outperforms SOTA baselines] --\x3e R1[\u5b9e\u9a8c/Experiments: Real-world datasets]\n    Results --\x3e R2[\u5b9e\u9a8c/Experiments: Diverse pipeline structures]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Iterative Deployment Improves Planning Skills in LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [iterative deployment, implicit reward, data curation, planning, fine-tuning]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Augusto B. Corr\xeaa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, Andr\xe9 G. Pereira, Yarin Gal"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Oxford, AI Sequrity Company, UFRGS"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24940",children:"https://arxiv.org/pdf/2512.24940"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that iterative deployment and fine-tuning on curated user data significantly improves LLM planning skills, including emergent generalization to longer plans. 2. Provides a theoretical analysis showing iterative deployment effectively implements an outer-loop reinforcement learning process with an implicit reward function. 3. Highlights the AI safety implications of this implicit training regime and positions it as an alternative to explicit RL training."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper shows that repeatedly deploying LLMs and fine-tuning them on curated data from previous deployments significantly improves their planning capabilities. This process is analyzed as an implicit form of reinforcement learning, which raises safety concerns due to the undefined reward function and offers an alternative training paradigm based on data curation."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Iterative Deployment Improves Planning Skills in LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLM\u89c4\u5212\u80fd\u529b/LLM Planning Skills]\n    C --\x3e C1[\u8fed\u4ee3\u90e8\u7f72\u4e0e\u5fae\u8c03/Iterative Deployment & Fine-tuning]\n    C1 --\x3e C2[\u7528\u6237\u6570\u636e\u7b5b\u9009/User Data Curation]\n    D --\x3e D1[\u89c4\u5212\u80fd\u529b\u63d0\u5347/Improved Planning Skills]\n    D --\x3e D2[\u53d1\u73b0\u9690\u5f0fRL/Discovering Implicit RL]\n    D2 --\x3e D3[AI\u5b89\u5168\u5f71\u54cd/AI Safety Implications]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [information retrieval], [relevance assessment, benchmark, long-tail, visual salience, e-commerce]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Taobao & Tmall Group of Alibaba"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24943",children:"https://arxiv.org/pdf/2512.24943"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RAIR, a comprehensive Chinese benchmark for e-commerce relevance assessment derived from real-world scenarios. 2. Establishes a standardized evaluation framework with universal rules to address the lack of standardized metrics. 3. Introduces a dataset with three specialized subsets (general, long-tail hard, visual salience) to evaluate fundamental, challenging, and multimodal capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes RAIR, a rule-aware benchmark for e-commerce search relevance assessment, to address the lack of complex and standardized evaluation datasets. It introduces a comprehensive dataset with three subsets to test different model capabilities. Experiments on 14 models show RAIR is challenging, with GPT-5 performing best, and it serves as a new industry benchmark."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[RAIR: \u4e00\u4e2a\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u89c4\u5219\u611f\u77e5\u57fa\u51c6 / RAIR: A Rule-Aware Benchmark for E-commerce Relevance Assessment]\n    A --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u590d\u6742\u6027\uff0c\u7f3a\u5c11\u6807\u51c6\u5316\u8bc4\u4f30 / Existing benchmarks lack complexity and standardized evaluation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u5305\u542b\u901a\u7528\u3001\u957f\u5c3e\u3001\u89c6\u89c9\u663e\u8457\u6027\u5b50\u96c6\u7684\u57fa\u51c6\u548c\u89c4\u5219\u6846\u67b6 / Propose benchmark with general, long-tail, visual-salience subsets and rule framework]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5bf914\u4e2a\u6a21\u578b\u6784\u6210\u6311\u6218\uff0cGPT-5\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u4f5c\u4e3a\u884c\u4e1a\u57fa\u51c6 / Presents challenge to 14 models, GPT-5 performs best, serves as industry benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [vision-language models], [LLM-as-a-Judge, training-free, few-shot, explainable AI, agricultural VQA]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wentao Zhang, Tao Fang, Lina Lu, Lifei Wang, Weihe Zhong"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shandong University of Technology, Macau Millennium College"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24947",children:"https://arxiv.org/pdf/2512.24947"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," ",(0,a.jsx)(n.a,{href:"https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis",children:"https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Caption-Prompt-Judge (CPJ) framework for training-free, few-shot agricultural pest diagnosis. 2. Introduces an LLM-as-Judge module for iterative refinement of structured, multi-angle image captions to enhance interpretability. 3. Demonstrates significant performance improvements on a benchmark dataset using lightweight models, advancing robust and explainable diagnosis without fine-tuning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/677e58cca4c831f09b054974c399f7092a40a64b068915b259e624f2728ef082_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/677e58cca4c831f09b054974c399f7092a40a64b068915b259e624f2728ef082_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes CPJ, a training-free framework that uses large vision-language models to generate and refine image captions, which are then used to improve agricultural visual question answering. The method significantly boosts disease classification and QA performance on the CDDMBench dataset, providing transparent, evidence-based reasoning without requiring model fine-tuning."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[CPJ: Explainable Agricultural Pest Diagnosis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing methods rely on costly fine-tuning and lack explainability under domain shifts.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: CPJ framework uses LVLMs to generate captions, refines them via LLM-as-Judge, and performs caption-informed VQA.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Significant performance gains (+22.7 pp in classification, +19.5 in QA score) on CDDMBench.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Classifying long legal documents using short random chunks"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [document classification], [DeBERTa V3, LSTM, random chunks, Temporal, long document processing]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Luis Adri\xe1n Cabrera-Diego"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Jus Mundi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24997",children:"https://arxiv.org/pdf/2512.24997"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A novel legal document classifier architecture combining DeBERTa V3 with an LSTM that processes only 48 randomly selected short chunks (max 128 tokens) per document, enabling efficient handling of long texts. 2. A robust deployment pipeline built using Temporal, a durable execution framework, ensuring reliable and fault-tolerant processing workflows. 3. Demonstrated effective performance on a multilingual legal document dataset with a weighted F-score of 0.898 and quantified processing efficiency (498 seconds per 100 files on CPU)."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92192ce5f7c11c2d86da5e296a17a8e7ae1b0794ecb76a29cb686c4b5b4f5f12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92192ce5f7c11c2d86da5e296a17a8e7ae1b0794ecb76a29cb686c4b5b4f5f12_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of classifying long legal documents by proposing a model that uses DeBERTa V3 and an LSTM to process only 48 randomly selected short text chunks per document. The method avoids the computational expense of processing full documents with Transformers and is deployed via a reliable Temporal-based pipeline. The system achieves a weighted F-score of 0.898 and processes 100 files in a median time of 498 seconds on CPU."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Classifying long legal documents using short random chunks] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Long legal documents are expensive/slow to process with full Transformers]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Classifier: DeBERTa V3 + LSTM on 48 random short chunks (128 tokens max)]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Weighted F-score: 0.898, Median time: 498s per 100 files (CPU)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal classification], [multi-agent collaboration, depressive symptoms identification, multimodal memes, Cognitive Analytic Therapy, large language model]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siddhant Agarwal, Adya Dhuler, Polly Ruhnke, Melvin Speisman, Md Shad Akhtar, Shweta Yadav"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Illinois at Chicago, Creighton University, Indraprastha Institute of Information Technology Delhi"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25015",children:"https://arxiv.org/pdf/2512.25015"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RESTOREx, a new dataset with LLM-generated and human-annotated explanations for detecting depressive symptoms in memes. 2. Proposes MAMAMemeia, a multi-agent multi-aspect discussion framework based on Cognitive Analytic Therapy (CAT) Competencies for multimodal analysis. 3. Achieves state-of-the-art performance, improving macro-F1 by 7.55% and establishing a new benchmark against over 30 methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea612586070aecc51cf0ff34b08e15ac3308fa3b6d60764b6e00c32e3a444b58_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea612586070aecc51cf0ff34b08e15ac3308fa3b6d60764b6e00c32e3a444b58_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the task of identifying depressive symptoms in multimodal memes from social media. It proposes MAMAMemeia, a multi-agent collaborative framework inspired by clinical psychology, and introduces the RESTOREx dataset with explanations. The method achieves a significant 7.55% improvement in macro-F1, setting a new benchmark for this task."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Identifying depressive symptoms in memes on social media]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Multi-agent multi-aspect framework (MAMAMemeia) based on Cognitive Analytic Therapy, using RESTOREx dataset with LLM/human explanations]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 7.55% macro-F1 improvement over SOTA, new benchmark vs. 30+ methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Modeling Language as a Sequence of Thoughts"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [recurrent transformer, thought gestalt, reversal curse, cross-attention, scaling efficiency]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nasim Borazjanizadeh, James McClelland"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Independent Researcher, Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25026",children:"https://arxiv.org/pdf/2512.25026"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"}),' 1. Introduced the Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels (tokens and sentence-level "thought" states). 2. Proposed a unified training scheme where token and sentence representations are generated with the same parameters and a single next-token objective, enabling gradient flow through memory. 3. Demonstrated improved data and parameter efficiency over GPT-2 and better performance on relational direction generalization (e.g., reversal curse).']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8e6d868bbc8b6b68328f1680dd184805a29e94d0e7b58137122aad5c0a6e9d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8e6d868bbc8b6b68328f1680dd184805a29e94d0e7b58137122aad5c0a6e9d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the limitations of standard Transformers, which rely on surface-level token statistics and lack globally consistent representations. It proposes the Thought Gestalt model, a recurrent Transformer that generates tokens while cross-attending to a memory of prior sentence-level "thought" states, trained with a unified next-token objective. The model shows improved scaling efficiency and reduces errors on relational generalization tasks like the reversal curse.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root("Modeling Language as a Sequence of Thoughts") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Transformer \u4f9d\u8d56\u8868\u5c42\u7edf\u8ba1/Transformers rely on surface-level statistics")\n    Problem --\x3e P2("\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f4\u8868\u793a/Lack globally consistent representations")\n    Problem --\x3e P3("\u5bfc\u81f4\u9006\u8f6c\u8bc5\u5492\u7b49\u95ee\u9898/Leads to issues like reversal curse")\n    Method --\x3e M1("\u63d0\u51fa\u601d\u60f3\u5b8c\u5f62\u6a21\u578b/Propose Thought Gestalt (TG) model")\n    Method --\x3e M2("\u53cc\u5c42\u5efa\u6a21: Token + \u53e5\u5b50\u7ea7\u601d\u60f3/Two-level modeling: tokens & sentence-level thoughts")\n    Method --\x3e M3("\u5faa\u73afTransformer + \u8de8\u6ce8\u610f\u529b\u8bb0\u5fc6/Recurrent Transformer with cross-attention memory")\n    Results --\x3e R1("\u6bd4GPT-2\u66f4\u9ad8\u6548/More efficient than GPT-2")\n    Results --\x3e R2("\u51cf\u5c11\u9006\u8f6c\u8bc5\u5492\u9519\u8bef/Reduces reversal curse errors")\n    Results --\x3e R3("\u7edf\u4e00\u53c2\u6570\u4e0e\u76ee\u6807\u8bad\u7ec3/Unified parameter & objective training")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.strong,{children:["[arXiv260101] AdaGReS",":Adaptive"," Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG"]})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [redundancy-aware selection, token-budgeted RAG, greedy selection, submodular optimization, adaptive calibration]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chao Peng, Bin Wang, Zhilei Long, Jinfang Sheng"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Central South University, Yizhi Intelligent (YZInt)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25052",children:"https://arxiv.org/pdf/2512.25052"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AdaGReS, a redundancy-aware context selection framework that optimizes a set-level objective combining query relevance and intra-set redundancy penalties under a token-budget constraint. 2. Introduces a closed-form, instance-adaptive calibration method for the relevance-redundancy trade-off parameter, eliminating manual tuning and adapting to candidate-pool statistics and budget limits. 3. Provides a theoretical analysis showing the proposed objective exhibits \u03b5-approximate submodularity, yielding near-optimality guarantees for the greedy selection algorithm."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85c85942576679b5e5fe4c0066c0977620d02d122c659a34dfe900bfed59c445_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85c85942576679b5e5fe4c0066c0977620d02d122c659a34dfe900bfed59c445_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of redundant context in token-budgeted RAG systems, which wastes budget and degrades generation quality. It proposes AdaGReS, an adaptive greedy selection framework that scores and selects chunks by balancing relevance and redundancy, with a theoretically-backed near-optimal guarantee. Experiments on QA and biomedical datasets show it improves redundancy control and end-to-end answer quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[AdaGReS: Adaptive Greedy Context Selection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Top-k\u68c0\u7d22\u8fd4\u56de\u5197\u4f59\u5757\uff0c\u6d6a\u8d39token\u9884\u7b97\u5e76\u964d\u4f4e\u751f\u6210\u8d28\u91cf/Top-k retrieval returns redundant chunks, wasting token budget and degrading generation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5197\u4f59\u611f\u77e5\u7684\u8d2a\u5a6a\u9009\u62e9\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u5f97\u5206\u4e0e\u5197\u4f59\u60e9\u7f5a\uff0c\u5e76\u8fdb\u884c\u81ea\u9002\u5e94\u53c2\u6570\u6821\u51c6/Redundancy-aware greedy selection framework with relevance-redundancy trade-off and adaptive calibration]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u5f00\u653e\u57dfQA\u548c\u751f\u7269\u533b\u5b66\u8bed\u6599\u4e0a\uff0c\u5197\u4f59\u63a7\u5236\u548c\u4e0a\u4e0b\u6587\u8d28\u91cf\u5f97\u5230\u6539\u5584\uff0c\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u7b54\u6848\u8d28\u91cf/Improved redundancy control and context quality on open-domain QA and biomedical corpus, leading to better end-to-end answer quality]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Many Minds from One Model: Bayesian Transformers for Population Intelligence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Bayesian Transformers, Variational Inference, Population Diversity, Normalization Layers, Wisdom of Crowds]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Diji Yang, Yi Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California Santa Cruz"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25063",children:"https://arxiv.org/pdf/2512.25063"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Population Bayesian Transformers (B-Trans), a method to convert a standard LLM into a Bayesian model by treating normalization layer biases as stochastic variables with a Gaussian variational approximation, enabling diverse model sampling from a single weight set. 2. Introduces sequence-level noise freezing to maintain temporal coherence within each sampled model instance's generation, ensuring consistent behavior across tokens. 3. Demonstrates that aggregating predictions from a population of sampled B-Trans instances enhances exploration and decision-making, leading to superior semantic diversity and task performance in zero-shot generation, RLVR, and RL without labels."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0113a2c263c7e9a33e3b5ce49ac7afb88b3a3baeb7fd88c121fac5ef4b745b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0113a2c263c7e9a33e3b5ce49ac7afb88b3a3baeb7fd88c121fac5ef4b745b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the lack of diversity and exploration in deterministic LLMs by proposing B-Trans, which transforms a standard LLM into a Bayesian model by making normalization biases stochastic. This allows sampling diverse "minds" from one model, and aggregating their predictions improves performance and semantic variety in reasoning tasks.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Many Minds from One Model: Bayesian Transformers for Population Intelligence] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u4ee3Transformer\u662f\u5355\u4e00\u601d\u7ef4\u7684/Modern Transformers are single-minded]\n    B --\x3e B2[\u7f3a\u4e4f\u591a\u6837\u6027\u963b\u788d\u63a2\u7d22/Lack of diversity hinders exploration]\n    C --\x3e C1[\u63d0\u51faB-Trans: \u8d1d\u53f6\u65afTransformer/Propose B-Trans: Bayesian Transformer]\n    C --\x3e C2[\u5f52\u4e00\u5316\u5c42\u504f\u7f6e\u4f5c\u4e3a\u968f\u673a\u53d8\u91cf/Normalization biases as stochastic variables]\n    C --\x3e C3[\u5e8f\u5217\u7ea7\u566a\u58f0\u51bb\u7ed3/Sequence-level noise freezing]\n    D --\x3e D1[\u589e\u5f3a\u8bed\u4e49\u591a\u6837\u6027/Improved semantic diversity]\n    D --\x3e D2[\u63d0\u5347\u4efb\u52a1\u6027\u80fd/Better task performance]\n    D --\x3e D3[\u5b9e\u73b0\u7fa4\u4f53\u667a\u6167/Achieves wisdom of crowds]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Scaling Open-Ended Reasoning to Predict the Future"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [language model forecasting], [open-ended forecasting, reinforcement learning, retrieval-augmented generation, calibration, Qwen3]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Max Planck Institute for Intelligent Systems, ELLIS Institute T\xfcbingen, T\xfcbingen AI Center, University of T\xfcbingen"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.25070",children:"https://arxiv.org/pdf/2512.25070"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"code:"})," /github (URL implied from first page content)"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A fully automated pipeline to synthesize a large-scale dataset (OpenForesight) for training language models on open-ended forecasting questions from news events. 2. A specialized forecasting system integrating retrieval and an improved RL reward function, trained on Qwen3, which prevents future information leakage. 3. The OpenForecaster 8B model, which demonstrates that specialized training improves accuracy, calibration, and consistency, matching larger proprietary models, with calibration benefits generalizing to other benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7433331ffccb9fb0f52db33a75b12ae808ae87c5410037274fcfdc5f22b3505_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7433331ffccb9fb0f52db33a75b12ae808ae87c5410037274fcfdc5f22b3505_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of training language models for open-ended future prediction. The authors propose an automated method to generate a large forecasting dataset from news and train a specialized model (OpenForecaster 8B) using retrieval and an improved RL reward. Their final model matches the performance of much larger proprietary models, showing improved prediction accuracy, calibration, and consistency."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Scaling Open-Ended Reasoning to Predict the Future<br>\u9884\u6d4b\u672a\u6765\u7684\u5f00\u653e\u5f0f\u63a8\u7406\u6269\u5c55] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u5f0f\u672a\u6765\u9884\u6d4b<br>Train LLMs for open-ended future forecasting]\n    C --\x3e C1[\u81ea\u52a8\u4ece\u65b0\u95fb\u751f\u6210\u6570\u636e\u96c6<br>Automated dataset generation from news]\n    C --\x3e C2[\u4f7f\u7528\u68c0\u7d22\u548c\u6539\u8fdb\u7684RL\u8fdb\u884c\u8bad\u7ec3<br>Training with retrieval & improved RL]\n    C --\x3e C3[\u9632\u6b62\u672a\u6765\u4fe1\u606f\u6cc4\u9732<br>Prevent future info leakage]\n    D --\x3e D1[OpenForecaster 8B \u5339\u914d\u66f4\u5927\u6a21\u578b<br>Matches larger proprietary models]\n    D --\x3e D2[\u63d0\u5347\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u4e00\u81f4\u6027<br>Improves accuracy, calibration, consistency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [word sense disambiguation], [quantum superposition, visual word sense disambiguation, gloss bias, quantum inference model, classical heuristic]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenbo Qiao, Peng Zhang, Qinghua Hu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tianjin University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24687",children:"https://arxiv.org/pdf/2512.24687"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Quantum Inference Model (Q-VWSD) for unsupervised visual word sense disambiguation, using quantum superposition to encode multiple glosses and mitigate semantic biases. 2. Showed that Q-VWSD is a quantum generalization of classical probability-based methods and designed a heuristic version that runs efficiently on classical computers. 3. Demonstrated superior performance over state-of-the-art classical methods, especially by effectively leveraging non-specialized glosses from large language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e5167ebf6e5d49153f09401dac307d10e811dc014412a548dd9a16e7d68f12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e5167ebf6e5d49153f09401dac307d10e811dc014412a548dd9a16e7d68f12_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of semantic bias in visual word sense disambiguation by proposing a Quantum Inference Model (Q-VWSD) that uses quantum superposition to encode multiple glosses. The method is shown to be a quantum generalization of classical approaches, and a heuristic version is developed for efficient classical computation. Experiments show the method outperforms existing classical techniques, particularly when using glosses from large language models."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TB\n    Root["Quantum Visual Word Sense Disambiguation<br/>\u91cf\u5b50\u89c6\u89c9\u8bcd\u4e49\u6d88\u6b67"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Glosses have semantic biases leading to biased disambiguation results.<br/>\u8bcd\u4e49\u89e3\u91ca\u5b58\u5728\u8bed\u4e49\u504f\u5dee\u5bfc\u81f4\u6d88\u6b67\u7ed3\u679c\u6709\u504f\u3002"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Propose Q-VWSD using quantum superposition to encode glosses.<br/>\u63d0\u51faQ-VWSD\uff0c\u7528\u91cf\u5b50\u53e0\u52a0\u6001\u7f16\u7801\u8bcd\u4e49\u89e3\u91ca\u3002"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Outperforms SOTA classical methods; heuristic version runs on classical computers.<br/>\u8d85\u8d8a\u73b0\u6709\u7ecf\u5178\u65b9\u6cd5\uff1b\u542f\u53d1\u5f0f\u7248\u672c\u53ef\u5728\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv260101] Large language models and the entropy of English"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [nlp], [language modeling], [conditional entropy, code length, long-range dependencies, statistical physics, large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Colin Scheibner, Lindsay M. Smith, William Bialek"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Princeton University, The CUNY Graduate Center"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.24969",children:"https://arxiv.org/pdf/2512.24969"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that the conditional entropy (code length) of English text continues to decrease with context length up to at least ~10^4 characters using LLMs, revealing long-range dependencies. 2. Shows empirically, independent of models, that small but significant correlations exist between characters at large separations. 3. Reveals that long-range structure in language is learned gradually during model training and that the distribution of code lengths shows emergent certainty for many characters at large context lengths."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b8a68a26c5fded01361a53044b69998ec40cb664dee0ff16491e09a2c2a4e3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b8a68a26c5fded01361a53044b69998ec40cb664dee0ff16491e09a2c2a4e3_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper uses large language models (LLMs) to analyze the conditional entropy of English text over long contexts. The key finding is that entropy continues to decrease with context length up to at least 10^4 characters, indicating the presence of long-range dependencies and correlations in language. These results provide new constraints for building statistical physics models of language and LLMs."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TB\n    A[Large language models and the entropy of English] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u82f1\u8bed\u6587\u672c\u662f\u5426\u5b58\u5728\u957f\u7a0b\u7ed3\u6784\uff1f<br>Does English text have long-range structure?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528LLMs\u5206\u6790\u6761\u4ef6\u71b5/\u4ee3\u7801\u957f\u5ea6<br>Use LLMs to analyze conditional entropy/code length]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u71b5\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u6301\u7eed\u4e0b\u964d\uff0c\u5b58\u5728\u957f\u7a0b\u4f9d\u8d56<br>Entropy decreases with context length, long-range dependencies exist]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);