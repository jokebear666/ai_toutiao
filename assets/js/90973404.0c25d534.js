"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5777],{7983:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CV/20251222-20251228","title":"20251222-20251228 (cs.CV)","description":"2025-12-22","source":"@site/docs/daily/cs_CV/20251222-20251228.md","sourceDirName":"daily/cs_CV","slug":"/daily/cscv/20251222-20251228","permalink":"/ai_toutiao/daily/cscv/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766487380000,"frontMatter":{"slug":"/daily/cscv/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.CV)","permalink":"/ai_toutiao/daily/cs_CV/20251215-20251221"},"next":{"title":"cs.CY","permalink":"/ai_toutiao/category/cscy"}}');var a=i(4848),r=i(8453);const t={slug:"/daily/cscv/20251222-20251228"},o="20251222-20251228 (cs.CV)",l={},d=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251222-20251228-cscv",children:"20251222-20251228 (cs.CV)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computational neuroscience], [Vision Transformer, modular subnetworks, condition-aware adaptation, structure-preserving framework, neural response modeling]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Xu, Shuai Gong, Xuming Ran, Haihua Luo, Yangfan Hu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dalian University of Technology, National University of Singapore, University of Jyvaskyla, Zhejiang University of Finance and Economics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16948",children:"https://arxiv.org/pdf/2512.16948"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces the Adaptive Visual Model (AVM), a structure-preserving framework that uses a frozen Vision Transformer encoder to capture stable visual features and separate modulation paths to adapt to stimulus and individual variations. It demonstrates improved generalization and interpretability in modeling mouse V1 neural responses, outperforming prior models in predictive correlation and explained variance across different experimental conditions."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [visual anchoring, asset-first mechanism, temporal bridge, diffusion models, large language model (LLM), text-to-video (T2V), character consistency, multi-stage pipeline]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," BITS Pilani"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16954",children:"https://arxiv.org/pdf/2512.16954"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method's necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [YOLOv8, Finer-CAM, saliency maps, cross-validation, TLS point cloud projections]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils N\xf6lke"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Applied Sciences and Art (HAWK), University of Groningen, University of G\xf6ttingen, Norwegian Institute of Bioeconomy Research (NIBIO)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16950",children:"https://arxiv.org/pdf/2512.16950"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models' perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [convolutional neural network, VGG-16, deep learning, image classification]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Reza Chandra, Adang Suhendra, Lintang Yuniar Banowosari, Prihandoko"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Gunadarma University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16947",children:"https://arxiv.org/pdf/2512.16947"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares two deep learning models, a custom CNN and the VGG-16 architecture, for the task of identifying pornographic image content. The study found that a CNN model with specific hyperparameters (50 epochs, learning rate 0.001) achieved a higher accuracy of 94.87% compared to the VGG-16 model, concluding that the CNN was more effective for fast and accurate detection."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] V-Agent: An Interactive Video Search System Using Vision-Language Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-language model, fine-tuning, retrieval vector, re-ranking, multi-agent system]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16925",children:"https://arxiv.org/pdf/2512.16925"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents\u2014routing, search, and chat\u2014to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [discrete video tokenization, transformer-based adaptive compressor, evidence lower bound (ELBO), information-theoretic compression, adaptive tokenization]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA, Stanford University, National University of Singapore"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16975",children:"https://arxiv.org/pdf/2512.16975"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [LongShOTBench, LongShOTAgent, agentic tool use, omni-modal reasoning, benchmark, open-ended questions, graded rubric, iterative refinement]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), American University of Beirut, Link\xf6ping University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16978",children:"https://arxiv.org/pdf/2512.16978"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces LongShOTBench, a diagnostic benchmark for long-form video understanding that features open-ended questions and tasks requiring multimodal reasoning and agentic tool use. It also presents LongShOTAgent, an agentic system that analyzes videos through preprocessing, search, and iterative refinement. The results show a significant performance gap, with state-of-the-art models achieving only up to 52.95%, highlighting the difficulty of real-world long-form video understanding."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical image segmentation], [semi-supervised learning, cross-supervision, uncertainty-guided pseudo-label, joint pseudo-label supervision, mutual learning, spatiotemporal corrective network]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hao Li, Daiwei Lu, Xing Yao, Nicholas Kavoussi, Ipek Oguz"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Vanderbilt University, Vanderbilt University Medical Center"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16977",children:"https://arxiv.org/pdf/2512.16977"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Endo-SemiS, a semi-supervised framework for endoscopic video segmentation that employs cross-supervision, uncertainty-guided pseudo-labeling, joint supervision, and mutual learning between two networks, along with a separate spatiotemporal corrective network. It demonstrates superior performance over state-of-the-art methods on kidney stone and polyp segmentation tasks when labeled data is limited."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [4D-RGPT, Perceptual 4D Distillation (P4D), R4D-Bench, region-level prompting, 4D Video Question Answering, multimodal LLM]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," NVIDIA, Purdue University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17012",children:"https://arxiv.org/pdf/2512.17012"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces 4D-RGPT, a multimodal LLM enhanced via a Perceptual 4D Distillation (P4D) framework to improve 4D (3D spatial + temporal) understanding from video inputs. It also proposes the R4D-Bench benchmark for evaluating region-level 4D reasoning. The method shows notable performance improvements on both existing and the new benchmark."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [hierarchical transformer model (PVTv2), airborne laser scanning (ALS), spatio-temporal total variation denoising, co-registration, SPOT-6/7 composites]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Martin Schwartz, Fajwel Fogel, Nikola Besic, Damien Robert, Louis Geist, Jean-Pierre Renaud, Jean-Matthieu Monnet, Clemens Mosig, C\xe9dric Vega, Alexandre d'Aspremont, Loic Landrieu, Philippe Ciais"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Laboratoire des Sciences du Climat et de l\u2019Environnement (LSCE), \xc9cole Normale Sup\xe9rieure \u2013 PSL, Universit\xe9 Gustave Eiffel, Universit\xe9 de Lorraine, University of Zurich, \xc9cole des Ponts, Office National des For\xeats, Universit\xe9 Grenoble Alpes, Institute of Earth System Science and Remote Sensing, Leipzig"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17021",children:"https://arxiv.org/pdf/2512.17021"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FORMSpoT, a method that uses a hierarchical transformer model trained on ALS data to derive high-resolution forest canopy height maps from SPOT satellite time series, combined with a post-processing pipeline for robust change detection. It demonstrates that this approach significantly outperforms existing products in detecting small-scale forest disturbances, enabling tree-level monitoring at a national scale. The results highlight the importance of very-high-resolution satellite data for accurate forest carbon loss quantification and monitoring under climate change."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [infinite homography warping, video diffusion model, data augmentation, camera-controlled video generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Min-Jung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17040",children:"https://arxiv.org/pdf/2512.17040"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces InfCam, a framework that uses infinite homography warping to encode 3D camera rotations in a 2D latent space of a video diffusion model, avoiding depth estimation errors. It also employs a data augmentation pipeline to create diverse camera trajectories for training. The method demonstrates improved camera-pose accuracy and visual fidelity compared to existing approaches."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Interpretable Similarity of Synthetic Image Utility"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical imaging], [Interpretable Utility Similarity (IUS), generalized neural additive models, synthetic data evaluation, deep learning, clinical decision support]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Panagiota Gatoula, George Dimas, Dimitris K. Iakovidis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Thessaly"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17080",children:"https://arxiv.org/pdf/2512.17080"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an interpretable measure called Interpretable Utility Similarity (IUS) to assess the similarity between synthetic and real medical images for deep learning-based clinical decision support. IUS uses generalized neural additive models to explain utility based on clinically relevant image features. Experiments show that selecting synthetic images with high IUS can improve classification performance by up to 54.6% across various medical imaging modalities."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] DGH: Dynamic Gaussian Hair"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision, graphics], [dynamic Gaussian hair, coarse-to-fine model, strand-guided optimization, differentiable rendering, 3D Gaussian splatting]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Junying Wang, Yuanlu Xu, Edith Tretschk, Ziyan Wang, Anastasia Ianina, Aljaz Bozic, Ulrich Neumann, Tony Tung"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Southern California, Meta Reality Labs Research"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17094",children:"https://arxiv.org/pdf/2512.17094"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Dynamic Gaussian Hair (DGH), a data-driven framework that learns hair dynamics and appearance using a coarse-to-fine motion model and a strand-guided 3D Gaussian representation. It enables photorealistic novel-view synthesis of hair under head motion without manual physics tuning. DGH provides a scalable, generalizable alternative to traditional simulation-based hair modeling."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Predictive Modeling of Maritime Radar Data Using Transformer Architecture"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [spatiotemporal forecasting], [transformer architecture, predictive modeling, radar frame prediction, spatiotemporal sequence forecasting]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Bjorna Qesaraku, Jan Steckel"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not explicitly provided in the given text."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17098",children:"https://arxiv.org/pdf/2512.17098"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper reviews predictive modeling approaches for maritime radar data, with a specific focus on the application of transformer architectures for spatiotemporal sequence forecasting. It concludes that while transformers have been used for AIS trajectory and sonar frame prediction, their use for maritime radar frame prediction remains an unexplored research gap, identifying a clear direction for future work."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Nan Zhou, Huandong Wang, Jiahao Li, Yang Li, Xiao-Ping Zhang, Yong Li, Xinlei Chen"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"institution:"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17152",children:"https://arxiv.org/pdf/2512.17152"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Johns Hopkins University, NVIDIA"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17137",children:"https://arxiv.org/pdf/2512.17137"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [canonical UV maps, SMPL-X poses, novel view synthesis, multi image finetuning, unpaired dataset]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sandeep Mishra, Yasamin Jafarian, Andreas Lugmayr, Yingwei Li, Varsha Ramakrishnan, Srivatsan Varadharajan, Alan C. Bovik, Ira Kemelmacher-Shlizerman"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin, Google"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17143",children:"https://arxiv.org/pdf/2512.17143"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Pro-Pose, a method for synthesizing professional full-body portraits from a single casual photo by transforming the person into a canonical UV map space and leveraging SMPL-X poses for reposing. This approach uses unpaired datasets and personalizes results through multi-image fine-tuning. It concludes that the method effectively preserves identity while generating high-quality, reposed avatars in varied poses and lighting."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Text-Conditioned Background Generation for Editable Multi-Layer Documents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [latent masking, Automated Readability Optimization (ARO), summarization-and-instruction process, multi-layer composition, WCAG 2.2]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Taewon Kang, Joseph K J, Chris Tensmeyer, Jihyung Kil, Wanrong Zhu, Ming C. Lin, Vlad I. Morariu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Maryland at College Park, Adobe Research"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17151",children:"https://arxiv.org/pdf/2512.17151"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a training-free framework for generating and editing document backgrounds using a diffusion model, ensuring text readability through latent masking and Automated Readability Optimization (ARO). It maintains multi-page thematic consistency via a summarization-and-instruction process and treats documents as editable multi-layer compositions. The method produces visually coherent, readable, and thematically aligned documents, bridging generative AI with practical design workflows."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Can Synthetic Images Serve as Effective and Efficient Class Prototypes?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal inference], [CLIP, large language model, diffusion model, zero-shot classification, visual prototypes, prompt generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Research Institute of Uranium Geology, Huazhong University of Science and Technology, The Hong Kong University of Science and Technology (Guangzhou), Great Bay University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17160",children:"https://arxiv.org/pdf/2512.17160"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LGCLIP, a framework that uses an LLM to generate class-specific prompts for a diffusion model to create synthetic images as visual prototypes, enabling zero-shot classification with only a visual encoder. It eliminates the need for annotated image-text pairs and reduces model complexity. Experiments show LGCLIP is effective and efficient, establishing a new lightweight paradigm for classification."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [CLIP, semantic refinement mechanism, local token-patch alignment, attribute-object binding, compositional image-text matching]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sichuan University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17178",children:"https://arxiv.org/pdf/2512.17178"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ABE-CLIP, a training-free method to enhance attribute-object binding in CLIP models. It uses a semantic refinement mechanism to improve text token embeddings and a local token-patch alignment strategy to compute image-text similarity. Experiments show the method significantly improves compositional matching performance, even surpassing some trained approaches."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [urban perception analysis], [Green View Index (GVI), street view imagery, pairwise ratings, perception survey]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Matias Quintana, Fangqi Liu, Jussi Torkko, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Tuuli Toivonen, Yi Lu, Filip Biljecki"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Singapore-ETH Centre, City University of Hong Kong, University of Helsinki, National University of Singapore"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17186",children:"https://arxiv.org/pdf/2512.17186"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes discrepancies between objective greenery measurements (like Green View Index from street view imagery) and subjective human perceptions collected via surveys across five countries. The main finding is that demographic and personality factors do not significantly influence perception, but the location where people live is a key factor, suggesting cultural and environmental experiences shape how urban greenery is observed."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [affine correspondences, global optimization, polynomial eigenvalue solver, generalized essential matrix, generalized camera model]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhenbao Yu, Banglei Guan, Shunkun Liang, Zibin Liu, Yang Shang, Qifeng Yu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology, Wuhan University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17188",children:"https://arxiv.org/pdf/2512.17188"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a globally optimal solver for estimating the generalized relative pose of multi-camera systems using affine correspondences and a known vertical direction. The method decouples rotation and translation, formulates a cost function, and solves it via polynomial eigenvalue techniques. Experimental results on synthetic and real data show the method outperforms state-of-the-art approaches in accuracy."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [contrastive decoding, anatomical mask, token re-weighting, attention re-weighting, logits re-weighting]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiao Liang, Chenxi Liu, Zhi Ma, Di Wang, Bin Jing, Quan Wang, Yuanyuan Shi"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xidian University, Capital Medical University, the Ninth Medical Center of the Chinese PLA General Hospital"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17189",children:"https://arxiv.org/pdf/2512.17189"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play method that uses an anatomical mask to guide a three-tiered contrastive decoding process at the token, attention, and logits levels to reduce hallucinations in Medical Vision-Language Models. Experiments across multiple medical imaging datasets show that ARCD effectively improves regional understanding, reduces factually incorrect outputs, and enhances diagnostic accuracy."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [one-step distillation, lightweight ensemble blocks, four-stage training, pansharpening, diffusion model, end-to-end network]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17202",children:"https://arxiv.org/pdf/2512.17202"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model's inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training], [latent modulation, variational autoencoder, reinforcement learning, supervised fine-tuning, reasoning strategies, controllable exploration]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Alibaba Group, Shanghai Jiao Tong University, Zhejiang University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17206",children:"https://arxiv.org/pdf/2512.17206"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Reasoning Palette, a framework that uses a latent variable from a VAE to modulate a model's reasoning trajectory via prepended token prefixes, enabling diverse strategic exploration. It shows that this approach improves exploration efficiency in RL training and leads to consistent performance gains over standard methods on reasoning benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xidian University, Brown University, Capital Medical University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17213",children:"https://arxiv.org/pdf/2512.17213"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [scene coordinate regression, global descriptors, covisibility graphs, contrastive loss, batch-mining]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Son Tung Nguyen, Tobias Fischer, Alejandro Fontan, Michael Milford"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Queensland University of Technology"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17226",children:"https://arxiv.org/pdf/2512.17226"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a method that learns global descriptors for visual localization by combining geometric structure and visual similarity, using a batch-mining strategy and modified contrastive loss to train without manual labels. This approach corrects errors from unreliable geometric constraints and improves disambiguation in large-scale environments. Experiments show significant gains in localization accuracy while maintaining computational and memory efficiency."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] DAVE: A VLM Vision Encoder for Document Understanding and Web Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [vision encoder, self-supervised pretraining, autoregressive pretraining, model merging, ensemble training, SigLIP2]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," MIT-IBM Watson AI Lab, UC Berkeley, University of Wisconsin\u2013Madison"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17221",children:"https://arxiv.org/pdf/2512.17221"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces DAVE, a vision encoder for VLMs designed for document understanding and web agents. Its training pipeline uses self-supervised pretraining on unlabeled data followed by supervised autoregressive pretraining, enhanced by model-merging and ensemble techniques to improve compatibility and performance. Experiments show DAVE is an effective vision encoder for document and web applications."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [curriculum learning, supervised fine-tuning, reinforcement learning, perception-grounded chain-of-thought, pivotal perception reward]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Meituan, Tsinghua University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17227",children:"https://arxiv.org/pdf/2512.17227"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a two-stage curriculum framework to address visual forgetting in multimodal reasoning. It first builds an abstract reasoning backbone using text-only data and then uses reinforcement learning with a novel reward to teach the model a strategic policy for when to perceive visual information. This approach transforms the model into a more strategic and visually grounded reasoner."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [remote sensing foundation model], [spectrum-independent tokenizer, multi-scale adaptive patch embedding, channel-wise self-supervised masking and reconstruction, multi-scale semantic alignment]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Xuyang Li, Chenyu Li, Danfeng Hong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Southeast University, Aerospace Information Research Institute, Chinese Academy of Sciences, University of Chinese Academy of Sciences"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17224",children:"https://arxiv.org/pdf/2512.17224"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Any-Optical-Model (AOM), a universal foundation model for optical remote sensing designed to handle arbitrary band compositions and spatial resolutions. Its core innovations include a spectrum-independent tokenizer, multi-scale adaptive patch embedding, and a channel-wise self-supervised pretraining strategy. Experiments show AOM achieves state-of-the-art performance in challenging scenarios like band missing and cross-sensor/cross-resolution settings."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [question-aware memory mechanism, recurrent processing, token compression, memory tokens, long video question-answering]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Renmin University of China, Tencent PCG"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17229",children:"https://arxiv.org/pdf/2512.17229"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes VideoDetective, a method that uses a question-aware memory mechanism to recurrently process long videos by compressing sub-segments into special memory tokens, enabling efficient question-answering. This approach allows models with limited context length to handle long videos with reduced memory and time. Experimental results show it effectively seeks critical clues from massive video information."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Mitty: Diffusion-based Human-to-Robot Video Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [robotics and computer vision], [diffusion transformer, in-context learning, video generation, human-to-robot translation, bidirectional attention]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yiren Song, Cheng Liu, Weijia Mao, Mike Zheng Shou"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17253",children:"https://arxiv.org/pdf/2512.17253"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Mitty is a Diffusion Transformer model that uses in-context learning to directly convert human demonstration videos into robot-execution videos without intermediate representations. It leverages a pretrained video diffusion model and an automatic synthesis pipeline to address data scarcity. The method achieves state-of-the-art performance and strong generalization in robot learning from human observations."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical imaging segmentation], [Multi-stage Domain Randomization (MSDR), Conditional Joint Annotation Regularization (CAR), synthetic data generation, zero-shot generalization, anatomical consistency]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dong Zifei, Wu Wenjie, Hao Jinkui, Chen Tianqi, Weng Ziqiao, Zhou Bo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northwestern University, Vanderbilt University, Shanxi Medical University, University of Sydney"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17263",children:"https://arxiv.org/pdf/2512.17263"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes AnyCXR, a framework for chest X-ray segmentation that uses synthetic data generated via Multi-stage Domain Randomization and a Conditional Joint Annotation Regularization learning strategy to handle imperfect labels. It demonstrates strong zero-shot generalization to real-world X-rays across different views, enabling accurate multi-organ segmentation and improving downstream clinical tasks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical image segmentation], [wavelet-guided enhancement, dual-attention feature fusion, U-shaped Mamba architecture, Wavelet-denoised High-Frequency-guided Feature (WHF), Dual Attention Feature Fusion (DAFF)]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing Jiaotong University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17278",children:"https://arxiv.org/pdf/2512.17278"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [deep learning, multi-task learning, domain generalization, segmentation, classification, Dice Similarity Coefficient, Area Under the Curve]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zehui Lin, Luyi Han, Xin Wang, Ying Zhou, Yanming Zhang, Tianyu Zhang, Lingyun Bao, Shandong Wu, Dong Xu, Tao Tan, UUSIC25 Challenge Consortium"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Macao Polytechnic University, Netherlands Cancer Institute, Zhejiang Cancer Hospital, The First People\u2019s Hospital of Hangzhou, University of Pittsburgh"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17279",children:"https://arxiv.org/pdf/2512.17279"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates general-purpose deep learning models for multi-organ classification and segmentation in ultrasound imaging through the UUSIC25 challenge. The top model demonstrated high accuracy and efficiency across tasks using a single architecture. However, performance degraded on data from unseen institutions, highlighting the critical need for improved domain generalization before clinical deployment."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Vision-Language Model Guided Image Restoration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-language model, CLIP, diffusion model, cross-attention, LoRA fine-tuning, degradation predictor]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cuixin Yang, Rongkang Dong, Kin-Man Lam"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong Polytechnic University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17292",children:"https://arxiv.org/pdf/2512.17292"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes the VLMIR framework, which uses a vision-language model (like CLIP) to extract aligned visual and textual features from images and integrates them via cross-attention into a diffusion model for restoration. It concludes that this approach, leveraging linguistic priors for semantic coherence, achieves superior performance in universal and degradation-specific image restoration tasks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [feature caching, selective computation, constraint-aware scheduling, temporal redundancy, activation schedule]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," South China University of Technology, South China Agricultural University, Pazhou Laboratory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17298",children:"https://arxiv.org/pdf/2512.17298"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ProCache, a training-free framework to accelerate Diffusion Transformers (DiTs) by using dynamic feature caching. It introduces a constraint-aware caching pattern search to create non-uniform activation schedules and a selective computation module to mitigate error accumulation. Experiments show ProCache achieves significant speedups with minimal quality loss, outperforming prior caching methods."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\ via Self-Supervised Image Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [self-supervised learning, image reconstruction, anomaly detection, SIR-Gate, ROPS]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wuyi Liu, Le Jin, Junxian Yang, Yuanchao Yu, Zishuo Peng, Jinfeng Xu, Xianzhi Li, Jun Zhou"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Siemens AG, University of Electronic Science and Technology of China, Peking University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17296",children:"https://arxiv.org/pdf/2512.17296"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces HiSIR-Net, a self-supervised image reconstruction framework for pixel-wise anomaly localization on high-resolution PCBA images. It uses two novel modules\u2014SIR-Gate to reduce reconstruction noise and ROPS for coherent patch selection\u2014to achieve accurate defect detection with low false positives. The method demonstrates superior performance on a new dataset (SIPCBA-500) and public benchmarks while maintaining practical inference speed."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [classifier-free guidance, attention modification, exponential moving average, adaptive layer selection, diffusion transformers]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ankit Yadav, Ta Duc Huy, Lingqiao Liu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Australian Institute for Machine Learning, The University of Adelaide"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17303",children:"https://arxiv.org/pdf/2512.17303"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Exponential Moving Average Guidance (EMAG), a training-free inference method for diffusion transformers that adaptively modifies attention maps to generate challenging negative samples. This allows the model to correct fine-grained artifacts, improving image quality and human preference scores over standard guidance. The method is shown to be compatible with other advanced guidance techniques for further gains."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [chain-of-thought, multi-turn reasoning, self-reflection, redundancy-penalized policy optimization, supervised fine-tuning, reinforcement learning]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University, Alibaba Group, Shanghai Jiao Tong University, Zhejiang University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17306",children:"https://arxiv.org/pdf/2512.17306"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes DRIM, a model that enhances multi-turn reasoning in vision-language models by integrating a self-reflective chain-of-thought with tool invocation. It uses a three-stage pipeline of data construction, supervised fine-tuning, and redundancy-penalized reinforcement learning to encourage reliable exploration and self-correction. Experiments show DRIM achieves superior performance on visual understanding benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [executable code, tool orchestration, reinforcement learning, visual reasoning, reward shaping]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qi Song, Honglin Li, Yingchen Yu, Haoyi Zhou, Lin Yang, Song Bai, Qi She, Zilong Huang, Yunqing Zhao"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beihang University, Westlake University, ByteDance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17312",children:"https://arxiv.org/pdf/2512.17312"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces CodeDance, a multimodal large language model that uses executable code to dynamically orchestrate multiple tools for visual reasoning. It employs a reinforcement learning reward to balance tool use, leading to adaptive and efficient reasoning. The method outperforms schema-driven and text-only baselines, and even surpasses advanced closed models like GPT-4o on various benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MatLat: Material Latent Space for PBR Texture Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [latent diffusion, VAE fine-tuning, material latent space, patch-based regularization, correspondence-aware attention]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kyeongmin Yeo, Yunhong Min, Jaihoon Kim, Minhyuk Sung"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17302",children:"https://arxiv.org/pdf/2512.17302"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes MatLat, a generative framework that fine-tunes a pretrained VAE to create a material latent space for generating high-quality PBR textures, addressing dataset scarcity and distribution shift issues. It introduces a patch-based regularization during VAE fine-tuning to preserve spatial locality between latent codes and image pixels, which is crucial for cross-view consistency. The method demonstrates improved PBR texture fidelity and achieves state-of-the-art performance, with each component being essential."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [vision-language models], [few-shot adaptation, parameter-efficient fine-tuning, auxiliary descriptive knowledge, large language model, non-parametric attention, compositional knowledge, instance-specific knowledge]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sungkyunkwan University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17313",children:"https://arxiv.org/pdf/2512.17313"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Auxiliary Descriptive Knowledge (ADK), a framework that enhances few-shot adaptation of vision-language models by using an LLM to generate offline descriptive prompts for each class. ADK enriches text representations through averaged compositional knowledge and a lightweight attention mechanism for instance-specific knowledge, improving classification without added inference cost. It consistently boosts existing parameter-efficient fine-tuning methods, achieving state-of-the-art performance across various scenarios."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [remote sensing, multimodal evaluation], [RSHR-Bench, adversarial filtering, high-resolution imagery, multimodal large language models, visual question answering, image captioning]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Nanjing University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17319",children:"https://arxiv.org/pdf/2512.17319"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [concept erasure, text-to-image generation, benchmark evaluation, semantic metrics, robustness testing, bias analysis]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Lu Wei, Yuta Nakashima, Noa Garcia"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Osaka University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17320",children:"https://arxiv.org/pdf/2512.17320"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces EMMA, a comprehensive benchmark for evaluating concept erasure techniques in text-to-image diffusion models. It tests five key dimensions across 12 metrics, including robustness to indirect prompts and bias. The main conclusion is that existing erasure methods struggle with implicit descriptions and visually similar concepts, and some can amplify gender and ethnicity bias."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [vision-language model, instruction tuning, visual question answering, whole-slide images, synthetic instruction generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sander Moonemans, Sebastiaan Ram, Fr\xe9d\xe9rique Meeuwsen, Carlijn Lems, Jeroen van der Laak, Geert Litjens, Francesco Ciompi"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Radboud University Medical Center"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17326",children:"https://arxiv.org/pdf/2512.17326"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Polysome, a tool for generating synthetic instructions, and uses it to create HISTAI-Instruct, a large instruction-tuning dataset from whole-slide images. It then trains a vision-language model called ANTONI-\u03b1 on this dataset, which is shown to outperform MedGemma on tasks like tissue identification and differential diagnosis."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Rotterdam artery-vein segmentation (RAV) dataset"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical imaging], [color fundus images, artery-vein segmentation, machine learning algorithms, connectivity validation, custom annotation interface]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Caroline Klaver"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Erasmus University Medical Center, Radboud University Medical Center, Institute of Molecular and Clinical Ophthalmology, University of Basel"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17322",children:"https://arxiv.org/pdf/2512.17322"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the Rotterdam Artery-Vein segmentation dataset, created by sampling and annotating color fundus images from the Rotterdam Study using a custom interface with connectivity validation tools. The dataset includes diverse, high-quality artery-vein segmentation masks and varied image modalities to support the development of robust machine learning models for retinal vascular analysis. The main conclusion is that this heterogeneous dataset enables benchmarking and training of clinically applicable algorithms under real-world variability in image quality and acquisition."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [diffusion model, residual training, variational autoencoder, event camera, frame synthesis]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiyun Kong, Jun-Hyuk Kim, Jong-Seok Lee"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yonsei University, Chung-Ang University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17323",children:"https://arxiv.org/pdf/2512.17323"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DESSERT, a method for event-driven video frame synthesis using a diffusion model trained on inter-frame residuals and conditioned on event data. It employs a two-stage pipeline with an Event-to-Residual Alignment VAE and a diffusion model, enhanced by Diverse-Length Temporal augmentation. The method outperforms existing approaches in producing sharper and more temporally consistent frames."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Multi-level distortion-aware deformable network for omnidirectional image super-resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [deformable attention, dilated deformable convolution, low-rank decomposition, multi-level feature fusion]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cuixin Yang, Rongkang Dong, Kin-Man Lam, Yuhang Zhang, Guoping Qiu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Hong Kong Polytechnic University, Guangzhou University, University of Nottingham"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17343",children:"https://arxiv.org/pdf/2512.17343"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a Multi-level Distortion-aware Deformable Network (MDDN) for omnidirectional image super-resolution, which uses parallel branches of deformable attention and dilated deformable convolutions to capture geometric distortions. It also employs a low-rank decomposition to reduce computational cost. Experiments show that MDDN outperforms existing state-of-the-art methods."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [attention-guided cooperative warping, 3D dense optical flow, cross-attention, 3D keypoints, confidence-guided fusion]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shihang Li, Zhiqiang Gong, Minming Ye, Yue Gao, Wen Yao"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Defense Innovation Institute Academy of Military Science, Intelligent Game and Decision Laboratory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17331",children:"https://arxiv.org/pdf/2512.17331"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SynergyWarpNet, a three-stage framework for neural portrait animation that combines explicit warping using 3D optical flow with reference-augmented correction via cross-attention and a confidence-guided fusion module. It aims to address the limitations of traditional warping and attention-based methods by integrating geometric alignment with semantic completion. The model achieves state-of-the-art performance on benchmark datasets for high-fidelity talking head synthesis."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [pixel-level mapping, high-frequency traces, semantic bias, cross-generator generalization]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Chenming Zhou, Jiaan Wang, Yu Li, Lei Li, Juan Cao, Sheng Tang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17350",children:"https://arxiv.org/pdf/2512.17350"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces a pixel-level mapping pre-processing step to disrupt pixel value distributions and break semantic shortcuts, forcing detectors to focus on generalizable high-frequency traces from image generation. This method significantly improves the cross-generator performance of state-of-the-art AI-generated image detectors, verifying that disrupting semantic cues is key to generalization."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [affective image filter, multi-modal transformer, diffusion models, emotional fidelity, content consistency]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Peixuan Zhang, Shuchen Weng, Jiajun Tang, Si Li, Boxin Shi"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications, Beijing Academy of Artificial Intelligence, Peking University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17376",children:"https://arxiv.org/pdf/2512.17376"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes AIF-D, an Affective Image Filter model that extends a multi-modal transformer baseline by leveraging generative priors from pre-trained large-scale diffusion models to reflect emotions from text into images. It demonstrates superior performance in content consistency and emotional fidelity compared to state-of-the-art methods and is more effective at evoking specific emotions according to user studies."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [vision-language models], [CulturalToM-VQA, visual question answering, chain-of-thought prompting, compositional chain-of-thought prompting, false belief reasoning, social desirability bias]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of California, Riverside, University of Dhaka, Stanford University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17394",children:"https://arxiv.org/pdf/2512.17394"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," L\xe9o Butsanets, Charles Corbi\xe8re, Julien Khlaut, Pierre Manceron, Corentin Dancette"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Raidium, Universit\xe9 de Paris Cit\xe9, H\xf4pital Europ\xe9en Georges Pompidou, AP-HP, INSERM"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17396",children:"https://arxiv.org/pdf/2512.17396"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [occlusion, GradCAM++, HiResCAM, Composite-L, CAM, explainable AI, convolutional neural networks]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Martin Krebs, Jan Obdr\u017e\xe1lek, V\xedt Musil, Tom\xe1\u0161 Br\xe1zdil"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Masaryk University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17416",children:"https://arxiv.org/pdf/2512.17416"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper seeks a faster alternative to the occlusion method for explaining CNN-based prostate cancer classification. By establishing comparison criteria and metrics, the authors evaluate several single-pass explanation methods like GradCAM++ and HiResCAM. They identify a method that reduces explanation time by at least a factor of 10 without compromising output quality, facilitating faster model development and clinical adoption."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision, disaster management], [semantic segmentation, visual question answering, image classification, deep learning, aerial imagery]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Georgios Simantiris, Konstantinos Bacharidis, Apostolos Papanikolaou, Petros Giannakakis, Costas Panagiotakis"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hellenic Mediterranean University, Institute of Computer Science, FORTH"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17432",children:"https://arxiv.org/pdf/2512.17432"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces AIFloodSense, a global aerial imagery dataset for flood detection, supporting tasks like semantic segmentation, image classification, and visual question answering. It establishes baseline benchmarks using state-of-the-art deep learning models. The main conclusion is that the dataset's global diversity and multi-task support advance the development of robust, domain-generalized AI tools for climate resilience and disaster assessment."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Xiaomi MiMo-VL-Miloco Technical Report"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [supervised fine-tuning, reinforcement learning, Group Relative Policy Optimization, chain-of-thought supervision, token-budget-aware reasoning, quantization]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xiaomi"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17436",children:"https://arxiv.org/pdf/2512.17436"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MiMo-VL-Miloco-7B, a vision-language model specialized for smart-home understanding, built via a two-stage training pipeline combining supervised fine-tuning and reinforcement learning. The model achieves leading performance on home-scenario tasks like gesture recognition and also shows gains on general multimodal and language reasoning benchmarks. The authors conclude that targeted home-scenario training enhances activity understanding and can improve text-only reasoning with minimal trade-offs on other tasks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [scene graph, agentic pipeline, video diffusion, 3D scene decomposition, natural language control, trajectory generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yun He, Francesco Pittaluga, Ziyu Jiang, Matthias Zwicker, Manmohan Chandraker, Zaid Tasneem"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Maryland, College Park, NEC Labs America, UC San Diego"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17445",children:"https://arxiv.org/pdf/2512.17445"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LangDriveCTRL is a framework that edits real-world driving videos using natural language instructions. It decomposes scenes into 3D graphs and uses an agentic pipeline with specialized modules for object grounding, behavior editing, and review, followed by video diffusion for refinement. The method achieves significantly higher instruction alignment and realism compared to previous state-of-the-art approaches."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [multimodal semantic segmentation, deep neural network, robust training strategies, synchronized sensor data, daytime training for nighttime performance]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jon Muhovi\u010d, Janez Per\u0161"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Ljubljana"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17450",children:"https://arxiv.org/pdf/2512.17450"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [3D scene reconstruction, generative models, differentiable optimization, compositional framework, camera recovery, spatial optimization]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tobias Sautter, Jan-Niklas Dihlmann, Hendrik P.A. Lensch"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of T\xfcbingen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17459",children:"https://arxiv.org/pdf/2512.17459"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," 3D-RE-GEN is a compositional framework that reconstructs a single image into textured 3D objects and a background by integrating models for detection, reconstruction, and placement, using generative models for occluded objects and a novel 4-DoF optimization for layout alignment. It achieves state-of-the-art performance in single-image 3D scene reconstruction, producing coherent, modifiable scenes suitable for VFX and game development."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [federated learning, digital twin, Vision Transformer, ViT-UNet, privacy-preserving AI, brain tumor segmentation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Towson University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17488",children:"https://arxiv.org/pdf/2512.17488"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical diagnostics], [artificial intelligence, digital pathology, prostate cancer, Gleason grading, external validation, Cohen's quadratically weighted kappa, compact scanner]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Nita Mulliqi, Abdulbasit K. Al-Talabani, Kimmo Kartasalo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Koya University, Karolinska Institutet"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17499",children:"https://arxiv.org/pdf/2512.17499"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study externally validated AI models for prostate cancer diagnosis and Gleason grading using a Middle Eastern cohort from Iraq. The AI models demonstrated performance comparable to pathologists and showed high consistency across different digital slide scanners, including low-cost compact models. The findings support the global adoption of AI in pathology, particularly in under-represented regions with limited resources."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [illuminant personalization, physics-based illuminant augmentation, edge-guided prompt disentanglement, masked reconstruction loss, contextual light adaptation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, Joost Van De Weijer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Computer Vision Center, Universitat Aut\xf2noma de Barcelona, City University of Hong Kong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17489",children:"https://arxiv.org/pdf/2512.17489"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LumiCtrl is a method for learning illuminant prompts from a single object image to control lighting in personalized text-to-image models. It uses physics-based augmentation, edge-guided prompt disentanglement, and a masked reconstruction loss to achieve contextual light adaptation. The method outperforms existing baselines in illuminant fidelity, aesthetic quality, and scene coherence, as confirmed by a human preference study."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal geo-spatial understanding], [cross-view retrieval, geolocalization, CLIP-inspired baseline, multimodal dataset, instance-level benchmark]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Oskar Kristoffersen, Alba R. S\xe1nchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Denmark, Pioneer Center for AI"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17492",children:"https://arxiv.org/pdf/2512.17492"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MMLANDMARKS, a multimodal dataset with aerial images, ground-view images, text, and GPS coordinates for geo-spatial tasks. Using a simple CLIP-inspired baseline, the authors show competitive performance across tasks like cross-view retrieval and geolocalization, highlighting the need for multimodal datasets for comprehensive geo-spatial understanding."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [4D scene geometry, diffusion-based video generation, occlusion consistency, illumination-aware dataset, mask generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST AI, SK Telecom"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17504",children:"https://arxiv.org/pdf/2512.17504"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [sys], [sensor fusion and filtering], [Error-State Extended Kalman Filter, Scaled Unscented Kalman Filter, visual-inertial odometry, quaternion estimation, adaptive covariance, loosely coupled architecture]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ufuk Asil, Efendi Nasibov"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Dokuz Eylul University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17505",children:"https://arxiv.org/pdf/2512.17505"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [multiple instance learning, AutoML, feature extraction, whole-slide images, benchmarking, computational pathology]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17517",children:"https://arxiv.org/pdf/2512.17517"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [source-free object detection, spatial prior-aware regularization, imbalance-aware noise robust pseudo-labeling, mean-teacher, OV-SAM, domain shift]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sairam VCR, Rishabh Lalla, Aveen Dayal, Tejal Kulkarni, Anuj Lalla, Vineeth N Balasubramanian, Muhammad Haris Khan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," IIT Hyderabad, MBZUAI, UC San Diego, IIT Jodhpur, Microsoft Research India"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17514",children:"https://arxiv.org/pdf/2512.17514"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FALCON-SFOD, a framework for source-free object detection that uses foundation model priors (SPAR) to enhance object-focused features and a noise-robust pseudo-labeling method (IRPL) to handle class imbalance. It concludes that this approach strengthens the feature space against domain shift, leading to more reliable pseudo-labels and competitive benchmark performance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal evaluation], [visual grounding, benchmark, multimodal large language models, discriminative, spatial, limited, rejection, test-time scaling, data-mixture training]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, Xiaomi, The University of Hong Kong, Renmin University of China"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17495",children:"https://arxiv.org/pdf/2512.17495"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GroundingME, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across four challenging dimensions: discriminative, spatial, limited, and rejection tasks. The evaluation of 25 MLLMs reveals a significant performance gap, with the best model achieving only 45.1% accuracy and most failing on rejection tasks by hallucinating objects. The authors propose test-time scaling and data-mixture training as strategies to partially improve model performance on these complex grounding challenges."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [3D reconstruction and language embedding], [feed-forward network, 3D Gaussians, instance-guided contrastive learning, geometry-semantic hierarchical sparsification, 2D-to-3D lifting]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qijian Tian, Xin Tan, Jiayu Ying, Xuhong Wang, Yuan Xie, Lizhuang Ma"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, East China Normal University, Shanghai Artificial Intelligence Laboratory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17541",children:"https://arxiv.org/pdf/2512.17541"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," FLEG is a feed-forward network that reconstructs language-embedded 3D Gaussians from arbitrary uncalibrated multi-view images without requiring 3D annotations. It uses instance-guided contrastive learning and hierarchical sparsification to align 2D semantics with 3D representations efficiently. The method outperforms existing approaches in generating accurate geometry, appearance, and language-aligned semantics from sparse or dense views."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [degradation-aware reasoning, structured reasoning chains, supervised fine-tuning, reward-driven alignment, dynamic reasoning depth scaling]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17532",children:"https://arxiv.org/pdf/2512.17532"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] G3Splat: Geometrically Consistent Generalizable Gaussian Splatting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [3D Gaussian splatting, geometrically consistent priors, view-synthesis loss, pose-free reconstruction, novel-view synthesis]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mehdi Hosseinzadeh, Shin-Fang Chng, Yi Xu, Simon Lucey, Ian Reid, Ravi Garg"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Australian Institute for Machine Learning, Goertek Alpha Labs, MBZUAI"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17547",children:"https://arxiv.org/pdf/2512.17547"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," G3Splat introduces geometric priors to address the ambiguity in learning 3D Gaussian splats from images under self-supervision, enabling geometrically consistent scene reconstruction without requiring camera poses. The method outperforms prior work in geometry recovery, relative pose estimation, and novel-view synthesis, demonstrating strong zero-shot generalization on datasets like ScanNet."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [clothing tailoring, body semantic estimation, body edge prediction, foundational human visual model (FHVM), 3D mesh recovery]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17545",children:"https://arxiv.org/pdf/2512.17545"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Attention U-Net, FLAIR hyperintensity segmentation, Dice score, Raidionics]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sj\xf6berg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Link\xf6ping University Hospital, Sk\xe5ne University Hospital, Ume\xe5 University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17566",children:"https://arxiv.org/pdf/2512.17566"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [diffusion model, parameter-sharing dual diffusion backbone, U-Net, DiT, inpainting, geometric coherence]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qilong Wang, Xiaofan Ming, Zhenyi Lin, Jinwen Li, Dongwei Ren, Wangmeng Zuo, Qinghua Hu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tianjin University, Harbin Institute of Technology"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17573",children:"https://arxiv.org/pdf/2512.17573"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes RoomEditor++, a diffusion-based architecture with a parameter-sharing dual diffusion backbone for high-fidelity virtual furniture synthesis. It introduces the RoomBench++ dataset for training and evaluation. Experiments show the method outperforms state-of-the-art approaches in metrics and human preference, demonstrating strong generalization."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Medical Imaging AI Competitions Lack Fairness"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical imaging], [benchmarking, fairness, FAIR principles, dataset bias, reproducibility, systematic review]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Annika Reinke, Evangelia Christodoulou, Sthuthi Sadananda, A. Emre Kavur, Khrystyna Faryna, Daan Schouten, Bennett A. Landman, Carole Sudre, Olivier Colliot, Nick Heller, Sophie Loizillon, Martin Ma\u0161ka, Ma\xeblys Solal, Arya Yazdan-Panah, Vilma Bozgo, \xd6mer S\xfcmer, Siem de Jong, Sophie Fischer, Michal Kozubek, Tim R\xe4dsch, Nadim Hammoud, Fruzsina Moln\xe1r-G\xe1bor, Steven Hicks, Michael A. Riegler, Anindo Saha, Vajira Thambawita, Pal Halvorsen, Amelia Jim\xe9nez-S\xe1nchez, Qingyang Yang, Veronika Cheplygina, Sabrina Bottazzi, Alexander Seitel, Spyridon Bakas, Alexandros Karargyris, Kiran Vaidhya Venkadesh, Bram van Ginneken, Lena Maier-Hein"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," German Cancer Research Center (DKFZ) Heidelberg"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17581",children:"https://arxiv.org/pdf/2512.17581"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper conducts a large-scale systematic study of 241 biomedical image analysis challenges to assess fairness in terms of dataset representativeness and accessibility. It finds substantial biases in dataset composition and restrictive access conditions, concluding that current benchmarks lack fairness and show a disconnect between leaderboard success and clinical relevance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Gaussian Discriminant Analysis, Z-score distance analysis, cluster-driven deep learning, two-stage framework]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Texas at El Paso, University of North Carolina"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17594",children:"https://arxiv.org/pdf/2512.17594"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] 3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [one-hot modulation, dual optical path, stochastic differential equation, video inpainting, one-step regression, one-step diffusion]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ge Wang, Xing Liu, Xin Yuan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Westlake University, Westlake Institute for Optoelectronics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17578",children:"https://arxiv.org/pdf/2512.17578"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel algorithm for video snapshot compressive imaging using one-hot modulation, which transforms the reconstruction into a video inpainting problem solved by combining a one-step regression initialization with a one-step diffusion refinement. To address spatial degradation, it introduces a dual optical path hardware design. Experiments show the method effectively reconstructs videos and is the first to integrate diffusion models into video SCI reconstruction."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [video anomaly detection], [attention heads, multi-criteria analysis, tuning-free, multimodal large language models (MLLMs), robust head identification]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhaolin Cai, Fan Li, Ziwei Zheng, Haixia Bi, Lijun He"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xinjiang University, Xi\u2019an Jiaotong University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17601",children:"https://arxiv.org/pdf/2512.17601"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes HeadHunt-VAD, a tuning-free video anomaly detection method that directly identifies and uses a sparse set of robust, anomaly-sensitive attention heads within a frozen Multimodal Large Language Model (MLLM), bypassing textual generation. It introduces a Robust Head Identification module to select expert heads based on saliency and stability across prompts, followed by a lightweight scorer for detection. The method achieves state-of-the-art performance among tuning-free approaches on major benchmarks, demonstrating the effectiveness of head-level probing in MLLMs for efficient and accurate anomaly detection."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical image registration], [mammography registration, anatomical landmarks, ANTs, VoxelMorph, TransMorph, IDIR, MammoRegNet, benchmark dataset]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Lomonosov Moscow State University, Third Opinion Platform"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17605",children:"https://arxiv.org/pdf/2512.17605"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [self-supervised learning, physics-informed deep learning, Bayesian maximum a posteriori inference, super-resolution, quantitative MRI relaxometry]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alireza Samadifardheris, Dirk H.J. Poot, Florian Wiesinger, Stefan Klein, Juan A. Hernandez-Tamames"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Erasmus MC, GE Healthcare, TU Delft"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17612",children:"https://arxiv.org/pdf/2512.17612"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a self-supervised, physics-informed deep learning framework for quantitative MRI super-resolution. It uses routinely acquired high-resolution weighted MRI scans as guidance to enhance low-resolution quantitative maps, eliminating the need for high-resolution ground truth during training. The method enables fast, high-quality quantitative MRI acquisitions, offering a practical pathway for clinical integration."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [semi-supervised learning, 3D segmentation, multi-output CNN, Slim UNETR, data augmentation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Denis Mikhailapov, Vladimir Berikov"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Sobolev Institute of Mathematics SB RAS"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17610",children:"https://arxiv.org/pdf/2512.17610"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a semi-supervised learning method for multi-output 3D CNNs, specifically using Slim UNETR, to segment aortic structures in Type-B Aortic Dissection. The method leverages data augmentation techniques like rotation and flipping to utilize both labeled and unlabeled data, overcoming the challenge of limited high-quality 3D annotations. It concludes that this approach is a universal and effective strategy for improving segmentation accuracy in medical imaging with complex, multi-class outputs."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [autonomous driving perception], [temporal stereo modeling, dynamic confidence gating, sparse query-based detection, multi-view 3D object detection]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Northwestern Polytechnical University, Suzhou University of Science and Technology"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17620",children:"https://arxiv.org/pdf/2512.17620"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," StereoMV2D integrates temporal stereo modeling into a 2D detection-guided multi-view 3D detector to enhance depth perception by exploiting cross-temporal disparities across adjacent frames. It refines query priors efficiently within 2D regions of interest and uses a dynamic confidence gating mechanism for robust detection under occlusion. The framework achieves superior performance on nuScenes and Argoverse 2 datasets without significant computational overhead."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [PathFLIP, fine-grained language-image pretraining, region-level subcaptions, text-conditioned region embeddings, visual-language grounding, large language models (LLMs), whole slide images (WSIs), computational pathology]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Shenzhen; National University of Singapore"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17621",children:"https://arxiv.org/pdf/2512.17621"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PathFLIP, a framework for computational pathology that decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings for fine-grained visual-language alignment. It leverages LLMs to follow clinical instructions and adapt to diagnostic contexts. Experiments show it outperforms existing pathological VLMs on multiple benchmarks while using less training data."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Region-Constraint In-Context Generation for Instructional Video Editing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [in-context generation, video diffusion, latent regularization, attention regularization, region-constraint, joint denoising]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, HiDream.ai Inc."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17650",children:"https://arxiv.org/pdf/2512.17650"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [computational behavioral measurement, video analysis, facial expression, head movement, body action, open-source toolkit, modularity, interpretability]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The Children's Hospital of Philadelphia, University of Pennsylvania, University of Pittsburgh"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17655",children:"https://arxiv.org/pdf/2512.17655"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Bitbox, an open-source behavioral imaging toolbox that provides a standardized interface for extracting high-level behavioral measurements from video using multiple face, head, and body processors. It is designed to bridge the translational gap by making advanced computational analysis accessible to behavioral and clinical researchers without requiring engineering expertise. The authors conclude that Bitbox will accelerate the integration of computational behavioral measurement into behavioral, clinical, and mental health research."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [cognitive steering conduit (CSC), hybrid interaction representations, hybrid guidance strategy, language modeling loss, auxiliary classification loss, open-vocabulary generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Xi'an Jiao Tong University, Shandong University, Tianjin University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17640",children:"https://arxiv.org/pdf/2512.17640"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes GRASP-HOI, a framework that reformulates human-object interaction detection as an open-vocabulary generation problem. It uses a lightweight cognitive steering module to inject visual features into a frozen multi-modal LLM for reasoning and employs a hybrid loss for training. This approach achieves state-of-the-art closed-set performance and strong zero-shot generalization."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [spatio-temporal feature representation, channel attention, self-attention, recurrent neural networks, video-based gaze estimation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alexandre Personnic, Mihai B\xe2ce"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KU Leuven"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17673",children:"https://arxiv.org/pdf/2512.17673"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [Diffusion Posterior Sampling (DPS), Manifold Constrained Gradient (MCG), conditioning step size, diffusion step count, ablation study]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yudhistira Arief Wibowo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Korea Advanced Institute of Science and Technology"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17675",children:"https://arxiv.org/pdf/2512.17675"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0]."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [semantic segmentation, monocular depth estimation, Time-to-Collision (TTC), Generalized Extreme Value (GEV) distribution, VideoLLaMA2, InternVL2.5 HiCo R16, domain adaptation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shaoyan Zhai, Mohamed Abdel-Aty, Chenzhu Wang, Rodrigo Vena Garcia"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Central Florida"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17724",children:"https://arxiv.org/pdf/2512.17724"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces SAVeD, a first-person video dataset from social media for analyzing ADAS vehicle near-misses and crashes. It proposes a framework using semantic segmentation and depth estimation to compute Time-to-Collision and uses extreme value theory to model risk. The dataset's annotations are shown to enhance the performance of video-language models through domain adaptation in complex scenarios."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [transformer-based reconstruction, 3D Gaussian Splatting, UV-space position maps, data distribution adjustment, lightweight UNet decoder]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Cheng Peng, Zhuo Su, Liao Wang, Chen Guo, Zhaohu Li, Chengjiang Long, Zheng Lv, Jingxiang Sun, Chenyangguang Zhang, Yebin Liu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Tsinghua University, ByteDance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17717",children:"https://arxiv.org/pdf/2512.17717"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," FlexAvatar is a flexible large reconstruction model that creates high-fidelity 3D head avatars from single or sparse images without camera poses or expression labels, using a transformer-based approach with structured head query tokens and a lightweight UNet decoder for real-time detailed deformations. It achieves superior 3D consistency and dynamic realism compared to previous methods, offering a practical solution for animatable avatar creation."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Mamba, multiple instance learning (MIL), overlapping scanning, selective stripe position encoder (S2PE), contextual token selection (CTS)]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Qian Zeng, Yihui Wang, Shu Yang, Yingxue Xu, Fengtao Zhou, Jiabo Ma, Dejia Cai, Zhengyu Zhang, Lijuan Qu, Yu Wang, Li Liang, Hao Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, Southern Medical University, 900th Hospital of Joint Logistic Support Force, PLA, Zhujiang Hospital, Southern Medical University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17726",children:"https://arxiv.org/pdf/2512.17726"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes MambaMIL+, a multiple instance learning framework that integrates spatial context modeling and long-range dependency for analyzing gigapixel whole-slide images. It introduces overlapping scanning, a selective stripe position encoder, and a contextual token selection mechanism to overcome memory decay and limited context in long sequences. The method achieves state-of-the-art performance across 20 benchmarks for diagnostic classification, molecular prediction, and survival analysis."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [deepfake detection], [CLIP, parameter-efficient transfer learning, textual prompts, visual adapters, layer ablation, diffusion models]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Waterloo, MBZUAI, University of Bergen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17730",children:"https://arxiv.org/pdf/2512.17730"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AdaptPrompt, a parameter-efficient framework that adapts the CLIP vision-language model for deepfake detection by jointly learning task-specific textual prompts and visual adapters while freezing the backbone. It also introduces the Diff-Gen dataset and shows that pruning the final transformer block of the vision encoder improves the retention of high-frequency artifacts. The method achieves state-of-the-art generalization across 25 test sets, including unseen generators, and demonstrates strong few-shot performance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [unsigned distance field, PCA, lightweight embedding, geodesic distance, shape correspondence]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yohanes Yudhi Adikusuma, Qixing Huang, Ying He"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin, Nanyang Technological University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17781",children:"https://arxiv.org/pdf/2512.17781"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LiteGE introduces a lightweight method for computing geodesic distances on 3D shapes by constructing compact shape descriptors using PCA on unsigned distance field samples. This approach eliminates the need for large neural networks, enabling significant reductions in memory usage and inference time while maintaining robustness on sparse point clouds. It also facilitates fast and accurate non-isometric shape correspondence, achieving up to 1000x speedup over state-of-the-art mesh-based methods."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [denoising diffusion, image inpainting, spatial gap-filling, pixel guided refinement, MODIS Terra LST]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Arya Chavoshi, Hassan Dashtian, Naveen Sudharsan, Dev Niyogi"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17782",children:"https://arxiv.org/pdf/2512.17782"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces UrbanDIFF, a denoising diffusion model for spatially reconstructing urban land surface temperature imagery obscured by dense clouds, using static urban structure data for conditioning and a pixel-guided refinement step. It demonstrates superior performance over baselines under high cloud coverage, with slower degradation as missing data increases."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [hybrid distortion model, neural network residual correction, stereo triangulation, camera calibration, long-range depth estimation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ami Pandat, Punna Rajasekhar, G.Aravamuthan, Gopika Vinod, Rohit Shukla"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Homi Bhabha National Institute, Bhabha Atomic Research Center"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17784",children:"https://arxiv.org/pdf/2512.17784"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid framework for long-range depth estimation by extending conventional camera distortion models with higher-order terms and then enhancing them using a neural network-based residual correction model. This approach improves 3D localization accuracy for distances up to 5 kilometers using CCTV cameras. The method is validated by transforming estimated 3D coordinates to GIS maps, offering a practical calibration solution for long-range photogrammetry."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [vision transformer, neural parametric head models, 3d morphable models, single-image 3d reconstruction, signed distance functions]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nie\xdfner"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Woven by Toyota, Toyota Motor Europe"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17773",children:"https://arxiv.org/pdf/2512.17773"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Animate Any Character in Any World"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [3DGS, conditional autoregressive video generation, pre-trained video generator, natural language control]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17796",children:"https://arxiv.org/pdf/2512.17796"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [3D Gaussian Splatting, multi-teacher pretraining, knowledge distillation, feed-forward encoder, open-vocabulary segmentation, render-and-distill]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"}),' University of Amsterdam, ETH Zurich, INSAIT (Sofia University "St. Kliment Ohridski"), Nanjing University of Aeronautics and Astronautics, University of Trento']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17817",children:"https://arxiv.org/pdf/2512.17817"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Chorus, a multi-teacher pretraining framework that learns a holistic 3D Gaussian Splatting scene encoder by distilling complementary signals from 2D foundation models. The method achieves strong performance on various 3D scene understanding tasks and demonstrates high data efficiency, requiring significantly fewer training scenes than point cloud baselines."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [autonomous coding agents, benchmark, end-to-end workflows, data preprocessing, model training, medical imaging competitions]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Harvard Medical School"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17838",children:"https://arxiv.org/pdf/2512.17838"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces ReX-MLE, a benchmark for evaluating autonomous coding agents on complex, end-to-end medical imaging challenges derived from real competitions. It finds that current state-of-the-art agents perform poorly, ranking near the 0th percentile compared to human experts, due to domain-knowledge and engineering limitations. The benchmark aims to expose these bottlenecks and guide the development of more capable, domain-aware autonomous AI systems."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [deep learning, noise modeling, cascaded neural network, simulation-driven framework, physics-informed learning]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengkun Chen, Sanidhya D. Tripathi, James W. Tunnell"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17852",children:"https://arxiv.org/pdf/2512.17852"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a simulation-driven deep learning framework that uses a comprehensive noise model to generate realistic Raman spectra for training a cascaded neural network. The network is designed to jointly suppress detector noise and fluorescence background. The results demonstrate that this physics-informed learning approach can improve spectral quality for faster and more accurate tissue analysis."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [cross-attention maps, inference-time optimization, compound loss, denoising step, spatial alignment]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17851",children:"https://arxiv.org/pdf/2512.17851"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Convolutional Neural Network (CNN), Attention Mechanism, CBAM, VGG16, Grad-CAM, Layer-wise Relevance Propagation (LRP)]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Balram Singh, Ram Prakash Sharma, Somnath Dey"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," National Institute of Technology Hamirpur, Indian Institute of Technology Indore"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17864",children:"https://arxiv.org/pdf/2512.17864"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [explainable AI], [Keypoint Counting Classifiers, Vision Transformers, self-explainable models, keypoint matching]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Kristoffer Wickstr\xf8m, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UiT The Arctic University of Norway, Technical University of Denmark"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17891",children:"https://arxiv.org/pdf/2512.17891"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Keypoint Counting Classifiers (KCCs), a method to convert any pre-trained Vision Transformer (ViT) into a self-explainable model without requiring retraining, by leveraging ViTs' ability to identify matching keypoints between images. The method creates an interpretable decision process that is directly visualizable. The authors conclude that KCCs improve human-machine communication and represent a step towards more transparent and reliable ViT-based foundation models."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Visually Prompted Benchmarks Are Surprisingly Fragile"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [visual prompting, benchmark evaluation, vision-language models, visual marker design, JPEG compression, dataset size, VPBench]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UC Berkeley"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17875",children:"https://arxiv.org/pdf/2512.17875"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] RadarGen: Automotive Radar Point Cloud Generation from Cameras"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [diffusion model, bird's-eye-view, radar cross section, Doppler, point cloud generation, foundation models]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technion, MIT, NVIDIA, University of Toronto, Vector Institute"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17897",children:"https://arxiv.org/pdf/2512.17897"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Diffusion Forcing for Multi-Agent Interaction Sequence Modeling"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent motion generation], [diffusion forcing, autoregressive diffusion, transformer, multi-agent interaction, denoising]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," UC Berkeley, Sony Group Corporation, Meta"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17900",children:"https://arxiv.org/pdf/2512.17900"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] InSPECT: Invariant Spectral Features Preservation of Diffusion Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [invariant spectral features, Fourier coefficients, feature-preserving diffusion, InSPECT, DDPM]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Columbia University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17873",children:"https://arxiv.org/pdf/2512.17873"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes InSPECT, a diffusion model that preserves invariant spectral features during the forward and backward processes, preventing the complete destruction of data into white noise. This approach leads to faster convergence, improved generation quality and diversity, and significant reductions in FID and improvements in IS compared to standard DDPM."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Adversarial Robustness of Vision in Open Foundation Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Projected Gradient Descent, adversarial robustness, Visual Question Answering, vision-language models]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jonathon Fox, William J Buchanan, Pavlos Papadopoulos"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Edinburgh Napier University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17902",children:"https://arxiv.org/pdf/2512.17902"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Vision Transformers, Pseudo-Color Enhancement, MRI, Multi-Class Classification]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Faisal Ahmed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Embry-Riddle Aeronautical University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16964",children:"https://arxiv.org/pdf/2512.16964"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer's disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Dexterous World Models"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [video diffusion, egocentric hand mesh rendering, hybrid interaction video dataset, scene-action-conditioned generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Seoul National University, RLWRLD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17907",children:"https://arxiv.org/pdf/2512.17907"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Dexterous World Models (DWM), a video diffusion framework that generates dynamic, egocentric videos of human-scene interactions by conditioning on static 3D scene renderings and hand motion sequences. It is trained on a hybrid dataset of synthetic and real-world videos. The method produces realistic and physically plausible interactions, representing a step toward interactive digital twins."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [test-time refinement, self-supervised learning, shape from shading, score distillation sampling, monocular depth estimation, diffusion models]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ananta R. Bhattarai, Helge Rhodin"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Bielefeld University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17908",children:"https://arxiv.org/pdf/2512.17908"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model's priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [latent diffusion models, representation encoders, semantic-pixel reconstruction, variational autoencoder, text-to-image generation, image editing]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Adobe Research, University of Chinese Academy of Sciences"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17909",children:"https://arxiv.org/pdf/2512.17909"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a framework to adapt discriminative representation encoders for generative tasks by introducing a semantic-pixel reconstruction objective, which compresses both semantic and fine-grained details into a compact latent space. The resulting model achieves state-of-the-art image reconstruction and enables unified text-to-image generation and editing, demonstrating that representation encoders can be effectively adapted into robust generative components."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion training], [StyleGAN2-ADA, Denoising Diffusion Probabilistic Models (DDPMs), FID, KID, Inception Score, ViT-B/16, synthetic data augmentation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Alliance University, \xd8stfold University College"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17585",children:"https://arxiv.org/pdf/2512.17585"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [medical imaging], [image registration, radiomics, deep learning, logistic regression, feature selection]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Nottingham, Nottingham City Hospital"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17759",children:"https://arxiv.org/pdf/2512.17759"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251222] MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [3D ConvNeXt, Global Response Normalization, depth scaling, width scaling, context scaling, supervised pretraining, volumetric segmentation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," German Cancer Research Center (DKFZ), Heidelberg University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17774",children:"https://arxiv.org/pdf/2512.17774"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network's architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Document Layout Analysis, Text-Editing Model, Fill in the Middle, hybrid editing-generation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Changxu Duan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Darmstadt"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18115",children:"https://arxiv.org/pdf/2512.18115"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces EditTrans, a hybrid editing-generation model that first identifies text requiring editing from a PDF using a Document Layout Analysis classifier before generating markup language. This approach reduces the need to regenerate dense text from scratch, significantly improving efficiency. Evaluations show EditTrans reduces transformation latency by up to 44.5% compared to end-to-end decoder transformer models while maintaining quality."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [pipeline parallelism, asynchronous execution, message-queue decoupling, graph compilation, mixed-precision quantization, kernel fusion, silence-detection, transformer]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Yildiz Technical University, Aktif Investment Bank Inc."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18318",children:"https://arxiv.org/pdf/2512.18318"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an asynchronous pipeline-parallel Transformer framework for real-time multilingual lip synchronization, which decouples translation, speech, and lip-sync modules using message queues and optimizes them with graph compilation and quantization. The method reduces end-to-end latency by up to 3.1 times compared to sequential approaches while maintaining accuracy, making it suitable for resource-constrained AIoT communication systems."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [cross-modal gated attention, positive-negative awareness attention, pseudo-matched pairs mitigation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Zhejiang University of Technology"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18660",children:"https://arxiv.org/pdf/2512.18660"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55a03600bbbf89f98190599bd29b1a2f12ec3a4ac217e9251256ba5b1f3c6a42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55a03600bbbf89f98190599bd29b1a2f12ec3a4ac217e9251256ba5b1f3c6a42_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PMPGuard, a retrieval framework that uses Cross-Modal Gated Attention and Positive-Negative Awareness Attention to handle noisy image-text pairs in remote sensing datasets. It dynamically regulates cross-modal information flow and distinguishes informative from misleading cues during alignment learning. Experiments on RSICD, RSITMD, and RS5M datasets show state-of-the-art performance, demonstrating robustness against pseudo-matched pairs."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [computer vision], [diffusion model, mixture-of-experts, tempo-aware, hierarchical routing, beat experts]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Xidian University, Hohai University, Institute for Infocomm Research (I2R), A*STAR"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18804",children:"https://arxiv.org/pdf/2512.18804"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4a50c7cb5fcadd432053a9cf11a5791010601ca6548cb28798420ee05830899_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4a50c7cb5fcadd432053a9cf11a5791010601ca6548cb28798420ee05830899_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TempoMoE, a hierarchical Mixture-of-Experts module for a diffusion model that uses tempo-structured motion experts and multi-scale beat experts to generate 3D dance from music. It dynamically routes and fuses these experts based on music features to achieve rhythm-aligned generation without needing genre labels. The method achieves state-of-the-art results in dance quality and rhythm synchronization."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [federated learning, differential privacy, VideoMAE, LoRA, DP-SGD, secure aggregation]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," The University of Sydney"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18809",children:"https://arxiv.org/pdf/2512.18809"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cebe9351bb53769823b1c88539e177d6fdfcc6f0486087e27fabdd74f958fb64_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cebe9351bb53769823b1c88539e177d6fdfcc6f0486087e27fabdd74f958fb64_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes FedVideoMAE, a federated learning framework for on-device video violence detection that integrates self-supervised VideoMAE, parameter-efficient LoRA adaptation, and privacy protection via DP-SGD and secure aggregation. It significantly reduces communication costs and trainable parameters while maintaining accuracy, demonstrating a viable privacy-preserving alternative to cloud-based video moderation."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [ai], [interpretable machine learning], [counterfactual explanations, cross-modal decompositionality, image-specific concepts, Decompose and Explain (DeX), multi-criterion selection, image privacy]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Alina Elena Baia, Andrea Cavallaro"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Not specified in provided text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18864",children:"https://arxiv.org/pdf/2512.18864"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces DeX, a training-free framework that uses cross-modal decompositionality and image-specific concepts to generate natural language counterfactual explanations for subjective image classification tasks. It identifies key decision factors and quantifies their contributions through a multi-criterion selection mechanism. The method uncovers principal decision factors and underlying dataset biases, enabling targeted strategies to improve fairness."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);