"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8311],{4833:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"daily/cs_LO/20251222-20251228","title":"20251222-20251228 (cs.LO)","description":"2025-12-22","source":"@site/docs/daily/cs_LO/20251222-20251228.md","sourceDirName":"daily/cs_LO","slug":"/daily/cslo/20251222-20251228","permalink":"/ai_toutiao/daily/cslo/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766477410000,"frontMatter":{"slug":"/daily/cslo/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.LO)","permalink":"/ai_toutiao/daily/cs_LO/20251215-20251221"},"next":{"title":"cs.MA","permalink":"/ai_toutiao/category/csma"}}');var s=n(4848),a=n(8453);const r={slug:"/daily/cslo/20251222-20251228"},o="20251222-20251228 (cs.LO)",l={},c=[{value:"2025-12-22",id:"2025-12-22",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251222-20251228-cslo",children:"20251222-20251228 (cs.LO)"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251222] Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [knowledge representation and reasoning], [entity set expansion, expansion graph, logical formula, semantic inclusion, computational complexity]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Pietro Cofone, Giovanni Amendola, Marco Manna, Aldo Ricioppo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Calabria, University of Cyprus"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16953",children:"https://arxiv.org/pdf/2512.16953"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a logic-based framework using expansion graphs, which are rooted directed acyclic graphs, to support taxonomic expansions of entity sets from knowledge bases. To avoid the impracticality of fully materializing these potentially large graphs, the authors formalize efficient reasoning tasks to check relationships between entity tuples within the graph structure. Their main conclusion is that, under realistic assumptions like bounded input, these tasks can be implemented efficiently, enabling local and incremental navigation without full graph construction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [ai], [reinforcement learning], [timed reward machines, tabular Q-learning, timed automata, counterfactual-imagining]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tata Institute of Fundamental Research, Max Planck Institute for Software Systems, University of Oxford"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.17637",children:"https://arxiv.org/pdf/2512.17637"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes timed reward machines (TRMs), an extension of reward machines that incorporates timing constraints into the reward specification for reinforcement learning. The authors develop model-free RL algorithms, specifically using tabular Q-learning integrated with abstractions of timed automata and counterfactual-imagining heuristics, to learn optimal policies. The experimental results show that their approach successfully learns policies that achieve high rewards while satisfying the specified timing constraints."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>r,x:()=>o});var t=n(6540);const s={},a=t.createContext(s);function r(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);