"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7964],{2305:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"daily/cs_GR/20251222-20251228","title":"20251222-20251228 (cs.GR)","description":"2025-12-22","source":"@site/docs/daily/cs_GR/20251222-20251228.md","sourceDirName":"daily/cs_GR","slug":"/daily/csgr/20251222-20251228","permalink":"/ai_toutiao/daily/csgr/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766994315000,"frontMatter":{"slug":"/daily/csgr/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.GR)","permalink":"/ai_toutiao/daily/cs_GR/20251215-20251221"},"next":{"title":"20251229-20260104 (cs.GR)","permalink":"/ai_toutiao/daily/csgr/20251229-20260104"}}');var a=i(4848),r=i(8453);const t={slug:"/daily/csgr/20251222-20251228"},d="20251222-20251228 (cs.GR)",l={},o=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2},{value:"2025-12-24",id:"2025-12-24",level:2},{value:"2025-12-25",id:"2025-12-25",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"20251222-20251228-csgr",children:"20251222-20251228 (cs.GR)"})}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"[arXiv251222] LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [unsigned distance field, PCA, lightweight embedding, geodesic distance, shape correspondence]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yohanes Yudhi Adikusuma, Qixing Huang, Ying He"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin, Nanyang Technological University"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.17781",children:"https://arxiv.org/pdf/2512.17781"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LiteGE introduces a lightweight method for computing geodesic distances on 3D shapes by constructing compact shape descriptors using PCA on unsigned distance field samples. This approach eliminates the need for large neural networks, enabling significant reductions in memory usage and inference time while maintaining robustness on sparse point clouds. It also facilitates fast and accurate non-isometric shape correspondence, achieving up to 1000x speedup over state-of-the-art mesh-based methods."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] MatSpray: Fusing 2D Material World Knowledge on 3D Geometry"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Philipp Langsteiner, Jan-Niklas Dihlmann, Hendrik P.A. Lensch"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18314",children:"https://arxiv.org/pdf/2512.18314"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," MatSpray: Fusing 2D Material World Knowledge on 3D Geometry"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhe Li, Kun Cheng, Hanyue Mo, Jintao Lu, Ziwen Kuang, Jianwen Ye, Lixu Xu, Xinya Meng, Jiahui Zhao, Shengda Ji, Shuyuan Liu, Mengyu Wang"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18597",children:"https://arxiv.org/pdf/2512.18597"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.18930",children:"https://arxiv.org/pdf/2512.18930"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.19390",children:"https://arxiv.org/pdf/2512.19390"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.19402",children:"https://arxiv.org/pdf/2512.19402"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251223] Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.19583",children:"https://arxiv.org/pdf/2512.19583"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-24",children:"2025-12-24"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251224] Generating the Past, Present and Future from a Motion-Blurred Image"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos, David B. Lindell"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.19817",children:"https://arxiv.org/pdf/2512.19817"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Generating the Past, Present and Future from a Motion-Blurred Image"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251224] Scaling Point-based Differentiable Rendering for Large-scale Reconstruction"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.20017",children:"https://arxiv.org/pdf/2512.20017"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Scaling Point-based Differentiable Rendering for Large-scale Reconstruction"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251224] Anisotropic Green Coordinates"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Dong Xiao, Renjie Chen, Bailin Deng"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.20386",children:"https://arxiv.org/pdf/2512.20386"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3142ca49761ee9bbf0364dc9bc86dd10c0898f07e22220a1a757c13e069379dc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3142ca49761ee9bbf0364dc9bc86dd10c0898f07e22220a1a757c13e069379dc_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," Anisotropic Green Coordinates"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251224] LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Vinayak Regmi, Christos Mousas"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.20550",children:"https://arxiv.org/pdf/2512.20550"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d09b76bd7e8408e9cbe2a9fb304bebde2b7918c5ae2ebfb71d37a7f10f36af5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d09b76bd7e8408e9cbe2a9fb304bebde2b7918c5ae2ebfb71d37a7f10f36af5_w640_q70.webp"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2025-12-25",children:"2025-12-25"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251225] Efficient Computation of Integer-constrained Cones for Conformal Parameterizations"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [geometric processing], [conformal parameterization, cone singularities, discrete optimization, integer-constrained angles, rotationally seamless]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Wei Du, Qing Fang, Ligang Liu, Xiao-Ming Fu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.20904",children:"https://arxiv.org/pdf/2512.20904"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. An explicit construction algorithm that drastically reduces the optimization problem scale for determining optimal cone angles, enabling efficient handling of high-genus surfaces., 2. A new derivative formula that allows for the effective movement of cone positions to reduce parameterization distortion., 3. A set of combined strategies (repositioning, adding, pairing cones, adaptive variable selection) to quickly achieve a favorable trade-off between cone count and distortion."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b941a6cb30753aa3bcef47884771c4a878dd646edafc8fcac32cf70713cc6388_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b941a6cb30753aa3bcef47884771c4a878dd646edafc8fcac32cf70713cc6388_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an efficient method to compute a small set of integer-constrained cone singularities for generating low-distortion, rotationally seamless conformal surface parameterizations. The method alternates optimization of discrete variables and employs novel techniques like an explicit construction algorithm and a new derivative formula to move cones, achieving an order-of-magnitude speedup over prior work while maintaining comparable quality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph LR\nA[Efficient Computation of Integer-constrained Cones] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: High distortion in conformal parameterizations)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Alternate optimization of discrete variables, explicit construction, derivative formula, combined strategies)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Order-of-magnitude speedup (30x), low distortion, rotationally seamless parameterizations)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [mlsys], [communication & networking], [4D Gaussian Splatting, video streaming, integer linear programming, pruning, keyframe selection]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Zhe Wang, Jinghang Li, Yifei Zhu"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.20943",children:"https://arxiv.org/pdf/2512.20943"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph LR\nA[AirGS: Real-Time 4D Gaussian Streaming] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: 4DGS\u8d28\u91cf\u4e0b\u964d\u4e0e\u9ad8\u5e26\u5bbd\u5f00\u9500/4DGS Quality Degradation & High Bandwidth Overhead]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6d41\u4f18\u5316\u6846\u67b6\u4e0e\u81ea\u9002\u5e94\u526a\u679d/Streaming-Optimized Framework & Adaptive Pruning]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u8d28\u91cf\u7a33\u5b9a\u3001\u8bad\u7ec3\u52a0\u901f\u3001\u4f20\u8f93\u51cf\u5c0f/Quality Stable, Training Faster, Transmission Smaller]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251225] A Design Study Process Model for Medical Visualization"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [other], [visualization], [design study, process model, medical visualization, visual analysis, interdisciplinary research]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Mengjie Fan, Liang Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University Health Science Center"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21034",children:"https://arxiv.org/pdf/2512.21034"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel design study process model specifically tailored for medical visualization, emphasizing stakeholder distinction, stage differentiation by analytic logic, and task classification. 2. Refines previous general visualization design models by incorporating characteristics of medical problems and providing actionable guidance for each step. 3. Demonstrates the model's utility by applying it to guide a new visual analysis method design and by reanalyzing three existing works, validating its practical framework."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5fe74c72d51a03dca133118f52d887d90317100c302c2b1d5fca972d1219ded_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5fe74c72d51a03dca133118f52d887d90317100c302c2b1d5fca972d1219ded_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a specialized design study process model for medical visualization, developed through literature review and interdisciplinary experience. The model emphasizes stakeholder analysis, task classification, and provides step-by-step guidance to make visualization design more targeted and adaptable to medical complexity. The authors demonstrate its application and argue it provides a systematic theoretical and practical framework for interdisciplinary medical visualization research."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph LR\nA[A Design Study Process Model for Medical Visualization<br>\u533b\u5b66\u53ef\u89c6\u5316\u8bbe\u8ba1\u7814\u7a76\u8fc7\u7a0b\u6a21\u578b] --\x3e B(Problem: Lack of systematic methodology for medical visualization design<br>\u6838\u5fc3\u95ee\u9898: \u7f3a\u4e4f\u7cfb\u7edf\u7684\u533b\u5b66\u53ef\u89c6\u5316\u8bbe\u8ba1\u65b9\u6cd5)\nA --\x3e C(Method: Propose a tailored design study process model<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51fa\u5b9a\u5236\u7684\u8bbe\u8ba1\u7814\u7a76\u8fc7\u7a0b\u6a21\u578b)\nA --\x3e D(Results: Model provides theoretical framework and practical guidance<br>\u5173\u952e\u7ed3\u679c: \u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u4e0e\u5b9e\u8df5\u6307\u5bfc)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251225] TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D avatar generation], [3D Gaussian Splatting, analytic rigging, texel-space deformation, hybrid representation, head reenactment]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," KAIST, Hanyang University"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21099",children:"https://arxiv.org/pdf/2512.21099"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph LR\n    A[TexAvatars] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee/Existing methods generalize poorly]\n    B --\x3e B2[\u96be\u4ee5\u5904\u7406\u6781\u7aef\u8868\u60c5\u4e0e\u59ff\u6001/Struggle with extreme expressions & poses]\n    C --\x3e C1[\u6df7\u5408\u8868\u793a/Hybrid Representation]\n    C --\x3e C2[UV\u7a7a\u95f4\u9884\u6d4b\uff0c3D\u7f51\u683c\u9a71\u52a8/UV-space prediction, 3D mesh-driven deformation]\n    D --\x3e D1[\u6cdb\u5316\u80fd\u529b\u63d0\u5347/Improved generalization]\n    D --\x3e D2[\u9ad8\u4fdd\u771f\u7ec6\u8282/High-fidelity details]\n    D --\x3e D3[\u72b6\u6001\u9886\u5148\u6027\u80fd/State-of-the-art performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"[arXiv251225] UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape generation], [diffusion models, geometric refinement, watertight processing, voxel-based refinement, RoPE]"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"authors:"})," Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"institution:"})," Peking University, The Hong Kong University of Science and Technology, National University of Singapore"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"link:"})," ",(0,a.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21185",children:"https://arxiv.org/pdf/2512.21185"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"contributions:"})," 1. A comprehensive data processing pipeline for 3D datasets, featuring a novel watertight processing method and high-quality filtering to improve geometric quality. 2. A two-stage 3D diffusion framework that decouples spatial localization from geometric detail synthesis, using voxel-based refinement with RoPE-encoded positional anchors. 3. Training and evaluation demonstrating competitive performance with existing open-source methods using only publicly available datasets and limited resources."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"thumbnail:"})," ",(0,a.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces UltraShape 1.0, a two-stage diffusion framework for generating high-fidelity 3D shapes. It first synthesizes a coarse structure and then refines it using a novel method that decouples spatial localization from detail synthesis via voxel queries and RoPE encoding. The approach, supported by an improved data processing pipeline, achieves competitive geometry generation quality using public datasets."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(n.mermaid,{value:"graph LR\nA[UltraShape 1.0] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: High-fidelity 3D shape generation \u9ad8\u4fdd\u771f3D\u5f62\u72b6\u751f\u6210)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Two-stage diffusion with geometric refinement \u4e24\u9636\u6bb5\u6269\u6563\u4e0e\u51e0\u4f55\u7ec6\u5316)\nC --\x3e C1(Stage 1: Coarse structure synthesis \u7c97\u7ed3\u6784\u5408\u6210)\nC --\x3e C2(Stage 2: Voxel-based detail refinement \u57fa\u4e8e\u4f53\u7d20\u7684\u7ec6\u8282\u7ec6\u5316)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Competitive generation quality \u5177\u6709\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>d});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);