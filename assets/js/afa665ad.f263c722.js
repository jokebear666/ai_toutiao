"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5951],{4921:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"daily/cs_GT/20251215-20251221","title":"20251215-20251221 (cs.GT)","description":"2025-12-18","source":"@site/docs/daily/cs_GT/20251215-20251221.md","sourceDirName":"daily/cs_GT","slug":"/daily/cs_GT/20251215-20251221","permalink":"/ai_toutiao/daily/cs_GT/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766642986000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.GT","permalink":"/ai_toutiao/category/csgt"},"next":{"title":"20251222-20251228 (cs.GT)","permalink":"/ai_toutiao/daily/csgt/20251222-20251228"}}');var r=n(4848),a=n(8453);const s={},l="20251215-20251221 (cs.GT)",o={},c=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function h(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"20251215-20251221-csgt",children:"20251215-20251221 (cs.GT)"})}),"\n",(0,r.jsx)(i.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"[arXiv251218] A Decision-Theoretic Approach for Managing Misalignment"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [ai], [decision theory], [value alignment, delegation, decision-theoretic framework, epistemic accuracy, reach, universal delegation, context-specific delegation]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Daniel A. Herrmann, Abinav Chari, Isabelle Qian, Sree Sharvesh, B. A. Levinstein"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," University of North Carolina at Chapel Hill, Georgia Institute of Technology, University of California, Berkeley, Amrita Vishwa Vidyapeetham, University of Illinois at Urbana-Champaign"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15584",children:"https://arxiv.org/pdf/2512.15584"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces a formal, decision-theoretic framework to analyze when to delegate decisions to an AI agent by balancing its value alignment, epistemic accuracy, and reach. It concludes that universal delegation requires near-perfect alignment, but context-specific delegation can be optimal even with significant misalignment if the agent's superior accuracy or expanded reach offers a net benefit."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [Shapley value, Direct Preference Optimization (DPO), data valuation, cooperative game theory, language model arithmetic]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," M\xe9lissa Tamine, Otmane Sakhi, Benjamin Heymann"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," Criteo AI Lab, Fairplay joint team"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.15765",children:"https://arxiv.org/pdf/2512.15765"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes an efficient method for computing Shapley values to value data contributions for LLM fine-tuning. It leverages the specific mathematical structure of Direct Preference Optimization (DPO) to enable scalable Shapley value approximation without requiring numerous model retrainings. The main conclusion is that this approach dramatically simplifies the computational challenge of data valuation for LLMs, unlocking applications in data markets and collaborative training."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [evolutionary game theory, replicator dynamics, trust mechanism, multi-agent systems, strategy equilibrium]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Shiduo Yang, Jiye Wang, Jiayu Qin, Jianbin Li, Yu Wang, Yuanhe Zhao, Kenan Guo"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," State Grid Corporation of China, North China Electric Power University"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16167",children:"https://arxiv.org/pdf/2512.16167"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes Ev-Trust, a trust mechanism based on evolutionary game theory that integrates direct trust, indirect trust, and expected revenue to guide agent behavior in LLM-based multi-agent systems. Theoretical analysis proves the stability of evolutionary equilibria, and experiments show the approach reduces malicious strategies and increases collective revenue in open service interactions."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.strong,{children:"[arXiv251219] Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game"})}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"tags:"})," [ai], [preference optimization], [Stackelberg game, sequential-move game, inference-time refinement, preference models, Nash equilibrium, Bradley-Terry model]"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"authors:"})," Barna P\xe1sztor, Thomas Kleine Buening, Andreas Krause"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"institution:"})," ETH Z\xfcrich"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"link:"})," ",(0,r.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.16626",children:"https://arxiv.org/pdf/2512.16626"})]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework that frames preference optimization as a sequential-move game between a Leader and a Follower policy. It demonstrates that this approach offers advantages in consistency, data sensitivity, and robustness to intransitive preferences compared to RLHF and NLHF. Experiments on large language models show that SLHF achieves strong alignment and enables inference-time refinements that transfer across model families."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>l});var t=n(6540);const r={},a=t.createContext(r);function s(e){const i=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(a.Provider,{value:i},e.children)}}}]);