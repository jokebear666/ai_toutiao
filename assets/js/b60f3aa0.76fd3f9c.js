"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[98],{3464:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/cs_PL/20251215-20251221","title":"20251215-20251221 (cs.PL)","description":"2025-12-18","source":"@site/docs/daily/cs_PL/20251215-20251221.md","sourceDirName":"daily/cs_PL","slug":"/daily/cs_PL/20251215-20251221","permalink":"/ai_toutiao/daily/cs_PL/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767008630000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.PL","permalink":"/ai_toutiao/category/cspl"},"next":{"title":"20251222-20251228 (cs.PL)","permalink":"/ai_toutiao/daily/cspl/20251222-20251228"}}');var s=i(4848),a=i(8453);const t={},o="20251215-20251221 (cs.PL)",l={},c=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function h(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251215-20251221-cspl",children:"20251215-20251221 (cs.PL)"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"[arXiv251218] Sharing State Between Prompts and Programs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [natural language programming, shared program state, natural function interface, interoperability, Nightjar]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ellie Y. Cheng, Logan Weber, Tian Jin, Michael Carbin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," MIT CSAIL"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14805",children:"https://arxiv.org/pdf/2512.14805"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces a programming abstraction called "shared program state" to enable seamless interoperability between natural language code (prompts) and formal program code (e.g., Python). It implements this abstraction in the Nightjar system, allowing natural code to directly read and write program variables. The results show that Nightjar programs can achieve higher task accuracy (+4-19%) and reduce lines of code by 39.6% on average, though with a runtime overhead of 0.4-4.3x compared to manual implementations.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15766",children:"https://arxiv.org/pdf/2512.15766"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251219] A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Hoare logic, weakest precondition reasoning, neurosymbolic AI, OpenJML, counterexample-guided repair]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Daragh King, Vasileios Koutavas, Laura Kovacs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Trinity College Dublin, Lero, TU Wien"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15816",children:"https://arxiv.org/pdf/2512.15816"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents NeuroInv, a neurosymbolic method for generating loop invariants that combines a neural module using LLMs and Hoare logic for backward-chaining weakest precondition reasoning with a symbolic module that iteratively repairs invariants using counterexamples from OpenJML. It achieves a 99.5% success rate on a benchmark of 150 Java programs and demonstrates scalability on complex multi-loop programs, substantially outperforming other approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative tool calls, tool cache, vLLM, prefix-caching]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Lawrence Livermore National Laboratory, University of Maryland"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15834",children:"https://arxiv.org/pdf/2512.15834"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new "tool cache" API to facilitate adoption of these optimizations.']}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);