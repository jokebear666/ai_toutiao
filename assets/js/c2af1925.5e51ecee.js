"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6916],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},88120:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_CV/20260105-20260111","title":"20260105-20260111 (cs.CV)","description":"2026-01-05","source":"@site/docs/daily/cs_CV/20260105-20260111.md","sourceDirName":"daily/cs_CV","slug":"/daily/cscv/20260105-20260111","permalink":"/ai_toutiao/daily/cscv/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/cscv/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.CV)","permalink":"/ai_toutiao/daily/cscv/20251229-20260104"},"next":{"title":"cs.CY","permalink":"/ai_toutiao/category/cscy"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/cscv/20260105-20260111"},o="20260105-20260111 (cs.CV)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260105-20260111-cscv",children:"20260105-20260111 (cs.CV)"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [generative AI, diffusion models, architectural intelligence, computational reasoning, vernacular architecture]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abolhassan Pishahang, Maryam Badiei"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida Atlantic University, North Carolina State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00029",children:"https://arxiv.org/pdf/2601.00029"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI\'s interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of "computational vernacular reasoning" as an analytical framework.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\nRoot["From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: How does generative AI interpret the architectural intelligence embedded in vernacular forms?"]\nRoot --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Test three diffusion models across three prompt stages (referential, adaptive, speculative) using a five-criteria evaluation framework."]\nRoot --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: AI reproduces geometry but misreads material/climatic reasoning; reference aids realism but limits creativity."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [diffusion models], [mode collapse, noise optimization, frequency characteristics, text-to-image generation, inference-time scaling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UC Berkeley, University of T\xfcbingen (T\xfcbingen AI Center), Technical University of Munich (MCML)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00090",children:"https://arxiv.org/pdf/2601.00090"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\nRoot("It\'s Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Mode collapse in text-to-image models")\nRoot --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Noise optimization with frequency analysis")\nRoot --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Improved generation diversity and quality")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [4D world model, autoregressive diffusion, Macro-from-Micro Planning (MMPL), Distribution Matching Distillation (DMD), generation-reconstruction-guidance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," TeleWorld Team (Institution inferred from corresponding author's email domain: ieee.org, but specific institution not explicitly stated in provided content. Likely an academic or research lab.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00051",children:"https://arxiv.org/pdf/2601.00051"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term memory in a closed-loop system. 2. Introduces a novel generation-reconstruction-guidance paradigm where a reconstructed 4D spatio-temporal representation guides subsequent generation for consistency. 3. Employs an autoregressive diffusion video model enhanced with Macro-from-Micro Planning (MMPL) and Distribution Matching Distillation (DMD) for efficient, long-horizon, real-time synthesis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents TeleWorld, a framework for building practical world models by integrating video generation and dynamic scene reconstruction into a closed-loop 4D system. It uses a novel generation-reconstruction-guidance paradigm and efficient planning/distillation techniques to achieve long-term consistency and real-time performance. The results demonstrate strong performance in world understanding and generation, advancing towards interactive, memory-enabled AI systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TeleWorld: 4D World Model<br/>TeleWorld: 4D\u4e16\u754c\u6a21\u578b] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Video models lack real-time interaction,<br/>long-horizon consistency, and memory<br/>\u89c6\u9891\u6a21\u578b\u7f3a\u4e4f\u5b9e\u65f6\u4ea4\u4e92\u3001<br/>\u957f\u65f6\u4e00\u81f4\u6027\u4e0e\u8bb0\u5fc6]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Generation-Reconstruction-Guidance paradigm<br/>with MMPL & DMD for real-time 4D synthesis<br/>\u751f\u6210-\u91cd\u5efa-\u5f15\u5bfc\u8303\u5f0f<br/>\u4f7f\u7528MMPL\u4e0eDMD\u5b9e\u73b0\u5b9e\u65f64D\u5408\u6210]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br/>Strong performance in static/dynamic<br/>understanding, consistency, and efficiency<br/>\u5728\u9759\u6001/\u52a8\u6001\u7406\u89e3\u3001<br/>\u4e00\u81f4\u6027\u4e0e\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal reasoning], [spatial intelligence, multimodal large language models, benchmark, spatiotemporal reasoning, evaluation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huawei Technologies, CUHK-Shenzhen, HKUST-GZ, Zhejiang University, Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00092",children:"https://arxiv.org/pdf/2601.00092"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://spatial4d-bench.github.io/spatial4d/",children:"https://spatial4d-bench.github.io/spatial4d/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Spatial4D-Bench, a large-scale benchmark with ~40,000 QA pairs for evaluating 4D spatial intelligence in MLLMs. 2. Systematically organizes 18 tasks into six cognitive categories for structured and comprehensive assessment. 3. Benchmarks state-of-the-art MLLMs, revealing their substantial limitations in 4D spatial reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Spatial4D-Bench, a large-scale benchmark designed to comprehensively evaluate the 4D spatial reasoning abilities of Multimodal Large Language Models (MLLMs). The benchmark covers 18 tasks across six cognitive categories. The evaluation reveals that current MLLMs have significant limitations in achieving human-level 4D spatial intelligence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SPATIAL4D-BENCH: A VERSATILE 4D SPATIAL INTELLIGENCE BENCHMARK] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8bc4\u4f30MLLMs\u76844D\u7a7a\u95f4\u667a\u80fd\u6c34\u5e73/Assess MLLMs' 4D spatial intelligence]\n    C --\x3e C1[\u6784\u5efa\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u57fa\u51c6/Build large-scale multi-task benchmark]\n    C --\x3e C2[\u8986\u76d618\u4e2a\u4efb\u52a1\uff0c6\u4e2a\u8ba4\u77e5\u7c7b\u522b/Cover 18 tasks, 6 cognitive categories]\n    D --\x3e D1[MLLMs\u57284D\u7a7a\u95f4\u63a8\u7406\u4e0a\u5b58\u5728\u663e\u8457\u5c40\u9650/MLLMs show substantial limitations in 4D spatial reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal image segmentation], [Synthetic Aperture Radar (SAR), Multispectral Imaging (MSI), Spatially Masked Adaptive Gated Network (SMAGNet), feature fusion, missing data robustness]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hyunho Lee, Wenwen Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00123",children:"https://arxiv.org/pdf/2601.00123"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SMAGNet, a novel multimodal deep learning model for post-flood water extent mapping that uses SAR as the primary input and adaptively integrates MSI data. 2. Introduces a method for handling incomplete or partially available MSI data, enhancing model applicability in real-world scenarios. 3. Demonstrates superior performance and robustness compared to other multimodal models, maintaining performance even when MSI data is completely missing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of integrating incomplete multispectral data with SAR imagery for post-flood water mapping. It proposes SMAGNet, a multimodal deep learning model that adaptively fuses features from both data sources. The model shows improved accuracy and robustness to missing data, making it more practical for real-world disaster response."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: How to adaptively integrate partially available MSI data with SAR for water mapping?"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose SMAGNet, a multimodal model using SAR as primary input with adaptive feature fusion."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms other models; robust to missing MSI data; enhances real-world applicability."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Explicit Abstention Knobs for Predictable Reliability in Video Question Answering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [selective prediction], [selective prediction, confidence-based abstention, risk-coverage tradeoff, distribution shift, video question answering]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jorge Ortiz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rutgers University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00138",children:"https://arxiv.org/pdf/2601.00138"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model's confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Explicit Abstention Knobs for Predictable Reliability in Video Question Answering] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Can confidence-based abstention provide reliable error rate control in video QA, especially under distribution shift?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use confidence thresholding on a VLM (Gemini 2.0 Flash) evaluated on the NExT-QA dataset, testing under evidence degradation.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: In-distribution control works, but confidence fails to decrease under shift, motivating warrant-based prediction.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Compressed Map Priors for 3D Perception"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D object detection], [spatial priors, binarized hashmap, map compression, nuScenes dataset, end-to-end training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Brady Zhou, Philipp Kr\xe4henb\xfchl"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UT Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00139",children:"https://arxiv.org/pdf/2601.00139"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Compressed Map Priors (CMP), a framework to learn spatial priors from historic traversals for 3D perception. 2. Introduces a highly efficient binarized hashmap representation requiring only 32KB/km\xb2, achieving a 20x memory reduction. 3. Demonstrates seamless integration into existing 3D perception architectures with minimal computational overhead, leading to consistent performance improvements on nuScenes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that autonomous vehicle perception systems ignore prior knowledge from repeated traversals of the same area. It proposes Compressed Map Priors (CMP), a framework that learns and stores spatial priors in a highly compressed binarized hashmap. The method integrates easily into existing systems with little computational cost and significantly improves 3D object detection performance on the nuScenes dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Compressed Map Priors for 3D Perception] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5ffd\u7565\u5386\u53f2\u904d\u5386\u7684\u5148\u9a8c\u77e5\u8bc6/Autonomous systems ignore prior knowledge from past traversals]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u538b\u7f29\u5730\u56fe\u5148\u9a8c(CMP)\u6846\u67b6\uff0c\u4f7f\u7528\u4e8c\u503c\u5316\u54c8\u5e0c\u56fe\u5b58\u50a8\u7a7a\u95f4\u5148\u9a8c/Propose CMP framework using binarized hashmap for spatial priors]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u51cf\u5c1120\u500d\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c3D\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347/20x memory reduction, low overhead, significant 3D detection improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics / ai-generated content detection], [GLASS, stratified sampling, global-local attention, high-resolution image detection, vision transformer]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lawrence Han"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent researcher (inferred from personal email domain)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00141",children:"https://arxiv.org/pdf/2601.00141"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," GitHub Project Code"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GLASS, a novel architecture that combines a globally resized view with multiple original-resolution local crops for AI-generated image detection. 2. Introduces a spatially stratified sampling method to efficiently select diverse, non-overlapping local regions from high-resolution images. 3. Demonstrates the integration and effectiveness of GLASS with various backbone models (ViT, ResNet, ConvNeXt), showing superior performance over standard transfer learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of losing fine-grained details when detecting AI-generated images due to standard downsampling. It proposes the GLASS architecture, which processes both a global resized view and multiple original-resolution local crops, aggregated via attention. Experiments show GLASS improves detection performance across different model backbones within practical computational limits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u9ad8\u5206\u8fa8\u7387AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\uff0c\u4e0b\u91c7\u6837\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e22\u5931/Fine-grained detail loss in high-resolution AI-generated image detection due to downsampling]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: GLASS\u67b6\u6784\uff0c\u7ed3\u5408\u5168\u5c40\u89c6\u56fe\u4e0e\u5206\u5c42\u91c7\u6837\u7684\u539f\u59cb\u5206\u8fa8\u7387\u5c40\u90e8\u88c1\u526a\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u805a\u5408/GLASS architecture combining global view & stratified-sampled original-resolution local crops with attention aggregation]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8d85\u8d8a\u6807\u51c6\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd/Outperforms standard transfer learning across various backbone models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [facial attribute analysis], [multi-attribute description, facial action units, vision-language model, fine-tuning, region-focal analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M.Jose, Han Hu, Xuri Ge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Glasgow, Shandong University, Institute of Computing Technology, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00156",children:"https://arxiv.org/pdf/2601.00156"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the novel problem (FaceFocalDesc) of generating multi-attribute natural language descriptions for arbitrarily selected face regions. 2. Constructs a new dataset with region-level annotations and natural language descriptions for this task. 3. Proposes the Focal-RegionFace model, a fine-tuned vision-language model that incrementally refines focus on localized features for interpretable multi-attribute analysis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a new task of generating fine-grained, multi-attribute descriptions for specific face regions. To address it, the authors create a new dataset and propose Focal-RegionFace, a model fine-tuned from Qwen2.5-VL that progressively focuses on local features. Experiments show the model achieves state-of-the-art performance on the new benchmark, demonstrating its effectiveness for region-focal facial analysis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u751f\u6210\u4efb\u610f\u9009\u5b9a\u9762\u90e8\u533a\u57df\u7684\u7ec6\u7c92\u5ea6\u591a\u5c5e\u6027\u63cf\u8ff0<br>Generating multi-attribute descriptions for arbitrarily selected face regions]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8eQwen2.5-VL\u7684\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u5fae\u8c03\u6a21\u578b<br>Multi-stage progressive fine-tuning of Qwen2.5-VL]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5728\u65b0\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd<br>Achieves best performance on the new benchmark]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal benchmark], [financial credit, multimodal AI, robustness evaluation, vision-language models, privacy-compliant dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Qifu Technology, Fudan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00150",children:"https://arxiv.org/pdf/2601.00150"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7f3a\u4e4f\u91d1\u878d\u4fe1\u8d37\u9886\u57df\u4e13\u7528\u591a\u6a21\u6001\u57fa\u51c6/Lack of domain-specific multimodal benchmark for financial credit]\n    C --\x3e C1[\u6784\u5efa\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210-\u91c7\u96c6\u7ba1\u9053/Build privacy-compliant synthesis-capture pipeline]\n    C --\x3e C2[\u8bbe\u8ba1\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]\n    D --\x3e D1[\u8bc4\u4f3023\u4e2aVLM/Evaluate 23 VLMs]\n    D --\x3e D2[Qfin-VL-Instruct\u6027\u80fd\u6700\u4f73/Qfin-VL-Instruct achieves top score]\n    D --\x3e D3[\u9c81\u68d2\u6027\u6311\u6218/Robustness remains a challenge]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [hybrid feature engineering, wavelet decomposition, graph-theoretic descriptors, linear separability, model compression]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Moirangthem Tiken Singh, Manibhushan Yaikhom"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00192",children:"https://arxiv.org/pdf/2601.00192"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 \xb5s latency) for real-time arrhythmia detection on edge devices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection<br>\u4f18\u5316\u7684\u6df7\u5408\u7279\u5f81\u5de5\u7a0b\u7528\u4e8e\u8d44\u6e90\u9ad8\u6548\u7684\u5fc3\u5f8b\u5931\u5e38\u68c0\u6d4b"] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B["Problem: Deep learning models are too heavy for edge devices.<br>\u6838\u5fc3\u95ee\u9898: \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927"]\n    C["Method: Hybrid feature engineering (wavelet + graph) for linear separability.<br>\u4e3b\u8981\u65b9\u6cd5: \u6df7\u5408\u7279\u5f81\u5de5\u7a0b\uff08\u5c0f\u6ce2+\u56fe\u8bba\uff09\u5b9e\u73b0\u7ebf\u6027\u53ef\u5206\u6027"]\n    D["Results: 98.44% accuracy, 8.54 KB model, 0.46 \xb5s latency.<br>\u5173\u952e\u7ed3\u679c: 98.44% \u51c6\u786e\u7387, 8.54 KB \u6a21\u578b, 0.46 \xb5s \u5ef6\u8fdf"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [underwater image restoration], [conditional generative adversarial network, hyperspectral imagery, underwater image formation equation, satellite imagery, PRISMA]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Deakin University, The University of Adelaide"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00194",children:"https://arxiv.org/pdf/2601.00194"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DichroGAN, a novel cGAN architecture for restoring in-air seafloor colours from satellite imagery. 2. Introduces a two-step simultaneous training process with four generators to model atmospheric radiance and underwater light transmission based on the image formation equation. 3. Demonstrates competitive performance on satellite and underwater datasets using a compact dataset derived from PRISMA imagery."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of restoring the true colours of the seafloor from satellite images, which are degraded by light absorption and scattering in water. The authors propose DichroGAN, a conditional GAN that uses a multi-generator architecture to estimate and remove underwater effects based on hyperspectral data. Experiments show the method achieves competitive results compared to state-of-the-art underwater restoration techniques."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Recovering in-air seafloor colours from degraded satellite imagery due to light attenuation in water.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: A cGAN (DichroGAN) with a two-step, four-generator architecture to estimate atmospheric radiance and underwater transmission.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Achieves competitive performance on satellite and underwater datasets.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D instance segmentation], [Neural Radiance Field (NeRF), 3D instance segmentation, crop counting, mask consistency, view synthesis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Ahmed Al Muzaddid, William J. Beksi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Arlington"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00207",children:"https://arxiv.org/pdf/2601.00207"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework for exact crop enumeration via 3D instance segmentation using multi-view images and NeRF. 2. Introduction of crop visibility and mask consistency scores to effectively segment instances in 3D. 3. Demonstration of consistent performance across diverse crops (cotton, apples, pears) without crop-specific parameter tuning and release of a new cotton plant dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces CropNeRF, a framework for accurate crop counting in agriculture. It uses multi-view 2D images and instance masks to train a Neural Radiance Field (NeRF), incorporating novel visibility and consistency scores to perform 3D instance segmentation and count crops. The method shows superior counting performance across different crop types and releases a new dataset to advance research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6237\u5916\u906e\u6321\u4e0e\u805a\u7c7b\u5bfc\u81f4\u8ba1\u6570\u56f0\u96be/Outdoor occlusions and clustering make counting hard]\n    C --\x3e C1[\u4f7f\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u4e0eNeRF\u8fdb\u884c3D\u5b9e\u4f8b\u5206\u5272/Use multi-view images & NeRF for 3D instance segmentation]\n    C --\x3e C2[\u5f15\u5165\u53ef\u89c1\u6027\u4e0e\u63a9\u7801\u4e00\u81f4\u6027\u5206\u6570/Introduce visibility & mask consistency scores]\n    D --\x3e D1[\u5728\u591a\u79cd\u4f5c\u7269\u4e0a\u5b9e\u73b0\u51c6\u786e\u8ba1\u6570/Achieve accurate counting on multiple crops]\n    D --\x3e D2[\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Outperforms state-of-the-art]\n    D --\x3e D3[\u8d21\u732e\u68c9\u82b1\u6570\u636e\u96c6/Contribute a cotton dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [domain adaptation], [style synthesis, contrastive learning, unpaired image translation, disentanglement]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Vanderbilt University, Siemens Healthineers, Johnson & Johnson Innovative Medicine"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00212",children:"https://arxiv.org/pdf/2601.00212"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/han-liu/IntraStyler",children:"https://github.com/han-liu/IntraStyler"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes IntraStyler, an exemplar-based style synthesis method for cross-modality domain adaptation that can capture diverse intra-domain styles without requiring prior knowledge of the variations. 2. Introduces a style encoder based on contrastive learning to discriminatively extract style-only features for guiding the synthesis. 3. Demonstrates the method's efficacy in controllable style synthesis and the benefits of diverse synthetic data for improving downstream segmentation performance on the CrossMoDA 2023 dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the under-explored issue of intra-domain variability in unsupervised domain adaptation by proposing IntraStyler, an exemplar-based method for synthesizing diverse target domain styles without prior knowledge. It uses a contrastive learning-based style encoder to extract style features and guide synthesis to match an exemplar's style. The method shows effective controllable style synthesis and improves downstream segmentation on a cross-modality medical imaging dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u57df\u5185\u591a\u6837\u6027 / Prior methods under-explore intra-domain variability]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u8303\u4f8b\u7684\u98ce\u683c\u5408\u6210 / Propose exemplar-based style synthesis]\n    C --\x3e C2[\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u98ce\u683c\u7f16\u7801\u5668 / Use contrastive learning style encoder]\n    D --\x3e D1[\u53ef\u63a7\u98ce\u683c\u5408\u6210\u6709\u6548 / Controllable style synthesis is effective]\n    D --\x3e D2[\u591a\u6837\u6570\u636e\u63d0\u5347\u5206\u5272\u6027\u80fd / Diverse data improves segmentation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D generation and morphing], [Structured Latent (SLAT), Morphing Cross-Attention (MCA), Temporal-Fused Self-Attention (TFSA), training-free, 3D morphing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanjing University, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00204",children:"https://arxiv.org/pdf/2601.00204"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A training-free framework (MorphAny3D) that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. 2. The introduction of Morphing Cross-Attention (MCA) for structural coherence and Temporal-Fused Self-Attention (TFSA) for temporal consistency. 3. An orientation correction strategy to mitigate pose ambiguity, enabling state-of-the-art morphing even for challenging cross-category cases."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of generating semantically consistent and temporally smooth 3D morphing sequences, especially across categories. It proposes MorphAny3D, a training-free framework that intelligently blends Structured Latent (SLAT) features within a 3D generator's attention mechanisms using novel MCA and TFSA modules. The method achieves state-of-the-art results and supports advanced applications like decoupled morphing and 3D style transfer."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: 3D Morphing Challenges)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: MorphAny3D Framework)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: State-of-the-Art Morphing)\n    B --\x3e B1("\u6311\u6218: \u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u65f6\u5e8f\u5e73\u6ed1\u6027 / Challenge: Semantic Consistency & Temporal Smoothness")\n    B --\x3e B2("\u5c24\u5176\u8de8\u7c7b\u522b / Especially Cross-Category")\n    C --\x3e C1("\u6838\u5fc3: \u7ed3\u6784\u5316\u6f5c\u5728\u8868\u793a / Core: Structured Latent (SLAT)")\n    C --\x3e C2("\u8bad\u7ec3\u65e0\u5173 / Training-Free")\n    C --\x3e C3("\u5173\u952e\u6a21\u5757: MCA & TFSA / Key Modules: MCA & TFSA")\n    D --\x3e D1("\u9ad8\u8d28\u91cf\u53d8\u5f62\u5e8f\u5217 / High-Quality Morphing Sequences")\n    D --\x3e D2("\u652f\u6301\u9ad8\u7ea7\u5e94\u7528 / Supports Advanced Applications")\n    D --\x3e D3("\u6cdb\u5316\u81f3\u5176\u4ed6\u6a21\u578b / Generalizable to Other Models")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, multimodal large language models, visual reasoning, group relative policy optimization, reward functions]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Omar Sharif, Eftekhar Hossain, Patrick Ng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dartmouth College, University of Central Florida"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00215",children:"https://arxiv.org/pdf/2601.00215"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: MLLMs lack visual grounding in reasoning"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: RL with reward functions & GRPO"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: 5.56% improvement on Qwen-2.5-VL-7B"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [model compression (quantization/pruning)], [vector quantization, compositional codebook, extrapolation-by-interpolation, codebook collapse, low-dimensional codevectors]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jie Li, Kwan-Yee K. Wong, Kai Han"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00222",children:"https://arxiv.org/pdf/2601.00222"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a parameter-efficient, low-dimensional compositional codebook that treats codevectors as compositional units, expanding the solution space and enabling a more compact representation. 2. Incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance feature smoothing and detail preservation during quantization. 3. Functions as a plug-and-play module for existing VQ-based methods, achieving state-of-the-art performance with full codebook usage and avoidance of collapse."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes LooC, a new vector quantization method that uses a low-dimensional, compositional codebook to achieve high capacity with a compact size. It introduces a novel way to combine codevectors and a feature smoothing mechanism, leading to better performance and full codebook utilization. Extensive evaluations show LooC outperforms existing VQ methods with a significantly smaller codebook."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Need for high-capacity yet compact VQ methods")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Low-dimensional compositional codebook & extrapolation-by-interpolation")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: SOTA performance with smaller codebook, plug-and-play")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image quality assessment], [synthetic data distribution, generalization error, distribution reshaping, upsampling, downsampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xidian University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00225",children:"https://arxiv.org/pdf/2601.00225"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Li-aobo/SynDR-IQA",children:"https://github.com/Li-aobo/SynDR-IQA"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified a key issue where models trained on synthetic IQA data learn discrete, clustered feature representations that hinder regression performance. 2. Proposed a novel framework, SynDR-IQA, which reshapes synthetic data distribution based on theoretical analysis of sample diversity and redundancy's impact on generalization. 3. Introduced two core strategies: distribution-aware diverse content upsampling to enhance visual diversity, and density-aware redundant cluster downsampling to balance sample density."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited generalization of Blind Image Quality Assessment (BIQA) models trained on synthetic data by identifying that the problem stems from the clustered distribution of synthetic data features. The authors propose the SynDR-IQA framework, which reshapes the data distribution through strategic upsampling and downsampling to improve sample diversity and balance. Extensive experiments across multiple cross-dataset settings demonstrate the effectiveness of their method in enhancing model generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Syn-to-Real IQA<br>\u91cd\u5851\u5408\u6210\u6570\u636e\u5206\u5e03] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited generalization of BIQA models trained on synthetic data due to clustered feature distributions]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>SynDR-IQA framework reshapes data distribution via diverse content upsampling and redundant cluster downsampling]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Method improves generalization across synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chao Yang, Haoyuan Zheng, Yue Ma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi\u2019an Jiaotong Liverpool University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00237",children:"https://arxiv.org/pdf/2601.00237"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: \u7ea2\u5916\u6570\u636e\u7a00\u7f3a / IR Data Scarcity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: CycleGAN\u8de8\u6a21\u6001\u751f\u6210 + YOLOv8\u68c0\u6d4b / CycleGAN Cross-modal Generation + YOLOv8 Detection"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63a5\u8fd1\u5168\u76d1\u7763\u57fa\u51c6 / Performance Significantly Improved, Approaches Fully Supervised Benchmark"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [few-shot object detection], [lightweight CNN, prototypical meta-learning, decision support system, precision agriculture, pest recognition]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anirudha Ghosh, Ritam Sarkar, Debaditya Barman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Visva-Bharati, Uttar Banga Krishi Viswavidyalaya"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00243",children:"https://arxiv.org/pdf/2601.00243"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A lightweight framework for pest detection and pesticide recommendation designed for low-resource devices like smartphones and drones. 2. A Pest Detection Module using a compact CNN with prototypical meta-learning for accurate identification with few training samples. 3. A Pesticide Recommendation Module that integrates environmental factors (crop type, growth stage) to suggest safe and eco-friendly pesticides."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a lightweight framework for precision agriculture that combines a few-shot pest detection model with a context-aware pesticide recommendation system. The method uses a compact CNN with prototypical learning for detection and a rule-based system incorporating environmental factors for recommendations. The framework achieves high accuracy with low computational cost, demonstrating potential for real-time use on resource-constrained devices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u5bb3\u866b\u7ba1\u7406\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u3001\u4e0d\u73af\u4fdd/Traditional pest management is costly, inefficient, and harmful]\n    C --\x3e C1[\u8f7b\u91cf\u7ea7\u6846\u67b6/Lightweight Framework]\n    C1 --\x3e C1_1[\u5bb3\u866b\u68c0\u6d4b\u6a21\u5757: \u8f7b\u91cfCNN+\u539f\u578b\u7f51\u7edc/Pest Detection: Lightweight CNN + Prototypical Network]\n    C1 --\x3e C1_2[\u519c\u836f\u63a8\u8350\u6a21\u5757: \u73af\u5883\u611f\u77e5\u51b3\u7b56\u7cfb\u7edf/Pesticide Recommendation: Context-Aware Decision System]\n    D --\x3e D1[\u9ad8\u7cbe\u5ea6\uff0c\u4f4e\u8ba1\u7b97\u6210\u672c/High accuracy, low computational cost]\n    D --\x3e D2[\u9002\u7528\u4e8e\u667a\u80fd\u624b\u673a\u548c\u65e0\u4eba\u673a/Suitable for smartphones and drones]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [wireless networking], [O-RAN, UAV, trajectory optimization, RIC, low-altitude economy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aly Sabri Abdalla, Vuk Marojevic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mississippi State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00257",children:"https://arxiv.org/pdf/2601.00257"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments"] --\x3e P1["\u5b50\u95ee\u9898/Sub-Problem: Absence of AI-integrated, context-aware control for LAE"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: O-RAN-enabled LAE framework with AI-driven RICs"] --\x3e M1["\u7ec4\u4ef6/Component: Semantic-aware rApp (terrain interpreter)"]\n    Method --\x3e M2["\u7ec4\u4ef6/Component: RL-enabled xApp (trajectory planner)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results: Framework enables closed-loop, AI-optimized LAE operations"] --\x3e R1["\u8bc4\u4f30/Evaluation: Feasibility and performance analysis presented"]\n    Results --\x3e R2["\u5c55\u671b/Outlook: Research challenges and standardization needs surveyed"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [3D-CT, vision-language model, organ separation, contrastive learning, zero-shot classification]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kohei Yamamoto, Tomohiro Kikuchi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Jichi Medical University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00260",children:"https://arxiv.org/pdf/2601.00260"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TotalFM, a 3D-CT vision foundation model based on an organ-separated learning framework to balance computational efficiency and representation capability. 2. Introduces an automated pipeline for creating organ volume and finding-sentence pairs using segmentation and LLM-based report processing. 3. Demonstrates superior zero-shot performance in organ-wise and finding-wise lesion classification compared to baselines like CT-CLIP and Merlin."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TotalFM, a 3D-CT vision foundation model that uses an organ-separated framework and combines self-supervised pre-training with contrastive learning to efficiently align volumetric images with text. It shows strong zero-shot classification performance on clinical tasks, outperforming existing models, and demonstrates the framework's effectiveness for practical 3D-CT model implementation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: 3D-CT\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u9ad8/High computational cost for training 3D-CT foundation models)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u5668\u5b98\u5206\u79bb\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60/Organ-separated framework, combining self-supervised pre-training and contrastive learning)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728\u96f6\u6837\u672c\u5668\u5b98\u548c\u75c5\u7076\u5206\u7c7b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u8d8a/Superior performance in zero-shot organ-wise and finding-wise lesion classification tasks)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal dataset], [multimodal learning, image-text alignment, semantic enhancement, Qwen-VL, CLIP score]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00264",children:"https://arxiv.org/pdf/2601.00264"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign",children:"https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces S1-MMAlign, a large-scale, multi-disciplinary dataset with over 15.5 million scientific image-text pairs from 2.5 million papers. 2. Proposes an AI-ready semantic enhancement pipeline using the Qwen-VL model to recaption images by synthesizing context from abstracts and citations, improving alignment. 3. Demonstrates significant data quality improvement via technical validation, showing reduced semantic ambiguity and an 18.21% increase in CLIP scores for image-text alignment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces S1-MMAlign, a large-scale multimodal dataset for scientific figure-text understanding, addressing the semantic gap between complex scientific imagery and sparse textual descriptions. It proposes a semantic enhancement pipeline using the Qwen-VL model to recaption images by integrating context from paper abstracts and citations, which significantly improves image-text alignment as validated by CLIP scores. The dataset serves as a foundational resource for advancing scientific reasoning and cross-modal understanding in AI for Science."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[S1-MMAlign: \u5927\u89c4\u6a21\u8de8\u5b66\u79d1\u6570\u636e\u96c6<br>S1-MMAlign: Large-Scale, Multi-Disciplinary Dataset] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u79d1\u5b66\u56fe\u50cf\u4e0e\u6587\u672c\u8bed\u4e49\u9e3f\u6c9f<br>Scientific Figure-Text Semantic Gap)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8eQwen-VL\u7684\u8bed\u4e49\u589e\u5f3a\u7ba1\u9053<br>Qwen-VL-based Semantic Enhancement Pipeline)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u63d0\u534718.21%<br>18.21% Improvement in Image-Text Alignment)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [concept erasure, activation patching, training-free, text-to-image, adversarial robustness]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Shenzhen; Tsinghua Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00267",children:"https://arxiv.org/pdf/2601.00267"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel training-free paradigm for concept erasure in diffusion models, eliminating the need for data-intensive fine-tuning. 2. Introduces a method that identifies and patches activation differences via prompt-pair analysis to precisely remove target concepts. 3. Demonstrates state-of-the-art performance across multiple erasure tasks while preserving model capability and showing robustness against adversarial attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ActErase, a training-free method for erasing sensitive concepts from text-to-image diffusion models. It works by identifying and patching activation differences during inference, avoiding costly fine-tuning. The method achieves strong erasure performance, maintains general generation quality, and is robust to attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u5185\u5bb9 / Diffusion models generate inappropriate content]\n    B --\x3e B2[\u73b0\u6709\u64e6\u9664\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u5fae\u8c03 / Existing erasure relies on costly fine-tuning]\n    C --\x3e C1[\u8bc6\u522b\u6fc0\u6d3b\u5dee\u5f02\u533a\u57df / Identify activation difference regions]\n    C --\x3e C2[\u63d0\u53d6\u5e76\u52a8\u6001\u66ff\u6362\u76ee\u6807\u6fc0\u6d3b / Extract and dynamically replace target activations]\n    D --\x3e D1[SOTA\u64e6\u9664\u6027\u80fd / SOTA erasure performance]\n    D --\x3e D2[\u4fdd\u6301\u751f\u6210\u80fd\u529b / Preserves generative capability]\n    D --\x3e D3[\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027 / Robust against adversarial attacks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [hallucination detection, vision-language models, uncertainty estimation, model-driven learning, LLM-as-a-Judge]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00269",children:"https://arxiv.org/pdf/2601.00269"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FaithSCAN: Faithful VQA Hallucination Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VQA\u6a21\u578b\u4ea7\u751f\u6d41\u7545\u4f46\u89c6\u89c9\u65e0\u6839\u636e\u7684\u7b54\u6848/Faithfulness Hallucinations in VQA]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5229\u7528VLM\u5185\u90e8\u4fe1\u53f7\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408/Exploit VLM Internal Signals & Uncertainty-Aware Fusion]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u9ad8\u6548\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/More Effective & Efficient than Prior Methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [long-tailed classification], [uncertainty estimation, evidential deep learning, remote sensing, long-tailed distribution, adaptive label smoothing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chi Ding, Junxiao Xue, Xinyi Yin, Shi Chen, Yunyun Shi, Yiduo Wang, Fengjian Xue, Xuecheng Wu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang Lab, Zhengzhou University, Xi'an Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00278",children:"https://arxiv.org/pdf/2601.00278"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a model-agnostic framework (DUAL) that disentangles prediction uncertainty into epistemic and aleatoric types to address long-tailed classification. 2. Introduces epistemic uncertainty to guide a reweighting strategy for hard-to-learn tail samples. 3. Leverages aleatoric uncertainty to quantify data ambiguity and employs an adaptive label smoothing mechanism to suppress noise."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of distinguishing hard-to-learn samples from noisy ones in long-tailed remote sensing image classification. The authors propose DUAL, an uncertainty-aware framework that uses epistemic uncertainty to reweight tail samples and aleatoric uncertainty to apply adaptive label smoothing for noise suppression. Experiments show the framework outperforms strong baselines and generalizes across different datasets and backbones."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Long-tailed distributions in remote sensing; need to disentangle hard tail samples from noisy ones."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose DUAL framework using Evidential Deep Learning to separate epistemic (for reweighting) and aleatoric (for label smoothing) uncertainty."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines (TGN, SADE); effective and generalizable across datasets and backbones."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [4D reconstruction], [Gaussian Splatting, Sparse View, Skeleton-Driven, Deformation Field, Motion Interpolation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jun-Jee Chao, Volkan Isler"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Minnesota, The University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00285",children:"https://arxiv.org/pdf/2601.00285"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A framework (SV-GS) for 4D reconstruction from sparse observations in both time and viewpoint, using a skeleton-driven Gaussian Splatting approach. 2. A novel skeleton-driven deformation field with a time-dependent joint pose estimator and a separate fine-grained deformation module, enabling smooth motion interpolation. 3. Demonstrating that the method's initial static reconstruction input can be replaced by a diffusion-based generative prior, enhancing practical applicability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenging problem of 4D reconstruction from sparse observations in time and viewpoint. It proposes SV-GS, a method that uses a skeleton-driven deformation field with Gaussian Splatting to simultaneously estimate motion and geometry. The approach outperforms existing methods under sparse settings and shows practical potential by relaxing input requirements with generative priors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: 4D reconstruction from sparse observations in time and viewpoint")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Skeleton-driven deformation field with Gaussian Splatting")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines under sparse views; comparable to dense methods with fewer frames; works with diffusion priors")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Swin Transformer, BatchFormer, Focal Loss, ReduceLROnPlateau, ISIC2019]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Technology Sydney, University of Sydney, Shaoyang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00286",children:"https://arxiv.org/pdf/2601.00286"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model's potential as a clinical support tool."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Automated Differential Diagnosis of Skin Diseases<br>\u76ae\u80a4\u75be\u75c5\u81ea\u52a8\u9274\u522b\u8bca\u65ad] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Limited dermatologist access & need for diagnostic tools<br>\u76ae\u80a4\u79d1\u533b\u751f\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u8bca\u65ad\u5de5\u5177]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Deep learning (Swin Transformer) with imbalance-aware strategies<br>\u6df1\u5ea6\u5b66\u4e60\uff08Swin Transformer\uff09\u4e0e\u4e0d\u5e73\u8861\u611f\u77e5\u7b56\u7565]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>87.71% accuracy on ISIC2019 dataset<br>\u5728ISIC2019\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.71%\u51c6\u786e\u7387]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] TimeColor: Flexible Reference Colorization via Temporal Concatenation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video colorization], [diffusion models, temporal concatenation, spatiotemporal correspondence-masked attention, modality-disjoint RoPE indexing, sketch-based colorization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bryan Constantine Sadihin, Yihao Meng, Michael Hua Wang, Matteo Jiahao Chen, Hang Su"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University, HKUST"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00296",children:"https://arxiv.org/pdf/2601.00296"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://bconstantine.github.io/TimeColor/",children:"https://bconstantine.github.io/TimeColor/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a method to support heterogeneous, variable-count references for video colorization via temporal concatenation of reference latents, keeping model parameters fixed. 2. Introduces spatiotemporal correspondence-masked attention and modality-disjoint RoPE indexing to enforce subject-reference binding and prevent shortcutting and palette leakage. 3. Demonstrates improved color fidelity, identity consistency, and temporal stability over prior baselines on the SAKUGA-42M dataset under single- and multi-reference protocols."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of existing video colorization models that rely on a single reference frame, which restricts the use of diverse references like character sheets. It proposes TimeColor, a sketch-based video colorization model that uses temporal concatenation to incorporate multiple references and novel attention mechanisms to bind subjects to references, improving color consistency and stability. Experiments show TimeColor outperforms prior methods in color fidelity and temporal coherence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TimeColor: Flexible Reference Colorization via Temporal Concatenation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4ec5\u652f\u6301\u5355\u53c2\u8003\u5e27/Existing methods condition on single reference]\n    B --\x3e B2[\u5ffd\u7565\u5176\u4ed6\u53c2\u8003\u6e90/Ignore other references (e.g., character sheets)]\n    C --\x3e C1[\u65f6\u95f4\u62fc\u63a5\u53c2\u8003\u9690\u53d8\u91cf/Temporally concatenate reference latents]\n    C --\x3e C2[\u65f6\u7a7a\u5bf9\u5e94\u63a9\u7801\u6ce8\u610f\u529b/Spatiotemporal correspondence-masked attention]\n    C --\x3e C3[\u6a21\u6001\u5206\u79bbRoPE\u7d22\u5f15/Modality-disjoint RoPE indexing]\n    D --\x3e D1[\u63d0\u5347\u989c\u8272\u4fdd\u771f\u5ea6/Improves color fidelity]\n    D --\x3e D2[\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027/Improves identity consistency]\n    D --\x3e D3[\u63d0\u5347\u65f6\u95f4\u7a33\u5b9a\u6027/Improves temporal stability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [person re-identification], [feature fusion, alpha-divergence loss, dynamic multi-task learning, semantic clustering, computational efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anns Ijaz, Muhammad Azeem Javed"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Management and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00307",children:"https://arxiv.org/pdf/2601.00307"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[VisNet: Efficient Person Re-Identification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Accuracy vs. Computational Cost Trade-off]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Feature Fusion, Semantic Clustering, Dynamic Multi-Task Learning, \u03b1-Divergence Loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 87.05% Rank-1, 77.65% mAP, 4.601 GFLOPs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video behavior recognition], [data augmentation, representation-aware mixing, spatiotemporal coherence, plug-and-play, motion-aware masking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Feng-Qi Cui, Jinyang Huang, Sirui Zhao, Jinglong Guo, Qifan Cai, Xin Yan, Zhi Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Hefei University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00311",children:"https://arxiv.org/pdf/2601.00311"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel plug-and-play, training-free augmentation strategy (ReMA) that formulates video mixing as a controlled replacement process to expand representations while preserving class-conditional stability. 2. Introduces a Representation Alignment Mechanism (RAM) to perform structured intra-class mixing under distributional constraints, suppressing irrelevant intra-class drift. 3. Introduces a Dynamic Selection Mechanism (DSM) to generate motion-aware spatiotemporal masks, localizing perturbations away from discrimination-sensitive regions to promote temporal coherence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that current perturbation-driven video data augmentation methods introduce uncontrolled variations that harm representation stability. To solve this, it proposes ReMA, a training-free plug-and-play method that uses representation alignment and dynamic spatiotemporal masking to control how and where mixing is applied. Experiments show ReMA consistently improves model generalization and robustness across various video behavior benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ReMA: \u89c6\u9891\u884c\u4e3a\u8bc6\u522b\u7684\u5373\u63d2\u5373\u7528\u6df7\u5408\u589e\u5f3a] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u89c6\u9891\u589e\u5f3a\u5f15\u5165\u975e\u5224\u522b\u6027\u53d8\u5316\uff0c\u7834\u574f\u7c7b\u5185\u5206\u5e03\u548c\u65f6\u5e8f\u4e00\u81f4\u6027]\n    C --\x3e C1[\u8868\u793a\u5bf9\u9f50\u673a\u5236 / Representation Alignment Mechanism (RAM)]\n    C --\x3e C2[\u52a8\u6001\u9009\u62e9\u673a\u5236 / Dynamic Selection Mechanism (DSM)]\n    D --\x3e D1[\u63d0\u5347\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027 / Improves generalization and robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image reflection separation], [Mamba, state-space model, depth-aware, memory expert, nighttime dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Siyan Fang, Long Peng, Yuntao Wang, Ruonan Wei, Yuehuan Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, University of Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00322",children:"https://arxiv.org/pdf/2601.00322"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/fashyon/DMDNet",children:"https://github.com/fashyon/DMDNet"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Depth-Memory Decoupling Network (DMDNet) with Depth-Aware Scanning (DAScan) and a Depth-Synergized State-Space Model (DS-SSM) to guide Mamba for better layer disentanglement. 2. Introduced a Memory Expert Compensation Module (MECM) that leverages historical knowledge to provide layer-specific compensation. 3. Constructed a new Nighttime Image Reflection Separation (NightIRS) dataset to address the lack of data for nighttime scenes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenging problem of separating reflection and transmission layers from a single blended image, especially in low-contrast nighttime scenes. The authors propose DMDNet, a novel network that integrates depth-aware guidance into a Mamba-based state-space model and utilizes a memory expert module for compensation. The method, evaluated on a newly created nighttime dataset, is shown to outperform existing state-of-the-art approaches for both daytime and nighttime reflection separation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5355\u56fe\u50cf\u4fe1\u606f\u6709\u9650\uff0c\u591c\u95f4\u5206\u79bb\u56f0\u96be/Single image info limited, severe at night]\n    C --\x3e C1[\u6df1\u5ea6\u8bb0\u5fc6\u89e3\u8026\u7f51\u7edc DMDNet/Depth-Memory Decoupling Network DMDNet]\n    C1 --\x3e C2[\u6df1\u5ea6\u611f\u77e5\u626b\u63cf DAScan/Depth-Aware Scanning DAScan]\n    C1 --\x3e C3[\u6df1\u5ea6\u534f\u540c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b DS-SSM/Depth-Synergized State-Space Model DS-SSM]\n    C1 --\x3e C4[\u8bb0\u5fc6\u4e13\u5bb6\u8865\u507f\u6a21\u5757 MECM/Memory Expert Compensation Module MECM]\n    C --\x3e C5[\u6784\u5efa\u591c\u95f4\u6570\u636e\u96c6 NightIRS/Construct NightIRS dataset]\n    D --\x3e D1[\u5728\u767d\u5929\u548c\u591c\u95f4\u5747\u8d85\u8d8aSOTA/Outperforms SOTA in daytime & nighttime]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [frequency-guided learning, structural attention, semantic consistency, dual-branch framework, CLIP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tianjin University of Science and Technology, The University of Sydney, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00327",children:"https://arxiv.org/pdf/2601.00327"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HarmoniAD: \u5f02\u5e38\u68c0\u6d4b/HarmoniAD: Anomaly Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7ed3\u6784-\u8bed\u4e49\u6743\u8861/Structure-Semantics Trade-off]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u9891\u7387\u5f15\u5bfc\u53cc\u5206\u652f\u6846\u67b6/Frequency-Guided Dual-Branch Framework]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA\u6027\u80fd/SOTA Performance]\n    B --\x3e B1[\u7ed3\u6784\u6a21\u578b\u566a\u58f0\u654f\u611f/Structure Models: Noise-Sensitive]\n    B --\x3e B2[\u8bed\u4e49\u6a21\u578b\u5ffd\u7565\u7ec6\u8282/Semantic Models: Miss Details]\n    C --\x3e C1[\u9ad8\u9891\u5206\u652f: FSAM\u6a21\u5757/High-Freq Branch: FSAM]\n    C --\x3e C2[\u4f4e\u9891\u5206\u652f: GSCM\u6a21\u5757/Low-Freq Branch: GSCM]\n    D --\x3e D1[\u6570\u636e\u96c6: MVTec-AD, VisA, BTAD/Datasets: MVTec-AD, VisA, BTAD]\n    D --\x3e D2[\u7ed3\u679c: \u9ad8\u654f\u611f\u6027\u4e0e\u9c81\u68d2\u6027/Results: High Sensitivity & Robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D human reconstruction], [bridge diffusion, latent representation, 3D Gaussian splatting, variational autoencoder, unified modeling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yingzhi Tang, Qijian Zhang, Junhui Hou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," City University of Hong Kong, Tencent Games"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00328",children:"https://arxiv.org/pdf/2601.00328"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/haiantyz/JGA-LBD",children:"https://github.com/haiantyz/JGA-LBD"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified framework (JGA-LBD) that jointly models human geometry and appearance in a single latent space, addressing inconsistency issues in decoupled pipelines. 2. Introduces a method to unify heterogeneous input conditions (e.g., depth, SMPL) into 3D Gaussian representations and compresses them into a shared latent space via a sparse VAE. 3. Formulates the generation process using a specialized bridge diffusion model that infers missing components from a partially observed latent code, enabling high-fidelity reconstruction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenging problem of reconstructing consistent 3D digital humans from a single RGB image. It proposes JGA-LBD, a novel framework that unifies geometry and appearance modeling into a joint latent representation and uses bridge diffusion for generation. Experiments show the method outperforms state-of-the-art approaches in both geometry and appearance quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Joint Geometry\u2013Appearance Human Reconstruction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u5355\u89c6\u56feRGB\u56fe\u50cf\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u6570\u5b57\u4eba/Single-view RGB to high-fidelity 3D human)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: JGA-LBD\u6846\u67b6/JGA-LBD Framework)\n    C --\x3e C1(\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4/Unified Latent Space)\n    C1 --\x3e C1a(3D\u9ad8\u65af\u8868\u793a/3D Gaussian Representation)\n    C1 --\x3e C1b(\u7a00\u758f\u53d8\u5206\u81ea\u7f16\u7801\u5668/Sparse VAE)\n    C --\x3e C2(\u6865\u63a5\u6269\u6563/Bridge Diffusion)\n    C2 --\x3e C2a(\u90e8\u5206\u89c2\u6d4b/Partial Observation)\n    C2 --\x3e C2b(\u63a8\u65ad\u7f3a\u5931\u7ec4\u4ef6/Infer Missing Components)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u8d85\u8d8aSOTA\u7684\u51e0\u4f55\u4e0e\u5916\u89c2\u8d28\u91cf/Superior geometry & appearance quality vs. SOTA)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLOv8, CNN, Transformer, License Plate Recognition, Speed Estimation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bruce Mugizi, Sudi Murindanyi, Olivia Nakacwa, Andrew Katumba"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Makerere University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00344",children:"https://arxiv.org/pdf/2601.00344"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a real-time intelligent traffic surveillance system specifically tailored for resource-constrained environments like Uganda, integrating vehicle detection, license plate recognition, and speed estimation. 2. Achieved high performance with a 97.9% mAP for license plate detection using YOLOv8 and a low 1.79% character error rate for recognition using a Transformer model, significantly improving over a CNN baseline. 3. Implemented a practical enforcement pipeline by creating a database to correlate vehicle data with user information and enabling automated ticket issuance via SMS using the Africa's Talking API."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a real-time computer vision system to tackle speeding in developing countries by detecting vehicles, recognizing license plates, and estimating speed. The method uses YOLOv8 for detection and a Transformer for character recognition, achieving high accuracy and integrating with an SMS-based ticketing system. The work demonstrates a practical, automated solution for traffic enforcement in resource-limited settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u8d85\u901f\u5bfc\u81f4\u4e8b\u6545/Speeding causes accidents")\n    Problem --\x3e P2("\u53d1\u5c55\u4e2d\u56fd\u5bb6\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd/Developing countries lack infrastructure")\n    Method --\x3e M1("\u8f66\u8f86\u68c0\u6d4b/Vehicle Detection")\n    Method --\x3e M2("\u8f66\u724c\u8bc6\u522b/License Plate Recognition")\n    Method --\x3e M3("\u901f\u5ea6\u4f30\u8ba1/Speed Estimation")\n    M1 --\x3e M1_1("\u4f7f\u7528YOLOv8/Using YOLOv8")\n    M2 --\x3e M2_1("\u4f7f\u7528CNN\u548cTransformer/Using CNN & Transformer")\n    M3 --\x3e M3_1("\u4f7f\u7528ROI/Using ROI")\n    Results --\x3e R1("\u9ad8\u68c0\u6d4b\u7cbe\u5ea6/High detection mAP: 97.9%")\n    Results --\x3e R2("\u4f4e\u5b57\u7b26\u9519\u8bef\u7387/Low CER: 1.79%")\n    Results --\x3e R3("\u901f\u5ea6\u4f30\u8ba1\u8bef\u5dee\u5c0f/Speed error: \xb110 km/h")\n    Results --\x3e R4("\u81ea\u52a8\u7f5a\u5355\u7cfb\u7edf/Automated ticketing via SMS")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal learning], [visual-tactile learning, domain generalization, fractional Fourier transform, multimodal fusion, hierarchical tree structure]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fuzhou University, University of Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00352",children:"https://arxiv.org/pdf/2601.00352"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates a new task, Single Domain Generalization for Multimodal Visual-Tactile Learning (SDG-VTL), to address modality and domain gaps in VTL. 2. Proposes the OmniVaT framework, which introduces a Multimodal Fractional Fourier Adapter (MFFA) to map visual and tactile embeddings into a unified embedding-frequency space, mitigating the modality gap. 3. Incorporates a Discrete Tree Generation (DTG) module to obtain diverse and reliable multimodal fractional representations via a hierarchical tree structure, enhancing adaptability to unseen domain shifts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of modality discrepancies and domain gaps in visual-tactile learning by proposing a new task (SDG-VTL) and a framework called OmniVaT. OmniVaT uses a Multimodal Fractional Fourier Adapter to unify visual and tactile features and a Discrete Tree Generation module to enhance generalization to unseen domains. Experiments show that OmniVaT achieves superior cross-domain generalization performance on the SDG-VTL task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[OmniVaT: \u5355\u57df\u6cdb\u5316\u591a\u6a21\u6001\u89c6\u89c9\u89e6\u89c9\u5b66\u4e60<br>OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6a21\u6001\u5dee\u5f02\u4e0e\u57df\u5dee\u8ddd<br>Modality & Domain Gaps]\n    B1 --\x3e B2[\u65b0\u4efb\u52a1: SDG-VTL<br>New Task: SDG-VTL]\n    C --\x3e C1[\u591a\u6a21\u6001\u5206\u6570\u5085\u91cc\u53f6\u9002\u914d\u5668 MFFA<br>Multimodal Fractional Fourier Adapter]\n    C --\x3e C2[\u79bb\u6563\u6811\u751f\u6210\u6a21\u5757 DTG<br>Discrete Tree Generation Module]\n    D --\x3e D1[\u5353\u8d8a\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd<br>Superior Cross-Domain Generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [dense visual embeddings, knowledge distillation, RGB-D transformer, real-time inference, Alpha-CLIP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," S\xf6hnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Ilmenau"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00359",children:"https://arxiv.org/pdf/2601.00359"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DVEFormer, an efficient RGB-D Transformer-based model for predicting dense, text-aligned visual embeddings via knowledge distillation from Alpha-CLIP. 2. Enables flexible, open-vocabulary scene understanding (e.g., text-based querying) beyond fixed-class semantic segmentation while maintaining the ability to perform classical segmentation. 3. Demonstrates real-time performance on embedded hardware (NVIDIA Jetson AGX Orin), making it suitable for mobile robotics applications like 3D mapping."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the need for robots to have a detailed, open-vocabulary understanding of indoor environments. It proposes DVEFormer, an efficient model that uses an RGB-D Transformer and knowledge distillation from Alpha-CLIP to predict dense visual embeddings, enabling both classical segmentation and flexible text-based querying. The method achieves competitive performance and real-time inference speeds, making it a practical drop-in replacement for traditional segmentation in mobile robotics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u673a\u5668\u4eba\u9700\u8981\u5168\u9762\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u573a\u666f\u7406\u89e3/Robots need comprehensive, open-vocabulary scene understanding]\n    C --\x3e C1[\u4f7f\u7528RGB-D Transformer\u548c\u77e5\u8bc6\u84b8\u998f/Use RGB-D Transformer and Knowledge Distillation]\n    C1 --\x3e C2[\u4eceAlpha-CLIP\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165/Learn Dense Visual Embeddings from Alpha-CLIP teacher]\n    D --\x3e D1[\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u4e0e\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c/Achieves real-time performance & competitive results]\n    D --\x3e D2[\u652f\u6301\u6587\u672c\u67e5\u8be2\u548c3D\u6620\u5c04/Enables text-based querying & 3D mapping]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction and inpainting], [voxel diffusion, mask-conditioned inpainting, joint geometry-color completion, 3D U-Net, cultural heritage]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aarya Sumuk"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00368",children:"https://arxiv.org/pdf/2601.00368"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A lightweight two-stage framework for joint geometry and color inpainting of 3D objects, separating damage localization from reconstruction. 2. A mask-conditioned volumetric diffusion model (3D U-Net) that directly inpaints voxel grids to reconstruct occupancy and color while preserving intact regions. 3. A composite training objective combining occupancy reconstruction, masked color reconstruction, and perceptual regularization for joint prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a two-stage method for digitally restoring damaged 3D cultural heritage artifacts. It first predicts a 3D damage mask from 2D RGB slices, then uses a mask-conditioned voxel diffusion model to jointly inpaint the missing geometry and color. The results show this approach produces more complete and coherent reconstructions than symmetry-based baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Mask-Conditioned Voxel Diffusion<br>\u63a9\u7801\u6761\u4ef6\u4f53\u7d20\u6269\u6563"] --\x3e Problem["Joint Geometry & Color Inpainting of Damaged 3D Objects<br>\u53d7\u635f3D\u7269\u4f53\u7684\u51e0\u4f55\u4e0e\u989c\u8272\u8054\u5408\u4fee\u590d"]\n    Root --\x3e Method["Two-Stage Framework: Mask Prediction + Mask-Conditioned Voxel Diffusion<br>\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u63a9\u7801\u9884\u6d4b + \u63a9\u7801\u6761\u4ef6\u4f53\u7d20\u6269\u6563"]\n    Root --\x3e Results["More Complete & Coherent Reconstructions vs. Baselines<br>\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5b8c\u6574\u3001\u66f4\u8fde\u8d2f\u7684\u91cd\u5efa\u7ed3\u679c"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [skeleton-based action recognition], [probabilistic fusion, multi-modal integration, reliability-aware learning, noisy-or, cross-modal ensemble]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Seungyeon Cho, Tae-kyun Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Imperial College London (Inferred from author Tae-Kyun Kim's affiliation)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00369",children:"https://arxiv.org/pdf/2601.00369"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A calibration-free preprocessing pipeline that learns directly from native skeleton coordinates, removing canonical-space transformations. 2. A probabilistic Noisy-OR fusion mechanism for reliability-aware dual-stream learning without explicit confidence supervision. 3. An intra- to cross-modal ensemble that couples four skeleton modalities with RGB representations in a unified framework."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of body-centric skeleton action recognition by proposing BHaRNet, a probabilistic dual-stream framework that integrates body and hand modalities for fine-grained recognition. The method introduces reliability-aware learning and a unified cross-modal ensemble of skeleton and RGB data. It demonstrates improved performance and robustness across multiple benchmarks, including a new hand-centric dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[BHaRNet: \u7ec6\u7c92\u5ea6\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b / Fine-grained Skeleton Action Recognition] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u624b\u90e8\u7ec6\u5fae\u52a8\u4f5c / Existing methods neglect subtle hand articulations]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6982\u7387\u53cc\u6d41\u6846\u67b6 / Probabilistic Dual-Stream Framework]\n    C --\x3e C1[\u6821\u51c6\u65e0\u5173\u9884\u5904\u7406 / Calibration-Free Preprocessing]\n    C --\x3e C2[\u566a\u58f0\u6216\u6982\u7387\u878d\u5408 / Probabilistic Noisy-OR Fusion]\n    C --\x3e C3[\u8de8\u6a21\u6001\u96c6\u6210 / Intra- to Cross-Modal Ensemble]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u591a\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u63d0\u5347 / Performance gains across multiple benchmarks & \u566a\u58f0\u4e0b\u9c81\u68d2\u6027 / Robustness under noise]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [small-object detection, dashcam dataset, roadside litter, long-tail distribution, transformer detectors]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tao Wu, Qing Xu, Xiangjian He, Oakleigh Weekes, James Brown, Wenting Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Nottingham Ningbo China, University of Lincoln"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00398",children:"https://arxiv.org/pdf/2601.00398"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/xq141839/RoLID-11K",children:"https://github.com/xq141839/RoLID-11K"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RoLID-11K, the first large-scale dataset for roadside litter detection from dashcam footage, featuring over 11k annotated images with extreme small-object and long-tail characteristics. 2. Provides a comprehensive benchmark of modern object detectors, including transformer and YOLO models, on this challenging task. 3. Establishes a new benchmark for extreme small-object detection in dynamic driving scenes to support scalable, low-cost litter monitoring systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RoLID-11K, a new dataset for detecting small roadside litter from dashcam videos, and benchmarks various modern object detectors on it. The results show that transformer-based models like CO-DETR achieve the best localization accuracy, while real-time models are limited by coarse feature hierarchies. The dataset aims to facilitate the development of scalable systems for automated roadside litter monitoring."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection] --\x3e B1\n    A --\x3e B2\n    A --\x3e B3\n    B1[\u6838\u5fc3\u95ee\u9898/Problem: \u8def\u8fb9\u5783\u573e\u76d1\u6d4b\u56f0\u96be\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u9002\u7528\u4e8e\u884c\u8f66\u8bb0\u5f55\u4eea\u573a\u666f/Roadside litter monitoring is challenging, existing datasets are unsuitable for dashcam footage]\n    B2[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u884c\u8f66\u8bb0\u5f55\u4eea\u8def\u8fb9\u5783\u573e\u68c0\u6d4b\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u57fa\u51c6\u6d4b\u8bd5/Propose the first large-scale dashcam roadside litter dataset and benchmark]\n    B3[\u5173\u952e\u7ed3\u679c/Results: CO-DETR\u7b49Transformer\u6a21\u578b\u5b9a\u4f4d\u6700\u51c6\uff0c\u5b9e\u65f6\u6a21\u578b\u53d7\u9650/CO-DETR and related transformers achieve best accuracy, real-time models are constrained]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [4D reconstruction and generation], [4D Gaussian Splatting, monocular video, feed-forward reconstruction, novel view synthesis, world model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," NLPR & MAIS, CASIA; CreateAI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00393",children:"https://arxiv.org/pdf/2601.00393"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://neoverse-4d.github.io",children:"https://neoverse-4d.github.io"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A scalable 4D world model (NeoVerse) built from diverse in-the-wild monocular videos, eliminating the need for expensive multi-view data or cumbersome pre-processing. 2. A pose-free, feed-forward 4D reconstruction method using 4D Gaussian Splatting (4DGS). 3. An online monocular degradation pattern simulation technique to enable high-quality, coherent novel-trajectory video generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes NeoVerse, a scalable 4D world model that performs feed-forward 4D reconstruction from monocular videos and generates novel-viewpoint videos. It addresses scalability limitations of prior methods by not requiring multi-view data or complex pre-processing. The model achieves state-of-the-art performance on standard benchmarks for reconstruction and generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5f53\u524d4D\u4e16\u754c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u53d7\u9650/Scalability limitation in current 4D world models]\n    B1 --\x3e B2[\u4f9d\u8d56\u6602\u8d35\u591a\u89c6\u89d2\u6570\u636e\u6216\u590d\u6742\u9884\u5904\u7406/Reliance on expensive multi-view data or cumbersome pre-processing]\n    C --\x3e C1[\u57fa\u4e8e\u5355\u76ee\u89c6\u9891\u7684\u53ef\u6269\u5c55\u6d41\u7a0b/Scalable pipeline using monocular videos]\n    C1 --\x3e C2[\u514d\u59ff\u6001\u524d\u99884D\u91cd\u5efa/Pose-free feed-forward 4D reconstruction]\n    C1 --\x3e C3[\u5728\u7ebf\u5355\u76ee\u9000\u5316\u6a21\u5f0f\u6a21\u62df/Online monocular degradation pattern simulation]\n    D --\x3e D1[\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd/Achieves state-of-the-art performance]\n    D1 --\x3e D2[\u5728\u6807\u51c6\u91cd\u5efa\u4e0e\u751f\u6210\u57fa\u51c6\u4e0a/On standard reconstruction & generation benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [Kolmogorov-Arnold Network (KAN), Functional Connectivity (FC), Transformer, Autism Spectrum Disorder (ASD), ABIDE I]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tyler Ward, Abdullah Imran"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Kentucky"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00416",children:"https://arxiv.org/pdf/2601.00416"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/tbwa233/ABFR-KAN",children:"https://github.com/tbwa233/ABFR-KAN"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ABFR-KAN, a novel transformer-based classification network that integrates advanced brain function representation components with Kolmogorov-Arnold Networks (KANs) to address limitations of atlas-based functional connectivity analysis. 2. Introduces a method designed to mitigate structural bias, improve anatomical conformity, and enhance the reliability of functional connectivity estimation for brain disorder diagnosis. 3. Demonstrates through extensive experiments, including cross-site evaluation and ablation studies on the ABIDE I dataset, that the proposed model consistently outperforms state-of-the-art baselines in autism spectrum disorder (ASD) classification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the issues of selection bias and lack of subject specificity in traditional atlas-based functional connectivity (FC) analysis for brain disorders. It proposes ABFR-KAN, a transformer-based classification network that incorporates novel brain function representation components and Kolmogorov-Arnold Networks (KANs) to improve FC estimation. Experiments on the ABIDE I dataset show that ABFR-KAN outperforms state-of-the-art methods in classifying autism spectrum disorder (ASD)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Atlas-based parcellation leads to selection bias and lacks subject specificity in functional connectivity analysis.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a transformer-based network (ABFR-KAN) with advanced brain function representation and Kolmogorov-Arnold Networks (KANs).]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms state-of-the-art baselines in ASD classification on the ABIDE I dataset.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Robust Assembly Progress Estimation via Deep Metric Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [metric learning], [assembly progress estimation, deep metric learning, quadruplet loss, anomaly detection, small-scale dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kazuma Miura, Sarthak Pathak, Kazunori Umeda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chuo University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00422",children:"https://arxiv.org/pdf/2601.00422"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Robust Assembly Progress Estimation via Deep Metric Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u88c5\u914d\u4efb\u52a1\u4e2d\u89c6\u89c9\u53d8\u5316\u7ec6\u5fae\u5bfc\u81f4\u8bef\u5206\u7c7b/Misclassification due to subtle visual changes in assembly tasks]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u56db\u5143\u7ec4\u635f\u5931\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u7f51\u7edc/Anomaly Quadruplet-Net with Quadruplet Loss & custom data loader]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5728PC\u88c5\u914d\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u53471.3%\uff0c\u76f8\u90bb\u4efb\u52a1\u8bef\u5206\u7c7b\u51cf\u5c111.9%/1.3% accuracy improvement & 1.9% reduction in adjacent task misclassification on PC dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Delta Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural network architecture], [residual networks, geometric transformation, spectral analysis, rank-1 perturbation, dynamic gating]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University, University of California, Los Angeles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00417",children:"https://arxiv.org/pdf/2601.00417"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/yifanzhang-pro/deep-delta-learning",children:"https://github.com/yifanzhang-pro/deep-delta-learning"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Deep Delta Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6b8b\u5dee\u7f51\u7edc\u9650\u5236/ResNet Limitation]\n    B1 --\x3e B2["\u521a\u6027\u76f8\u52a0\u504f\u7f6e/Rigid Additive Bias"]\n    B2 --\x3e B3["\u9650\u5236\u590d\u6742\u72b6\u6001\u8f6c\u6362/Limits Complex State Transitions"]\n    C --\x3e C1[Delta \u7b97\u5b50/Delta Operator]\n    C1 --\x3e C2["\u79e9-1\u6270\u52a8/ Rank-1 Perturbation"]\n    C2 --\x3e C3["\u53ef\u5b66\u4e60\u51e0\u4f55\u53d8\u6362/Learnable Geometric Transform"]\n    C3 --\x3e C4["\u52a8\u6001\u95e8\u63a7/Dynamic Gating (\u03b2)"]\n    D --\x3e D1["\u8c31\u5206\u6790/Spectral Analysis"]\n    D1 --\x3e D2["\u63d2\u503c\u8eab\u4efd/\u6295\u5f71/\u53cd\u5c04/Interpolates Identity/Projection/Reflection"]\n    D --\x3e D3["\u540c\u6b65\u79e9-1\u6ce8\u5165/Synchronous Rank-1 Injection"]\n    D3 --\x3e D4["\u63a7\u5236\u8f6c\u6362\u8c31/Controls Transition Spectrum"]\n    D4 --\x3e D5["\u4fdd\u6301\u7a33\u5b9a\u8bad\u7ec3/Preserves Stable Training"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning for human feedback], [reinforcement learning from human feedback (RLHF), flow matching, stochastic differential equations (SDE), group relative policy optimization (GRPO), entropy-aware sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00423",children:"https://arxiv.org/pdf/2601.00423"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/shengjun-zhang/VisualGRPO",children:"https://github.com/shengjun-zhang/VisualGRPO"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e2a\u53bb\u566a\u6b65\u4e0a\u4f18\u5316\uff0c\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u6a21\u7cca/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n    C --\x3e C1[\u63d0\u51faE-GRPO: \u71b5\u611f\u77e5\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n    C1 --\x3e C2[\u5408\u5e76\u4f4e\u71b5\u6b65\u4e3a\u9ad8\u71b5SDE\u91c7\u6837\u6b65\uff0c\u5176\u4ed6\u6b65\u7528ODE\u91c7\u6837/Merge low-entropy steps for SDE, use ODE elsewhere]\n    C1 --\x3e C3[\u5f15\u5165\u591a\u6b65\u5206\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u8ba1\u7b97/Introduce multi-step group normalized advantage]\n    D --\x3e D1[\u5728\u4e0d\u540c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027/Method effectiveness demonstrated across different reward settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CPPO: Contrastive Perception for Vision Language Policy Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [contrastive perception loss, entropy shift, vision-language models, policy optimization, multimodal reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huawei Technologies Canada Co. Ltd., Huawei Cloud"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00501",children:"https://arxiv.org/pdf/2601.00501"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CPPO: Contrastive Perception for Vision Language Policy Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Disentangling perception and reasoning tokens in VLMs is difficult with prior methods requiring extra LLMs or indiscriminate rewards]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Detect perception tokens via entropy shifts under perturbed images; apply Contrastive Perception Loss (CPL) for consistency/sensitivity]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: CPPO surpasses previous perception-rewarding methods, avoids extra models, and improves training efficiency/scalability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [physics-based simulation], [motion distillation, differentiable simulation, multimodal large language model, material parameter estimation, video diffusion models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Miaowei Wang, Jakub Zadro\u017cny, Oisin Mac Aodha, Amir Vaxman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Edinburgh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00504",children:"https://arxiv.org/pdf/2601.00504"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://wangmiaowei.github.io/MotionPhysics.github.io/",children:"https://wangmiaowei.github.io/MotionPhysics.github.io/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7269\u7406\u53c2\u6570\u8c03\u4f18\u8017\u65f6\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6/Physical parameter tuning is time-consuming and requires expertise]\n    C --\x3e C1[\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u53c2\u6570/Use multimodal LLM to estimate parameters]\n    C --\x3e C2[\u63d0\u51fa\u53ef\u5b66\u4e60\u8fd0\u52a8\u84b8\u998f\u635f\u5931/Propose learnable motion distillation loss]\n    C2 --\x3e C2a[\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u53d6\u8fd0\u52a8\u5148\u9a8c/Extract motion priors from video diffusion models]\n    D --\x3e D1[\u572830+\u573a\u666f\u4e2d\u8bc4\u4f30/Evaluated across 30+ scenarios]\n    D --\x3e D2[\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Surpasses state-of-the-art]\n    D --\x3e D3[\u81ea\u52a8\u786e\u5b9a\u5408\u7406\u53c2\u6570/Automatically determines plausible parameters]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video restoration], [all-in-one restoration, smoothly evolving degradation, prompt learning, coarse intensity estimation, flow prompt generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, University of Macau, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00533",children:"https://arxiv.org/pdf/2601.00533"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Friskknight/ORCANet-SEUD",children:"https://github.com/Friskknight/ORCANet-SEUD"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Smoothly Evolving Unknown Degradations (SEUD) scenario for video restoration, where degradation types and intensities change continuously over time. 2. Proposes ORCANet, featuring a Coarse Intensity Estimation Dehazing (CIED) module for initialization and a Flow Prompt Generation (FPG) module that generates static and dynamic prompts to capture degradation types and intensity variations. 3. Designs a flexible synthesis pipeline to generate temporally coherent videos with single, compound, and evolving degradations for training and evaluation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of all-in-one video restoration under temporally continuous and evolving weather degradations. It proposes ORCANet, a network that uses coarse intensity estimation and a novel prompting mechanism to adapt to degradation changes over time. Experiments show the method achieves superior restoration quality and temporal consistency compared to existing baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Extending all-in-one restoration to videos with temporally continuous, evolving degradations]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ORCANet with CIED for initialization and FPG for static/dynamic prompt generation]\n    D[\u5173\u952e\u7ed3\u679c/Results: Superior restoration quality, temporal consistency, and robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [text-to-image generation], [diffusion transformers, attention localization, spectral glyph injection, training-free, text rendering]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00535",children:"https://arxiv.org/pdf/2601.00535"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a training-free framework (FreeText) that decomposes text rendering into "where to write" and "what to write" problems. 2. Introduces a method to localize writing regions using endogenous image-to-text attention with sink-like tokens and topology-aware refinement. 3. Presents Spectral-Modulated Glyph Injection (SGMI) to inject a noise-aligned glyph prior with frequency-domain modulation to enhance structure and suppress semantic leakage.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FreeText, a training-free framework to improve text rendering in Diffusion Transformer models. It localizes writing regions via attention mechanisms and injects glyph structure using spectral modulation. Experiments show it improves text readability while preserving image quality with minimal overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[FreeText: Training-Free Text Rendering in Diffusion Transformers] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Diffusion models struggle with precise text rendering, especially for multi-line layouts and long-tailed scripts]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Decomposes into "where to write" (attention localization) and "what to write" (spectral-modulated glyph injection)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Consistent gains in text readability, preserves semantic alignment and aesthetics, modest inference overhead]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image editing], [drag-style editing, motion prediction, predict-and-move framework, motion supervision]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiacheng Sui, Yujie Zhou, Li Niu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00542",children:"https://arxiv.org/pdf/2601.00542"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel "predict-and-move" framework for drag-style image editing, which is the first of its kind. 2. Introduces an iterative method combining Motion Prediction and Motion Supervision to proactively guide handle points. 3. Proposes a dynamic adjustment mechanism for valid handle points to improve editing performance.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper identifies issues like miss tracking and poor editability in existing drag-style image editing methods. It proposes DynaDrag, a new method under a "predict-and-move" framework that iteratively uses motion prediction and motion supervision to guide handle points. Experiments on face and human datasets show it outperforms previous works.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u8ddf\u8e2a\u4e22\u5931\u3001\u6a21\u7cca\u8ddf\u8e2a\u3001\u7f16\u8f91\u6027\u5dee\u7b49\u95ee\u9898]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u9996\u4e2a"\u9884\u6d4b-\u79fb\u52a8"\u6846\u67b6\uff0c\u8fed\u4ee3\u6267\u884c\u8fd0\u52a8\u9884\u6d4b\u548c\u8fd0\u52a8\u76d1\u7763\uff0c\u52a8\u6001\u8c03\u6574\u6709\u6548\u63a7\u5236\u70b9]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u4eba\u8138\u548c\u4eba\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image segmentation], [Segment Anything Model, zero-shot segmentation, low-level features, fine-tuning, visually non-salient]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Guangqian Guo, Pengfei Chen, Yong Guo, Huafeng Chen, Boqiang Zhang, Shan Gao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northwestern Polytechnical University, University of Chinese Academy of Sciences, Max Planck Institute for Informatics, University of Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00537",children:"https://arxiv.org/pdf/2601.00537"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://guangqian-guo.github.io/VNS-SAM/",children:"https://guangqian-guo.github.io/VNS-SAM/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed VNS-SAM, a method to enhance SAM's performance in visually non-salient (low-contrast) scenarios while preserving its zero-shot generalizability. 2. Introduced two key designs: a Mask-Edge Token Interactive decoder and a Non-Salient Feature Mining module to effectively exploit SAM's low-level features. 3. Created VNS-SEG, a large-scale unified dataset with over 35K images for training and benchmarking models on various visually non-salient segmentation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem where the Segment Anything Model (SAM) struggles with visually non-salient scenarios where foreground and background have low contrast. The authors propose VNS-SAM, which enhances SAM's perception using a novel decoder and feature mining module, and introduce a new dataset called VNS-SEG. Experiments show VNS-SAM achieves superior performance, especially in zero-shot settings, demonstrating its practical potential."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[SAM\u5728\u89c6\u89c9\u975e\u663e\u8457\u573a\u666f\u8868\u73b0\u4e0d\u4f73/SAM underperforms in visually non-salient scenarios]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faVNS-SAM\u6a21\u578b/Propose VNS-SAM model]\n    M1 --\x3e M2[\u8bbe\u8ba1Mask-Edge Token Interactive\u89e3\u7801\u5668/Design Mask-Edge Token Interactive decoder]\n    M1 --\x3e M3[\u8bbe\u8ba1Non-Salient Feature Mining\u6a21\u5757/Design Non-Salient Feature Mining module]\n    M1 --\x3e M4[\u6784\u5efaVNS-SEG\u6570\u636e\u96c6/Build VNS-SEG dataset]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6027\u80fd\u663e\u8457\u63d0\u5347/Superior performance]\n    Results --\x3e R2[\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b/Preserves zero-shot generalizability]\n    Results --\x3e R3[\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u4f4e/Low parameter & computational cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Comprehensive Dataset for Human vs. AI Generated Image Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [AI-Generated Images, Detection Techniques, Synthetic Media, Generative AI, Multimodal AI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San Jos\xe9 State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00553",children:"https://arxiv.org/pdf/2601.00553"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Comprehensive Dataset for Human vs. AI Generated Image Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["AI\u751f\u6210\u56fe\u50cf\u96be\u4ee5\u533a\u5206/AI-Generated Images Hard to Distinguish"]\n    Problem --\x3e P2["\u8bef\u5bfc\u6027\u5185\u5bb9\u4f20\u64ad/Spread of Misleading Content"]\n    Method --\x3e M1["\u6784\u5efaMS COCOAI\u6570\u636e\u96c6/Build MS COCOAI Dataset"]\n    Method --\x3e M2["\u4f7f\u7528\u4e94\u79cd\u751f\u6210\u5668/Use Five Generators"]\n    Results --\x3e R1["\u63d0\u4f9b96000\u4e2a\u6570\u636e\u70b9/Provide 96k Datapoints"]\n    Results --\x3e R2["\u5b9a\u4e49\u4e24\u9879\u68c0\u6d4b\u4efb\u52a1/Define Two Detection Tasks"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging reconstruction], [photoacoustic imaging, point cloud, iterative reconstruction, irregular array, hierarchical optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Peking University, Nanjing University, Pohang University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00551",children:"https://arxiv.org/pdf/2601.00551"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ShuangLiPKU/SlingBAG-Pro",children:"https://github.com/ShuangLiPKU/SlingBAG-Pro"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SingBAG Pro: 3D\u5149\u58f0\u6210\u50cf/SlingBAG Pro: 3D Photoacoustic Imaging] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e0d\u89c4\u5219\u9635\u5217\u91cd\u5efa\u6162/Irregular Array Reconstruction is Slow]\n    C --\x3e C1[\u70b9\u4e91\u8fed\u4ee3\u4e0e\u5206\u5c42\u4f18\u5316/Point Cloud Iteration & Hierarchical Optimization]\n    D --\x3e D1[\u901f\u5ea6\u63d0\u53472.2\u500d/Speedup of 2.2x]\n    D --\x3e D2[\u4eff\u771f\u4e0e\u6d3b\u4f53\u9a8c\u8bc1/Simulation & In Vivo Validation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] A Cascaded Information Interaction Network for Precise Image Segmentation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image segmentation], [cascaded convolutional neural network, Global Information Guidance Module, multi-scale feature fusion]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hewen Xiao, Jie Mei, Guangfu Ma, Weiren Wu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology, Shenzhen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00562",children:"https://arxiv.org/pdf/2601.00562"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a cascaded convolutional neural network architecture for robust image segmentation. 2. Introduces a novel Global Information Guidance Module to fuse low-level texture and high-level semantic features. 3. Demonstrates superior performance on benchmark datasets, particularly in cluttered or blurred environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of robust image segmentation in complex scenarios by proposing a cascaded CNN with a novel Global Information Guidance Module. This module effectively fuses multi-scale features to overcome the limitations of single-scale extraction. Experimental results show the method outperforms state-of-the-art approaches, highlighting its potential for practical robotic applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Cascaded Information Interaction Network for Precise Image Segmentation] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u590d\u6742\u573a\u666f\u4e0b\u9c81\u68d2\u5206\u5272\u662f\u6311\u6218/Robust segmentation in complex scenarios is a challenge]\n    C --\x3e C1[\u7ea7\u8054\u5377\u79ef\u795e\u7ecf\u7f51\u7edc/Cascaded CNN]\n    C --\x3e C2[\u5168\u5c40\u4fe1\u606f\u5f15\u5bfc\u6a21\u5757/Global Information Guidance Module]\n    C2 --\x3e C3[\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81/Fuse multi-scale features]\n    D --\x3e D1[\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02/Superior performance on benchmarks]\n    D --\x3e D2[\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5/Outperforms SOTA methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multimodal evaluation], [unified multimodal models, world knowledge, deterministic evaluation, benchmark, reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Hong Kong, The Hong Kong Polytechnic University, The Chinese University of Hong Kong, Huawei Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00561",children:"https://arxiv.org/pdf/2601.00561"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes AEGIS, a comprehensive multi-task benchmark for evaluating world knowledge capabilities of Unified Multimodal Models across visual understanding, generation, editing, and interleaved generation. 2. Introduces Deterministic Checklist-based Evaluation (DCE), a protocol using atomic "Y/N" judgments to replace ambiguous prompt-based scoring for more reliable evaluation. 3. Conducts extensive experiments revealing severe world knowledge deficits in current UMMs, performance degradation with complex reasoning, and the partial mitigation offered by plug-in reasoning modules.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies a critical gap in evaluating Unified Multimodal Models' (UMMs) ability to apply world knowledge. To address this, it proposes the AEGIS benchmark and a Deterministic Checklist-based Evaluation (DCE) protocol. The experiments show that current UMMs have significant world knowledge deficiencies, especially in complex reasoning tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Multimodal Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u4e0d\u8db3/Existing benchmarks are insufficient]\n    C --\x3e C1[\u63d0\u51faAEGIS\u57fa\u51c6/Propose AEGIS benchmark]\n    C --\x3e C2[\u63d0\u51faDCE\u8bc4\u4f30\u534f\u8bae/Propose DCE evaluation protocol]\n    D --\x3e D1[\u6a21\u578b\u5b58\u5728\u77e5\u8bc6\u7f3a\u9677/Models have knowledge deficits]\n    D --\x3e D2[\u590d\u6742\u63a8\u7406\u6027\u80fd\u4e0b\u964d/Performance degrades with complex reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video moment retrieval], [zero-shot, granularity mismatch, query rewriting, caption generation, vision-language models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chung-Ang University, Korea Advanced Institute of Science and Technology (KAIST), Electronics and Telecommunications Research Institute (ETRI)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00584",children:"https://arxiv.org/pdf/2601.00584"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a training-free framework (GranAlign) to address semantic granularity mismatch in zero-shot video moment retrieval. 2. Introduces granularity-based query rewriting to generate queries at varied semantic levels. 3. Introduces query-aware caption generation to embed query intent into video content descriptions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the granularity mismatch problem in zero-shot video moment retrieval by proposing GranAlign, a training-free framework that uses query rewriting and query-aware caption generation to align multi-level semantic representations. The method achieves state-of-the-art performance on three major benchmarks, including a 3.23% mAP@avg improvement on QVHighlights."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[GranAlign] --\x3e Problem(\u6838\u5fc3\u95ee\u9898: \u7c92\u5ea6\u4e0d\u5339\u914d / Problem: Granularity Mismatch)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5: \u7c92\u5ea6\u611f\u77e5\u5bf9\u9f50 / Method: Granularity-Aware Alignment)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c: SOTA\u6027\u80fd / Results: SOTA Performance)\n    Problem --\x3e P1[\u67e5\u8be2\u4e0e\u89c6\u9891\u5185\u5bb9\u8bed\u4e49\u7c92\u5ea6\u4e0d\u4e00\u81f4 / Query-Video Semantic Granularity Mismatch]\n    Method --\x3e M1[\u7c92\u5ea6\u67e5\u8be2\u91cd\u5199 / Granularity-based Query Rewriting]\n    Method --\x3e M2[\u67e5\u8be2\u611f\u77e5\u63cf\u8ff0\u751f\u6210 / Query-aware Caption Generation]\n    Results --\x3e R1[\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA / SOTA on Three Major Benchmarks]\n    Results --\x3e R2[\u5728QVHighlights\u4e0a\u663e\u8457\u63d0\u5347 / Notable Improvement on QVHighlights]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [RGB-Infrared fusion, modality imbalance, cross-modal learning, optimization bias, multimodal perception]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tongji University, Soochow University, The University of Arizona"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00598",children:"https://arxiv.org/pdf/2601.00598"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Modality Dominance Index (MDI) to quantify optimization bias caused by asymmetric modality characteristics in RGB-IR fusion. 2. Develops the Modality Dominance-Aware Cross-modal Learning (MDACL) framework to regulate cross-modal optimization. 3. Introduces Hierarchical Cross-modal Guidance (HCG) and Adversarial Equilibrium Regularization (AER) within MDACL to enhance feature alignment and balance optimization dynamics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the optimization bias problem in RGB-Infrared multimodal object detection, where disparities in information density cause training to over-rely on a dominant modality. The authors propose a Modality Dominance Index (MDI) to measure this bias and a Modality Dominance-Aware Cross-modal Learning (MDACL) framework to mitigate it. Experiments show that MDACL effectively reduces optimization bias and achieves state-of-the-art performance on RGB-IR benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modality Dominance-Aware Optimization for Embodied RGB\u2013Infrared Perception] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6a21\u6001\u4f18\u5316\u504f\u5dee/Modality Optimization Bias]\n    C --\x3e C1[\u6a21\u6001\u4e3b\u5bfc\u6307\u6570/MDI]\n    C --\x3e C2[\u6a21\u6001\u4e3b\u5bfc\u611f\u77e5\u5b66\u4e60\u6846\u67b6/MDACL]\n    C2 --\x3e C2_1[\u5206\u5c42\u8de8\u6a21\u6001\u6307\u5bfc/HCG]\n    C2 --\x3e C2_2[\u5bf9\u6297\u5747\u8861\u6b63\u5219\u5316/AER]\n    D --\x3e D1[\u7f13\u89e3\u4f18\u5316\u504f\u5dee/Mitigates Bias]\n    D --\x3e D2[\u5b9e\u73b0SOTA\u6027\u80fd/Achieves SOTA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [text-to-motion generation], [machine unlearning, diffusion models, motion safety, continuous kinematics, safe dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Australian National University, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00590",children:"https://arxiv.org/pdf/2601.00590"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/YilingWang98/SafeMo",children:"https://github.com/YilingWang98/SafeMo"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SafeMo, a trustworthy motion generation framework integrating a two-stage Minimal Motion Unlearning (MMU) strategy for safe generation in continuous space. 2. Introduced the first safe text-to-motion dataset, SafeMoVAE-29K, with rewritten safe prompts and refined motions. 3. Demonstrated superior safety-utility trade-offs, achieving significantly higher forget-set FID scores than prior state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses safety concerns in text-to-motion generation by proposing SafeMo, a framework that uses a two-stage machine unlearning strategy to remove unsafe behaviors while preserving motion quality in continuous space. It also introduces a new safe dataset for training. Experiments show SafeMo effectively forgets unsafe prompts while maintaining good performance on safe ones, outperforming previous methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u7f3a\u9677/Existing Method Flaws]\n    B1 --\x3e B1a[\u79bb\u6563\u4ee3\u7801\u672c\u66ff\u6362\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d/Discrete codebook replacement degrades benign performance]\n    B1 --\x3e B1b[\u91cf\u5316\u5bfc\u81f4\u4f2a\u5f71\u548c\u4e0d\u8fde\u8d2f/Quantization causes artifacts & jerky transitions]\n    B --\x3e B2[\u6570\u636e\u96c6\u5305\u542b\u4e0d\u5b89\u5168\u5185\u5bb9/Datasets contain unsafe intents & motions]\n    C --\x3e C1[\u63d0\u51faSafeMo\u6846\u67b6/Propose SafeMo Framework]\n    C1 --\x3e C1a[\u6700\u5c0f\u5316\u8fd0\u52a8\u9057\u5fd8\u7b56\u7565/Minimal Motion Unlearning (MMU)]\n    C1 --\x3e C1b[\u8fde\u7eed\u7a7a\u95f4\u751f\u6210/Generation in Continuous Space]\n    C --\x3e C2[\u6784\u5efa\u5b89\u5168\u6570\u636e\u96c6/Build Safe Dataset SafeMoVAE-29K]\n    D --\x3e D1[\u6709\u6548\u9057\u5fd8\u4e0d\u5b89\u5168\u63d0\u793a/Effective Forgetting on Unsafe Prompts]\n    D1 --\x3e D1a[\u9057\u5fd8\u96c6FID\u663e\u8457\u63d0\u5347/Forget-set FID greatly improved]\n    D --\x3e D2[\u4fdd\u6301\u826f\u6027\u6027\u80fd/Benign Performance Preserved]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Noise-Robust Tiny Object Localization with Flows"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [Tiny Object Detection, Noise Robustness, Normalizing Flows, Uncertainty-Guided Optimization, Flow-Based Error Modeling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beihang University, Communication University of China, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00617",children:"https://arxiv.org/pdf/2601.00617"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Noise-Robust Tiny Object Localization with Flows] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5fae\u5c0f\u7269\u4f53\u68c0\u6d4b\u5bf9\u6807\u6ce8\u566a\u58f0\u654f\u611f/Tiny object detection is sensitive to annotation noise]\n    C --\x3e C1[\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u8bef\u5dee\u5efa\u6a21/Flow-based error modeling]\n    C --\x3e C2[\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4f18\u5316/Uncertainty-guided optimization]\n    D --\x3e D1[\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548/Validated on multiple datasets]\n    D --\x3e D2[\u63d0\u5347DINO\u57fa\u7ebf1.2% AP/Boosts DINO baseline by 1.2% AP]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [Hypergraph Neural Network, Learning Using Privileged Information, Knowledge Distillation, Severed Graph Strategy, dual-stream distillation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuren Gabriel Yu, Sikang Ren, Yongji Tian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Beijing Tiantan Hospital"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00626",children:"https://arxiv.org/pdf/2601.00626"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HyperPriv-EPN] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Pre-op prognosis lacks semantic insights from post-op reports]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hypergraph LUPI with Severed Graph Strategy & dual-stream distillation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA accuracy & survival stratification on 311 patients]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [human pose estimation], [real-time 3D pose estimation, biomechanical analysis, SmoothNet, multi-camera tracking, Unity visualization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junxiao Xue, Pavel Smirnov, Ziao Li, Yunyun Shi, Shi Chen, Xinyi Yin, Xiaohan Yue, Lei Wang, Yiduo Wang, Feng Lin, Yijia Chen, Xiao Ma, Xiaoran Yan, Qing Zhang, Fengjian Xue, Xuecheng Wu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang Lab, Xi'an Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00625",children:"https://arxiv.org/pdf/2601.00625"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A unified, end-to-end pipeline for real-time 3D human pose estimation and motion analysis from multi-camera RGB video for rehabilitation. 2. A fast tracking method for multi-person scenarios in medical rehabilitation, achieving tracking in under 1ms per frame. 3. A modification of SmoothNet for real-time posture estimation to reduce errors and produce smoother, more accurate motion states."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes RePose, a real-time framework for 3D human pose estimation and biomechanical analysis to assist rehabilitation training. The method uses a multi-camera pipeline, a fast multi-person tracker, and a modified SmoothNet for smooth pose estimation, integrated with Unity for real-time feedback and muscle stress visualization. The system aims to provide immediate guidance to help patients perform exercises correctly."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RePose: \u5eb7\u590d\u5b9e\u65f63D\u59ff\u6001\u4f30\u8ba1\u4e0e\u751f\u7269\u529b\u5b66\u5206\u6790\u6846\u67b6<br/>RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Need for automatic, real-time monitoring and assessment in rehabilitation training] --\x3e B1[\u8fdc\u7a0b\u5eb7\u590d\u76d1\u63a7\u9700\u6c42<br/>Remote rehabilitation monitoring need]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Real-time 3D pose estimation and analysis pipeline] --\x3e C1[\u591a\u6444\u50cf\u5934RGB\u89c6\u9891\u8f93\u5165<br/>Multi-camera RGB video input]\n    C --\x3e C2[\u5feb\u901f\u591a\u4eba\u8ddf\u8e2a (<1ms/\u5e27)<br/>Fast multi-person tracking (<1ms/frame)]\n    C --\x3e C3[\u6539\u8fdbSmoothNet\u7528\u4e8e\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1<br/>Modified SmoothNet for real-time pose estimation]\n    C --\x3e C4[Unity\u5e73\u53f0\u5b9e\u65f6\u76d1\u63a7\u4e0e\u808c\u8089\u5e94\u529b\u663e\u793a<br/>Unity platform for real-time monitoring & muscle stress display]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>A unified framework for real-time motion correction and guidance] --\x3e D1[\u5b9e\u65f6\u53cd\u9988\u8f85\u52a9\u6b63\u786e\u5eb7\u590d\u8bad\u7ec3<br/>Real-time feedback to assist correct rehabilitation training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image classification], [transfer learning, vision transformer, DenseNet, sprout detection, shelf-life prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," TCS Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00645",children:"https://arxiv.org/pdf/2601.00645"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Designed a high-precision binary classifier for potato sprout detection using transfer learning, achieving 98.03% accuracy with DenseNet. 2. Developed an advanced multi-class predictor for estimating weight loss and forecasting remaining shelf-life, demonstrating best performance with coarse class divisions. 3. Demonstrated the feasibility of integrating image-based deep learning models into automated sorting systems for improved inventory management and reduced food waste."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using transfer learning with CNN and Vision Transformer models for non-invasive quality detection of stored potatoes. The method involves a binary classifier for sprout detection and a multi-class predictor for weight loss and shelf-life estimation. The study concludes that this approach is effective for automated sorting, with coarse class divisions yielding robust performance for practical applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Quality Detection of Stored Potatoes via Transfer Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u76d1\u6d4b\u5b58\u50a8\u9a6c\u94c3\u85af\u8d28\u91cf/Monitoring stored potato quality]\n    B --\x3e B2[\u68c0\u6d4b\u53d1\u82bd\u3001\u4f30\u8ba1\u91cd\u91cf\u635f\u5931/Predicting sprouting, weight loss & shelf-life]\n    C --\x3e C1[\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b/Using pre-trained models (ResNet, VGG, DenseNet, ViT)]\n    C --\x3e C2[\u8bbe\u8ba1\u4e8c\u5206\u7c7b\u4e0e\u591a\u5206\u7c7b\u5668/Designing binary & multi-class classifiers]\n    D --\x3e D1[\u9ad8\u7cbe\u5ea6\u53d1\u82bd\u68c0\u6d4b/High-accuracy sprout detection (98.03%)]\n    D --\x3e D2[\u7c97\u7c92\u5ea6\u4fdd\u8d28\u671f\u9884\u6d4b\u66f4\u4f18/Coarse class shelf-life prediction works best]\n    D --\x3e D3[\u652f\u6301\u81ea\u52a8\u5316\u5206\u62e3\u7cfb\u7edf/Supports automated sorting systems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [hallucination mitigation, contrastive decoding, vision-language models, training-free]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology (ISM) Dhanbad, Transmute AI, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00659",children:"https://arxiv.org/pdf/2601.00659"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models",children:"https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel hallucinated model that captures hallucination effects by selectively removing key text tokens, addressing the limitation of visual information propagation. 2. Introduces Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. 3. Presents CRoPS, a training-free framework that significantly improves hallucination metrics (e.g., 20% CHAIR score gain) across multiple benchmarks and LVLM families."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of hallucinations in Large Vision-Language Models (LVLMs). It proposes CRoPS, a training-free framework that introduces a novel hallucinated model based on text token removal and a Generalized Contrastive Decoding method to mitigate diverse hallucination sources. The method achieves significant improvements in hallucination metrics across several benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LVLMs tend to generate hallucinated content, undermining reliability.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Novel hallucinated model via text token removal + Generalized Contrastive Decoding.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improves CHAIR scores by 20%, gains across 6 benchmarks & 3 LVLM families.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [SAR tomography, point cloud, deep learning, building height estimation, dual-topology network]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaiyu Chen, Yuanyuan Wang, Yilei Shi, Xiao Xiang Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Munich (TUM)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00658",children:"https://arxiv.org/pdf/2601.00658"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/zhu-xlab/tomosar2height",children:"https://github.com/zhu-xlab/tomosar2height"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel learning-based framework for generating high-resolution building height maps directly from raw, noisy TomoSAR point clouds. 2. Introduces a dual-topology network architecture that alternates between a point branch and a grid branch to jointly model irregular scatterer features and enforce spatial consistency. 3. Demonstrates the first proof of concept for large-scale urban height mapping from TomoSAR and shows the framework's extensibility to incorporate optical imagery for enhanced quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of accurate building height reconstruction from noisy and incomplete spaceborne TomoSAR point clouds. It proposes a dual-topology deep learning network that processes both point-based and grid-based representations to denoise and inpaint the data, producing continuous height maps. Experiments on urban datasets validate the method's effectiveness, and the framework is shown to be extensible by fusing with optical imagery."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[TomoSAR\u70b9\u4e91\u566a\u58f0\u591a\u3001\u5206\u5e03\u4e0d\u5747\u3001\u6709\u6570\u636e\u7a7a\u6d1e/TomoSAR point clouds are noisy, anisotropic, and have voids]\n    C --\x3e C1[\u63d0\u51fa\u53cc\u62d3\u6251\u7f51\u7edc/Propose a dual-topology network]\n    C1 --\x3e C1_1[\u70b9\u5206\u652f\u5904\u7406\u4e0d\u89c4\u5219\u6563\u5c04\u4f53/Point branch models irregular scatterers]\n    C1 --\x3e C1_2[\u7f51\u683c\u5206\u652f\u4fdd\u8bc1\u7a7a\u95f4\u4e00\u81f4\u6027/Grid branch enforces spatial consistency]\n    D --\x3e D1[\u6709\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u9ad8\u5ea6\u56fe/Effectively produces high-resolution height maps]\n    D --\x3e D2[\u9996\u4e2a\u5927\u89c4\u6a21\u57ce\u5e02\u9ad8\u5ea6\u6d4b\u7ed8\u6982\u5ff5\u9a8c\u8bc1/First proof of concept for large-scale urban height mapping]\n    D --\x3e D3[\u53ef\u6269\u5c55\u878d\u5408\u5149\u5b66\u5f71\u50cf/Extensible to incorporate optical imagery]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [talking head generation], [diffusion forcing, direct preference optimization, real-time interaction, low latency, multimodal inputs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KAIST, NTU Singapore, DeepAuto.ai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00664",children:"https://arxiv.org/pdf/2601.00664"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://taekyungki.github.io/AvatarForcing",children:"https://taekyungki.github.io/AvatarForcing"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u7f3a\u4e4f\u771f\u6b63\u4e92\u52a8/Lacks truly interactive communication]\n    Problem --\x3e P2[\u5355\u5411\u53cd\u5e94\u7f3a\u4e4f\u60c5\u611f/One-way responses lack emotional engagement]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6269\u6563\u9a71\u52a8\u6846\u67b6/Diffusion forcing framework]\n    Method --\x3e M2[\u65e0\u6807\u7b7e\u76f4\u63a5\u504f\u597d\u4f18\u5316/Label-free direct preference optimization]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u4ea4\u4e92/Low-latency real-time interaction (~500ms)]\n    Results --\x3e R2[6.8\u500d\u52a0\u901f/6.8x speedup]\n    Results --\x3e R3[80%\u7528\u6237\u504f\u597d/Over 80% user preference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [3D Gaussians, camera-controlled generation, temporal consistency, single-image-conditioned]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Glasgow"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00678",children:"https://arxiv.org/pdf/2601.00678"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://melonienimasha.github.io/Pixel-to-4D-Website/",children:"https://melonienimasha.github.io/Pixel-to-4D-Website/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion from a single image in a single forward pass., 2. Enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into rendered frames., 3. Achieves state-of-the-art video quality and inference efficiency on multiple datasets (KITTI, Waymo, RealEstate10K, DL3DV-10K)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of generating temporally consistent and geometrically sound videos from a single image with user-controlled camera paths. It proposes Pixel-to-4D, a method that builds a dynamic 3D Gaussian representation of the scene in one pass to enable fast, camera-guided video synthesis. The approach demonstrates superior video quality and efficiency compared to existing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Pixel-to-4D: Camera-Controlled Image-to-Video Generation<br>Pixel-to-4D: \u76f8\u673a\u63a7\u5236\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u7f3a\u4e4f\u7528\u6237\u53ef\u63a7\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u76f8\u673a\u8fd0\u52a8\u5efa\u6a21\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u5b8c\u6574\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218<br>Lack of user controllability, challenges in camera motion modeling, temporal consistency, and geometric integrity]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u6784\u5efa\u52a8\u60013D\u9ad8\u65af\u573a\u666f\u8868\u793a\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u91c7\u6837\u7269\u4f53\u8fd0\u52a8<br>Construct dynamic 3D Gaussian scene representation, sample object motion in a single forward pass]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u89c6\u9891\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387<br>Achieves SOTA video quality and inference efficiency on multiple datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DefVINS: Visual-Inertial Odometry for Deformable Scenes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [visual-inertial odometry], [deformable scenes, observability analysis, embedded deformation graph, IMU anchoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Samuel Cerezo, Javier Civera"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad de Zaragoza"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00702",children:"https://arxiv.org/pdf/2601.00702"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A VIO framework (DefVINS) that explicitly separates rigid motion (IMU-anchored) from non-rigid deformation (modeled by an embedded deformation graph). 2. An observability analysis characterizing how inertial measurements constrain rigid motion and identify modes in deformable scenes. 3. A conditioning-based activation strategy that progressively enables non-rigid degrees of freedom to prevent ill-posed updates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces DefVINS, a visual-inertial odometry framework designed for deformable scenes. It separates rigid and non-rigid motion, uses an observability analysis to guide a progressive activation strategy for deformation, and shows improved robustness in non-rigid environments through ablation studies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DefVINS: Visual-Inertial Odometry for Deformable Scenes] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u521a\u6027\u5047\u8bbe\u5931\u6548 / Rigidity Assumption Violated]\n    B --\x3e B2[VIO\u5728\u975e\u521a\u6027\u573a\u666f\u4e2d\u6f02\u79fb / VIO Drift in Non-Rigid Scenes]\n    C --\x3e C1[\u5206\u79bb\u521a\u6027\u72b6\u6001\u4e0e\u975e\u521a\u6027\u5f62\u53d8 / Separate Rigid & Non-Rigid State]\n    C --\x3e C2[\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u4e0eIMU\u951a\u5b9a / Observability Analysis & IMU Anchoring]\n    C --\x3e C3[\u57fa\u4e8e\u6761\u4ef6\u7684\u6e10\u8fdb\u6fc0\u6d3b / Conditioning-Based Progressive Activation]\n    D --\x3e D1[\u63d0\u5347\u975e\u521a\u6027\u73af\u5883\u9c81\u68d2\u6027 / Improved Robustness in Non-Rigid Environments]\n    D --\x3e D2[\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027 / Ablation Studies Validate Benefits]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image demosaicing], [isotropic networks, spatial downsampling, joint-demosaicing-and-denoising (JDD), DeepMAD, JD3Net]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Cory Fan, Wenchao Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cornell University, OmniVision Technologies"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00703",children:"https://arxiv.org/pdf/2601.00703"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes that spatial downsampling can improve the efficiency and performance of isotropic networks for demosaicing, contrary to common design practices. 2. Designs and validates simple fully convolutional networks with downsampling using a mathematical architecture design technique adapted from DeepMAD. 3. Introduces JD3Net, a downsampled variant, which demonstrates strong empirical performance on various image demosaicing and JDD tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational inefficiency of isotropic networks for image demosaicing on mobile platforms. It proposes using spatial downsampling within these networks to improve efficiency and performance, validating the claim through network design and empirical testing of a model called JD3Net. The results show that the downsampled networks achieve strong performance on demosaicing and joint-demosaicing-and-denoising tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u79fb\u52a8\u8bbe\u5907\u4e0a\u6df1\u5ea6\u5b66\u4e60\u53bb\u9a6c\u8d5b\u514b\u7684\u8ba1\u7b97\u6210\u672c\u9ad8/High computational cost of deep learning demosaicing on mobile")\n    Method --\x3e M1("\u5728\u5404\u9879\u540c\u6027\u7f51\u7edc\u4e2d\u4f7f\u7528\u7a7a\u95f4\u4e0b\u91c7\u6837/Using spatial downsampling in isotropic networks")\n    Method --\x3e M2("\u57fa\u4e8eDeepMAD\u8bbe\u8ba1\u5168\u5377\u79ef\u7f51\u7edc/Designing fully convolutional networks based on DeepMAD")\n    Method --\x3e M3("\u63d0\u51faJD3Net\u6a21\u578b/Proposing the JD3Net model")\n    Results --\x3e R1("\u4e0b\u91c7\u6837\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd/Downsampling improves efficiency and performance")\n    Results --\x3e R2("JD3Net\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2/JD3Net shows strong performance on various tasks")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00705",children:"https://arxiv.org/pdf/2601.00705"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GS-SLAM\u7684\u6b8b\u5dee\u9a71\u52a8\u81f4\u5bc6\u5316\u6548\u7387\u4f4e/Inefficient residual-driven densification in GS-SLAM]\n    C --\x3e C1[\u4e00\u6b21\u6027\u5bc6\u96c6\u521d\u59cb\u5316/One-shot dense initialization]\n    C1 --\x3e C2[\u4f7f\u7528DINOv3\u7279\u5f81\u4e0e\u7f6e\u4fe1\u5185\u70b9\u5206\u7c7b\u5668/Using DINOv3 features & confidence-aware inlier classifier]\n    C2 --\x3e C3[\u591a\u89c6\u89d2\u4e09\u89d2\u5316\u751f\u6210\u9ad8\u65af\u5148\u9a8c/Multi-view triangulation for Gaussian prior]\n    D --\x3e D1[\u6536\u655b\u52a0\u901f~20%/~20% faster convergence]\n    D --\x3e D2[\u66f4\u9ad8\u6e32\u67d3\u4fdd\u771f\u5ea6/Higher rendering fidelity]\n    D --\x3e D3[\u5b9e\u65f6\u6027\u80fd\u8fbe925 FPS/Real-time performance up to 925 FPS]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [continual learning], [multi-level feature fusion, catastrophic forgetting, visual quality inspection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Johannes C. Bauer, Paul Geng, Stephan Trattnig, Petr Dokl\xe1dal, R\xfcdiger Daub"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, MINES Paris, Fraunhofer IGCV"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00725",children:"https://arxiv.org/pdf/2601.00725"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a multi-level feature fusion (MLFF) approach for continual learning that utilizes representations from different depths of a pretrained network. 2. Demonstrates that MLFF matches end-to-end training performance for quality inspection tasks while using significantly fewer trainable parameters. 3. Shows that the approach reduces catastrophic forgetting and improves generalization robustness to new product types or defects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of continual learning for visual quality inspection in volatile manufacturing scenarios like remanufacturing. It proposes a Multi-Level Feature Fusion (MLFF) method that fuses features from different network depths to enable efficient model adaptation. The results show that MLFF achieves performance comparable to full retraining with fewer parameters, while also mitigating catastrophic forgetting and improving robustness to new defects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u6a21\u578b\u5728\u52a8\u6001\u573a\u666f\uff08\u5982\u518d\u5236\u9020\uff09\u4e2d\u9700\u8981\u6301\u7eed\u9002\u5e94\uff0c\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u591a\u5c42\u7ea7\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u4e0d\u540c\u6df1\u5ea6\u7684\u8868\u5f81]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u5339\u914d\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u53c2\u6570\u66f4\u5c11\uff0c\u51cf\u5c11\u9057\u5fd8\uff0c\u63d0\u5347\u6cdb\u5316\u9c81\u68d2\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [domain adaptation], [data shift detection, performance degradation monitoring, vision-language model, confidence-based indicator, digital pathology]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hao Guan, Li Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Brigham and Women's Hospital, Harvard Medical School"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00716",children:"https://arxiv.org/pdf/2601.00716"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[VLM\u6027\u80fd\u5728\u6570\u636e\u504f\u79fb\u540e\u4e0b\u964d/VLM performance degrades after data shift]\n    C --\x3e C1[\u5f00\u53d1\u8f93\u5165\u6570\u636e\u504f\u79fb\u68c0\u6d4b\u5de5\u5177/DomainSAT toolbox for input shift]\n    C --\x3e C2[\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8f93\u51fa\u76d1\u6d4b\u6307\u6807/Confidence-based output indicator]\n    D --\x3e D1[\u8f93\u5165\u504f\u79fb\u68c0\u6d4b\u6709\u6548\u4f46\u4e0d\u603b\u5bf9\u5e94\u6027\u80fd\u4e0b\u964d/Input shift detection effective but not always correlates with degradation]\n    D --\x3e D2[\u7f6e\u4fe1\u5ea6\u6307\u6807\u4e0e\u6027\u80fd\u4e0b\u964d\u5bc6\u5207\u76f8\u5173/Confidence indicator closely related to degradation]\n    D --\x3e D3[\u7ed3\u5408\u4e24\u8005\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u76d1\u6d4b/Combining both enables more reliable monitoring]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Grading Handwritten Engineering Exams with Multimodal Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [multimodal LLM, handwritten exam grading, reference grounding, ensemble grading, deterministic validation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Janez Per\u0161, Jon Muhovi\u010d, Andrej Ko\u0161ir, Bo\u0161tjan Murovec"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Ljubljana, Faculty of Electrical Engineering"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00730",children:"https://arxiv.org/pdf/2601.00730"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An end-to-end workflow for grading scanned handwritten engineering exams using multimodal LLMs that preserves standard paper-based exam processes, requiring only a handwritten reference solution and grading rules from the lecturer. 2. A multi-stage reliability design featuring format/presence checks, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. 3. Empirical evaluation demonstrating the pipeline's effectiveness with state-of-the-art backends (GPT-5.2, Gemini-3 Pro), achieving \u22488-point mean absolute difference to lecturer grades and low bias, while ablations confirm the necessity of structured prompting and reference grounding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an automated workflow for grading handwritten STEM exams using multimodal large language models. The method uses a lecturer's handwritten reference solution and grading rules, processed through a multi-stage pipeline with checks and ensemble grading for reliability. Evaluation shows the system achieves close agreement with human grades, confirming that structured prompting and reference grounding are essential for accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Grading Handwritten Engineering Exams with Multimodal LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u624b\u52a8\u8bc4\u5206\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55/Manual grading is slow and hard to scale]\n    C --\x3e C1[\u591a\u6a21\u6001LLM\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41/Multimodal LLM end-to-end workflow]\n    C --\x3e C2[\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u4e0e\u5206\u7ea7\u89c4\u5219/Reference solution & grading rules]\n    C --\x3e C3[\u591a\u9636\u6bb5\u53ef\u9760\u8bbe\u8ba1/Multi-stage reliability design]\n    D --\x3e D1[\u22488\u5206\u5e73\u5747\u7edd\u5bf9\u5dee/\u22488-point mean absolute difference]\n    D --\x3e D2[\u4f4e\u504f\u5dee\u4e0e\u89e6\u53d1\u7387/Low bias & trigger rate]\n    D --\x3e D3[\u7ed3\u6784\u5316\u63d0\u793a\u4e0e\u53c2\u8003\u81f3\u5173\u91cd\u8981/Structured prompting & reference essential]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Unified Primitive Proxies for Structured Shape Completion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape completion], [primitive proxies, structured shape completion, primitive assembly, online target updates, Chamfer distance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaiyu Chen, Yuqing Wang, Xiao Xiang Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Munich Center for Machine Learning"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00759",children:"https://arxiv.org/pdf/2601.00759"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://unico-completion.github.io",children:"https://unico-completion.github.io"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes UniCo, a model that predicts a complete set of primitives (geometry, semantics, inlier membership) in a single feed-forward pass for structured shape completion. 2. Introduces primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. 3. Presents a training strategy that couples primitives and points with online target updates for consistent optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of structured 3D shape completion from partial scans. It proposes UniCo, a model that uses primitive proxies to jointly predict a complete set of parametric primitives in a single pass, outperforming baselines by significantly lowering Chamfer distance and improving normal consistency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Unified Primitive Proxies for Structured Shape Completion] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Incomplete 3D scans lack structural regularities for downstream tasks)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: UniCo model with primitive proxies and a dedicated decoding pathway)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Lowers Chamfer distance by up to 50%, improves normal consistency by up to 7%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [audio deepfake detection], [Multimodal Large Language Models, audio deepfake detection, zero-shot, fine-tuning, multi-prompt]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Ropar, Birla Institute of Technology and Science Pilani Hyderabad Campus, Monash University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00777",children:"https://arxiv.org/pdf/2601.00777"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Pioneering exploration of Multimodal Large Language Models for audio deepfake detection, a largely unexplored area. 2. Introduction of a text-aware, context-rich, question-answer based multi-prompt approach to facilitate multimodal understanding for the task. 3. Comprehensive evaluation of models (Qwen2-Audio-7B-Instruct, SALMONN) in zero-shot and fine-tuned modes, demonstrating their potential on in-domain data with minimal supervision."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the use of Multimodal Large Language Models for detecting audio deepfakes by combining audio inputs with text prompts. The method employs a multi-prompt, question-answer approach and evaluates models in zero-shot and fine-tuned settings. The results show that while models struggle without training and on out-of-domain data, they achieve promising performance on in-domain data with minimal supervision, indicating a viable path forward."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Investigating MLLMs for Audio Deepfake Detection<br/>\u63a2\u7a76MLLMs\u7528\u4e8e\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["MLLMs for audio deepfakes unexplored<br/>MLLMs\u7528\u4e8e\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u672a\u88ab\u63a2\u7d22"]\n    Method["Audio + Multi-prompt QA approach<br/>\u97f3\u9891+\u591a\u63d0\u793a\u95ee\u7b54\u65b9\u6cd5"]\n    Results["Poor zero-shot, good fine-tuned on in-domain data<br/>\u96f6\u6837\u672c\u6548\u679c\u5dee\uff0c\u57df\u5185\u5fae\u8c03\u6548\u679c\u597d"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [self-supervised learning, auxiliary task, feature fusion, cross-dataset generalization, local directional pattern]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shukesh Reddy, Srijan Das, Abhijit Das"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani; University of North Carolina at Charlotte"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00789",children:"https://arxiv.org/pdf/2601.00789"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework that uses self-supervised learning as an auxiliary task to optimize the primary task of generalized deepfake detection. 2. Introduces a feature fusion strategy to combine representations from the self-supervised and primary tasks, creating a more powerful and unique feature set. 3. Demonstrates superior cross-dataset generalization performance on multiple deepfake datasets compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of poor cross-dataset generalization in deepfake detection. The proposed method, Fusion-SSAT, leverages self-supervised learning as an auxiliary task and fuses its features with those from the primary detection task to create a more robust representation. The results show that this approach achieves better generalizability across multiple datasets than current state-of-the-art detectors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Poor cross-dataset generalization of deepfake detectors)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Fuse features from self-supervised auxiliary task with primary detection task)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Better generalizability on cross-dataset evaluation)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [hypernetwork, conditional VAE, differential privacy, MMD alignment, client heterogeneity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sunny Gupta, Amit Sethi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Bombay"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00785",children:"https://arxiv.org/pdf/2601.00785"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/sunnyinAI/FedHypeVAE"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FedHypeVAE] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u975eIID\u6570\u636e\u4e0e\u9690\u79c1\u6311\u6218/non-IID & Privacy]\n    C --\x3e C1[\u8d85\u7f51\u7edc\u751f\u6210\u6761\u4ef6VAE/Hypernetwork-Generated Conditional VAE]\n    C --\x3e C2[\u5dee\u5206\u9690\u79c1\u8bad\u7ec3/Differentially Private Training]\n    C --\x3e C3[MMD\u5bf9\u9f50\u4e0e\u6b63\u5219\u5316/MMD Alignment & Regularization]\n    D --\x3e D1[\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u7edf\u4e00/Unified Personalization & Privacy]\n    D --\x3e D2[\u53ef\u63a7\u591a\u57df\u5408\u6210/Controllable Multi-Domain Synthesis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [U-Net, layer normalization, instance-batch normalization, left ventricle segmentation, cardiac MRI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenhui Chu, Nikolaos V. Tsekos"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Houston"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00794",children:"https://arxiv.org/pdf/2601.00794"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies\u2014layer normalization and a combined instance-batch normalization\u2014to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5de6\u5fc3\u5ba4\u5206\u5272\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981/Left ventricle segmentation is critical for clinical diagnosis]\n    C --\x3e C1[\u63d0\u51faLNU-Net\u548cIBU-Net/Propose LNU-Net and IBU-Net]\n    C1 --\x3e C2[\u57fa\u4e8eU-Net\uff0c\u91c7\u7528\u4e0d\u540c\u5f52\u4e00\u5316\u7b56\u7565/Based on U-Net with different normalization strategies]\n    D --\x3e D1[\u5728805\u5f20MRI\u56fe\u50cf\u4e0a\u8bc4\u4f30/Evaluated on 805 MRI images]\n    D1 --\x3e D2[\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Outperforms state-of-the-art approaches]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [quantum computing], [quantum dot, charge stability diagram, automated characterization, machine learning, image processing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00067",children:"https://arxiv.org/pdf/2601.00067"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method's effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Automated electrostatic characterization of quantum dot devices<br>\u91cf\u5b50\u70b9\u5668\u4ef6\u81ea\u52a8\u9759\u7535\u8868\u5f81] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Manual interpretation of CSDs is slow and error-prone<br>CSD\u624b\u52a8\u89e3\u91ca\u6162\u4e14\u6613\u9519]\n    C --\x3e C1[Integrates ML, image processing, object detection<br>\u96c6\u6210ML\u3001\u56fe\u50cf\u5904\u7406\u3001\u76ee\u6807\u68c0\u6d4b]\n    D --\x3e D1[Enables rapid extraction of device parameters (lever arms, couplings)<br>\u5b9e\u73b0\u5668\u4ef6\u53c2\u6570\u5feb\u901f\u63d0\u53d6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [Convolutional Neural Network, Transfer Learning, Chest X-ray, Pediatric Pneumonia, RegNet]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zahedan University of Medical Sciences, Guilan University of Medical Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00041",children:"https://arxiv.org/pdf/2601.00041"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Deep Learning Approach for Pediatric Pneumonia Diagnosis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u513f\u7ae5\u80ba\u708e\u8bca\u65ad\u56f0\u96be / Pediatric Pneumonia Diagnosis is Challenging]\n    C --\x3e C1[\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60 / Use Pretrained CNNs with Transfer Learning]\n    C --\x3e C2[\u8bc4\u4f30ResNetRS, RegNet, EfficientNetV2 / Evaluate ResNetRS, RegNet, EfficientNetV2]\n    D --\x3e D1[RegNet\u6027\u80fd\u6700\u4f73 / RegNet Achieved Best Performance]\n    D --\x3e D2[\u51c6\u786e\u738792.4%, \u654f\u611f\u5ea690.1% / Accuracy 92.4%, Sensitivity 90.1%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural fields, signal processing], [Neural Radiance Fields (NeRF), EEG, brain-computer interfaces, signal reconstruction, continuous representation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ben Gurion University of the Negev, Tel Aviv University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00012",children:"https://arxiv.org/pdf/2601.00012"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Shaharak88/neural-brain-fields",children:"https://github.com/Shaharak88/neural-brain-fields"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>EEG data challenges: low SNR, variability, limited datasets"] --\x3e P1["\u5177\u4f53\u6311\u6218/Challenges<br>Varying length, low SNR, participant differences"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>NeRF-inspired neural network for EEG"] --\x3e M1["\u6838\u5fc3\u7c7b\u6bd4/Core Analogy<br>Viewpoints (NeRF) \u2194 Electrodes (EEG)"]\n    Method --\x3e M2["\u6280\u672f\u5b9e\u73b0/Technique<br>Train on single sample to get fixed weight vector"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Enables continuous visualization & reconstruction"] --\x3e R1["\u529f\u80fd\u4e00/Capability 1<br>Render signal at unseen times/positions"]\n    Results --\x3e R2["\u529f\u80fd\u4e8c/Capability 2<br>Simulate non-existent electrodes"]\n    Results --\x3e R3["\u5b9e\u8bc1\u7ed3\u679c/Empirical Result<br>Improves standard EEG network performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [dynamic scene reconstruction], [Adaptive Gabor Representation, Cubic Hermite Splines, Temporal Curvature Regularization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University, University of Zaragoza"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00796",children:"https://arxiv.org/pdf/2601.00796"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Adaptive Gabor Representation, which extends Gaussian primitives with learnable frequency weights and adaptive energy compensation to capture high-frequency details while maintaining stability. 2. Introduced a temporal continuity modeling approach using Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution and reduce interpolation artifacts. 3. Designed an Adaptive Initialization mechanism that leverages depth estimation, point tracking, and foreground masks to establish a stable initial point cloud distribution for efficient training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AdaGaR, a novel framework for dynamic 3D scene reconstruction from monocular videos. It introduces an Adaptive Gabor Representation to capture high-frequency details and uses Cubic Hermite Splines with regularization to ensure temporal smoothness. Experiments show state-of-the-art performance on the DAVIS dataset, achieving superior rendering quality and strong generalization in tasks like frame interpolation and video editing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u95ee\u9898: Existing Method Issues]\n    B1 --\x3e B1_1[\u9ad8\u65af\u539f\u8bed\u4f4e\u901a\u6ee4\u6ce2: Gaussian Primitives Low-Pass Filtering]\n    B1 --\x3e B1_2[\u6807\u51c6Gabor\u80fd\u91cf\u4e0d\u7a33\u5b9a: Standard Gabor Energy Instability]\n    B1 --\x3e B1_3[\u7f3a\u4e4f\u65f6\u95f4\u8fde\u7eed\u6027\u5bfc\u81f4\u4f2a\u5f71: Lack of Temporal Continuity Causes Artifacts]\n    C --\x3e C1[\u81ea\u9002\u5e94Gabor\u8868\u793a: Adaptive Gabor Representation]\n    C1 --\x3e C1_1[\u53ef\u5b66\u4e60\u9891\u7387\u6743\u91cd: Learnable Frequency Weights]\n    C1 --\x3e C1_2[\u81ea\u9002\u5e94\u80fd\u91cf\u8865\u507f: Adaptive Energy Compensation]\n    C --\x3e C2[\u65f6\u95f4\u8fde\u7eed\u6027\u5efa\u6a21: Temporal Continuity Modeling]\n    C2 --\x3e C2_1[\u4e09\u6b21Hermite\u6837\u6761: Cubic Hermite Splines]\n    C2 --\x3e C2_2[\u65f6\u95f4\u66f2\u7387\u6b63\u5219\u5316: Temporal Curvature Regularization]\n    C --\x3e C3[\u81ea\u9002\u5e94\u521d\u59cb\u5316: Adaptive Initialization]\n    C3 --\x3e C3_1[\u6df1\u5ea6\u4f30\u8ba1: Depth Estimation]\n    C3 --\x3e C3_2[\u70b9\u8ddf\u8e2a: Point Tracking]\n    C3 --\x3e C3_3[\u524d\u666f\u63a9\u7801: Foreground Masks]\n    D --\x3e D1[\u6700\u5148\u8fdb\u6027\u80fd: State-of-the-Art Performance]\n    D1 --\x3e D1_1[PSNR: 35.49]\n    D1 --\x3e D1_2[SSIM: 0.9433]\n    D1 --\x3e D1_3[LPIPS: 0.0723]\n    D --\x3e D2[\u5f3a\u5927\u6cdb\u5316\u80fd\u529b: Strong Generalization]\n    D2 --\x3e D2_1[\u5e27\u63d2\u503c: Frame Interpolation]\n    D2 --\x3e D2_2[\u6df1\u5ea6\u4e00\u81f4\u6027: Depth Consistency]\n    D2 --\x3e D2_3[\u89c6\u9891\u7f16\u8f91: Video Editing]\n    D2 --\x3e D2_4[\u7acb\u4f53\u89c6\u56fe\u5408\u6210: Stereo View Synthesis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [lesion attention, explainable AI, Grad-CAM, sensitivity analysis, transfer learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tanay Donde"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00355",children:"https://arxiv.org/pdf/2601.00355"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Analyzed the relationship between model attention on lesion areas and diagnostic performance metrics (precision, recall, F1-score) in melanoma classification. 2. Employed a multi-faceted methodology involving masked images, bounding box detection, and transfer learning to investigate lesion focus. 3. Demonstrated that models with higher focus on lesion areas achieve better diagnostic performance, highlighting the value of interpretable AI for building trustworthy medical models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how the focus (attention) of AI models on lesion areas affects their performance in classifying melanoma from skin images. The authors used methods like masked images and explainability techniques (e.g., Grad-CAM) to analyze this relationship. They found that models which pay more attention to the actual lesion regions perform better, suggesting that improving model interpretability can lead to more accurate and reliable diagnostic tools."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Impact of Lesion Focus on AI-Based Melanoma Classification<br>\u75c5\u7076\u7126\u70b9\u5bf9AI\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>CNN models lack reliability due to inconsistent focus on lesion areas.<br>CNN\u6a21\u578b\u56e0\u5bf9\u75c5\u7076\u533a\u57df\u5173\u6ce8\u4e0d\u4e00\u81f4\u800c\u7f3a\u4e4f\u53ef\u9760\u6027\u3002]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Analyze lesion attention using masked images, bounding box detection, transfer learning, and explainability methods.<br>\u4f7f\u7528\u63a9\u7801\u56fe\u50cf\u3001\u8fb9\u754c\u6846\u68c0\u6d4b\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790\u75c5\u7076\u6ce8\u610f\u529b\u3002]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Higher model focus on lesion areas correlates with better diagnostic performance (precision, recall, F1-score).<br>\u6a21\u578b\u5bf9\u75c5\u7076\u533a\u57df\u7684\u66f4\u9ad8\u5173\u6ce8\u4e0e\u66f4\u597d\u7684\u8bca\u65ad\u6027\u80fd\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\uff09\u76f8\u5173\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);