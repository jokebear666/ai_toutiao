"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4142],{1202:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"daily/cs_NE/20260105-20260111","title":"20260105-20260111 (cs.NE)","description":"2026-01-05","source":"@site/docs/daily/cs_NE/20260105-20260111.md","sourceDirName":"daily/cs_NE","slug":"/daily/csne/20260105-20260111","permalink":"/ai_toutiao/daily/csne/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/csne/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.NE)","permalink":"/ai_toutiao/daily/csne/20251229-20260104"},"next":{"title":"cs.NI","permalink":"/ai_toutiao/category/csni"}}');var s=i(74848),a=i(28453);const t={slug:"/daily/csne/20260105-20260111"},o="20260105-20260111 (cs.NE)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20260105-20260111-csne",children:"20260105-20260111 (cs.NE)"})}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [ferroelectric synapses, spiking neural networks, EEG signal processing, adaptive learning, neuromorphic computing]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nikhil Garg, Anxiong Song, Niklas Plessnig, Nathan Savoia, Laura B\xe9gon-Lours"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Zurich (Integrated Systems Laboratory, Department of Information Technology and Electrical Engineering)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00020",children:"https://arxiv.org/pdf/2601.00020"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrated the deployment and adaptation of Spiking Neural Networks (SNNs) on fabricated ferroelectric memristive synaptic devices for EEG-based motor imagery decoding under realistic device constraints. 2. Introduced a device-aware weight-update strategy that accumulates gradient updates digitally and triggers discrete programming events only when a threshold is exceeded, reducing programming frequency and emulating device dynamics. 3. Evaluated two complementary deployment strategies (device-aware training and transfer learning with on-device re-tuning) that achieve performance comparable to software-based SNNs and show improved accuracy through subject-specific adaptation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce95488f8704205e4a01e64825c78c800662f36da33df71b138881b21ab7b9bb_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of adapting EEG-based brain-computer interfaces to non-stationary neural signals on resource-constrained hardware. It proposes deploying Spiking Neural Networks on ferroelectric memristive synapses with a novel device-aware update strategy and demonstrates two effective deployment methods for personalized, low-overhead adaptation. The results show that programmable ferroelectric hardware can support robust, efficient adaptation for personalized neuromorphic processing."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: EEG\u4fe1\u53f7\u975e\u5e73\u7a33\u6027\u9650\u5236\u6a21\u578b\u6cdb\u5316\uff0c\u9700\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e2a\u6027\u5316\u9002\u5e94/Non-stationary EEG signals limit model generalization, requiring personalized adaptation on resource-constrained platforms]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5728\u94c1\u7535\u5fc6\u963b\u7a81\u89e6\u4e0a\u90e8\u7f72SNN\uff0c\u91c7\u7528\u8bbe\u5907\u611f\u77e5\u7684\u6743\u91cd\u66f4\u65b0\u7b56\u7565/Deploy SNNs on ferroelectric memristive synapses with a device-aware weight-update strategy]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u4e24\u79cd\u90e8\u7f72\u7b56\u7565\u6027\u80fd\u5ab2\u7f8e\u8f6f\u4ef6SNN\uff0c\u7279\u5b9a\u5bf9\u8c61\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u51c6\u786e\u7387/Two deployment strategies achieve performance comparable to software SNNs, subject-specific transfer learning improves accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Covariance Matrix Adaptation Evolution Strategy without a matrix"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [evolutionary computation], [CMA-ES, matrix-free, high-dimensional optimization, step-size adaptation, black-box optimization]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jaros\u0142aw Arabas, Adam Stelmaszczyk, Eryk Warchulski, Dariusz Jagodzi\u0144ski, Rafa\u0142 Biedrzycki"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Warsaw University of Technology"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00102",children:"https://arxiv.org/pdf/2601.00102"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel, matrix-free variant of CMA-ES that eliminates the need for covariance matrix decomposition by using an archive of normalized difference vectors. 2. Provides a theoretical proof that the probability distribution of individuals generated by the matrix-free method is identical to that of the standard CMA-ES. 3. Demonstrates experimentally that the method maintains or improves optimization efficiency, especially when combined with step-size adaptation, and allows for a reduced archive size without performance loss."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57b7923e9e9cac21a68aeaefb1bed6421d350b6a739556df9a54b09abc025039_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57b7923e9e9cac21a68aeaefb1bed6421d350b6a739556df9a54b09abc025039_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) in high dimensions, which stems from the cubic cost of matrix decomposition. It introduces a matrix-free CMA-ES that generates new solutions by taking weighted combinations of past normalized difference vectors stored in an archive, eliminating the need for an explicit covariance matrix. The method is proven to be distributionally equivalent to standard CMA-ES and is shown to achieve comparable or superior performance, particularly when coupled with step-size adaptation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Covariance Matrix Adaptation Evolution Strategy without a matrix] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>CMA-ES\u5728\u9ad8\u7ef4\u7a7a\u95f4\u7684\u8ba1\u7b97\u74f6\u9888<br>Computational bottleneck of CMA-ES in high dimensions]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u63d0\u51fa\u65e0\u77e9\u9635CMA-ES\uff0c\u4f7f\u7528\u5b58\u6863\u5411\u91cf\u52a0\u6743\u7ec4\u5408<br>Propose matrix-free CMA-ES using weighted combination of archive vectors]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5206\u5e03\u7b49\u4ef7\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u6536\u655b\u66f4\u5feb<br>Distributionally equivalent, comparable/superior performance, faster convergence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Reservoir Computing, Sequential Architecture, Spatiotemporal Forecasting, High-dimensional Data, Training Efficiency]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ata Akbari Asanjan, Filip Wudarski, Daniel O'Connor, Shaun Geaney, Elena Strbac, P. Aaron Lott, Davide Venturelli"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," USRA Research Institute for Advanced Computer Science (RIACS), Standard Chartered Bank"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00172",children:"https://arxiv.org/pdf/2601.00172"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a Sequential Reservoir Computing architecture that decomposes a large reservoir into smaller, interconnected ones to reduce computational and memory costs. 2. Demonstrates superior performance with longer forecast horizons and lower error metrics on chaotic and high-dimensional physical systems compared to RNN/LSTM baselines. 3. Achieves up to three orders of magnitude lower training cost, maintaining RC's efficiency while improving scalability for high-dimensional forecasting."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80a4bf491108d4e92c2c08807229cd230f08dd3e916c820595ac48efa8952783_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Sequential Reservoir Computing, a novel architecture that breaks a large reservoir into a sequence of smaller ones to efficiently forecast high-dimensional spatiotemporal systems. It outperforms traditional RNNs and LSTMs in forecast horizon and accuracy while drastically reducing training costs, offering a path to real-time, energy-efficient forecasting."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>RNN/LSTM\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u4f20\u7edfRC\u6269\u5c55\u6027\u5dee"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u987a\u5e8f\u50a8\u5c42\u8ba1\u7b97\u67b6\u6784"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u9884\u6d4b\u66f4\u957f\uff0c\u8bef\u5dee\u66f4\u4f4e\uff0c\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u964d\u4f4e"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [neuromorphic computing, state-space models, sparse attention, surrogate gradients, local learning rules]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Osvaldo Simeone"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Northeastern University London (Intelligent Networked Systems Institute - INSI)"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00245",children:"https://arxiv.org/pdf/2601.00245"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel conceptual framework for analyzing modern neuromorphic AI through the lens of intra-token (feature-level) and inter-token (contextual) processing. 2. Systematically reviews the convergence of neuromorphic principles (e.g., sparse, discrete activations) with state-of-the-art AI architectures like state-space models and transformers. 3. Reviews and categorizes training methodologies for neuromorphic models, from surrogate gradients to local learning rules based on reinforcement learning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9154dccb8e64d45d3b60bd0f6bb1e84fa9423cf041633e14a8cb6272baa46_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high energy costs of modern AI by exploring the convergence of neuromorphic computing principles with contemporary architectures. It proposes a framework distinguishing between intra-token and inter-token processing to analyze how neuromorphic ideas like sparse activations and state dynamics are embodied in models such as transformers and state-space models. The main conclusion is that modern AI is increasingly adopting brain-inspired, energy-efficient neuromorphic principles for both processing types, offering a path toward more sustainable intelligent systems."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u80fd\u8017\u589e\u957f / Escalating AI Energy Requirements]\n    C --\x3e C1[\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u539f\u5219 / Neuromorphic Computing Principles]\n    C1 --\x3e C2[\u79bb\u6563\u7a00\u758f\u6fc0\u6d3b / Discrete & Sparse Activations]\n    C1 --\x3e C3[\u5faa\u73af\u52a8\u6001 / Recurrent Dynamics]\n    C --\x3e C4[\u5904\u7406\u6846\u67b6: \u4ee4\u724c\u5185\u4e0e\u4ee4\u724c\u95f4 / Processing Framework: Intra-Token vs. Inter-Token]\n    D --\x3e D1[\u73b0\u4ee3AI\u4f53\u73b0\u795e\u7ecf\u5f62\u6001\u539f\u5219 / Modern AI Embodies Neuromorphic Principles]\n    D --\x3e D2[\u8fde\u63a5SNN\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001Transformer / Connects SNNs, State-Space Models, Transformers]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Rectifying Adversarial Examples Using Their Vulnerabilities"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial defense], [adversarial examples, label rectification, re-attack, white-box attack, black-box attack]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Fumiya Morimoto, Ryuto Morita, Satoshi Ono"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Kagoshima University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00270",children:"https://arxiv.org/pdf/2601.00270"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel adversarial example rectification method based on "re-attacking" AEs to move them beyond the decision boundary for correct label estimation. 2. The method is designed to be straightforward, requiring only AEs as input without parameter adjustments or preliminary training, enabling it to address diverse attack types. 3. Demonstrates consistent performance and superior stability against various attacks, including targeted and black-box attacks, compared to conventional rectification and input transformation methods.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c32aaba927fb42e32f98d767a04b8c60ecebe5d8f0f13c4c25f72d7d23e5138c_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the problem of rectifying adversarial examples (AEs) to recover the correct labels of the original inputs, which is crucial for applications like autonomous driving. The proposed method works by "re-attacking" the AEs to push them across the model\'s decision boundary. The results show that this method performs consistently across different attack types and is more stable than existing approaches.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Rectifying Adversarial Examples Using Their Vulnerabilities] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: DNNs misclassify adversarial examples, needing correct label recovery)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Re-attack AEs to move them beyond decision boundary)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Consistent performance across attacks, outperforms conventional methods in stability)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Vehicle Painting Robot Path Planning Using Hierarchical Optimization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [robotic path planning], [hierarchical optimization, vehicle routing problem (VRP), constraint handling, evolutionary computation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuya Nagai, Hiromitsu Nakamura, Narito Shinmachi, Yuta Higashizono, Satoshi Ono"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Kagoshima University, TOYOTA Body Research & Development Co., Ltd."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00271",children:"https://arxiv.org/pdf/2601.00271"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Formulates vehicle painting robot path planning as a hierarchical optimization problem, separating high-level task assignment (VRP-like) from low-level detailed path planning. 2. Proposes a flexible constraint handling framework for the painting process through custom variable representation, repair operators, and initialization. 3. Demonstrates the method's effectiveness by automatically generating paths for commercial vehicles that are comparable in quality to manual designs."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd6fcb32eb9c80960a0ed996281f691e7732ffdfe85f78eaf940b5a4d0c7ce64_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd6fcb32eb9c80960a0ed996281f691e7732ffdfe85f78eaf940b5a4d0c7ce64_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the manual and time-consuming task of planning paint paths for multiple robotic arms in vehicle factories. It proposes a hierarchical optimization method that treats the problem as a high-level vehicle routing task and a low-level detailed path planning task, enabling automated design. Experiments on real vehicle models show the method can generate constraint-satisfying paths of comparable quality to those created by human engineers."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root("Vehicle Painting Robot Path Planning Using Hierarchical Optimization") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Manual paint path design for multiple robotic arms is time-consuming")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical optimization (Upper: VRP-like assignment, Lower: detailed path planning)")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Automatically generates constraint-satisfying paths comparable to manual designs")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-healing, distributed computing continuum, language model agents, multi-agent systems, fault tolerance]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lov\xe9n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Lule\xe5 University of Technology, Peking University, TU Wien"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00339",children:"https://arxiv.org/pdf/2601.00339"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ReCiSt, a novel bio-inspired framework that maps biological self-healing phases (Hemostasis, Inflammation, Proliferation, Remodeling) to computational layers (Containment, Diagnosis, Meta-Cognitive, Knowledge) for resilience in DCCS. 2. Proposes the use of Language Model (LM)-powered agents to autonomously interpret logs, diagnose faults, and reconfigure resources with minimal human intervention. 3. Demonstrates the framework's capability for self-healing within tens of seconds with low resource overhead (e.g., 10% CPU usage) through evaluation on public fault datasets."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61eb83e4510dadeebc7e84fe1a44a89c218981f7cc3a6c7ef337ea51860d2146_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes ReCiSt, a bio-inspired, agent-based framework that uses Language Model-powered agents to autonomously detect, diagnose, and recover from faults in Distributed Computing Continuum Systems. The framework is evaluated on public datasets, showing it can achieve self-healing in tens of seconds with minimal resource overhead."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["Bio-inspired Agentic Self-healing Framework<br>\u751f\u7269\u542f\u53d1\u7684\u667a\u80fd\u4f53\u81ea\u6108\u6846\u67b6"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>DCCS\u4e2d\u7684\u590d\u6742\u6027\u4e0e\u6545\u969c\u9891\u53d1<br>Complexity & Frequent Faults in DCCS"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>ReCiSt\u6846\u67b6: \u4eff\u751f\u56db\u5c42\u4e0eLM\u667a\u80fd\u4f53<br>ReCiSt Framework: Bio-inspired Layers & LM Agents"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u6570\u5341\u79d2\u5185\u81ea\u6108\uff0c\u4f4eCPU\u5f00\u9500<br>Self-healing in tens of seconds, low CPU overhead"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [efficient transformers], [astrocyte-inspired computing, long-term plasticity (LTP), short-term plasticity (STP), memory compression, Long Range Arena (LRA)]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Md Zesun Ahmed Mia, Malyaban Bal, Abhronil Sengupta"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Pennsylvania State University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00426",children:"https://arxiv.org/pdf/2601.00426"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RMAAT, a novel Transformer architecture that integrates abstracted astrocyte functionalities for efficient long-context processing. 2. Proposes an adaptive memory compression mechanism governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP). 3. Develops Astrocytic Memory Replay Backpropagation (AMRB), a novel training algorithm designed for memory efficiency in recurrent networks."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48db0bbab3b8ca0fcbec4bfde72ebb384d63651cf925a575ef2f9e06a070abc5_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the quadratic complexity problem of Transformer self-attention for long sequences by proposing RMAAT, an architecture inspired by astrocyte functions in biological memory. The method uses recurrent segment-based processing with adaptive memory compression and a linear-complexity attention mechanism. Evaluations on the Long Range Arena benchmark show that RMAAT achieves competitive accuracy with substantial improvements in computational and memory efficiency."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    Root[RMAAT: Astrocyte-Inspired Memory Compression and Replay] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6/Quadratic Complexity of Self-Attention]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u542f\u53d1\u7684\u5faa\u73af\u8bb0\u5fc6\u67b6\u6784/Astrocyte-Inspired Recurrent Memory Architecture]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728LRA\u57fa\u51c6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387/Competitive Accuracy & Efficiency on LRA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Benchmarking ERP Analysis: Manual Features, Deep Learning, and Foundation Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interfaces], [event-related potential, EEG, deep learning, transformer, benchmark]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yihe Wang, Zhiqiao Kang, Bohan Chen, Yu Zhang, Xiang Zhang"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of North Carolina-Charlotte, South China University of Technology, University of California-Davis, Stanford University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00573",children:"https://arxiv.org/pdf/2601.00573"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"code:"})," ",(0,s.jsx)(n.a,{href:"https://github.com/DL4mHealth/ERP-Benchmark",children:"https://github.com/DL4mHealth/ERP-Benchmark"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comprehensive benchmark comparing manual features, deep learning models, and EEG foundation models for ERP analysis. 2. Established a unified data preprocessing and training pipeline and evaluated methods on two tasks across 12 public datasets. 3. Investigated various patch-embedding strategies within Transformer architectures to identify designs better suited for ERP data."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58ac79c471f534c1c0facf539a3aecc0242ffa2b33fb14835383bc5dbe5a8d85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58ac79c471f534c1c0facf539a3aecc0242ffa2b33fb14835383bc5dbe5a8d85_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper benchmarks methods for analyzing Event-Related Potentials (ERPs) in EEG data. It systematically compares traditional manual features, deep learning models, and pre-trained foundation models using a unified pipeline across 12 datasets. The study provides a framework for method selection and identifies effective Transformer embedding strategies for ERP data."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Benchmarking ERP Analysis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[ERP\u5206\u6790\u4e2d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u6027\u672a\u5145\u5206\u63a2\u7d22/Deep learning effectiveness on ERP data underexplored]\n    C --\x3e C1[\u5efa\u7acb\u7edf\u4e00\u9884\u5904\u7406\u4e0e\u8bad\u7ec3\u6d41\u6c34\u7ebf/Establish unified preprocessing & training pipeline]\n    C --\x3e C2[\u7cfb\u7edf\u6bd4\u8f83\u4e09\u7c7b\u65b9\u6cd5/Systematically compare three method categories]\n    C --\x3e C3[\u7814\u7a76Transformer\u7684Patch\u5d4c\u5165\u7b56\u7565/Investigate Transformer patch-embedding strategies]\n    D --\x3e D1[\u63d0\u4f9b\u65b9\u6cd5\u9009\u62e9\u4e0e\u6a21\u578b\u8bbe\u8ba1\u6846\u67b6/Provide framework for method selection & model design]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Three factor delay learning rules for spiking neural networks"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [spiking neural networks, delay learning, three-factor learning, online learning, neuromorphic processors]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Luke Vassallo, Nima Taherinejad"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Heidelberg University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00668",children:"https://arxiv.org/pdf/2601.00668"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduced learnable synaptic and axonal delays into LIF-based SNNs and proposed novel three-factor learning rules for online, simultaneous learning of both weights and delays. 2. Employed a smooth Gaussian surrogate gradient exclusively for eligibility trace calculation to enable gradient-equivalent delay parameter updates. 3. Demonstrated significant improvements in model efficiency, achieving up to 6.6x model size reduction and 67% lower inference latency with minimal accuracy loss, enabling on-device learning for resource-constrained environments."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58cabcbc7350cb70af9599a2c224309ae64c370ecf64ef754054d42693a8f504_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limited temporal learning capability of Spiking Neural Networks (SNNs) by introducing learnable synaptic and axonal delays and proposing online three-factor learning rules to train them. The method uses a Gaussian surrogate gradient for eligibility traces and achieves competitive accuracy on temporal tasks like speech recognition while drastically reducing model size and latency. The findings facilitate efficient, on-device learning for power and area-constrained neuromorphic hardware."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Three factor delay learning rules for spiking neural networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[SNNs lack temporal parameters/SNNs\u7f3a\u4e4f\u65f6\u95f4\u53c2\u6570]\n    B --\x3e B2[Existing delay learning is offline & large/\u73b0\u6709\u5ef6\u8fdf\u5b66\u4e60\u662f\u79bb\u7ebf\u7684\u4e14\u6a21\u578b\u5927]\n    C --\x3e C1[Learnable synaptic & axonal delays/\u53ef\u5b66\u4e60\u7684\u7a81\u89e6\u548c\u8f74\u7a81\u5ef6\u8fdf]\n    C --\x3e C2[Three-factor online learning rules/\u4e09\u56e0\u7d20\u5728\u7ebf\u5b66\u4e60\u89c4\u5219]\n    C --\x3e C3[Gaussian surrogate for eligibility trace/\u7528\u4e8e\u8d44\u683c\u8ff9\u7684\u9ad8\u65af\u4ee3\u7406\u68af\u5ea6]\n    D --\x3e D1[Accuracy improved up to 20%/\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe20%]\n    D --\x3e D2[Model size reduced 6.6x/\u6a21\u578b\u5927\u5c0f\u51cf\u5c116.6\u500d]\n    D --\x3e D3[Latency reduced 67%/\u5ef6\u8fdf\u964d\u4f4e67%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [quantization, spike-driven language models (SLMs), memory footprint, tiered search, embedded systems]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Rachmad Vidya Wicaksana Putra, Pasindu Wickramasinghe, Muhammad Shafique"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," New York University (NYU) Abu Dhabi"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00679",children:"https://arxiv.org/pdf/2601.00679"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes QSLM, an automated quantization framework for compressing pre-trained Spike-driven Language Models (SLMs) to meet performance and memory constraints. 2. Introduces a tiered quantization strategy (global-, block-, and module-level) guided by network hierarchy and layer sensitivity analysis. 3. Leverages a multi-objective performance-and-memory trade-off function to select the final quantization setting, achieving significant memory and power reduction while maintaining high task performance."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/911838f93e33219fcf586121369dd081b9908c86a1169c367cfdd1bb7e50139b_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes QSLM, an automated framework for quantizing Spike-driven Language Models (SLMs) to reduce their memory footprint for embedded deployment. It uses a tiered search strategy based on network hierarchy and layer sensitivity, along with a multi-objective trade-off function, to find optimal quantization settings. Experimental results show QSLM can reduce memory by up to 86.5% and power by up to 20% while maintaining performance close to the original model."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: SLMs\u5185\u5b58\u5360\u7528\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907/SLMs have large memory footprints, challenging for resource-constrained embedded deployment]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u81ea\u52a8\u5316\u5206\u5c42\u91cf\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u7f51\u7edc\u5c42\u6b21\u3001\u5c42\u654f\u611f\u6027\u548c\u591a\u76ee\u6807\u6743\u8861\u51fd\u6570/Automated tiered quantization strategy using network hierarchy, layer sensitivity, and multi-objective trade-off]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u5360\u7528\u51cf\u5c11\u9ad8\u8fbe86.5%\uff0c\u529f\u8017\u964d\u4f4e\u9ad8\u8fbe20%\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b/Memory footprint reduced by up to 86.5%, power by up to 20%, performance close to original model]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Cost Optimization in Production Line Using Genetic Algorithm"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [combinatorial optimization], [genetic algorithm, task scheduling, production line, chromosome encoding, JGAP]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Alireza Rezaee"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Tehran"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00689",children:"https://arxiv.org/pdf/2601.00689"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and compares two chromosome encoding strategies (station-based and task-based) for a GA applied to a production line scheduling problem. 2. Adapts standard GA operators (crossover, mutation, etc.) to preserve solution feasibility under precedence and capacity constraints. 3. Empirically demonstrates that the task-based encoding yields smoother convergence and more reliable cost minimization, especially for problems with a large number of valid schedules."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d46632061e4499187a195e89a09ba37b6eff28c44f9881da5b237c2b781953b_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper applies a genetic algorithm to optimize task scheduling in a production line to minimize cost. It investigates two different ways to represent the schedule (encoding) within the algorithm and finds that a task-based encoding performs better, converging more smoothly to lower-cost solutions. The study shows GAs are advantageous for this type of complex, constrained scheduling problem."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Cost Optimization in Production Line Using Genetic Algorithm] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6700\u5c0f\u5316\u751f\u4ea7\u7ebf\u6210\u672c/Minimize Production Line Cost]\n    B --\x3e B2[\u4efb\u52a1\u8c03\u5ea6\u4e0e\u7ea6\u675f/Task Scheduling with Constraints]\n    C --\x3e C1[\u9057\u4f20\u7b97\u6cd5/Genetic Algorithm]\n    C --\x3e C2[\u4e24\u79cd\u7f16\u7801\u7b56\u7565/Two Encoding Strategies]\n    C2 --\x3e C21[\u57fa\u4e8e\u5de5\u4f4d\u7684\u7f16\u7801/Station-based Encoding]\n    C2 --\x3e C22[\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7801/Task-based Encoding]\n    D --\x3e D1[\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7801\u6027\u80fd\u66f4\u4f18/Task-based Encoding Performs Better]\n    D --\x3e D2[\u66f4\u5e73\u6ed1\u7684\u6536\u655b/Smoother Convergence]\n    D --\x3e D3[\u66f4\u53ef\u9760\u7684\u4f18\u5316/More Reliable Optimization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260105] Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [binary neural networks, quadratic unconstrained binary optimization, ising machine, regularization, simulated annealing]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jonas Christoffer Villumsen, Yusuke Sugita"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Hitachi Europe Ltd., Hitachi Ltd."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00449",children:"https://arxiv.org/pdf/2601.00449"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Extends existing QUBO models for training Binary Neural Networks to accommodate arbitrary network topologies. 2. Proposes a novel regularization method that maximizes neuron margins to bias training toward configurations with larger pre-activation magnitudes. 3. Proposes a second novel, dropout-inspired iterative regularization scheme that trains reduced subnetworks to adjust linear penalties on network parameters."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28133ab4719e6b11d3441bf2c924e33139268781e601ff1ef434dd50fa7de88c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28133ab4719e6b11d3441bf2c924e33139268781e601ff1ef434dd50fa7de88c_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of efficiently training discrete Binary Neural Networks (BNNs) by formulating the training as a Quadratic Unconstrained Binary Optimization (QUBO) problem. It extends existing QUBO models to arbitrary network topologies and introduces two new regularization methods to improve generalization. Experimental results on a GPU-based Ising machine show that the proposed methods modify training behavior and improve classification accuracy on unseen data."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Quadratic Unconstrained Binary Optimisation for Training and Regularisation of Binary Neural Networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u8bad\u7ec3\u4e8c\u5143\u795e\u7ecf\u7f51\u7edc\u7684\u6311\u6218/Challenge of training BNNs]\n    C --\x3e C1[\u6269\u5c55QUBO\u6a21\u578b/Extend QUBO models]\n    C --\x3e C2[\u63d0\u51fa\u4e24\u79cd\u6b63\u5219\u5316\u65b9\u6cd5/Propose two regularization methods]\n    C2 --\x3e C2_1[\u6700\u5927\u5316\u795e\u7ecf\u5143\u95f4\u9694/Maximize neuron margins]\n    C2 --\x3e C2_2[Dropout\u542f\u53d1\u5f0f\u8fed\u4ee3\u65b9\u6848/Dropout-inspired iterative scheme]\n    D --\x3e D1[\u6b63\u5219\u5316\u6539\u53d8\u8bad\u7ec3\u884c\u4e3a/Regularization modifies training behavior]\n    D --\x3e D2[\u63d0\u5347\u672a\u89c1\u6570\u636e\u5206\u7c7b\u7cbe\u5ea6/Improves classification accuracy on unseen data]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(96540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);