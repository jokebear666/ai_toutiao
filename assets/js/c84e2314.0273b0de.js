"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8449],{4145:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"daily/cs_HC/20251229-20260104","title":"20251229-20260104 (cs.HC)","description":"2025-12-29","source":"@site/docs/daily/cs_HC/20251229-20260104.md","sourceDirName":"daily/cs_HC","slug":"/daily/cshc/20251229-20260104","permalink":"/ai_toutiao/daily/cshc/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766994315000,"frontMatter":{"slug":"/daily/cshc/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.HC)","permalink":"/ai_toutiao/daily/cshc/20251222-20251228"},"next":{"title":"cs.IR","permalink":"/ai_toutiao/category/csir"}}');var s=i(4848),t=i(8453);const r={slug:"/daily/cshc/20251229-20260104"},o="20251229-20260104 (cs.HC)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251229-20260104-cshc",children:"20251229-20260104 (cs.HC)"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Dartmouth College"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21506",children:"https://arxiv.org/pdf/2512.21506"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [human-ai interaction], [bidirectional alignment, value-centered design, interactive alignment]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21551",children:"https://arxiv.org/pdf/2512.21551"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Human-AI Interaction Alignment] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Establishes agenda for reciprocal, responsible human-AI futures]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [ai for education], [human-ai alignment, trustworthy ai, adaptive learning, educational technology, ai ethics]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hua Shen"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," NYU Shanghai, New York University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21552",children:"https://arxiv.org/pdf/2512.21552"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the novel concept of "bidirectional human-AI alignment" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI\'s role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: Bidirectional Human-AI Alignment in Education] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: AI in education introduces risks to equity, privacy, and autonomy.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Emotion-Aware Smart Home Automation Based on the eBICA Model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [affective computing], [eBICA, emotion-aware automation, psychological safety, STAI-S, smart home]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Masaaki Yamauchi, Yiyuan Liang, Hiroko Hara, Hideyuki Shimonishi, Masayuki Murata"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The University of Osaka"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21589",children:"https://arxiv.org/pdf/2512.21589"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposed an emotion-aware smart home automation framework guided by the eBICA model for dynamic control based on emotional state. 2. Conducted a proof-of-concept experiment demonstrating a significant reduction in state anxiety (STAI-S) through comfort-inducing automation. 3. Found that individual personality and anxiety traits modulate the relief effect, indicating a pathway for personalized emotion-adaptive systems."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5260e7567742ea94b88d5dab934224f8814c9ad506e334dbc2220c01eec9093d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5260e7567742ea94b88d5dab934224f8814c9ad506e334dbc2220c01eec9093d_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study proposes a smart home automation framework that uses the eBICA model to adapt to a user's emotional state. A proof-of-concept experiment showed that anxiety-inducing automation significantly reduced user anxiety, demonstrating the framework's effectiveness in promoting psychological safety and its potential for personalization."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Emotion-Aware Smart Home Automation Based on the eBICA Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u81ea\u52a8\u5316\u7f3a\u4e4f\u60c5\u611f\u9002\u5e94<br/>Traditional automation lacks emotional adaptation]\n    C --\x3e C1[\u57fa\u4e8eeBICA\u7684\u6846\u67b6<br/>eBICA-based framework]\n    C --\x3e C2[\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c<br/>Proof-of-concept experiment]\n    D --\x3e D1[\u7126\u8651\u663e\u8457\u964d\u4f4e<br/>Significant anxiety reduction]\n    D --\x3e D2[\u4e2a\u6027\u5316\u6f5c\u529b<br/>Personalization potential]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Ghostcrafting AI: Under the Rug of Platform Labor"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [human-computer interaction], [platform labor, ghostcrafting, ethnography, ethical AI, situated learning]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," ATM Mizanur Rahman, Sharifa Sultana"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21649",children:"https://arxiv.org/pdf/2512.21649"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the novel conceptual framework of "Ghostcrafting AI" to describe the invisible and essential labor of platform workers in building and sustaining AI systems. 2. Provides an in-depth ethnographic account of the situated learning practices and coping tactics of platform workers in Bangladesh, revealing their resourcefulness and agency. 3. Highlights the structural precarity and exploitation faced by these workers, arguing for urgent design, policy, and governance interventions to ensure fairness and recognition.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49785e109f95da7d4a140badf3830b8bc2770c67e7d5e4a226476af7aecf0903_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49785e109f95da7d4a140badf3830b8bc2770c67e7d5e4a226476af7aecf0903_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the hidden labor of platform workers in the Global South who build and sustain AI systems. Through an eight-month ethnography in Bangladesh, it conceptualizes this as "Ghostcrafting AI" and documents how workers learn and cope with exploitative conditions. The study concludes that AI is fundamentally dependent on this invisible labor and calls for interventions to ensure fairness and sustainability in platform work.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root["Ghostcrafting AI: Under the Rug of Platform Labor"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Platform laborers are indispensable yet invisible in building AI systems."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Eight-month ethnography in Bangladesh\'s platform labor industry."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Reveals workers\' situated learning, coping tactics, and the need for fairness interventions."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface (BCI)], [EEG, TSception, Adaptive Average Pooling, spatiotemporal features, drowsiness detection]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology Roorkee, OP Jindal University, Lule\xe5 University of Technology, Indian Institute of Technology Dhanbad"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21747",children:"https://arxiv.org/pdf/2512.21747"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Driver drowsiness detection for road safety)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved stability on SEED-VIG, SOTA on STEW)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [nlp], [image captioning], [scientific figure captioning, large-scale dataset, domain-specific training, human evaluation, large language models (LLMs)]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University, Adobe Research"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21789",children:"https://arxiv.org/pdf/2512.21789"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u79d1\u5b66\u56fe\u8868\u8bf4\u660e\u8d28\u91cf\u5dee/Poor quality of scientific figure captions]\n    Problem --\x3e P2[\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6/Lack of large-scale real-world dataset]\n    Method --\x3e M1[\u6784\u5efaarXiv\u56fe\u8868-\u8bf4\u660e\u5bf9\u6570\u636e\u96c6/Construct arXiv figure-caption dataset]\n    Method --\x3e M2[\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u4e0e\u8bc4\u4f30/Domain-specific training & evaluation]\n    Method --\x3e M3[\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5174\u8d77/Navigate rise of LLMs]\n    Results --\x3e R1[\u603b\u7ed3\u6280\u672f\u65b9\u6cd5\u7ecf\u9a8c/Summarize technical & methodological lessons]\n    Results --\x3e R2[\u63d0\u51fa\u672a\u6765\u6311\u6218\u4e0e\u65b9\u5411/Outline future challenges & directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Generative Lecture: Making Lecture Videos Interactive with LLMs and AI Clone Instructors"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI)], [interactive videos, personalized learning, AI clone instructor, on-demand content generation, generative AI]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hye-Young Jo, Ada Zhao, Xiaoan Liu, Ryo Suzuki"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Colorado Boulder"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21796",children:"https://arxiv.org/pdf/2512.21796"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"}),' 1) Introduces the "Generative Lecture" concept for transforming passive lecture videos into interactive, two-way learning experiences using AI. 2) Proposes a system architecture that integrates an AI clone instructor (via HeyGen, ElevenLabs, GPT-5) with on-demand content generation to respond to student queries. 3) Identifies and implements eight key system features (e.g., on-demand clarification, adaptive quiz) based on a design study, and validates the system\'s usability and effectiveness through user studies.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/047a789db6b06e8758007487ecf18eb3b59ea0d4d56a243c23b590b8ad50497f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/047a789db6b06e8758007487ecf18eb3b59ea0d4d56a243c23b590b8ad50497f_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Generative Lecture, a system that uses generative AI and AI clone instructors to make existing lecture videos interactive, allowing students to ask questions and receive personalized, generated explanations. The system was developed based on user goals and features like on-demand clarification and adaptive quizzes. User studies suggest it enables effective two-way communication and supports personalized learning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    Root("Generative Lecture<br>\u751f\u6210\u5f0f\u8bb2\u5ea7") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Lecture videos are passive<br>\u8bb2\u5ea7\u89c6\u9891\u662f\u88ab\u52a8\u7684")\n    Method --\x3e M1("Use AI Clone Instructor & LLMs<br>\u4f7f\u7528AI\u514b\u9686\u8bb2\u5e08\u548cLLMs")\n    Method --\x3e M2("Generate on-demand content<br>\u751f\u6210\u6309\u9700\u5185\u5bb9")\n    Results --\x3e R1("Enables two-way communication<br>\u5b9e\u73b0\u53cc\u5411\u4ea4\u6d41")\n    Results --\x3e R2("Supports personalized learning<br>\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Conserved active information"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [information theory], [conserved active information, No-Free-Lunch, KL divergence, search space, information conservation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yanchen Chen, Daniel Andr\xe9s D\xedaz-Pach\xf3n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Miami"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21834",children:"https://arxiv.org/pdf/2512.21834"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Introduces conserved active information (I\u2295), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I\u2295 can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new information-theoretic measure called conserved active information (I\u2295) to quantify net information change in search problems while respecting conservation laws. It shows that I\u2295 uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    Root[Conserved active information] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem: Limitations of average-focused information measures like KL divergence]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce conserved active information I\u2295, a symmetric extension respecting No-Free-Lunch]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results: I\u2295 reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Positive Narrativity Enhances Sense of Agency toward a VR Avatar"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [virtual reality and embodiment], [full-body illusion, sense of agency, avatar narrativity, Proteus effect, bodily self-consciousness]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Kureha Hamagashira, Miyuki Azuma, Sotaro Shimada"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Meiji University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21968",children:"https://arxiv.org/pdf/2512.21968"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Investigated the explicit manipulation of avatar impressions using narrative context (positive vs. negative stories) to modulate the full-body illusion. 2. Demonstrated that positive narratives significantly enhance the sense of agency toward a VR avatar. 3. Found a positive correlation between the sense of agency and participants' perceived personal familiarity with the avatar."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d658df90fd7ef654f1504e1b44262071ad05b5dab0737598e4b949dd3f39383_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d658df90fd7ef654f1504e1b44262071ad05b5dab0737598e4b949dd3f39383_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study explores how narrative context affects embodiment in VR by having participants embody an avatar after hearing either a positive or negative story about it. The results show that positive narratives significantly increase the user's sense of agency over the avatar, and this feeling is linked to how familiar the avatar feels. This suggests that storytelling can be a tool to modulate virtual embodiment experiences."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Positive Narrativity Enhances Sense of Agency toward a VR Avatar] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How does narrative context affect the full-body illusion?);\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Participants embodied an avatar after listening to a positive or negative narrative about it.);\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Positive narratives enhanced sense of agency, which correlated with perceived familiarity.);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [other], [Human-Computer Interaction (HCI)], [Gestural Interaction, Physics Simulation, Air-drawn Sketches, VR Content Creation]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Beijing Technology and Business University, The University of Sydney"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22016",children:"https://arxiv.org/pdf/2512.22016"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Context-Aware Intelligent Chatbot Framework Leveraging Mobile Sensing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [mobile sensing, context-aware, large language models, structured prompting, digital health]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ziyan Zhang, Nan Gao, Zhiqiang Nie, Shantanu Pal, Haining Zhang"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Nankai University, Tsinghua University, Deakin University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22032",children:"https://arxiv.org/pdf/2512.22032"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a context-sensitive conversational assistant framework that integrates mobile sensing data with large language models. 2. Abstracts raw mobile sensing signals into 16 contextual scenarios and translates them into natural language prompts. 3. Designs a structured prompting system to guide the LLM in generating personalized and contextually relevant dialogue."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ee41bc75f2109be1e0a7714aeb8ea0c7f389bba53adcab3bd17db8b8d415623_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ee41bc75f2109be1e0a7714aeb8ea0c7f389bba53adcab3bd17db8b8d415623_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of LLMs in understanding real-world user behavior by proposing a chatbot framework that uses mobile sensing data. The method abstracts sensor data into contextual scenarios and converts them into natural language prompts to guide the LLM. The work demonstrates the potential of passive behavioral data for creating personalized, context-aware conversational agents, particularly for digital health applications."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[Context-Aware Intelligent Chatbot Framework<br>\u4e0a\u4e0b\u6587\u611f\u77e5\u667a\u80fd\u804a\u5929\u673a\u5668\u4eba\u6846\u67b6] --\x3e B[Problem: LLMs lack real-world user context<br>\u95ee\u9898\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u73b0\u5b9e\u7528\u6237\u60c5\u5883]\n    A --\x3e C[Method: Integrate mobile sensing & structured prompts<br>\u65b9\u6cd5\uff1a\u96c6\u6210\u79fb\u52a8\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63d0\u793a]\n    A --\x3e D[Results: Personalized, context-relevant dialogue<br>\u7ed3\u679c\uff1a\u4e2a\u6027\u5316\u3001\u60c5\u5883\u76f8\u5173\u7684\u5bf9\u8bdd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [autoregressive distillation, adversarial refinement, real-time streaming, reference-anchored positional re-encoding, consistency-aware discriminator]"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22065",children:"https://arxiv.org/pdf/2512.22065"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"code:"})," ",(0,s.jsx)(n.a,{href:"https://streamavatar.github.io",children:"https://streamavatar.github.io"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"contributions:"})," 1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"thumbnail:"})," ",(0,s.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TB\n    A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var a=i(6540);const s={},t=a.createContext(s);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);