"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4877],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},76737:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_RO/20260105-20260111","title":"20260105-20260111 (cs.RO)","description":"2026-01-05","source":"@site/docs/daily/cs_RO/20260105-20260111.md","sourceDirName":"daily/cs_RO","slug":"/daily/csro/20260105-20260111","permalink":"/ai_toutiao/daily/csro/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767583031000,"frontMatter":{"slug":"/daily/csro/20260105-20260111"},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104 (cs.RO)","permalink":"/ai_toutiao/daily/csro/20251229-20260104"},"next":{"title":"cs.SC","permalink":"/ai_toutiao/category/cssc"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/csro/20260105-20260111"},o="20260105-20260111 (cs.RO)",l={},d=[{value:"2026-01-05",id:"2026-01-05",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260105-20260111-csro",children:"20260105-20260111 (cs.RO)"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Metric Interval Temporal Logic (MITL), Timed Limit-Deterministic Generalized B\xfcchi Automata (Timed-LDGBA), Q-learning, POMDP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaoan Wang, Junchao Li, Mahdi Mohammad, Shaoping Xiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Iowa, Talus Renewables, Inc., Roma Tre University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00087",children:"https://arxiv.org/pdf/2601.00087"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified automata-based RL framework for synthesizing policies under MITL specifications in both MDPs and POMDPs. 2. Introduces a translation of MITL formulas into Timed-LDGBA and their synchronization with decision processes to create product timed models for Q-learning. 3. Validates the framework with simulation studies showing it satisfies time-bounded requirements, scales to larger state spaces, and works in partially observable environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/556427a99f7edc810f0aa638c014e36ca104d451bbd5edf5c81df58176b568d9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the challenge of integrating formal temporal logic (MITL) with reinforcement learning for robotic motion planning under stochastic and partially observable dynamics. It proposes a method that translates MITL specifications into timed automata and synchronizes them with the decision process to enable policy learning via Q-learning. The results demonstrate that the learned policies successfully satisfy strict time constraints in various simulated environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement learning with timed constraints for robotics motion planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u590d\u6742\u4efb\u52a1\u5e8f\u5217\u4e0e\u4e25\u683c\u65f6\u95f4\u7ea6\u675f/Complex task sequences & strict temporal constraints]\n    B --\x3e B2[\u968f\u673a\u52a8\u6001\u4e0e\u90e8\u5206\u53ef\u89c2\u6d4b\u6027/Stochastic dynamics & partial observability]\n    C --\x3e C1[MITL\u516c\u5f0f\u8f6c\u6362\u4e3aTimed-LDGBA/MITL to Timed-LDGBA translation]\n    C --\x3e C2[\u6784\u5efa\u4ea7\u54c1\u5b9a\u65f6\u6a21\u578b\u4e0eQ\u5b66\u4e60/Construct product timed models for Q-learning]\n    C --\x3e C3[\u7b80\u5355\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5956\u52b1\u7ed3\u6784/Simple yet expressive reward structure]\n    D --\x3e D1[\u6ee1\u8db3\u65f6\u95f4\u7ea6\u675f\u7684\u7b56\u7565/Policies satisfy time-bounded requirements]\n    D --\x3e D2[\u6269\u5c55\u5230\u66f4\u5927\u72b6\u6001\u7a7a\u95f4/Scales to larger state spaces]\n    D --\x3e D3[\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u6709\u6548/Effective in partially observable environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [geometric reinforcement learning, Hamiltonian optimization, simultaneous navigation and mapping, local sensory observations, path differential]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aditya Sai Ellendula, Yi Wang, Minh Nguyen, Chandrajit Bajaj"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Austin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00116",children:"https://arxiv.org/pdf/2601.00116"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/",children:"https://github.com/"}),' (as per the abstract "The code is publicly available on Github." The specific URL is not provided in the given text, only a placeholder link.)']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel geometric reinforcement learning framework (GRL-SNAM) for Simultaneous Navigation and Mapping that relies exclusively on local sensory observations without constructing a global map. 2. Formulates navigation and mapping as a dynamic shortest path search using controlled Hamiltonian optimization, translating sensory inputs into local energy landscapes and evolving policies via updating Hamiltonians. 3. Demonstrates that the method enables high-quality navigation with minimal exploration through local energy refinement, preserving clearance and generalizing to unseen environments in 2D navigation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/776eec0810502d9188877dd04875e090e89eaeb9b6aaa3dec59b37da20fe81b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping (SNAM) in unknown environments. It formulates the problem as a dynamic shortest path search using Hamiltonian optimization, where local sensory data is used to update energy landscapes and refine trajectories without building a global map. The method is shown to achieve efficient, high-quality navigation with minimal exploration and good generalization in 2D tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRL-SNAM: Geometric RL for SNAM] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Simultaneous Navigation and Mapping in mapless environments]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Geometric RL with Path Differential Hamiltonians, local energy landscapes]\n    D[\u5173\u952e\u7ed3\u679c/Results: High-quality navigation with minimal exploration, generalizes to unseen layouts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Compositional Diffusion with Guided search for Long-Horizon Planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [compositional generation, mode averaging, guided search, diffusion denoising, long-horizon planning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Utkarsh A Mishra, David He, Yongxin Chen, Danfei Xu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00126",children:"https://arxiv.org/pdf/2601.00126"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://cdgsearch.github.io/",children:"https://cdgsearch.github.io/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Compositional Diffusion with Guided Search (CDGS) to solve the mode averaging problem in compositional generative models. 2. Embeds a search mechanism within the diffusion denoising process, combining population-based sampling, likelihood-based filtering, and iterative resampling. 3. Demonstrates strong performance on robot manipulation tasks and generalizes to domains like panoramic image synthesis and long video generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41034b12b6ab0eabc5d0ac493a8094944df0b3944c3c4fe217daaa776e14f15b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41034b12b6ab0eabc5d0ac493a8094944df0b3944c3c4fe217daaa776e14f15b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the mode averaging problem in compositional generative models for long-horizon planning. It proposes CDGS, a method that integrates guided search into the diffusion process to explore and prune local mode combinations for globally coherent outputs. The approach matches oracle performance on robot tasks and generalizes across domains without requiring long-horizon training data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Compositional Diffusion with Guided Search for Long-Horizon Planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Mode Averaging in Compositional Models<br/>\u7ec4\u5408\u6a21\u578b\u4e2d\u7684\u6a21\u5f0f\u5e73\u5747\u95ee\u9898]\n    C --\x3e C1[Embedded Search in Diffusion<br/>\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u641c\u7d22]\n    C --\x3e C2[Population Sampling & Filtering<br/>\u7fa4\u4f53\u91c7\u6837\u4e0e\u8fc7\u6ee4]\n    C --\x3e C3[Iterative Resampling<br/>\u8fed\u4ee3\u91cd\u91c7\u6837]\n    D --\x3e D1[Matches Oracle Performance<br/>\u5339\u914dOracle\u6027\u80fd]\n    D --\x3e D2[Generalizes Across Domains<br/>\u8de8\u9886\u57df\u6cdb\u5316]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [multi-robot systems], [heterogeneous fleets, collaborative 3D exploration, intermittent communication, multi-layer planning, adaptive inspection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junfeng Chen, Yuxiao Zhu, Xintong Zhang, Bing Luo, Meng Guo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Peking University, Duke Kunshan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00163",children:"https://arxiv.org/pdf/2601.00163"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel planning and coordination framework (SLEI3D) for simultaneous 3D exploration, adaptive inspection, and timely communication under limited communication constraints. 2. A multi-layer and multi-rate planning mechanism to handle uncertainties in feature number/location and coordinate plans within and between robot subgroups. 3. Validation of the framework through extensive high-fidelity simulations with up to 48 robots and hardware experiments with 7 robots, demonstrating efficiency and robustness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d02320f0074d134e681c8dbb5de6d7da4b9e27eafb70994ef7ddece708f1a6f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d02320f0074d134e681c8dbb5de6d7da4b9e27eafb70994ef7ddece708f1a6f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SLEI3D, a planning framework for heterogeneous robot fleets to simultaneously explore unknown 3D environments, inspect identified features, and relay findings back to a control station under limited communication. The method employs a multi-layer, multi-rate planning mechanism and intermittent/proactive communication protocols to coordinate subgroups of robots online. The framework is validated as efficient and reliable through large-scale simulations and hardware experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Coordinating heterogeneous fleets for simultaneous exploration and inspection in unknown environments with limited communication"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Multi-layer, multi-rate planning with intermittent/proactive communication protocols"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Validated via large-scale simulation (48 robots) and hardware experiments (7 robots)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D instance segmentation], [Neural Radiance Field (NeRF), 3D instance segmentation, crop counting, mask consistency, view synthesis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Ahmed Al Muzaddid, William J. Beksi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Arlington"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00207",children:"https://arxiv.org/pdf/2601.00207"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel framework for exact crop enumeration via 3D instance segmentation using multi-view images and NeRF. 2. Introduction of crop visibility and mask consistency scores to effectively segment instances in 3D. 3. Demonstration of consistent performance across diverse crops (cotton, apples, pears) without crop-specific parameter tuning and release of a new cotton plant dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces CropNeRF, a framework for accurate crop counting in agriculture. It uses multi-view 2D images and instance masks to train a Neural Radiance Field (NeRF), incorporating novel visibility and consistency scores to perform 3D instance segmentation and count crops. The method shows superior counting performance across different crop types and releases a new dataset to advance research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6237\u5916\u906e\u6321\u4e0e\u805a\u7c7b\u5bfc\u81f4\u8ba1\u6570\u56f0\u96be/Outdoor occlusions and clustering make counting hard]\n    C --\x3e C1[\u4f7f\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u4e0eNeRF\u8fdb\u884c3D\u5b9e\u4f8b\u5206\u5272/Use multi-view images & NeRF for 3D instance segmentation]\n    C --\x3e C2[\u5f15\u5165\u53ef\u89c1\u6027\u4e0e\u63a9\u7801\u4e00\u81f4\u6027\u5206\u6570/Introduce visibility & mask consistency scores]\n    D --\x3e D1[\u5728\u591a\u79cd\u4f5c\u7269\u4e0a\u5b9e\u73b0\u51c6\u786e\u8ba1\u6570/Achieve accurate counting on multiple crops]\n    D --\x3e D2[\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Outperforms state-of-the-art]\n    D --\x3e D3[\u8d21\u732e\u68c9\u82b1\u6570\u636e\u96c6/Contribute a cotton dataset]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [CycleGAN, YOLOv8, infrared image generation, data augmentation, PCB defect detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chao Yang, Haoyuan Zheng, Yue Ma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi\u2019an Jiaotong Liverpool University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00237",children:"https://arxiv.org/pdf/2601.00237"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: \u7ea2\u5916\u6570\u636e\u7a00\u7f3a / IR Data Scarcity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: CycleGAN\u8de8\u6a21\u6001\u751f\u6210 + YOLOv8\u68c0\u6d4b / CycleGAN Cross-modal Generation + YOLOv8 Detection"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63a5\u8fd1\u5168\u76d1\u7763\u57fa\u51c6 / Performance Significantly Improved, Approaches Fully Supervised Benchmark"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [robotics], [perching drone, slapband gripper, failure recovery, vertical surface, autonomous flight]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Julia Di, Kenneth A. W. Hoffmann, Tony G. Chen, Tian-Ao Ren, Mark R. Cutkosky"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University, Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00238",children:"https://arxiv.org/pdf/2601.00238"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel gentle perching approach for larger drones on vertical surfaces using a fast active elastic gripper with microspines made from commercially-available slapbands. 2. A system-level integration featuring vision-based perch site detection, IMU-based failure detection, and an attitude controller for soft perching. 3. Demonstrated autonomous perching with failure recovery, achieving a 75% success rate in initial indoor flight experiments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4fbed8060d82defaf56320fa59f4bb60a1215cf321aee59a80fa75b633d2f4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd4fbed8060d82defaf56320fa59f4bb60a1215cf321aee59a80fa75b633d2f4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents SLAP, a system for enabling drones to autonomously and gently perch on vertical tree trunks. The key innovation is a gripper made from slapbands with microspines, combined with detection and control modules for soft landing and failure recovery. Initial experiments on a modified quadrotor showed a 75% perch success rate and 100% recovery from induced failures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SLAP: Slapband-based Autonomous Perching Drone] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5782\u76f4\u8868\u9762\u6816\u606f\u56f0\u96be/Vertical Surface Perching is Hard]\n    B1 --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u9ad8\u901f\u4e14\u5371\u9669/Existing methods are high-speed and dangerous]\n    C --\x3e C1[\u7cfb\u7edf\u96c6\u6210/System Integration]\n    C1 --\x3e C2[\u57fa\u4e8e\u89c6\u89c9\u7684\u6816\u606f\u70b9\u68c0\u6d4b/Vision-based perch site detector]\n    C1 --\x3e C3[\u57fa\u4e8eIMU\u7684\u5931\u8d25\u68c0\u6d4b/IMU-based failure detector]\n    C1 --\x3e C4[\u8f6f\u6816\u606f\u59ff\u6001\u63a7\u5236\u5668/Attitude controller for soft perching]\n    C1 --\x3e C5[\u62cd\u8155\u5e26\u5f39\u6027\u5939\u722a/Slapband-based elastic gripper]\n    D --\x3e D1[75%\u6816\u606f\u6210\u529f\u7387/75% perch success rate]\n    D --\x3e D2[100%\u5931\u8d25\u6062\u590d\u7387/100% failure recovery rate]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Vehicle Painting Robot Path Planning Using Hierarchical Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [robotic path planning], [hierarchical optimization, vehicle routing problem (VRP), constraint handling, evolutionary computation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuya Nagai, Hiromitsu Nakamura, Narito Shinmachi, Yuta Higashizono, Satoshi Ono"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kagoshima University, TOYOTA Body Research & Development Co., Ltd."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00271",children:"https://arxiv.org/pdf/2601.00271"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates vehicle painting robot path planning as a hierarchical optimization problem, separating high-level task assignment (VRP-like) from low-level detailed path planning. 2. Proposes a flexible constraint handling framework for the painting process through custom variable representation, repair operators, and initialization. 3. Demonstrates the method's effectiveness by automatically generating paths for commercial vehicles that are comparable in quality to manual designs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd6fcb32eb9c80960a0ed996281f691e7732ffdfe85f78eaf940b5a4d0c7ce64_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd6fcb32eb9c80960a0ed996281f691e7732ffdfe85f78eaf940b5a4d0c7ce64_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the manual and time-consuming task of planning paint paths for multiple robotic arms in vehicle factories. It proposes a hierarchical optimization method that treats the problem as a high-level vehicle routing task and a low-level detailed path planning task, enabling automated design. Experiments on real vehicle models show the method can generate constraint-satisfying paths of comparable quality to those created by human engineers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Vehicle Painting Robot Path Planning Using Hierarchical Optimization") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Manual paint path design for multiple robotic arms is time-consuming")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical optimization (Upper: VRP-like assignment, Lower: detailed path planning)")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Automatically generates constraint-satisfying paths comparable to manual designs")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [inertial navigation], [wheel-mounted inertial sensors, chassis-mounted inertial sensors, extended Kalman filter, pure inertial navigation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dusan Nemec, Gal Versano, Itai Savin, Vojtech Simak, Juraj Kekelak, Itzik Klein"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Zilina, University of Haifa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00275",children:"https://arxiv.org/pdf/2601.00275"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed WiCHINS, a novel wheeled and chassis inertial navigation system combining sensors from different vehicle locations. 2. Derived a three-stage estimation framework, each stage utilizing a dedicated Extended Kalman Filter. 3. Demonstrated improved accuracy for pure inertial navigation, achieving an average position error of 2.4% of the traveled distance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf2c7ce1d6afe2abf0464e5b8ec3ff0435d40e0b4e57f8ff6a56d9f2f35553e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf2c7ce1d6afe2abf0464e5b8ec3ff0435d40e0b4e57f8ff6a56d9f2f35553e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inertial navigation drift for autonomous vehicles in GNSS-denied or visually degraded environments. It proposes WiCHINS, a system that fuses data from wheel-mounted and chassis-mounted inertial sensors using a three-stage Extended Kalman Filter framework. The method significantly reduces position error, enabling more robust pure inertial navigation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Pure Inertial Navigation in Challenging Environments<br/>\u6311\u6218\u73af\u5883\u4e0b\u7684\u7eaf\u60ef\u6027\u5bfc\u822a] --\x3e B(Problem: Inertial drift in GNSS/vision-limited conditions<br/>\u95ee\u9898: GNSS/\u89c6\u89c9\u53d7\u9650\u4e0b\u7684\u60ef\u6027\u6f02\u79fb)\n    A --\x3e C(Method: WiCHINS - Fuse wheel & chassis IMUs with 3-stage EKF<br/>\u65b9\u6cd5: WiCHINS - \u878d\u5408\u8f6e\u5f0f\u4e0e\u5e95\u76d8IMU\u7684\u4e09\u9636\u6bb5EKF\u6846\u67b6)\n    A --\x3e D(Results: 11.4m avg error (2.4% of distance)<br/>\u7ed3\u679c: 11.4\u7c73\u5e73\u5747\u8bef\u5dee(\u8ddd\u79bb\u76842.4%))"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Replaceable Bit-based Gripper for Picking Cluttered Food Items"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [robotics, food handling], [replaceable gripper, bit-based system, weight-specific dropping, cluttered food, bento box automation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Prashant Kumar, Yukiyasu Domae, Weiwei Wan, Kensuke Harada"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Based on author names and context, likely a Japanese research institution (e.g., Osaka University, AIST). Specific institution not explicitly stated in provided text."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00305",children:"https://arxiv.org/pdf/2601.00305"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel replaceable bit-based gripper system designed for handling diverse, cluttered, and flexible food items. 2. Introduced specialized, food-specific attachment bits (e.g., for ikura and spaghetti) to enhance grasping capabilities for challenging food categories. 3. Demonstrated a system capable of weight-specific picking and dropping with high accuracy (>80% for spaghetti, >95% for ikura) and quick bit switching for operational flexibility."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10fd54305281ee91551575494169fb7c342e6d6d74a74b115dfc4ee87dcd441_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10fd54305281ee91551575494169fb7c342e6d6d74a74b115dfc4ee87dcd441_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of automating the handling of cluttered, flexible, and granular food items for bento box packaging. It proposes a gripper system with replaceable, food-specific bits and a belt replacement mechanism to grasp and accurately drop target weights of different foods. The system successfully demonstrated high-accuracy, weight-specific handling of ikura and spaghetti and allows for rapid adaptation between food types."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Replaceable Bit-based Gripper for Picking Cluttered Food Items] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Handling cluttered, flexible food items with weight control for packaging)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Replaceable bit-based gripper system with food-specific attachments)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Successful picking & >80%/95% weight-drop accuracy for spaghetti/ikura; Quick bit switching)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [dense visual embeddings, knowledge distillation, RGB-D transformer, real-time inference, Alpha-CLIP]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," S\xf6hnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Ilmenau"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00359",children:"https://arxiv.org/pdf/2601.00359"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DVEFormer, an efficient RGB-D Transformer-based model for predicting dense, text-aligned visual embeddings via knowledge distillation from Alpha-CLIP. 2. Enables flexible, open-vocabulary scene understanding (e.g., text-based querying) beyond fixed-class semantic segmentation while maintaining the ability to perform classical segmentation. 3. Demonstrates real-time performance on embedded hardware (NVIDIA Jetson AGX Orin), making it suitable for mobile robotics applications like 3D mapping."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the need for robots to have a detailed, open-vocabulary understanding of indoor environments. It proposes DVEFormer, an efficient model that uses an RGB-D Transformer and knowledge distillation from Alpha-CLIP to predict dense visual embeddings, enabling both classical segmentation and flexible text-based querying. The method achieves competitive performance and real-time inference speeds, making it a practical drop-in replacement for traditional segmentation in mobile robotics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u673a\u5668\u4eba\u9700\u8981\u5168\u9762\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u573a\u666f\u7406\u89e3/Robots need comprehensive, open-vocabulary scene understanding]\n    C --\x3e C1[\u4f7f\u7528RGB-D Transformer\u548c\u77e5\u8bc6\u84b8\u998f/Use RGB-D Transformer and Knowledge Distillation]\n    C1 --\x3e C2[\u4eceAlpha-CLIP\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165/Learn Dense Visual Embeddings from Alpha-CLIP teacher]\n    D --\x3e D1[\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u4e0e\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c/Achieves real-time performance & competitive results]\n    D --\x3e D2[\u652f\u6301\u6587\u672c\u67e5\u8be2\u548c3D\u6620\u5c04/Enables text-based querying & 3D mapping]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [autonomous agents, multi-agent systems, low-power embedded systems, nano-satellites, space debris removal]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dennis Christmann, Juan F. Gutierrez, Sthiti Padhi, Patrick Pl\xf6rer, Aditya Takur, Simona Silvestri, Andres Gomez"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Braunschweig (TU Braunschweig)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00465",children:"https://arxiv.org/pdf/2601.00465"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel application of low-power autonomous agents for space debris removal using nano-satellite swarms. 2. Implements autonomous agent software on resource-constrained wireless microcontrollers. 3. Demonstrates the feasibility and energy efficiency of the approach through experiments on a specialized test-bed."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/162af2b08fc214951045fe887fa77c6a53132c5f821aafaa9132122ac6d19730_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/162af2b08fc214951045fe887fa77c6a53132c5f821aafaa9132122ac6d19730_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of space debris by proposing the use of autonomous nano-satellite swarms for de-orbiting. The method involves implementing low-power autonomous agent software on wireless microcontrollers to control these swarms. The work concludes by demonstrating the feasibility and energy efficiency of this approach through experimental validation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Space Debris Removal using Nano-Satellites<br>\u7a7a\u95f4\u788e\u7247\u6e05\u9664\u4f7f\u7528\u7eb3\u536b\u661f"] --\x3e Problem["\u7a7a\u95f4\u788e\u7247\u95ee\u9898<br>Space Debris Problem"]\n    Root --\x3e Method["\u4f4e\u529f\u8017\u81ea\u4e3b\u667a\u80fd\u4f53\u63a7\u5236\u7eb3\u536b\u661f\u7fa4<br>Low-Power Autonomous Agents Control Nano-Satellite Swarm"]\n    Root --\x3e Results["\u5c55\u793a\u53ef\u884c\u6027\u4e0e\u80fd\u6548<br>Demonstrates Feasibility & Energy Efficiency"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [probabilistic graphical models], [hybrid factor graphs, variable elimination, conditional linear gaussian, exact inference, slam]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Varun Agrawal, Frank Dellaert"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00545",children:"https://arxiv.org/pdf/2601.00545"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Hybrid Factor Graph framework with a hybrid Gaussian factor and hybrid conditional for modeling discrete-continuous problems. 2. Derives a hybrid variable elimination algorithm under the Conditional Linear Gaussian scheme to produce exact posteriors as a hybrid Bayes network. 3. Introduces a tree-structured factor representation with pruning and probabilistic assignment to bound discrete hypotheses and ensure tractable inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba63dbee87ea804f9a68c06addc0916e514ecc262180b1bcb37f36f06370de49_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba63dbee87ea804f9a68c06addc0916e514ecc262180b1bcb37f36f06370de49_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of performing exact inference in hybrid problems involving both discrete and continuous variables, common in robotics. It proposes a new Hybrid Factor Graph framework and a variable elimination algorithm that yields exact Maximum A Posteriori estimates and marginals. The method is demonstrated to be accurate and tractable on a SLAM dataset with ambiguous data association."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Variable Elimination in Hybrid Factor Graphs<br>\u6df7\u5408\u56e0\u5b50\u56fe\u4e2d\u7684\u53d8\u91cf\u6d88\u9664] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Hybrid discrete-continuous inference is difficult<br>\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u63a8\u7406\u56f0\u96be]\n    B --\x3e B2[Existing approaches are approximate<br>\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u8fd1\u4f3c]\n    C --\x3e C1[Novel hybrid factor & conditional<br>\u65b0\u578b\u6df7\u5408\u56e0\u5b50\u4e0e\u6761\u4ef6\u5206\u5e03]\n    C --\x3e C2[Hybrid variable elimination algorithm<br>\u6df7\u5408\u53d8\u91cf\u6d88\u9664\u7b97\u6cd5]\n    C --\x3e C3[Tree-structured pruning for tractability<br>\u6811\u7ed3\u6784\u526a\u679d\u4fdd\u8bc1\u53ef\u5904\u7406\u6027]\n    D --\x3e D1[Exact MAP estimation & marginalization<br>\u7cbe\u786eMAP\u4f30\u8ba1\u4e0e\u8fb9\u7f18\u5316]\n    D --\x3e D2[Demonstrated on SLAM with ambiguous data<br>\u5728SLAM\u6a21\u7cca\u6570\u636e\u5173\u8054\u4e2d\u9a8c\u8bc1]\n    D --\x3e D3[Shows accuracy, generality, simplicity<br>\u5c55\u793a\u7cbe\u5ea6\u3001\u901a\u7528\u6027\u3001\u7b80\u6d01\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Optimal Transport-Based Decentralized Multi-Agent Distribution Matching"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [optimal transport, Wasserstein distance, decentralized control, distribution matching, sequential weight-update]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kooktae Lee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New Mexico Institute of Mining and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00548",children:"https://arxiv.org/pdf/2601.00548"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Reformulated the global optimal transport distribution-matching problem into a tractable per-agent decision process using only local information. 2. Introduced a sequential weight-update rule and a memory-based correction mechanism to handle intermittent communication. 3. Established convergence guarantees for the proposed decentralized framework under both linear and nonlinear agent dynamics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8fe088e282f40c32d9ba8ce2686dcb5df3a1b0a69d86f1cf5042766dd92e8e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8fe088e282f40c32d9ba8ce2686dcb5df3a1b0a69d86f1cf5042766dd92e8e1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a decentralized control framework for multi-agent distribution matching using optimal transport theory. It enables each agent to determine its target location locally via a sequential weight-update rule and a memory-based correction mechanism, avoiding a centralized optimal transport solver. The method is proven to converge and is demonstrated through simulations to be effective and scalable."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Optimal Transport-Based Decentralized Multi-Agent Distribution Matching"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Multi-agent systems need to achieve a prescribed terminal spatial distribution under decentralized, local information constraints."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Reformulates global OT into per-agent process; uses sequential weight-update and memory-based correction for local plans."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Convergence guarantees established; framework achieves effective, scalable distribution matching fully decentralized."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [semantic mapping, LLM-based decision, modular motion primitives, AprilTag, ROS]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abu Hanif Muhammad Syarubany, Farhan Zaki Rahmani, Trio Widianto"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science & Technology (KAIST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00555",children:"https://arxiv.org/pdf/2601.00555"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An end-to-end LLM-based agentic exploration system for indoor shopping tasks, integrating perception, mapping, and action. 2. A lightweight semantic mapping approach that incrementally builds a map from detected signboards and uses AprilTags as repeatable anchors for navigation. 3. A modular ROS-based execution stack where an LLM provides high-level discrete commands and a finite-state controller gates low-level motion primitives for safe task execution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/535aa877825dca3fad1c33dd6360354f727986e717cee5cc6c3ee4ffaea1881a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/535aa877825dca3fad1c33dd6360354f727986e717cee5cc6c3ee4ffaea1881a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an LLM-based robotic system for autonomous indoor shopping. The robot explores an environment, builds a semantic map from visual cues like signboards and AprilTags, and uses an LLM to interpret natural language requests and generate navigation and manipulation decisions, which are executed by a modular ROS controller. The integrated system successfully demonstrates end-to-end task execution from instruction to object retrieval in both simulation and a real-world corridor setup."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: How to integrate perception, semantic mapping, and LLM decision-making for end-to-end robotic task execution in an unknown indoor environment?"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Robot builds a lightweight semantic map from signboards and AprilTags; LLM generates discrete actions; ROS controller executes actions using modular motion primitives."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: The system can perform end-to-end shopping tasks from instruction to object retrieval in simulation and real-world, remaining modular and debuggable."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Priority-Aware Multi-Robot Coverage Path Planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-robot path planning], [coverage path planning, priority-weighted latency, lexicographic optimization, spanning-tree, Steiner-tree]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kanghoon Lee, Hyeonjun Kim, Jiachen Li, Jinkyoo Park"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), Korea Military Academy (KMA), University of California, Riverside (UCR)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00580",children:"https://arxiv.org/pdf/2601.00580"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formally defines the Priority-Aware Multi-Robot Coverage Path Planning (PA-MCPP) problem, introducing priority weights and a lexicographic objective to minimize priority-weighted latency and makespan. 2. Proposes a scalable two-phase framework combining greedy zone assignment with local search and Steiner-tree-guided residual coverage. 3. Demonstrates through experiments that the method significantly reduces priority-weighted latency compared to baselines while maintaining competitive makespan and scales well with the number of robots."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4091ae11f186a26844a6a1c27c13cf633fa92da4e6636d53275a7d839f775d9f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4091ae11f186a26844a6a1c27c13cf633fa92da4e6636d53275a7d839f775d9f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitation of standard multi-robot coverage path planning, which treats all areas equally, by introducing a priority-aware version (PA-MCPP) where certain zones have higher urgency. The authors propose a two-phase method that first assigns and covers priority zones efficiently and then handles the remaining area. Experiments show their approach successfully reduces coverage delay for high-priority zones without significantly compromising the overall completion time."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Priority-Aware Multi-Robot Coverage Path Planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6807\u51c6MCPP\u5ffd\u89c6\u533a\u57df\u4f18\u5148\u7ea7/Standard MCPP ignores zone priority]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u6846\u67b6: \u8d2a\u5fc3\u5206\u914d\u4e0e\u65af\u5766\u7eb3\u6811\u5f15\u5bfc\u8986\u76d6/Two-phase framework: greedy assignment & Steiner-tree-guided coverage]\n    D --\x3e D1[\u663e\u8457\u964d\u4f4e\u4f18\u5148\u7ea7\u52a0\u6743\u5ef6\u8fdf/Significantly reduces priority-weighted latency]\n    D --\x3e D2[\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5b8c\u5de5\u65f6\u95f4/Maintains competitive makespan]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [robotics control], [nonlinear model predictive control, robust adaptive control, deep neural network, visual pose estimation, safety module]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tampere University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00609",children:"https://arxiv.org/pdf/2601.00609"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A comprehensive four-module navigation and control framework integrating visual pose estimation, high-level NMPC, a low-level DNN policy with robust adaptive control, and a safety module for large-scale mobile robots. 2. A low-level control framework that guarantees uniform exponential stability for the actuation subsystem. 3. A logarithmic safety module designed to monitor the entire robot stack and ensure system-level safety during operation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858a2542ad097a42549017d22fa566990bbeaddab1e84e94c1b158ffa217822d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858a2542ad097a42549017d22fa566990bbeaddab1e84e94c1b158ffa217822d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a navigation and control framework for large-scale mobile robots operating on slip-prone terrain. The method combines visual pose estimation, nonlinear model predictive control, a deep neural network control policy augmented with robust adaptive control, and a safety module to ensure stability and safety. The framework was validated on a 6,000 kg robot, demonstrating robust operation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LSMR\u5728\u677e\u6563\u5730\u5f62\u4e0a\u6613\u6253\u6ed1/LSMR prone to slip on loose terrain]\n    C --\x3e C1[\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1/Visual Pose Estimation]\n    C --\x3e C2[\u9ad8\u5c42NMPC/High-level NMPC]\n    C --\x3e C3[\u4f4e\u5c42DNN\u63a7\u5236\u7b56\u7565/Low-level DNN Control Policy]\n    C --\x3e C4[\u5b89\u5168\u6a21\u5757/Safety Module]\n    D --\x3e D1[\u4fdd\u8bc1\u6267\u884c\u5668\u5b50\u7cfb\u7edf\u7a33\u5b9a/Guarantees actuation subsystem stability]\n    D --\x3e D2[\u786e\u4fdd\u7cfb\u7edf\u7ea7\u5b89\u5168/Ensures system-level safety]\n    D --\x3e D3[\u57286000kg\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1/Validated on a 6000kg robot]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] From 2D to 3D terrain-following area coverage path planning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [robotics path planning], [terrain-following, area coverage, inverse distance weighting, working height, agricultural robotics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mogens Plessen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Findklein GmbH"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00614",children:"https://arxiv.org/pdf/2601.00614"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel 3D terrain-following area coverage path planning algorithm that simultaneously accounts for a specific working width and working height, a gap identified in existing literature. 2. Highlights algorithmic complexities compared to 2D planning, including uniformly spaced elevation data generation using Inverse Distance Weighting and a local search. 3. Validates the algorithm using real-world 3D terrain data within an agricultural context."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b87ce191d5ee1f6c0b66e89f7d4260d7328c4767f9b3668dbfe5ab4e429e010_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b87ce191d5ee1f6c0b66e89f7d4260d7328c4767f9b3668dbfe5ab4e429e010_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a new algorithm for generating 3D area coverage paths for ground vehicles that must follow terrain while maintaining a specific working width and height, such as for agricultural spraying. The method addresses limitations of prior 2D projection approaches by directly planning in 3D, using techniques like Inverse Distance Weighting for elevation data. It is validated with real-world agricultural terrain data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[From 2D to 3D terrain-following area coverage path planning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: 2D\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u8d77\u4f0f\u5730\u5f62\u4e0a\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u5de5\u4f5c\u5bbd\u5ea6\u548c\u9ad8\u5ea6/2D path planning fails to maintain working width and height over 3D terrain)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa3D\u5730\u5f62\u8ddf\u968f\u533a\u57df\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5/Propose 3D terrain-following area coverage path planning algorithm)\n    C --\x3e D(\u4f7f\u7528\u53cd\u8ddd\u79bb\u52a0\u6743\u751f\u6210\u5747\u5300\u9ad8\u7a0b\u6570\u636e/Use Inverse Distance Weighting for elevation data)\n    C --\x3e E(\u4f7f\u7528\u5c40\u90e8\u641c\u7d22/Local search)\n    A --\x3e F(\u5173\u952e\u7ed3\u679c/Results: \u5728\u771f\u5b9e\u519c\u4e1a3D\u5730\u5f62\u6570\u636e\u4e0a\u9a8c\u8bc1\u7b97\u6cd5/Validate algorithm on real-world agricultural 3D terrain data)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, robust adaptive control, visual pose estimation, hierarchical learning, safety supervisor]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tampere University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00610",children:"https://arxiv.org/pdf/2601.00610"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A hierarchical learning framework that decomposes the goal-reaching control problem into tightly coupled modules, including RL for planning and supervised learning for dynamics modeling. 2. Integration of a model-based robust adaptive controller with the learned dynamics model to guarantee wheel command tracking on slip-prone terrain, ensuring uniform exponential stability. 3. Design of a mathematical safety supervisor to autonomously monitor the robot, stop it on unsafe faults, and guide it back to a safe area, reducing human intervention."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/588ba572bdf33b4091ce883350b57f99b3a43b7383f0188394f0a36af791cfc3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/588ba572bdf33b4091ce883350b57f99b3a43b7383f0188394f0a36af791cfc3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hierarchical learning framework for safe, vision-based goal-reaching control of large mobile robots. The method combines reinforcement learning for motion planning, supervised learning for robot dynamics modeling, and a robust adaptive controller for stable actuation, all overseen by a safety supervisor. Experiments on a 6,000 kg robot confirm the framework's effectiveness and safety guarantees."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[RL\u63a2\u7d22\u4e0d\u5b89\u5168/Unsafe RL Exploration]\n    B --\x3e B2[\u5927\u578b\u673a\u5668\u4eba\u5e94\u7528\u53d7\u9650/Limited Application for Large Robots]\n    C --\x3e C1[\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1/Visual Pose Estimation]\n    C --\x3e C2[RL\u8fd0\u52a8\u89c4\u5212\u5668/RL Motion Planner]\n    C --\x3e C3[\u76d1\u7763\u5b66\u4e60\u52a8\u529b\u5b66\u6a21\u578b/Supervised Learning Dynamics Model]\n    C --\x3e C4[\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u5668/Robust Adaptive Controller]\n    C --\x3e C5[\u6570\u5b66\u5b89\u5168\u76d1\u7763\u5668/Mathematical Safety Supervisor]\n    D --\x3e D1[\u4fdd\u8bc1\u7a33\u5b9a\u6027/Guarantees Stability]\n    D --\x3e D2[\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027/Experimental Validation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RoboReward: General-Purpose Vision-Language Reward Models for Robotics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [vision-language models, reward modeling, reinforcement learning, data augmentation, robotics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University, UC Berkeley"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00675",children:"https://arxiv.org/pdf/2601.00675"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RoboReward, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment and RoboArena. 2. Proposes a negative examples data augmentation pipeline to generate calibrated negatives and near-misses for training. 3. Trains and deploys general-purpose 4B/8B vision-language reward models that outperform larger VLMs and improve real-robot RL policy learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3d93ea0589a39158950d54cbe397e38c6b0ab250252ddc7e117e74f8d59010_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3d93ea0589a39158950d54cbe397e38c6b0ab250252ddc7e117e74f8d59010_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of designing rewards for robotic reinforcement learning by introducing RoboReward, a dataset and benchmark for training vision-language reward models. The method includes a data augmentation pipeline to create negative examples and trains compact 4B/8B parameter models. The results show these models outperform larger VLMs on short-horizon tasks and significantly improve real-robot policy learning compared to a frontier VLM."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RoboReward: General-Purpose Vision-Language Reward Models for Robotics] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Reward design for RL is labor-intensive or brittle/RL\u5956\u52b1\u8bbe\u8ba1\u8d39\u65f6\u6216\u8106\u5f31]\n    C --\x3e C1[Build dataset & benchmark from OXE & RoboArena/\u57fa\u4e8eOXE\u548cRoboArena\u6784\u5efa\u6570\u636e\u96c6\u4e0e\u57fa\u51c6]\n    C --\x3e C2[Propose negative examples augmentation pipeline/\u63d0\u51fa\u8d1f\u6837\u672c\u6570\u636e\u589e\u5f3a\u6d41\u7a0b]\n    C --\x3e C3[Train RoboReward 4B/8B VLMs/\u8bad\u7ec3RoboReward 4B/8B VLM]\n    D --\x3e D1[No existing VLM excels across all tasks/\u73b0\u6709VLM\u65e0\u5168\u80fd\u6a21\u578b]\n    D --\x3e D2[RoboReward models outperform larger VLMs/RoboReward\u6a21\u578b\u4f18\u4e8e\u66f4\u5927VLM]\n    D --\x3e D3[Improves real-robot RL over Gemini Robotics-ER/\u5728\u771f\u5b9e\u673a\u5668\u4ebaRL\u4e2d\u5927\u5e45\u8d85\u8d8aGemini]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] DefVINS: Visual-Inertial Odometry for Deformable Scenes"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [visual-inertial odometry], [deformable scenes, observability analysis, embedded deformation graph, IMU anchoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Samuel Cerezo, Javier Civera"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad de Zaragoza"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00702",children:"https://arxiv.org/pdf/2601.00702"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A VIO framework (DefVINS) that explicitly separates rigid motion (IMU-anchored) from non-rigid deformation (modeled by an embedded deformation graph). 2. An observability analysis characterizing how inertial measurements constrain rigid motion and identify modes in deformable scenes. 3. A conditioning-based activation strategy that progressively enables non-rigid degrees of freedom to prevent ill-posed updates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces DefVINS, a visual-inertial odometry framework designed for deformable scenes. It separates rigid and non-rigid motion, uses an observability analysis to guide a progressive activation strategy for deformation, and shows improved robustness in non-rigid environments through ablation studies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DefVINS: Visual-Inertial Odometry for Deformable Scenes] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u521a\u6027\u5047\u8bbe\u5931\u6548 / Rigidity Assumption Violated]\n    B --\x3e B2[VIO\u5728\u975e\u521a\u6027\u573a\u666f\u4e2d\u6f02\u79fb / VIO Drift in Non-Rigid Scenes]\n    C --\x3e C1[\u5206\u79bb\u521a\u6027\u72b6\u6001\u4e0e\u975e\u521a\u6027\u5f62\u53d8 / Separate Rigid & Non-Rigid State]\n    C --\x3e C2[\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u4e0eIMU\u951a\u5b9a / Observability Analysis & IMU Anchoring]\n    C --\x3e C3[\u57fa\u4e8e\u6761\u4ef6\u7684\u6e10\u8fdb\u6fc0\u6d3b / Conditioning-Based Progressive Activation]\n    D --\x3e D1[\u63d0\u5347\u975e\u521a\u6027\u73af\u5883\u9c81\u68d2\u6027 / Improved Robustness in Non-Rigid Environments]\n    D --\x3e D2[\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027 / Ablation Studies Validate Benefits]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Bayesian Inverse Games with High-Dimensional Multi-Modal Observations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [inverse reinforcement learning], [Bayesian inference, variational autoencoder, Nash equilibrium, inverse games, multimodal observations]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yash Jain, Xinjie Liu, Lasse Peters, David Fridovich-Keil, Ufuk Topcu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas at Austin, Delft University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00696",children:"https://arxiv.org/pdf/2601.00696"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Bayesian inference framework for inverse games to quantify uncertainty in estimating agent objectives, addressing the overconfidence of point-estimate methods. 2. Introduces a structured variational autoencoder with an embedded differentiable Nash game solver, enabling posterior sampling without requiring labeled objective data. 3. Demonstrates that multimodal inference reduces uncertainty when trajectory data is insufficient, leading to safer downstream planning decisions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9263c0ad6078bf92eb8f7a7ca21579f0a55a6e710fa80a06f40a66a735ce24a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inferring agents' hidden objectives in multi-agent interactions, where existing maximum likelihood methods produce overconfident point estimates. The authors propose a Bayesian inverse game framework using a structured variational autoencoder with a differentiable Nash solver to generate posterior samples from multimodal observations. Experiments show the method improves inference quality, quantifies uncertainty, and enables safer autonomous decision-making compared to prior approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Bayesian Inverse Games with High-Dimensional Multi-Modal Observations") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("MLE\u65b9\u6cd5\u53ea\u63d0\u4f9b\u70b9\u4f30\u8ba1\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u88ab\u5ffd\u7565/MLE methods provide only point estimates, ignoring uncertainty")\n    Problem --\x3e P2("\u4e0b\u6e38\u89c4\u5212\u53ef\u80fd\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u52a8\u4f5c/Downstream planning can be overconfident, leading to unsafe actions")\n    Method --\x3e M1("\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6/Approximate Bayesian inference framework")\n    Method --\x3e M2("\u7ed3\u6784\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5d4c\u5165\u53ef\u5fae\u7eb3\u4ec0\u6c42\u89e3\u5668/Structured VAE with embedded differentiable Nash solver")\n    Method --\x3e M3("\u5229\u7528\u591a\u6a21\u6001\u89c2\u6d4b\u6570\u636e/Utilizes multi-modal observation data")\n    Results --\x3e R1("\u6210\u529f\u5b66\u4e60\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03/Successfully learns prior and posterior distributions")\n    Results --\x3e R2("\u63a8\u7406\u8d28\u91cf\u4f18\u4e8eMLE\u65b9\u6cd5/Improves inference quality over MLE")\n    Results --\x3e R3("\u591a\u6a21\u6001\u63a8\u7406\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027/Multimodal inference further reduces uncertainty")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [SLAM (Simultaneous Localization and Mapping)], [Gaussian Splatting, Dense Initialization, DINOv3, Multi-view Triangulation, Real-time Mapping]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Yang Ming Chiao Tung University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00705",children:"https://arxiv.org/pdf/2601.00705"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[GS-SLAM\u7684\u6b8b\u5dee\u9a71\u52a8\u81f4\u5bc6\u5316\u6548\u7387\u4f4e/Inefficient residual-driven densification in GS-SLAM]\n    C --\x3e C1[\u4e00\u6b21\u6027\u5bc6\u96c6\u521d\u59cb\u5316/One-shot dense initialization]\n    C1 --\x3e C2[\u4f7f\u7528DINOv3\u7279\u5f81\u4e0e\u7f6e\u4fe1\u5185\u70b9\u5206\u7c7b\u5668/Using DINOv3 features & confidence-aware inlier classifier]\n    C2 --\x3e C3[\u591a\u89c6\u89d2\u4e09\u89d2\u5316\u751f\u6210\u9ad8\u65af\u5148\u9a8c/Multi-view triangulation for Gaussian prior]\n    D --\x3e D1[\u6536\u655b\u52a0\u901f~20%/~20% faster convergence]\n    D --\x3e D2[\u66f4\u9ad8\u6e32\u67d3\u4fdd\u771f\u5ea6/Higher rendering fidelity]\n    D --\x3e D3[\u5b9e\u65f6\u6027\u80fd\u8fbe925 FPS/Real-time performance up to 925 FPS]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260105] Calling for Backup: How Children Navigate Successive Robot Communication Failures"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [human-robot interaction], [successive robot error, child-robot interaction, error recovery, performance error, social error]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Maria Teresa Parreira, Isabel Neto, Filipa Rocha, Wendy Ju"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cornell University, Universidade de Lisboa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.00754",children:"https://arxiv.org/pdf/2601.00754"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Reproduced the successive robot failure paradigm with children (ages 8-10) to explore their unique responses to repeated conversational errors. 2. Identified key behavioral differences between children and adults, such as children's increased disengagement (e.g., ignoring the robot, seeking adult help) and more flexible conversational expectations. 3. Provided empirical findings to inform the design of more effective and developmentally appropriate human-robot interaction systems for young users."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6516606de03da03b1c7297f4e0a7fcb61612ec1bea932c4290e5d1ba746ac52a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6516606de03da03b1c7297f4e0a7fcb61612ec1bea932c4290e5d1ba746ac52a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates how children respond to repeated robot communication failures by reproducing an adult-focused error paradigm with child participants. The method involved children interacting with a robot that failed to understand their prompts three times, with their behavioral responses recorded and analyzed. The main conclusion is that while children share some error-response strategies with adults, they exhibit more disengagement behaviors and maintain a stable perception of the robot, suggesting different interaction needs that should guide robot design for young users."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Calling for Backup: How Children Navigate Successive Robot Communication Failures] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u513f\u7ae5\u5bf9\u8fde\u7eed\u673a\u5668\u4eba\u9519\u8bef\u7684\u53cd\u5e94/Children's response to successive robot errors]\n    C --\x3e C1[\u590d\u5236\u6210\u4eba\u7814\u7a76\u8303\u5f0f\uff0c\u4e0e\u513f\u7ae5\u8fdb\u884c\u673a\u5668\u4eba\u4ea4\u4e92/Reproduce adult study paradigm, child-robot interaction]\n    C --\x3e C2[\u5206\u6790\u884c\u4e3a\u89c6\u9891\u8bb0\u5f55/Analyze behavioral video recordings]\n    D --\x3e D1[\u513f\u7ae5\u4e0e\u6210\u4eba\u53cd\u5e94\u7684\u5f02\u540c/Similarities and differences vs. adult responses]\n    D --\x3e D2[\u66f4\u591a\u8131\u79bb\u884c\u4e3a\uff0c\u5982\u5bfb\u6c42\u6210\u4eba\u5e2e\u52a9/More disengagement, e.g., seeking adult help]\n    D --\x3e D3[\u5bf9\u673a\u5668\u4eba\u7684\u611f\u77e5\u672a\u53d7\u5f71\u54cd/Robot perception unaffected]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);