"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6316],{15174:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"daily/cs_MM/20251229-20260104","title":"20251229-20260104 (cs.MM)","description":"2025-12-29","source":"@site/docs/daily/cs_MM/20251229-20260104.md","sourceDirName":"daily/cs_MM","slug":"/daily/csmm/20251229-20260104","permalink":"/ai_toutiao/daily/csmm/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767092843000,"frontMatter":{"slug":"/daily/csmm/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.MM)","permalink":"/ai_toutiao/daily/csmm/20251222-20251228"},"next":{"title":"cs.MS","permalink":"/ai_toutiao/daily/csms"}}');var r=i(74848),t=i(28453);const a={slug:"/daily/csmm/20251229-20260104"},o="20251229-20260104 (cs.MM)",d={},l=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251229-20260104-csmm",children:"20251229-20260104 (cs.MM)"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [steganography], [raster domain steganography, glyph perturbation, deterministic rasterization, multimodal embedding, text-based data hiding]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," A V Uday Kiran Kandala"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Queen Mary University of London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21698",children:"https://arxiv.org/pdf/2512.21698"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified Glyph Perturbation Cardinality (GPC) framework for embedding heterogeneous data (text, images, audio, video) directly into the pixel space of rendered text glyphs. 2. Operates exclusively in the raster domain after font rendering, modifying bitmap pixels with minimal, visually imperceptible intensity increments for covert communication. 3. Introduces a decoding method based on re-rasterizing cover text, subtracting canonical glyph rasters, and recovering payload via pixel count analysis, leveraging deterministic raster behavior."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a raster domain steganography framework that embeds multimodal data into text by minimally perturbing the interior pixels of rendered glyphs. The method is visually imperceptible and computationally lightweight, enabling ordinary text to serve as a covert medium for secure data embedding. It generalizes beyond traditional linguistic steganography by operating directly on the deterministic bitmap output of text rendering pipelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: How to embed multimodal data covertly into ordinary text?")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Glyph Perturbation Cardinality (GPC) Framework")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Visually imperceptible, lightweight embedding in raster domain")\n    Problem --\x3e P1("\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650/Limitations of linguistic & structural methods")\n    Method --\x3e M1("\u64cd\u4f5c\u4e8e\u6805\u683c\u5316\u540e/Operates post-rasterization")\n    Method --\x3e M2("\u6270\u52a8\u5b57\u5f62\u5185\u90e8\u50cf\u7d20/Perturbs interior ink pixels")\n    Method --\x3e M3("\u57fa\u4e8e\u50cf\u7d20\u57fa\u6570\u7f16\u7801/Encodes via pixel cardinality")\n    Results --\x3e R1("\u652f\u6301\u591a\u6a21\u6001\u6570\u636e/Supports multimodal data")\n    Results --\x3e R2("\u89e3\u7801\u7a33\u5b9a\u53ef\u9760/Stable & decodable signal")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [recommender systems], [frozen large video language models, micro-video recommendation, feature fusion, intermediate hidden states, dual feature fusion]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology, Singapore Management University, University of Wisconsin-Madison, GienTech Technology Co., Ltd."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21863",children:"https://arxiv.org/pdf/2512.21863"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted the first systematic empirical study on integrating frozen LVLMs into micro-video recommendation, evaluating feature extraction paradigms (captions vs. hidden states) and integration strategies (replacement vs. fusion) with ID embeddings. 2. Derived three key principles: intermediate hidden states outperform captions, ID embeddings are irreplaceable (fusion > replacement), and the effectiveness of hidden states varies across layers. 3. Proposed the Dual Feature Fusion (DFF) Framework, a lightweight plug-and-play method that adaptively fuses multi-layer LVLM representations with ID embeddings, achieving state-of-the-art performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper systematically studies how to best integrate frozen Large Video-Language Models (LVLMs) as feature extractors for micro-video recommendation. It finds that using intermediate decoder hidden states and fusing them with item ID embeddings is superior to using generated captions or replacing IDs. Based on these insights, the authors propose the Dual Feature Fusion (DFF) framework, which achieves state-of-the-art results on benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Frozen LVLMs for Micro-Video Recommendation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("LVLM\u96c6\u6210\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30/Lack of systematic evaluation for LVLM integration")\n    Method --\x3e M1("\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76/Systematic empirical study")\n    Method --\x3e M2("\u63d0\u51faDFF\u6846\u67b6/Propose DFF Framework")\n    M1 --\x3e M1a("\u6bd4\u8f83\u7279\u5f81\u63d0\u53d6\u8303\u5f0f/Compare feature extraction paradigms")\n    M1 --\x3e M1b("\u6bd4\u8f83ID\u96c6\u6210\u7b56\u7565/Compare ID integration strategies")\n    M2 --\x3e M2a("\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c42\u7279\u5f81/Adaptively fuse multi-layer features")\n    M2 --\x3e M2b("\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528/Lightweight plug-and-play")\n    Results --\x3e R1("\u4e2d\u95f4\u9690\u85cf\u6001\u4f18\u4e8e\u63cf\u8ff0/Intermediate hidden states > captions")\n    Results --\x3e R2("ID\u5d4c\u5165\u4e0d\u53ef\u66ff\u4ee3/Fusion > replacement")\n    Results --\x3e R3("DFF\u5b9e\u73b0SOTA\u6027\u80fd/DFF achieves SOTA performance")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yiquan Gao, John See"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Heriot-Watt University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21944",children:"https://arxiv.org/pdf/2512.21944"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")\n    Results("\u5173\u952e\u7ed3\u679c/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")'}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [visual question answering], [signboard VQA, OCR-integrated VQA, multimodal dataset, Vietnamese, multi-agent framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Information Technology, Vietnam National University, Ho Chi Minh City"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22218",children:"https://arxiv.org/pdf/2512.22218"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: \u7406\u89e3\u81ea\u7136\u573a\u666f\u4e2d\u7684\u62db\u724c\u6587\u672c\u5bf9\u4e8eVQA\u7684\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: 1. \u5f15\u5165\u9996\u4e2a\u5927\u89c4\u6a21\u8d8a\u5357\u8bed\u62db\u724cVQA\u6570\u636e\u96c6ViSignVQA\u3002 2. \u901a\u8fc7\u96c6\u6210\u8d8a\u5357\u8bedOCR\u548c\u8bed\u8a00\u6a21\u578b\u6765\u9002\u914dSOTA VQA\u6a21\u578b\u3002 3. \u63d0\u51fa\u7ed3\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u667a\u80fd\u4f53\u53caGPT-4\u7684\u591a\u667a\u80fd\u4f53VQA\u6846\u67b6\u3002"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: 1. \u6dfb\u52a0OCR\u6587\u672c\u4f7fF1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe209%\u3002 2. \u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u591a\u6570\u6295\u7968\u8fbe\u523075.98%\u51c6\u786e\u7387\u3002 3. \u521b\u5efa\u4e86\u9996\u4e2a\u6355\u83b7\u771f\u5b9e\u4e16\u754c\u573a\u666f\u6587\u672c\u7279\u5f81\u7684\u5927\u89c4\u6a21\u8d8a\u5357\u8bed\u591a\u6a21\u6001\u57fa\u51c6\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [motion capture, wearable computing, human-computer interaction], [IMU, WebXR, SLAM, IoT, edge computing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidan\xe9, Tejaswi Gowda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22690",children:"https://arxiv.org/pdf/2512.22690"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5\xb0 joint-angle error) at approximately 5% of the cost, with low latency and high reliability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Motion capture is costly and complex, limiting accessibility] --\x3e P1[\u6602\u8d35\u4e14\u590d\u6742/Expensive & Complex]\n    Problem --\x3e P2[\u5c40\u9650\u4e8e\u4e13\u4e1a\u5b9e\u9a8c\u5ba4/Limited to Specialized Labs]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Open-source, low-cost system using IoT sensors and web tech] --\x3e M1[\u8eab\u4f53\u4f69\u6234IMU\u4f20\u611f\u5668\u7f51\u7edc/Body-worn IMU Sensor Network]\n    Method --\x3e M2[\u667a\u80fd\u624b\u673aWebXR SLAM\u5b9a\u4f4d/Smartphone WebXR SLAM for Positioning]\n    Method --\x3e M3[\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u5e94\u7528/Web-Browser-Based Application]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Affordable, accurate, and real-time performance] --\x3e R1[\u6210\u672c\u7ea6\u4e3a\u5546\u4e1a\u7cfb\u7edf\u76845%/~5% Cost of Commercial System]\n    Results --\x3e R2[\u5e73\u5747\u5173\u8282\u89d2\u5ea6\u8bef\u5dee2-5\u5ea6/Mean Joint-Angle Error 2-5\xb0]\n    Results --\x3e R3[\u5b9e\u65f6\u4f4e\u5ef6\u8fdf/Real-Time with Low Latency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Federated Multi-Task Clustering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [federated clustering, spectral clustering, multi-task learning, tensor methods, ADMM]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," South China University of Technology, Dalian University of Technology, Xidian University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22897",children:"https://arxiv.org/pdf/2512.22897"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Federated Multi-Task Clustering] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Centralized models inapplicable to decentralized environments/\u96c6\u4e2d\u5f0f\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u73af\u5883]\n    B --\x3e B2[Poor generalization due to unreliable pseudo-labels/\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u5dee]\n    B --\x3e B3[Failure to capture latent client correlations/\u672a\u80fd\u6355\u83b7\u5ba2\u6237\u7aef\u95f4\u7684\u6f5c\u5728\u5173\u8054]\n    C --\x3e C1[Client-side personalized clustering module/\u5ba2\u6237\u7aef\u4e2a\u6027\u5316\u805a\u7c7b\u6a21\u5757]\n    C --\x3e C2[Server-side tensorial correlation module/\u670d\u52a1\u5668\u7aef\u5f20\u91cf\u5173\u8054\u6a21\u5757]\n    C --\x3e C3[ADMM-based distributed algorithm/\u57fa\u4e8eADMM\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5]\n    D --\x3e D1[Outperforms baselines and SOTA/\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u548c\u524d\u6cbf\u65b9\u6cd5]\n    D --\x3e D2[Validated on real-world datasets/\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Bridging Your Imagination with Audio-Video Generation via a Unified Director"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [unified director model, Mixture-of-Transformers, interleaved concept learning, disentangled expert learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," ByteDance Intelligent Creation, Nanyang Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23222",children:"https://arxiv.org/pdf/2512.23222"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://kebii.github.io/UniMAGE",children:"https://kebii.github.io/UniMAGE"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a "first interleaving, then disentangling" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel "first interleaving, then disentangling" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u7cfb\u7edf\u5c06\u811a\u672c\u8d77\u8349\u4e0e\u5173\u952e\u955c\u5934\u8bbe\u8ba1\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1/Existing systems treat script drafting and key-shot design as disjoint tasks]\n    C --\x3e C1[\u63d0\u51fa\u7edf\u4e00\u5bfc\u6f14\u6a21\u578bUniMAGE/Propose unified director model UniMAGE]\n    C --\x3e C2[\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784/Employ Mixture-of-Transformers architecture]\n    C --\x3e C3[\u5f15\u5165"\u5148\u4ea4\u9519\uff0c\u540e\u89e3\u8026"\u8bad\u7ec3\u8303\u5f0f/Introduce "first interleaving, then disentangling" training paradigm]\n    D --\x3e D1[\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd/Achieves SOTA performance among open-source models]\n    D --\x3e D2[\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u7684\u811a\u672c\u548c\u89c6\u89c9\u4e00\u81f4\u7684\u56fe\u50cf/Generates logically coherent scripts and visually consistent keyframes]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multi Agents Semantic Emotion Aligned Music to Image Generation with Music Derived Captions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [multi-modal generation], [music-to-image generation, multi-agent system, valence-arousal alignment, diffusion models, CLIP fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junchang Shi, Gang Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology Beijing"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23320",children:"https://arxiv.org/pdf/2512.23320"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A multi-agent semantic-emotion aligned framework (MESA MIG) that uses cooperating agents to refine music-derived captions for image generation. 2. Integration of continuous Valence-Arousal regression from music and a CLIP-based visual emotion head to enforce emotional alignment between music and generated images. 3. Empirical demonstration that the framework outperforms caption-only and single-agent baselines in aesthetic quality, semantic consistency, and emotional alignment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf4833583da2d40740efcb2a747ee2d1334ebcef0ce1d2e3d3717470dea449e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf4833583da2d40740efcb2a747ee2d1334ebcef0ce1d2e3d3717470dea449e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of generating images from music by externalizing the visual imagery evoked by listening. The proposed MESA MIG framework uses a multi-agent system to produce and refine structured music captions, and enforces emotional alignment using Valence-Arousal regression. Experiments show it outperforms baselines in semantic and emotional consistency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Multi Agents Semantic Emotion Aligned Music to Image Generation<br>\u591a\u667a\u80fd\u4f53\u8bed\u4e49\u60c5\u611f\u5bf9\u9f50\u7684\u97f3\u4e50\u5230\u56fe\u50cf\u751f\u6210"] --\x3e Problem["Externalize visual imagery from music<br>\u5c06\u97f3\u4e50\u5f15\u53d1\u7684\u89c6\u89c9\u610f\u8c61\u5916\u663e\u5316"]\n    Root --\x3e Method["Multi-agent caption refinement & VA alignment<br>\u591a\u667a\u80fd\u4f53\u63cf\u8ff0\u4f18\u5316\u4e0e\u60c5\u611f\u5524\u9192\u5ea6\u5bf9\u9f50"]\n    Root --\x3e Results["Outperforms baselines in quality & alignment<br>\u5728\u8d28\u91cf\u4e0e\u5bf9\u9f50\u4e0a\u8d85\u8d8a\u57fa\u7ebf"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D reconstruction], [benchmark, multi-view, physical degradation, neural radiance field, Gaussian splatting]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Tokyo, NII, Tohoku University, University of W\xfcrzburg, RIKEN"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23437",children:"https://arxiv.org/pdf/2512.23437"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RealX3D: A Physically-Degraded 3D Benchmark] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u771f\u5b9e\u4e16\u754c\u9000\u5316\u5f71\u54cd3D\u91cd\u5efa/Real-world degradations hinder 3D reconstruction]\n    C --\x3e C1[\u6784\u5efa\u771f\u5b9e\u6355\u83b7\u57fa\u51c6/Build real-capture benchmark]\n    C --\x3e C2[\u56db\u7c7b\u9000\u5316, \u591a\u4e25\u91cd\u7ea7\u522b/Four corruption families, multiple severity levels]\n    C --\x3e C3[\u63d0\u4f9b\u5bf9\u9f50\u7684LQ/GT\u89c6\u56fe, RAW\u6570\u636e, \u6fc0\u5149\u626b\u63cf/Provide aligned LQ/GT views, RAW, laser scans]\n    D --\x3e D1[\u5f53\u524d\u65b9\u6cd5\u8d28\u91cf\u663e\u8457\u4e0b\u964d/Current methods show substantial quality degradation]\n    D --\x3e D2[\u7a81\u51fa\u73b0\u5b9e\u6311\u6218\u6027/Underscores fragility in challenging real environments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Unlocking WebRTC for End User Driven Innovation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [web systems], [WebRTC, browser extension, API interception, real-time customization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kundan Singh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Intencity Cloud Technologies"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23688",children:"https://arxiv.org/pdf/2512.23688"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A software architecture enabling end-user-driven customization of WebRTC applications via a browser extension. 2. A tool (RTC Helper) that intercepts and modifies WebRTC APIs in real-time to change web app behavior. 3. Support for rapid prototyping and over a hundred built-in examples across ten+ customization categories for novel communication use cases."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54e58edaa93822049a8bb88a45bce19b8d615d6c0c4f64e6c5e2c16cc6a757d3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54e58edaa93822049a8bb88a45bce19b8d615d6c0c4f64e6c5e2c16cc6a757d3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of vendor lock-in and limited innovation in WebRTC-based applications by proposing RTC Helper, a browser extension that intercepts WebRTC APIs to allow real-time customization by end-users and developers. This enables novel use cases and rapid prototyping without app redeployment. The conclusion is that this approach unlocks user-driven innovation in web multimedia communication."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Unlocking WebRTC for End User Driven Innovation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Vendor lock-in hinders innovation in WebRTC apps]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: RTC Helper browser extension intercepts & modifies WebRTC APIs]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Enables real-time customization & rapid prototyping with many examples]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SemCovert: Secure and Covert Video Transmission via Deep Semantic-Level Hiding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [semantic communication security], [semantic hiding, randomized embedding, secret semantic extractor]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhihan Cao, Xiao Yang, Gaolei Li, Jun Wu, Jianhua Li, Yuchen Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, North Carolina State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22233",children:"https://arxiv.org/pdf/2512.22233"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SemCovert, a deep semantic-level hiding framework for secure and covert video transmission, integrating co-designed hiding and extraction models into the semantic communication pipeline. 2. Introduces a randomized semantic hiding strategy to break embedding determinism and introduce unpredictable distribution patterns, improving resistance to analysis. 3. Demonstrates through experiments that the framework effectively mitigates eavesdropping/detection risks, reliably conceals secret videos with minor video quality degradation, preserving transmission fidelity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b368b0e0e2b9ae1e6c7bf4629ba64b86e8e570fdf21089b033a631ae92af3c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b368b0e0e2b9ae1e6c7bf4629ba64b86e8e570fdf21089b033a631ae92af3c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses privacy leakage in video semantic communication by proposing SemCovert, a framework that hides secret information at the semantic level using co-designed models and a randomized hiding strategy. The method enables authorized recovery while remaining imperceptible to regular users and resistant to statistical analysis. Experimental results confirm its effectiveness for secure and covert transmission without significantly compromising video quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SemCovert: Secure and Covert Video Transmission] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9690\u79c1\u6cc4\u9732\u98ce\u9669/Privacy Leakage Risk]\n    B --\x3e B2[\u4f20\u7edf\u5b89\u5168\u6280\u672f\u4e0d\u9002\u7528/Traditional Security Inapplicable]\n    C --\x3e C1[\u8bed\u4e49\u9690\u85cf\u6a21\u578b\u4e0e\u63d0\u53d6\u5668/Semantic Hiding Model & Extractor]\n    C --\x3e C2[\u968f\u673a\u5316\u8bed\u4e49\u9690\u85cf\u7b56\u7565/Randomized Semantic Hiding Strategy]\n    D --\x3e D1[\u6709\u6548\u7f13\u89e3\u7a83\u542c\u4e0e\u68c0\u6d4b\u98ce\u9669/Effectively Mitigates Eavesdropping & Detection]\n    D --\x3e D2[\u53ef\u9760\u9690\u85cf\u79d8\u5bc6\u89c6\u9891/Reliably Conceals Secret Videos]\n    D --\x3e D3[\u89c6\u9891\u8d28\u91cf\u8f7b\u5fae\u4e0b\u964d/Minor Video Quality Degradation]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);