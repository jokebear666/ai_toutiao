"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9991],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},59541:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"daily/cs_SD/20251229-20260104","title":"20251229-20260104 (cs.SD)","description":"2025-12-29","source":"@site/docs/daily/cs_SD/20251229-20260104.md","sourceDirName":"daily/cs_SD","slug":"/daily/cssd/20251229-20260104","permalink":"/ai_toutiao/daily/cssd/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767092843000,"frontMatter":{"slug":"/daily/cssd/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.SD)","permalink":"/ai_toutiao/daily/cssd/20251222-20251228"},"next":{"title":"cs.SE","permalink":"/ai_toutiao/daily/csse"}}');var t=i(74848),r=i(28453);const a={slug:"/daily/cssd/20251229-20260104"},o="20251229-20260104 (cs.SD)",d={},l=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",mermaid:"mermaid",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"20251229-20260104-cssd",children:"20251229-20260104 (cs.SD)"})}),"\n",(0,t.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Liuyang Bai, Weiyi Lu, Li Guo"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," NYU Shanghai"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21653",children:"https://arxiv.org/pdf/2512.21653"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251229] Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [sec], [audio deepfake detection], [transfer learning, zero-shot inference, fine-tuning, Bengali audio, BanglaFake dataset]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21702",children:"https://arxiv.org/pdf/2512.21702"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Bengali Deepfake Audio Detection is unexplored)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Zero-shot inference & Fine-tuning of pre-trained models)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251229] Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [nlp], [speech recognition & translation], [task vector, rare word recognition, catastrophic forgetting, speech-to-text, parameter arithmetic]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Ruihao Jing, Cheng Gong, Yu Jiang, Boyu Zhu, Shansong Liu, Chi Zhang, Xiao-Lei Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21894",children:"https://arxiv.org/pdf/2512.21894"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a training-free paradigm for rare word handling using task vectors, eliminating the need for fine-tuning. 2. Introduces word-level task vector arithmetic for flexible composition and reuse of rare-word capabilities. 3. Demonstrates that the method matches or surpasses fine-tuning on target words, improves general performance (~5 BLEU), and mitigates catastrophic forgetting."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd10590bdc1608f8eae90820d97325d5b60e598c9d06e1275430238b0313b56_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd10590bdc1608f8eae90820d97325d5b60e598c9d06e1275430238b0313b56_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the bottleneck of rare word recognition in speech-to-text systems. It proposes a training-free method based on task vector arithmetic to compose rare-word capabilities, which avoids the costs and forgetting issues of fine-tuning. Experiments show the method performs comparably to fine-tuning on target words while improving overall translation quality and reducing catastrophic forgetting."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Rare words are a bottleneck for speech-to-text systems. Fine-tuning is costly and causes forgetting.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Training-free paradigm using task vector arithmetic for flexible composition of rare-word capabilities.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Matches/surpasses fine-tuning on target words, improves general performance (~5 BLEU), mitigates forgetting.]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [speaker verification], [Layer Attentive Pooling, Attentive Statistical Temporal Pooling, pre-trained speech models, multi-level features, speaker embeddings]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Korea University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22148",children:"https://arxiv.org/pdf/2512.22148"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/sadPororo/LAP",children:"https://github.com/sadPororo/LAP"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    Root[\u91cd\u65b0\u601d\u8003\u5229\u7528\u9884\u8bad\u7ec3\u591a\u5c42\u8868\u793a\u8fdb\u884c\u8bf4\u8bdd\u4eba\u9a8c\u8bc1<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br/>\u9759\u6001\u52a0\u6743\u5e73\u5747\u805a\u5408\u591a\u5c42\u7279\u5f81\u7684\u5c40\u9650\u6027<br/>Limitations of static weighted average for multi-layer feature aggregation] --\x3e Problem_Detail[\u7ec6\u8282/Detail<br/>\u672a\u5145\u5206\u5229\u7528\u9ad8\u5c42\u8868\u793a<br/>Underutilization of high-level representations]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u63d0\u51fa\u5c42\u6ce8\u610f\u529b\u6c60\u5316<br/>Propose Layer Attentive Pooling (LAP)] --\x3e Method_Detail1[\u7ec6\u8282/Detail<br/>\u52a8\u6001\u591a\u89c6\u89d2\u8bc4\u4f30\u5c42\u91cd\u8981\u6027<br/>Time-dynamically assess layer significance from multiple perspectives]\n    Method --\x3e Method_Detail2[\u7ec6\u8282/Detail<br/>\u4f7f\u7528\u6700\u5927\u6c60\u5316\u800c\u975e\u5e73\u5747<br/>Employ max pooling instead of averaging]\n    Method --\x3e Method_Detail3[\u7ec6\u8282/Detail<br/>\u8f7b\u91cf\u7ea7\u540e\u7aef\u6a21\u578b (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br/>\u5728VoxCeleb\u4e0a\u8fbe\u5230SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --\x3e Results_Detail[\u7ec6\u8282/Detail<br/>\u6027\u80fd\u4f18\u8d8a\u4e14\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4<br/>Superior performance and greatly reduced training time]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] A Robust framework for sound event localization and detection on real recordings"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [audio event detection], [SELD, data augmentation, test time augmentation, ensemble, ACCDOA]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Korea University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22156",children:"https://arxiv.org/pdf/2512.22156"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. A robust training framework for SELD that mixes real recordings and emulated data with augmentation to improve generalization on real-world scenes. 2. A test time augmentation and clustering-based ensemble method to aggregate confident predictions and reject abnormal ones. 3. Application of the framework to a ResNet-based model, achieving competitive performance on the DCASE2022 challenge task."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12addf93fd64ffb5001f18188632e3d13c2b6e99ad52d2339022072d1c08696c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12addf93fd64ffb5001f18188632e3d13c2b6e99ad52d2339022072d1c08696c_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a robust framework for sound event localization and detection (SELD) to improve performance on real-world recordings. The method combines data augmentation, a pipeline mixing real and emulated datasets, and a test-time clustering ensemble. Experimental results show it outperforms baselines and achieves competitive performance in the DCASE2022 challenge."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\nA[A Robust framework for sound event localization and detection on real recordings] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: SELD generalization on real-world recordings)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: ResNet-based model with augmentation, real+emulated data mixing, test time ensemble)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms baseline, achieves competitive performance)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [domain adaptation, fine-tuning, learning rate optimization, data augmentation, word error rate]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Xuanfan Ni, Fei Yang, Fengping Tian, Qingjuan Li, Chenyang Lyu, Yichao Du, Longyue Wang, Weihua Luo, Kaifu Zhang"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Alibaba International Digital Commerce"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22165",children:"https://arxiv.org/pdf/2512.22165"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. A principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. 2. A learning rate optimization strategy based on performance metrics (e.g., WER) rather than just loss, to prevent overfitting and instability. 3. A domain-specific data analysis and augmentation pipeline to address data mismatch and linguistic variability."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edef09334bdd6f2c438c4ea7c62b2c8a134ebb61ef00ffed51f52a270a2fa141_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edef09334bdd6f2c438c4ea7c62b2c8a134ebb61ef00ffed51f52a270a2fa141_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a principled fine-tuning framework to adapt large-scale ASR models like Whisper and Qwen2-Audio to specialized domains. The framework uses metric-driven learning rate optimization and domain-specific data augmentation. Empirical results validate the framework and establish practical protocols for improving domain-specific performance while preventing overfitting."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root["MARCO-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: ASR\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u6027\u80fd\u4e0b\u964d/Degraded ASR performance in domain-specific applications"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u6307\u6807\u7684\u5fae\u8c03\u6846\u67b6/Metric-driven fine-tuning framework with learning rate optimization & data augmentation"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u6846\u67b6\u9a8c\u8bc1\u4e0e\u6027\u80fd\u63d0\u5347/Validated framework and improved performance while preventing overfitting"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [Generative Adversarial Networks (GANs), Single-Double-Triple (SDT) Attention, Time-Frequency Cross-Attention (TF-CA), contrastive losses, real-time generation]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," HaeChun Chung"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," KT Corporation"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22166",children:"https://arxiv.org/pdf/2512.22166"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AudioGAN, the first successful GAN-based framework for text-to-audio generation, enabling single-pass inference. 2. Introduces novel architectural components, Single-Double-Triple (SDT) Attention and Time-Frequency Cross-Attention (TF-CA), to enhance model capability. 3. Integrates multiple contrastive losses to overcome the inherent training difficulties of GANs, improving stability and performance."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb3eeaa0b35c7e10c5a5530b703d70eeb9b9309a57df9a97ae1234ec0c25981_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb3eeaa0b35c7e10c5a5530b703d70eeb9b9309a57df9a97ae1234ec0c25981_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces AudioGAN, a GAN-based framework for efficient text-to-audio generation. It overcomes GAN training challenges with novel attention mechanisms and contrastive losses, achieving state-of-the-art results with 90% fewer parameters and 20x faster inference than diffusion models."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root["AudioGAN: A Compact and Efficient Framework for Real-Time High-Fidelity Text-to-Audio Generation"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Diffusion-based TTA models are slow and computationally expensive"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: First successful GAN-based TTA framework with SDT Attention, TF-CA, and contrastive losses"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: SOTA performance, 90% fewer parameters, 20x faster inference (<1 second)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Chord Recognition with Deep Learning"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [music information retrieval], [chord recognition, deep learning, generative models, pitch augmentation, beat detection]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Pierre Mackenzie"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," University of Edinburgh"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22621",children:"https://arxiv.org/pdf/2512.22621"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    Root(Chord Recognition with Deep Learning) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u8fdb\u5c55\u7f13\u6162/Slow Progress)\n    Method --\x3e M1(\u5b9e\u9a8c\u73b0\u6709\u65b9\u6cd5/Experiment with Existing Methods)\n    Method --\x3e M2(\u6d4b\u8bd5\u751f\u6210\u6a21\u578b\u5047\u8bbe/Test Generative Model Hypotheses)\n    Results --\x3e R1(\u7f55\u89c1\u548c\u5f26\u8868\u73b0\u5dee/Poor Performance on Rare Chords)\n    Results --\x3e R2(\u97f3\u9ad8\u589e\u5f3a\u63d0\u5347\u51c6\u786e\u7387/Pitch Augmentation Boosts Accuracy)\n    Results --\x3e R3(\u8282\u62cd\u68c0\u6d4b\u63d0\u5347\u53ef\u89e3\u91ca\u6027/Beat Detection Improves Interpretability)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [DistilHuBERT, 8-bit quantization, cross-corpus validation, Leave-One-Session-Out (LOSO), model compression]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Saifelden M. Ismail"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," University of Science and Technology, Zewail City"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23435",children:"https://arxiv.org/pdf/2512.23435"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a "theatricality effect" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A["Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem: SER\u90e8\u7f72\u53d7\u9650\u4e8e\u5927\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42/SER deployment constrained by computational demands of large models"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u84b8\u998f\u4e0e8\u4f4d\u91cf\u5316\u7684DistilHuBERT\uff0c\u5e76\u8fdb\u884c\u8de8\u8bed\u6599\u5e93\u8bad\u7ec3/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results: \u6a21\u578b\u4ec523MB\uff0c\u7cbe\u5ea6\u8fbe\u57fa\u51c691%\uff0c\u8de8\u8bed\u6599\u5e93\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u6027/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [nlp], [spoken language understanding], [spoken language models, style amnesia, multi-turn conversation, paralinguistic features, instruction following]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Yu-Xiang Lin, Cheng-Han Chiang, Hung-yi Lee"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," National Taiwan University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23578",children:"https://arxiv.org/pdf/2512.23578"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and defines the "style amnesia" problem where spoken language models fail to maintain a user-specified speaking style across multiple conversation turns. 2. Provides a comprehensive evaluation across multiple proprietary and open-source SLMs, demonstrating the pervasiveness of the issue across different emotion, accent, volume, and speed styles. 3. Investigates mitigation strategies, finding that explicit recall prompts can partially alleviate the problem and revealing a counter-intuitive weakness when style instructions are placed in system messages.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7d00f21d056c5ef5d2a037960cefe1ed2dea85d21396e8409388c6f482ecbf8_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates the problem of "style amnesia" in spoken language models (SLMs), where models instructed to adopt a specific speaking style fail to maintain it over a multi-turn conversation. The authors evaluate several SLMs and find that explicitly prompting the model to recall the style instruction can partially mitigate the issue. The study concludes that current SLMs struggle with long-term style consistency, a critical challenge for natural spoken interactions.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    Root("Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("SLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u65e0\u6cd5\u7ef4\u6301\u6307\u5b9a\u7684\u526f\u8bed\u8a00\u98ce\u683c/SLMs cannot maintain specified paralinguistic style in multi-turn conversation")\n    Method --\x3e M1("\u5728\u591a\u8f6e\u5bf9\u8bdd\u5f00\u59cb\u65f6\u6307\u5b9a\u98ce\u683c\u5e76\u8bc4\u4f30/Instruct style at conversation start and evaluate")\n    Method --\x3e M2("\u4f7f\u7528\u81ea\u52a8\u8bc4\u4f30\u5668\u6d4b\u91cf\u6307\u4ee4\u9075\u5faa\u7387/Use automatic judges to measure instruction-following rate")\n    Method --\x3e M3("\u6d4b\u8bd5\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565/Test different prompting strategies")\n    Results --\x3e R1("\u53d1\u73b0\u98ce\u683c\u9057\u5fd8\u73b0\u8c61\uff0c\u6307\u4ee4\u9075\u5faa\u7387\u968f\u8f6e\u6b21\u4e0b\u964d/Style amnesia found, IF rate degrades over turns")\n    Results --\x3e R2("\u663e\u5f0f\u56de\u5fc6\u6307\u4ee4\u53ef\u90e8\u5206\u7f13\u89e3\u95ee\u9898/Explicit recall can partially mitigate")\n    Results --\x3e R3("\u7cfb\u7edf\u63d0\u793a\u4e2d\u7684\u6307\u4ee4\u6548\u679c\u4e0d\u4f73/Instructions in system prompts perform poorly")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [nlp], [speech recognition], [context-conditioned ASR, entity-aware evaluation, professional speech]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Deepak Babu Piskala"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Independent Researcher (affiliation inferred from email domain: gmail.com)"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23686",children:"https://arxiv.org/pdf/2512.23686"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"code:"})," ",(0,t.jsx)(n.a,{href:"https://github.com/prdeepakbabu/ProfASR-Bench",children:"https://github.com/prdeepakbabu/ProfASR-Bench"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces ProfASR-Bench, a benchmark for evaluating context-conditioned ASR in high-stakes professional domains (finance, medicine, legal, technology). 2. Identifies and defines the "context-utilization gap" (CUG), showing current promptable models underuse textual context for improving recognition. 3. Provides a standardized evaluation framework with a context ladder, entity/slice-aware reporting, and a reproducible testbed for comparing fusion strategies.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e17670ba26d26dd9fe7f42150241a5910106d394ee058ad7a75c1fc5815bdcd9_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces ProfASR-Bench, a benchmark for evaluating Automatic Speech Recognition (ASR) in high-stakes professional settings. It tests models like Whisper and Qwen-Omni with various contextual prompts and finds a "context-utilization gap," where current systems fail to effectively use available side information to improve accuracy, despite being promptable. The benchmark provides tools for entity-aware and slice-wise evaluation to advance context-conditioned ASR.']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[PROFASR-BENCH: A Benchmark for Context-Conditioned ASR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u57fa\u51c6\u4f4e\u4f30\u4e13\u4e1a\u573a\u666f\u6311\u6218 / Existing benchmarks underplay professional challenges]\n    B1 --\x3e B2[\u5bc6\u96c6\u672f\u8bed, \u6b63\u5f0f\u8bed\u4f53, \u5173\u952e\u5b9e\u4f53\u96f6\u5bb9\u5fcd / Dense terminology, formal register, zero tolerance for entity errors]\n    C --\x3e C1[\u6784\u5efa\u4e13\u4e1a\u8bed\u97f3\u8bc4\u4f30\u5957\u4ef6 / Build professional-talk evaluation suite]\n    C1 --\x3e C2[\u914d\u5bf9\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e0e\u76ee\u6807\u8bdd\u8bed / Pair natural-language prompts with target utterances]\n    C2 --\x3e C3[\u652f\u6301\u5b9e\u4f53\u611f\u77e5\u548c\u5206\u7247\u62a5\u544a / Support entity-aware and slice-wise reporting]\n    D --\x3e D1[\u53d1\u73b0\u4e0a\u4e0b\u6587\u5229\u7528\u5dee\u8ddd(CUG) / Uncover context-utilization gap (CUG)]\n    D1 --\x3e D2[\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\u5bf9WER\u6539\u5584\u751a\u5fae / Lightweight textual context yields little WER change]\n    D2 --\x3e D3[\u5bf9\u6297\u6027\u63d0\u793a\u4e0d\u4f1a\u53ef\u9760\u964d\u4f4e\u6027\u80fd / Adversarial prompts do not reliably degrade performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface], [EEG-to-Voice, mel-spectrogram, domain adaptation, automatic speech recognition, language model correction]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Hanbeot Park, Yunjeong Cho, Hunhee Kim"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Pukyong National University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22146",children:"https://arxiv.org/pdf/2512.22146"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[EEG-to-Voice Decoding of Spoken and Imagined speech] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: EEG-based speech reconstruction is challenging due to noise, low resolution, and lack of aligned targets for imagined speech.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Direct, open-loop EEG-to-mel-spectrogram generation with subject-specific generators, domain adaptation from spoken to imagined speech, and optional LM-based ASR correction.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Feasibility demonstrated for both speech types; stable acoustic/linguistic performance; LM correction reduces CER/WER without semantic distortion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"tags:"})," [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"authors:"})," Atakan I\u015f\u0131k, Selin Vulga I\u015f\u0131k, Ahmet Feridun I\u015f\u0131k, Mah\u015fuk Taylan"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"institution:"})," Ba\u015fkent University, Gaziantep University"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"link:"})," ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22564",children:"https://arxiv.org/pdf/2512.22564"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"thumbnail:"})," ",(0,t.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\n    A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>\u547c\u5438\u58f0\u97f3\u5206\u7c7b\u7684\u51e0\u4f55\u611f\u77e5\u4f18\u5316] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n\n    B --\x3e B1[\u6570\u636e\u9650\u5236\u4e0e\u8fc7\u62df\u5408<br/>Data Constraints & Overfitting]\n    B1 --\x3e B2[\u6570\u636e\u96c6\u5c0f\u3001\u566a\u58f0\u5927\u3001\u7c7b\u522b\u4e0d\u5e73\u8861<br/>Small, Noisy, Imbalanced Dataset]\n\n    C --\x3e C1[\u4f7f\u7528SAM\u4f18\u5316AST<br/>Enhance AST with SAM]\n    C1 --\x3e C2[\u4f18\u5316\u635f\u5931\u66f2\u9762\u51e0\u4f55<br/>Optimize Loss Surface Geometry]\n    C --\x3e C3[\u52a0\u6743\u91c7\u6837\u7b56\u7565<br/>Weighted Sampling Strategy]\n\n    D --\x3e D1[SOTA\u5206\u6570: 68.10%<br/>SOTA Score: 68.10%]\n    D --\x3e D2[\u9ad8\u654f\u611f\u5ea6: 68.31%<br/>High Sensitivity: 68.31%]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);