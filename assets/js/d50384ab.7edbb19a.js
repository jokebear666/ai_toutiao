"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[820],{945:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_SD/20251215-20251221","title":"20251215-20251221 (cs.SD)","description":"2025-12-18","source":"@site/docs/daily/cs_SD/20251215-20251221.md","sourceDirName":"daily/cs_SD","slug":"/daily/cs_SD/20251215-20251221","permalink":"/ai_toutiao/daily/cs_SD/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766635085000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"cs.SD","permalink":"/ai_toutiao/daily/cssd"},"next":{"title":"20251222-20251228 (cs.SD)","permalink":"/ai_toutiao/daily/cssd/20251222-20251228"}}');var r=i(4848),t=i(8453);const a={},o="20251215-20251221 (cs.SD)",l={},d=[{value:"2025-12-18",id:"2025-12-18",level:2},{value:"2025-12-19",id:"2025-12-19",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251215-20251221-cssd",children:"20251215-20251221 (cs.SD)"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-18",children:"2025-12-18"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [learnable Gabor filters, ResNeXt, squeeze-and-excitation attention, spectrograms, deep learning]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Flinders University, PrioriAnalytica"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14714",children:"https://arxiv.org/pdf/2512.14714"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [end-to-end spoken dialogue systems, audio language models, multi-turn evaluation, speech-to-speech, audio-native benchmark, inference memory, instruction retention, self coherence, voice editing]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda U\xe7ar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Scale AI"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14865",children:"https://arxiv.org/pdf/2512.14865"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new "Voice Editing" axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Snap Inc."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14938",children:"https://arxiv.org/pdf/2512.14938"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [multi-task learning, cross-attention, gated fusion, confidence-weighted fusion, adaptive fusion]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aref Farhadipour, Teodora Vukovic, Volker Dellwo, Petr Motlicek, Srikanth Madikeri"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Zurich, Idiap Research Institute"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14961",children:"https://arxiv.org/pdf/2512.14961"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a robust trimodal person recognition framework that integrates voice, face, and gesture data using multi-task learning, cross-attention, and a gated, confidence-weighted fusion strategy to handle missing or degraded modalities. It achieves high accuracy on the CANDOR and VoxCeleb1 datasets and maintains performance even when modalities are unavailable, demonstrating robustness for real-world applications."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] BEAT2AASIST model with layer fusion for ESDD 2026 Challenge"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [audio deepfake detection], [BEATs, AASIST, transformer layer fusion, vocoder-based data augmentation, dual-branch architecture]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sanghyeok Chung, Eujin Kim, Donggun Kim, Gaeun Heo, Jeongbin You, Nahyun Lee, Sunmook Choi, Soyul Han, Seungsang Oh, Il-Youp Kwak"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea University, Chung-Ang University, Cornell University, Hannam University"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15180",children:"https://arxiv.org/pdf/2512.15180"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes BEAT2AASIST, a model for environmental sound deepfake detection that extends BEATs-AASIST by splitting features into dual AASIST branches and incorporating transformer layer fusion strategies. It also uses vocoder-based data augmentation for robustness. The approach demonstrates competitive performance on the ESDD 2026 Challenge test sets."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [EEND-EDA, RNN-based stitching, centroid refinement decoder, permutation-invariant training (PIT), online speaker diarization]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Elio Gruttadauria, Mathieu Fontaine, Jonathan Le Roux, Slim Essid"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," T\xe9l\xe9com Paris, Institut polytechnique de Paris, Mitsubishi Electric Research Laboratories (MERL)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15229",children:"https://arxiv.org/pdf/2512.15229"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces O-EENC-SD, an efficient online end-to-end neural clustering system for speaker diarization. It is based on EEND-EDA and features a novel RNN-based stitching mechanism and a centroid refinement decoder for online prediction. The system is shown to be competitive with state-of-the-art methods on the CallHome dataset while offering a better trade-off between diarization error rate and computational complexity."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] Time-Varying Audio Effect Modeling by End-to-End Adversarial Training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Generative Adversarial Network (GAN), convolutional-recurrent architecture, adversarial training, State Prediction Network (SPN), chirp-train signals]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yann Bourdin, Pierrick Legrand, Fanny Roche"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arturia, Inria, IMS, University of Bordeaux, Bordeaux INP"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15313",children:"https://arxiv.org/pdf/2512.15313"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a two-stage GAN framework for black-box modeling of time-varying audio effects, using only input-output recordings without needing control signals. The method involves an adversarial training phase followed by supervised fine-tuning with a State Prediction Network for synchronization. Experiments on a vintage phaser demonstrate the approach's effectiveness in capturing time-varying dynamics."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251218] A Conditioned UNet for Music Source Separation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [conditioned UNet, music source separation, QSCNet, Sparse Compressed Network, Bandsplit RNN, Banquet, MoisesDb]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ken O'Hanlon, Basil Woods, Lin Wang, Mark Sandler"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Queen Mary University of London, AudioStrip Ltd."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.15532",children:"https://arxiv.org/pdf/2512.15532"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-19",children:"2025-12-19"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [transformer, causal attention, hierarchical representation, multi-task learning, domain-adversarial training, controlled perturbation training, counterfactual simulation]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Strathmore University, Google Research, Queen Mary University of London"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16271",children:"https://arxiv.org/pdf/2512.16271"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes DACH-TIC, a domain-agnostic causal-aware hierarchical audio transformer that integrates causal attention, multi-task learning, and adversarial domain generalization for robust infant cry classification. It outperforms state-of-the-art baselines in accuracy and macro-F1 score and demonstrates strong generalization to unseen acoustic environments with minimal performance degradation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [SpeechLLMs, cascaded systems, speech foundation models, speech-to-text translation, benchmarking, Whisper, SeamlessM4T, Gemma, Tower+]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vil\xe9m Zouhar, Carlos Escolano, Gerard I. G\xe1llego, Jorge Iranzo-S\xe1nchez, Ahrii Kim, Dominik Mach\xe1\u010dek, Patricia Schmidtova, Maike Z\xfcfle"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fondazione Bruno Kessler, Barcelona Supercomputing Center, University of Zurich, ETH Zurich, Universitat Polit\xe8cnica de Catalunya, Universitat Polit\xe8cnica de Val\xe8ncia, AI-Bio Convergence Research Institute, Charles University, KIT"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16378",children:"https://arxiv.org/pdf/2512.16378"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces the "Hearing to Translate" test suite to benchmark SpeechLLMs against cascaded and direct speech-to-text translation systems. It finds that cascaded systems, which combine speech recognition with LLM-based translation, remain the most reliable overall, while current SpeechLLMs only match them in specific scenarios. The study concludes that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.']}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251219] Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [speech synthesis], [cepstrum, pitch shifting, mel-spectrogram, DCT, pseudo-inverse mel transform, neural vocoder]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nikolaos Ellinas, Alexandra Vioni, Panos Kakoulidis, Georgios Vamvoukakis, Myrsini Christidou, Konstantinos Markopoulos, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Innoetics, Samsung Electronics, Samsung Electronics Mobile eXperience Business"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.16519",children:"https://arxiv.org/pdf/2512.16519"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a cepstrum-based pitch modification method that directly manipulates the cepstral feature space to shift the harmonic structure in a mel-spectrogram, making it compatible with any mel-based neural vocoder without retraining. The method is validated through objective and subjective evaluations, showing its effectiveness compared to traditional pitch modification techniques."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);