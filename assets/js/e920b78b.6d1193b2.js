"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4612],{3532:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>o,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/cs_CL/20251222-20251228","title":"20251222-20251228 (cs.CL)","description":"2025-12-22","source":"@site/docs/daily/cs_CL/20251222-20251228.md","sourceDirName":"daily/cs_CL","slug":"/daily/cscl/20251222-20251228","permalink":"/ai_toutiao/daily/cscl/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766988500000,"frontMatter":{"slug":"/daily/cscl/20251222-20251228"},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221 (cs.CL)","permalink":"/ai_toutiao/daily/cs_CL/20251215-20251221"},"next":{"title":"20251229-20260104 (cs.CL)","permalink":"/ai_toutiao/daily/cscl/20251229-20260104"}}');var a=i(4848),r=i(8453);const t={slug:"/daily/cscl/20251222-20251228"},l="20251222-20251228 (cs.CL)",d={},c=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2},{value:"2025-12-24",id:"2025-12-24",level:2},{value:"2025-12-25",id:"2025-12-25",level:2}];function h(n){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mermaid:"mermaid",mn:"mn",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"20251222-20251228-cscl",children:"20251222-20251228 (cs.CL)"})}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] PAACE: A Plan-Aware Automated Agent Context Engineering Framework"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [context engineering, plan-aware compression, next-k-task relevance, instruction co-refinement, function-preserving compression, synthetic data generation, knowledge distillation]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kamer Ali Yuksel"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," aiXplain Inc"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16970",children:"https://arxiv.org/pdf/2512.16970"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Shanghai Artificial Intelligence Laboratory"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16969",children:"https://arxiv.org/pdf/2512.16969"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] A Women's Health Benchmark for Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [healthcare AI evaluation], [women's health benchmark, large language models, error types, model stumps, query types]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel G\xfchner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17028",children:"https://arxiv.org/pdf/2512.17028"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces the Women's Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women's health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women's health advice."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [knowledge distillation, chain-of-thought, structured reasoning, query execution plan, text-to-sql]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Khushboo Thaker, Yony Bresler"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Crater Labs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17053",children:"https://arxiv.org/pdf/2512.17053"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/906b49597857a3cad8e1c9c8d6cdbec46e7807fe819943d1e6d91facfb7f18bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/906b49597857a3cad8e1c9c8d6cdbec46e7807fe819943d1e6d91facfb7f18bd_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Struct-SQL, a knowledge distillation framework that trains a small language model using a structured chain-of-thought derived from query execution plans, rather than unstructured reasoning traces. The distilled model achieves an 8.1% absolute improvement over an unstructured baseline, primarily due to a reduction in syntactic errors. This demonstrates that structured logical blueprints are beneficial for reliable SQL generation in small models."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Perturb Your Data: Paraphrase-Guided Training Data Watermarking"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [SPECTRA, watermarking, training data detection, membership inference attack, paraphrase generation, scoring model, token probability comparison]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," JPMorgan AI Research"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17075",children:"https://arxiv.org/pdf/2512.17075"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b47f34679362a94a5413b86758bfca6d1690e7158a9b8bc7a21706264c5e833c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces SPECTRA, a watermarking method that subtly paraphrases text using an LLM to embed a detectable signature into training data without altering its statistical distribution. It verifies unauthorized use by comparing token probabilities between a suspect model and a scoring model. The approach reliably detects watermarked data even when it constitutes a minuscule fraction of the training corpus, providing a scalable pre-release watermark for data owners."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [dialogue topic segmentation], [window-tolerant F1, boundary density, segment coherence, granularity-aware evaluation]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Michael H. Coen"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Independent Researcher"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17083",children:"https://arxiv.org/pdf/2512.17083"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a new evaluation framework for dialogue topic segmentation that emphasizes boundary density and segment coherence alongside window-tolerant F1. It demonstrates through cross-dataset experiments that reported performance differences are often artifacts of annotation granularity mismatches, not model quality. The core conclusion is that topic segmentation should be viewed as selecting an appropriate granularity rather than predicting a single correct boundary set."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [solver-in-the-loop, instruction-tuning, supervised fine-tuning, best-of-N sampling, answer set programming, semantic parsing]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Timo Pierre Schrader, Lukas Lange, Tobias Kaminski, Simon Razniewski, Annemarie Friedrich"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Bosch Center for AI, University of Augsburg, ScaDS.AI & TU Dresden"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17093",children:"https://arxiv.org/pdf/2512.17093"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38c83df2ce552270bc09f323934a96a0aad16af58e736a7049ccfd73afeed0d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38c83df2ce552270bc09f323934a96a0aad16af58e736a7049ccfd73afeed0d4_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a solver-in-the-loop framework that uses an ASP solver to provide feedback on LLM-generated code, creating a dataset of chosen and rejected instances for supervised fine-tuning. The method improves LLM performance on generating Answer Set Programming code for logic puzzles, demonstrating consistent gains across different prompting settings and datasets."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [error level noise embedding, n-best hypotheses, noise-aware modeling, whisper, llama-2, word error rate, fine-tuning]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zahra Rahmani, Hossein Sameti"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Sharif University of Technology"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17247",children:"https://arxiv.org/pdf/2512.17247"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a robust noise-sensitive ASR error correction framework for Persian. It introduces Error Level Noise (ELN) embeddings, derived from disagreements in multiple ASR hypotheses, to condition a fine-tuned LLaMA-2 model, enabling it to reason about noise-induced uncertainty. The ELN-conditioned model significantly reduces Word Error Rate compared to text-only baselines, demonstrating the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust speech recognition in noisy environments."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [evaluation framework], [LLM-as-a-Judge, regression, MetricBank, retrieval, human feedback correlation]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Michael J. Ryan, Yanzhe Zhang, Amol Salunkhe, Yi Chu, Di Xu, Diyi Yang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Stanford University, American Express"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17267",children:"https://arxiv.org/pdf/2512.17267"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents AutoMetrics, a framework that synthesizes evaluation metrics by combining retrieved metrics from a curated bank with automatically generated LLM-as-a-Judge criteria, composed via regression to maximize correlation with human feedback. It demonstrates that AutoMetrics significantly improves correlation with human judgments over standard LLM-as-a-Judge approaches while requiring minimal human feedback data. The method can serve as an effective proxy reward for optimizing AI applications."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Wuhan University, Tongyi Lab, Zhongguancun Academy"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17270",children:"https://arxiv.org/pdf/2512.17270"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Subjective Question Generation and Answer Evaluation using NLP"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [large language models, instruct-tuning, bloom's taxonomy, subjective evaluation, question generation, answer evaluation]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," G. M. Refatul Islam, Safwan Shaheer, Yaseen Nur, Mohammad Rafid Hamid"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Brac University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17289",children:"https://arxiv.org/pdf/2512.17289"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f03ca77cc9ebde43ea5a7c83c936ce7c8843b9c39c6b6c58614cb92eb1ce8fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f03ca77cc9ebde43ea5a7c83c936ce7c8843b9c39c6b6c58614cb92eb1ce8fc_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This research proposes a framework that uses instruct-tuned large language models (LLMs) to generate subjective questions and evaluate student answers, particularly for higher-order thinking skills. The study concludes that this approach can effectively automate the assessment of complex, subjective understanding, a task traditionally requiring human evaluators."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Large Language Models as Pok\xe9mon Battle Agents: Strategic Play and Content Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [large language models, turn-based battle system, strategic decision-making, content generation, procedural generation, adaptive difficulty]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Birla Institute of Technology and Science, Pilani"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17308",children:"https://arxiv.org/pdf/2512.17308"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper develops a turn-based Pok\xe9mon battle system where LLMs act as agents, making tactical decisions based on a structured battle state without domain-specific training. The core method involves evaluating LLMs on strategic reasoning and their ability to generate novel game content. The main conclusion is that LLMs can function as dynamic game opponents and designers, offering a practical alternative to reinforcement learning for strategic games."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Task Schema and Binding: A Double Dissociation Study of In-Context Learning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [in-context learning], [activation patching, double dissociation, task schema, binding, transformer, mamba]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Chaeha Kim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Changwon National University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17325",children:"https://arxiv.org/pdf/2512.17325"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper uses activation patching experiments across multiple Transformer models and Mamba to causally dissect in-context learning. It concludes that ICL decomposes into two separable mechanisms: Task Schema (abstract task recognition) and Binding (specific input-output associations), with their reliance governed by a trade-off with the model's prior knowledge."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [adversarial control tokens, beam-search exploration, last-layer logit gap, LoRA-based adversarial training, reward hacking]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tung-Ling Li, Yuhao Wu, Hongliang Liu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Palo Alto Networks"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17375",children:"https://arxiv.org/pdf/2512.17375"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper introduces AdvJudge-Zero, a method that uses beam-search on a model\'s next-token distribution to discover short, low-perplexity control token sequences that can flip the binary decisions of LLM-as-a-Judge systems from "No" to "Yes". It concludes that these tokens represent a realistic reward-hacking vulnerability in post-training pipelines, and shows that adversarial training can mitigate the issue while preserving evaluation quality.']}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [vision-language models], [CulturalToM-VQA, visual question answering, chain-of-thought prompting, compositional chain-of-thought prompting, false belief reasoning, social desirability bias]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of California, Riverside, University of Dhaka, Stanford University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17394",children:"https://arxiv.org/pdf/2512.17394"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [visual question answering, vision-language models, fine-tuning, benchmark dataset, CT, MRI]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," L\xe9o Butsanets, Charles Corbi\xe8re, Julien Khlaut, Pierre Manceron, Corentin Dancette"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Raidium, Universit\xe9 de Paris Cit\xe9, H\xf4pital Europ\xe9en Georges Pompidou, AP-HP, INSERM"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17396",children:"https://arxiv.org/pdf/2512.17396"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [SWE-Bench++, automated benchmark generation, pull request harvesting, environment synthesis, test oracle extraction, hint-guided trajectory synthesis, fine-tuning]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Turing"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17419",children:"https://arxiv.org/pdf/2512.17419"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/077c20705c707ee562f1935988b006695cf25f213f2df392cb27846fedaf0d4a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces SWE-Bench++, an automated framework that generates software engineering benchmarks by harvesting pull requests from GitHub to create reproducible, execution-based coding tasks across multiple languages. The method involves programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance, with a final step to create training trajectories from failed instances. The main conclusion is that this scalable, multilingual approach provides a valuable benchmark for evaluating and improving LLMs on repository-level code generation, as demonstrated by model performance metrics and fine-tuning improvements."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [natural language processing], [ensemble learning, weighted voting, Condorcet\u2019s Jury Theorem, fine-tuning, transformer models]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Menna Elgabry, Ali Hamdi"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," MSA University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17630",children:"https://arxiv.org/pdf/2512.17630"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a confidence- and credibility-weighted ensemble framework using diverse small transformer models (BERT, RoBERTa, etc.) for emotion detection. The method combines global validation performance and instance-level confidence to weight model votes. The ensemble achieves a 93.5% macro F1-score on the DAIR-AI dataset, outperforming larger LLMs while being more parameter-efficient."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [AncientBench, benchmark evaluation, ancient character comprehension, excavated documents, glyph comprehension, pronunciation comprehension, meaning comprehension, contextual comprehension]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhihan Zhou, Daqian Shi, Rui Song, Lida Shi, Xiaolei Diao, Hao Xu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Jilin University, Queen Mary University of London, University of Trento"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17756",children:"https://arxiv.org/pdf/2512.17756"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a15f186f48b4cc77c0d16272783515a0f56f75b51cf745384fc582f7e77f37a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a15f186f48b4cc77c0d16272783515a0f56f75b51cf745384fc582f7e77f37a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces AncientBench, a comprehensive benchmark designed to evaluate large language models' comprehension of ancient Chinese, particularly focusing on excavated documents. It assesses four competencies (glyph, pronunciation, meaning, and contextual) through ten tasks. The experimental results show that while LLMs have significant potential in ancient text scenarios, a performance gap remains compared to human experts."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [BERT, DistilBERT, ELECTRA, RoBERTa, Multi-BERT Ensemble, transformer models, medical entity recognition]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tanjim Taharat Aurpa, Farzana Akter, Md. Mehedi Hasan, Shakil Ahmed, Shifat Ara Rafiq, Fatema Khan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Frontier Technology, University of Liberal Arts Bangladesh"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17769",children:"https://arxiv.org/pdf/2512.17769"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Multi-BERT Ensemble approach for Bangla Medical Entity Recognition (MedER), evaluating models like BERT, DistilBERT, ELECTRA, and RoBERTa. The ensemble method achieved 89.58% accuracy, an 11.80% improvement over single-layer BERT, and the authors also created a new annotated dataset for this low-resource language task."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] ShareChat: A Dataset of Chatbot Conversations in the Wild"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [dataset collection, multi-turn conversations, platform affordances, source citations, temporal analysis, cross-platform corpus]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Indiana University"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17843",children:"https://arxiv.org/pdf/2512.17843"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces ShareChat, a large-scale dataset of real-world chatbot conversations collected from five major platforms, preserving interface-specific features like reasoning traces and source links. It demonstrates the dataset's utility through analyses of user intent satisfaction, citation behaviors, and evolving usage patterns, providing a resource for studying authentic user-LLM interactions."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251222] When Reasoning Meets Its Laws"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [large reasoning models], [laws of reasoning, compute law, accuracy law, monotonicity, compositionality, LoRe-Bench, finetuning]"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Massachusetts Institute of Technology, University of Pennsylvania, New York University, NTT Research"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17901",children:"https://arxiv.org/pdf/2512.17901"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9db7b3665dbba1bcaed95897dff8a53103ef5bfe963b50f03c044189965a72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f9db7b3665dbba1bcaed95897dff8a53103ef5bfe963b50f03c044189965a72_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces the Laws of Reasoning (LoRe), a framework that formalizes desired reasoning behaviors in large reasoning models, including compute and accuracy laws. It proposes LoRe-Bench to evaluate monotonicity and compositionality, and develops a finetuning method to improve compositionality. The study finds that better compliance with these laws leads to enhanced reasoning performance across benchmarks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Lihui Liu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17912",children:"https://arxiv.org/pdf/2512.17912"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Rahul Baxi"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17920",children:"https://arxiv.org/pdf/2512.17920"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e85dcb740e46985e03fe90bf075468b334e1528a3de2a4858fac5b2ddbc2dc9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e85dcb740e46985e03fe90bf075468b334e1528a3de2a4858fac5b2ddbc2dc9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17911",children:"https://arxiv.org/pdf/2512.17911"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Minh Tri L\xca, Ali Ait-Bachir"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17916",children:"https://arxiv.org/pdf/2512.17916"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a9a26225e6a2c495c48df9cb6a0e4bd0c624c036e56d8d0cf9885d0d909a745_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a9a26225e6a2c495c48df9cb6a0e4bd0c624c036e56d8d0cf9885d0d909a745_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Aomufei Yuan, Zhiming Wang, Ruijie Miao, Dayu Wang, Yuxuan Tian, Zihan Wang, Yebo Peng, Yuhan Wu, Bairen Yi, Xin Liu, Tong Yang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17917",children:"https://arxiv.org/pdf/2512.17917"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca40de411c47ab6f32fb67699fbcdce0808ef0d5179742bf46c64d088a640d3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca40de411c47ab6f32fb67699fbcdce0808ef0d5179742bf46c64d088a640d3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Boris Kriuk, Logic Ng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17914",children:"https://arxiv.org/pdf/2512.17914"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d4fd665329b7eb837ca00f1091de17f8b4c1eb920670c43995680ca539562ae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d4fd665329b7eb837ca00f1091de17f8b4c1eb920670c43995680ca539562ae_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Nick Rossenbach, Robin Schmitt, Tina Raissi, Simon Berger, Larissa Kleppel, Ralf Schl\xfcter"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17915",children:"https://arxiv.org/pdf/2512.17915"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6036cb69e2cb0f832a1b1088209441a1b5309cc94cf18961ca3b07bffec7a52c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18014",children:"https://arxiv.org/pdf/2512.18014"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18004",children:"https://arxiv.org/pdf/2512.18004"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] CoPE: A Small Language Model for Steerable and Scalable Content Labeling"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Samidh Chakrabarti, David Willner, Kevin Klyman, Tiffany Saade, Emily Capstick, Sabina Nong"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18027",children:"https://arxiv.org/pdf/2512.18027"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/423158010714f415c807a9ae93864ccaf58e7d33b462039083dce106e5d195f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/423158010714f415c807a9ae93864ccaf58e7d33b462039083dce106e5d195f2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," CoPE: A Small Language Model for Steerable and Scalable Content Labeling"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Roger A. Finger, Eduardo G. Cortes, Sandro J. Rigo, Gabriel de O. Ramos"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18041",children:"https://arxiv.org/pdf/2512.18041"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/405c807df8e2ce95661469b072db6898b3dcf1bb175cd81aa868ecc9d2c06c12_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Statistical laws and linguistics inform meaning in naturalistic and fictional conversation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ashley M. A. Fehr, Calla G. Beauregard, Julia Witte Zimmerman, Katie Ekstr\xf6m, Pablo Rosillo-Rodes, Christopher M. Danforth, Peter Sheridan Dodds"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18072",children:"https://arxiv.org/pdf/2512.18072"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0119bf39897b4bb6f848dd8818e4a75f43e327cbc8223e8cff6bafd0a8277d08_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0119bf39897b4bb6f848dd8818e4a75f43e327cbc8223e8cff6bafd0a8277d08_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Statistical laws and linguistics inform meaning in naturalistic and fictional conversation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Changxu Duan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18115",children:"https://arxiv.org/pdf/2512.18115"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jian Yan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18190",children:"https://arxiv.org/pdf/2512.18190"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Training LLMs with LogicReward for Faithful and Rigorous Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jundong Xu, Hao Fei, Huichi Zhou, Xin Quan, Qijun Huang, Shengqiong Wu, William Yang Wang, Mong-Li Lee, Wynne Hsu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18196",children:"https://arxiv.org/pdf/2512.18196"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a4cb0521ac4192e49080ee2ad4da1d452c4469bd6d7c8279e1ca666dd437a6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10a4cb0521ac4192e49080ee2ad4da1d452c4469bd6d7c8279e1ca666dd437a6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Training LLMs with LogicReward for Faithful and Rigorous Reasoning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18215",children:"https://arxiv.org/pdf/2512.18215"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Stable and Efficient Single-Rollout RL for Multimodal Reasoning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] GeoSense-AI: Fast Location Inference from Crisis Microblogs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Deepit Sapru"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18225",children:"https://arxiv.org/pdf/2512.18225"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c58318afd02a0e78246cd4d9114fc0566ca0ff40a0a1ad3e92124da0582633_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c58318afd02a0e78246cd4d9114fc0566ca0ff40a0a1ad3e92124da0582633_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," GeoSense-AI: Fast Location Inference from Crisis Microblogs"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Investigating Spatial Attention Bias in Vision-Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Aryan Chaudhary, Sanchit Goyal, Pratik Narang, Dhruv Kumar"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18231",children:"https://arxiv.org/pdf/2512.18231"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Investigating Spatial Attention Bias in Vision-Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sudip Chakrabarty, Pappu Bishwas, Rajdeep Chatterjee"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18298",children:"https://arxiv.org/pdf/2512.18298"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2d1fa5f1ea12d3598f0bd43290aa418e4ebda7629f53494201d317e1a493a95_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2d1fa5f1ea12d3598f0bd43290aa418e4ebda7629f53494201d317e1a493a95_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wenkai Li, Lynnette Hui Xian Ng, Andy Liu, Daniel Fried"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18292",children:"https://arxiv.org/pdf/2512.18292"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bca157c21b274184903d6db27d719924ad61a7ebd545421b88ee4959f34a8c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bca157c21b274184903d6db27d719924ad61a7ebd545421b88ee4959f34a8c8_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tanjim Taharat Aurpa, Md Shoaib Ahmed, Md Mahbubur Rahman, Md. Golam Moazzam"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18301",children:"https://arxiv.org/pdf/2512.18301"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7a5ce8e8f2666469be8a7a0bffad58a614870a283a7d6d1dca8f43d9f0a9304_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7a5ce8e8f2666469be8a7a0bffad58a614870a283a7d6d1dca8f43d9f0a9304_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tianlun Liu, Zhiliang Tian, Zhen Huang, Xingzhi Zhou, Wanlong Yu, Tianle Liu, Feng Liu, Dongsheng Li"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18321",children:"https://arxiv.org/pdf/2512.18321"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3cf2be9f3faea6343f7391291b649e6555516f57a7f71804673fdebc50f3e4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a3cf2be9f3faea6343f7391291b649e6555516f57a7f71804673fdebc50f3e4_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsxs)(e.strong,{children:["[arXiv251223] LIR",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsx)(e.mrow,{children:(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mn,{children:"3"})]})}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^3"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mord mtight",children:"3"})})]})})})})})]})]})})]}),"AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation"]})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Guo Chen, Junjie Huang, Huaijin Xie, Fei Sun, Tao Jia"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18329",children:"https://arxiv.org/pdf/2512.18329"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dd393abec358205f73a078d58f2d077d68b22ce62c1b8d3eb7bc25329e59e8a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dd393abec358205f73a078d58f2d077d68b22ce62c1b8d3eb7bc25329e59e8a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," LIR",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsx)(e.mrow,{children:(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mn,{children:"3"})]})}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^3"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mord mtight",children:"3"})})]})})})})})]})]})})]}),"AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Towards Efficient Agents: A Co-Design of Inference Architecture and System"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Weizhe Lin, Hui-Ling Zhen, Shuai Yang, Xian Wang, Renxi Liu, Hanting Chen, Wangze Zhang, Chuansai Zhou, Yiming Li, Chen Chen, Xing Li, Zhiyuan Yang, Xiaosong Li, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan, Yunhe Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18337",children:"https://arxiv.org/pdf/2512.18337"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7ed4e66482149eeb38c88b95b26442424c5cb783935dd1d2ae998dcbe934a3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7ed4e66482149eeb38c88b95b26442424c5cb783935dd1d2ae998dcbe934a3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Towards Efficient Agents: A Co-Design of Inference Architecture and System"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] LLM-based Few-Shot Early Rumor Detection with Imitation Agent"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Fengzhu Zeng, Qian Shao, Ling Cheng, Wei Gao, Shih-Fen Cheng, Jing Ma, Cheng Niu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18352",children:"https://arxiv.org/pdf/2512.18352"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68253881479be80c6f5156b8929dcea7c9affda2463411703dbff80daa9a6787_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68253881479be80c6f5156b8929dcea7c9affda2463411703dbff80daa9a6787_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," LLM-based Few-Shot Early Rumor Detection with Imitation Agent"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] SRS-Stories: Vocabulary-constrained multilingual story generation for language learning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wiktor Kamzela, Mateusz Lango, Ondrej Dusek"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18362",children:"https://arxiv.org/pdf/2512.18362"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bb1bdec7e9243a15f59e7c93c8e0fc83a1b1ee3982dfb0225c524424979ac69_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bb1bdec7e9243a15f59e7c93c8e0fc83a1b1ee3982dfb0225c524424979ac69_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SRS-Stories: Vocabulary-constrained multilingual story generation for language learning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mateusz Lango, Ond\u0159ej Du\u0161ek"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18360",children:"https://arxiv.org/pdf/2512.18360"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e70af4d7e4c119643e3631c8815a5d46438a6f1b8881a5821764fb495f8608f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e70af4d7e4c119643e3631c8815a5d46438a6f1b8881a5821764fb495f8608f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] DACE For Railway Acronym Disambiguation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," El Mokhtar Hribach, Oussama Mechhour, Mohammed Elmonstaser, Yassine El Boudouri, Othmane Kabal"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18357",children:"https://arxiv.org/pdf/2512.18357"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/491cdc5ad5360c6518de9b4fdc6851733d3db21d281c60aa6fe7296ba2016a65_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/491cdc5ad5360c6518de9b4fdc6851733d3db21d281c60aa6fe7296ba2016a65_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," DACE For Railway Acronym Disambiguation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mark Kashirskiy, Artiom Lipinski, Ilya Makarov"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18399",children:"https://arxiv.org/pdf/2512.18399"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb721fc55d6ea689ae069e1c50129b86f7fc25c4c3433aa154aaa38bf9378cd3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb721fc55d6ea689ae069e1c50129b86f7fc25c4c3433aa154aaa38bf9378cd3_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] An Agentic AI Framework for Training General Practitioner Student Skills"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Victor De Marez, Jens Van Nooten, Luna De Bruyne, Walter Daelemans"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18440",children:"https://arxiv.org/pdf/2512.18440"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937c34e04859d95b3aed57029c9c5791e5aa2e5a17dbc8d2d6ac7882d0933e37_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937c34e04859d95b3aed57029c9c5791e5aa2e5a17dbc8d2d6ac7882d0933e37_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," An Agentic AI Framework for Training General Practitioner Student Skills"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Christopher Rom\xe1n Jaimes"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18462",children:"https://arxiv.org/pdf/2512.18462"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Research on a hybrid LSTM-CNN-Attention model for text-based web content classification"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mykola Kuz, Ihor Lazarovych, Mykola Kozlenko, Mykola Pikuliak, Andrii Kvasniuk"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18475",children:"https://arxiv.org/pdf/2512.18475"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d6cee54c1b292cf87f190d497421a3a323bc276532b71bc07f75f60e88c9a5d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Research on a hybrid LSTM-CNN-Attention model for text-based web content classification"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Teaching and Critiquing Conceptualization and Operationalization in NLP"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Vagrant Gautam"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18505",children:"https://arxiv.org/pdf/2512.18505"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124f7904bd6c9c1e7dda9c02619db7d1384dfe77987878e3270100575989ec37_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124f7904bd6c9c1e7dda9c02619db7d1384dfe77987878e3270100575989ec37_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Teaching and Critiquing Conceptualization and Operationalization in NLP"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," S Mahmudul Hasan, Shaily Roy, Akib Jawad Nafis"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18533",children:"https://arxiv.org/pdf/2512.18533"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ae98cacefd250f660449f7354216ce0a7f9d0395ed396c825595eada053e771_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sungjoon Park, Varun Ramamurthi, Owen Terry"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18551",children:"https://arxiv.org/pdf/2512.18551"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fa6d5c65b00f54388a5a5958e1d779544648a0e4bcc5878bdac10755ae186e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fa6d5c65b00f54388a5a5958e1d779544648a0e4bcc5878bdac10755ae186e1_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] LLMs on Drugs: Language Models Are Few-Shot Consumers"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Alexander Doudkin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18546",children:"https://arxiv.org/pdf/2512.18546"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fccef5b4c858e71f601c763675964730c550bdc2fa2972235024cc1e538c6bf7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fccef5b4c858e71f601c763675964730c550bdc2fa2972235024cc1e538c6bf7_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," LLMs on Drugs: Language Models Are Few-Shot Consumers"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Scott Thornton"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18542",children:"https://arxiv.org/pdf/2512.18542"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18552",children:"https://arxiv.org/pdf/2512.18552"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Toward Training Superintelligent Software Agents through Self-Play SWE-RL"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Amit Barman, Atanu Mandal, Sudip Kumar Naskar"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18593",children:"https://arxiv.org/pdf/2512.18593"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Prabigya Acharya, Liza Shrestha"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18608",children:"https://arxiv.org/pdf/2512.18608"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58ef9e00613b3a5bfbc5cce08d668a7661f6fb069cae338e88db1f35673ee946_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58ef9e00613b3a5bfbc5cce08d668a7661f6fb069cae338e88db1f35673ee946_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] On Finding Inconsistencies in Documents"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Charles J. Lovering, Seth Ebner, Brandon Smock, Michael Krumdick, Saad Rabbani, Ahmed Muhammad, Varshini Reddy, Chris Tanner"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18601",children:"https://arxiv.org/pdf/2512.18601"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c61e64bf42843ef58fd5d03121ce8b9fe2e96bcf5d34bd8dd2619416e3f08f59_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c61e64bf42843ef58fd5d03121ce8b9fe2e96bcf5d34bd8dd2619416e3f08f59_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," On Finding Inconsistencies in Documents"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng, Kaitong Cai, Jian Wang, Keze Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18623",children:"https://arxiv.org/pdf/2512.18623"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18622",children:"https://arxiv.org/pdf/2512.18622"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Pierre Colombo, Malik Boudiaf, Allyn Sweet, Michael Desa, Hongxi Wang, Kevin Candra, Sym\xe9on del Marmol"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18658",children:"https://arxiv.org/pdf/2512.18658"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99cb2c0d15c54813fefddf4670e9d0dc5b70cdc79ff9a13a7d5fb4f3a38f7143_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99cb2c0d15c54813fefddf4670e9d0dc5b70cdc79ff9a13a7d5fb4f3a38f7143_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] brat: Aligned Multi-View Embeddings for Brain MRI Analysis"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Maxime Kayser, Maksim Gridnev, Wanting Wang, Max Bain, Aneesh Rangnekar, Avijit Chatterjee, Aleksandr Petrov, Harini Veeraraghavan, Nathaniel C. Swinburne"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18679",children:"https://arxiv.org/pdf/2512.18679"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," brat: Aligned Multi-View Embeddings for Brain MRI Analysis"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yuchen Li, Handing Wang, Bing Xue, Mengjie Zhang, Yaochu Jin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18682",children:"https://arxiv.org/pdf/2512.18682"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d33a421c48ad59f7420161a73adc3cf5979c2e95cd4ded4b6c5ac3a603e0e95_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d33a421c48ad59f7420161a73adc3cf5979c2e95cd4ded4b6c5ac3a603e0e95_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Code2Doc: A Quality-First Curated Dataset for Code Documentation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Recep Kaan Karaman, Meftun Akarsu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18748",children:"https://arxiv.org/pdf/2512.18748"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b6da5096797c358f77d8022914985853333b12b54cf68425fe470f42a60638b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b6da5096797c358f77d8022914985853333b12b54cf68425fe470f42a60638b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Code2Doc: A Quality-First Curated Dataset for Code Documentation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] MemEvolve: Meta-Evolution of Agent Memory Systems"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, Shuicheng Yan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18746",children:"https://arxiv.org/pdf/2512.18746"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bcb0edebf98e4279185979654fe8f92c41ebcbd3b45786b67e305d7a65d04f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bcb0edebf98e4279185979654fe8f92c41ebcbd3b45786b67e305d7a65d04f6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MemEvolve: Meta-Evolution of Agent Memory Systems"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18745",children:"https://arxiv.org/pdf/2512.18745"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Thorsten Hellert, Nikolay Agladze, Alex Giovannone, Jan Jug, Frank Mayet, Mark Sherwin, Antonin Sulc, Chris Tennant"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18779",children:"https://arxiv.org/pdf/2512.18779"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4811b7a6317b8d33b2f92f5249b3e215e280fef25b4d76030d5495c78a7d02f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4811b7a6317b8d33b2f92f5249b3e215e280fef25b4d76030d5495c78a7d02f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sultan Alrashed, Francesco Orabona"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18834",children:"https://arxiv.org/pdf/2512.18834"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b37cd3dd620b849ae8ae05331dd8a70314996ab6497e0c72c29ceac5b60b6899_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b37cd3dd620b849ae8ae05331dd8a70314996ab6497e0c72c29ceac5b60b6899_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] From Word to World: Can Large Language Models be Implicit Text-based World Models?"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, Mengdi Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18832",children:"https://arxiv.org/pdf/2512.18832"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7359ba2b2edde0aa35db68272c398f7be194a19334cd0996c3e220c7df0b0c05_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7359ba2b2edde0aa35db68272c398f7be194a19334cd0996c3e220c7df0b0c05_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," From Word to World: Can Large Language Models be Implicit Text-based World Models?"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tung Duong Ta, Tim Oates"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18841",children:"https://arxiv.org/pdf/2512.18841"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/904d7953eb4dbc5c149d55e1575a3ed4dcd44d5ed3985c2f466d1edfe62296e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/904d7953eb4dbc5c149d55e1575a3ed4dcd44d5ed3985c2f466d1edfe62296e9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Application of deep learning approaches for medieval historical documents transcription"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18865",children:"https://arxiv.org/pdf/2512.18865"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Application of deep learning approaches for medieval historical documents transcription"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Toward Human-Centered AI-Assisted Terminology Work"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Antonio San Martin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18859",children:"https://arxiv.org/pdf/2512.18859"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9d1c947f7ae3816e7677add1e89a2192fc6c70c465f3c393e895ab239f5a20f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9d1c947f7ae3816e7677add1e89a2192fc6c70c465f3c393e895ab239f5a20f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Toward Human-Centered AI-Assisted Terminology Work"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18880",children:"https://arxiv.org/pdf/2512.18880"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Qiyu Wu, Toshiyuki Sekiya, Christof Monz"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18906",children:"https://arxiv.org/pdf/2512.18906"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fe48c6335a2271e131697f68ce5e1bd4c38bb02d11e569ba97f897ef4100cd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fe48c6335a2271e131697f68ce5e1bd4c38bb02d11e569ba97f897ef4100cd_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] FASTRIC: Prompt Specification Language for Verifiable LLM Interactions"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wen-Long Jin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18940",children:"https://arxiv.org/pdf/2512.18940"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5496d384aad293547e961ed7f2ce4568121f384e9e0b977144128668dc8445cb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5496d384aad293547e961ed7f2ce4568121f384e9e0b977144128668dc8445cb_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," FASTRIC: Prompt Specification Language for Verifiable LLM Interactions"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jinyan Liu, Zikang Chen, Qinchuan Wang, Tan Xie, Heming Zheng, Xudong Lv"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18999",children:"https://arxiv.org/pdf/2512.18999"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e96bd25bfb48e12bba787eb322582c438c99e6cb5614ad626a47b788b4598038_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e96bd25bfb48e12bba787eb322582c438c99e6cb5614ad626a47b788b4598038_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18987",children:"https://arxiv.org/pdf/2512.18987"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19011",children:"https://arxiv.org/pdf/2512.19011"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19004",children:"https://arxiv.org/pdf/2512.19004"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shijian Ma, Yunqi Huang, Yan Lin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19012",children:"https://arxiv.org/pdf/2512.19012"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61a55d81fc2fbbe12d82874badb4ccf6267053cf79d4294994aa54685dbd78fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61a55d81fc2fbbe12d82874badb4ccf6267053cf79d4294994aa54685dbd78fd_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ruiqi Ma, Yu Yan, Chunhong Zhang, Minghao Yin, XinChao Liu, Zhihong Jin, Zheng Hu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19070",children:"https://arxiv.org/pdf/2512.19070"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ziyan Zhang, Chao Wang, Zhuo Chen, Lei Chen, Chiyi Li, Kai Song"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19092",children:"https://arxiv.org/pdf/2512.19092"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4872220afe78cb4c58d8419cbef85a647f38abc69798722db414463aacc8757f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4872220afe78cb4c58d8419cbef85a647f38abc69798722db414463aacc8757f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mahir Labib Dihan, Sadif Ahmed, Md Nafiu Rahman"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19122",children:"https://arxiv.org/pdf/2512.19122"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf739862e73c646805e217bdf5e2cd5a0f6ec312b673bc4801a828112773cb1d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf739862e73c646805e217bdf5e2cd5a0f6ec312b673bc4801a828112773cb1d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Amar Lakel"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19117",children:"https://arxiv.org/pdf/2512.19117"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f46c06d65cdf6f99dbc4e1a6dd9ba52726ca28bfeec49c8ae2faf170ca9124e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f46c06d65cdf6f99dbc4e1a6dd9ba52726ca28bfeec49c8ae2faf170ca9124e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao, Guojun Yin, Wei Lin, Ran He"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19126",children:"https://arxiv.org/pdf/2512.19126"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea562f5265303d30f48412af8f0c2c84f8e98bc5e8f45118efe7f417df403e8d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea562f5265303d30f48412af8f0c2c84f8e98bc5e8f45118efe7f417df403e8d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] SAP: Syntactic Attention Pruning for Transformer-based Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19125",children:"https://arxiv.org/pdf/2512.19125"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3141b4a53facc55a384f90382f9ca7bbbdeafdff36d3027e35548bc8ba2ea87_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SAP: Syntactic Attention Pruning for Transformer-based Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Dehai Min, Kailin Zhang, Tongtong Wu, Lu Cheng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19134",children:"https://arxiv.org/pdf/2512.19134"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829ce6a74a55abeea6f6d10fec5338ad05441e95283687a525b0f2525bbf8a12_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/829ce6a74a55abeea6f6d10fec5338ad05441e95283687a525b0f2525bbf8a12_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Alessandro Lucca, Francesco Pierri"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19161",children:"https://arxiv.org/pdf/2512.19161"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53c07d75233bf3941b1b0c41209affef3033b0d43a91f61416cc01001baa79ef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53c07d75233bf3941b1b0c41209affef3033b0d43a91f61416cc01001baa79ef_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Bingyang Kelvin Liu, Ziyu Patrick Chen"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19171",children:"https://arxiv.org/pdf/2512.19171"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8bb7209d6fc574cd2b7517828b640ea0eaabfd4d5cb2bba4396c7e2c1fa0a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8bb7209d6fc574cd2b7517828b640ea0eaabfd4d5cb2bba4396c7e2c1fa0a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Dazhen Deng, Sen Yang, Yuchen He, Yuan Tian, Yingcai Wu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19173",children:"https://arxiv.org/pdf/2512.19173"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Anna-Maria Gueorguieva, Aylin Caliskan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19238",children:"https://arxiv.org/pdf/2512.19238"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mingxu Zhang, Dazhong Shen, Qi Zhang, Ying Sun"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19240",children:"https://arxiv.org/pdf/2512.19240"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeb3b37133d9071af2bfeeecd0da8171d70b3d3d9b3b0658553484d1919570f5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeb3b37133d9071af2bfeeecd0da8171d70b3d3d9b3b0658553484d1919570f5_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Do Minh Duc, Quan Xuan Truong, Nguyen Tat Dat, Nguyen Van Vinh"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19247",children:"https://arxiv.org/pdf/2512.19247"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3e85ac8e144e835ca46b7dcdc94a82085e98b53bc5c52cfa6addea751cf9af2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3e85ac8e144e835ca46b7dcdc94a82085e98b53bc5c52cfa6addea751cf9af2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Javier Vela-Tambo, Jorge Gracia, Fernando Dominguez-Castro"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19305",children:"https://arxiv.org/pdf/2512.19305"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e378c47c250804c75f74ae7dab654f342496632f097065fedb247e2353e13310_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e378c47c250804c75f74ae7dab654f342496632f097065fedb247e2353e13310_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] MAGIC: Achieving Superior Model Merging via Magnitude Calibration"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19320",children:"https://arxiv.org/pdf/2512.19320"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MAGIC: Achieving Superior Model Merging via Magnitude Calibration"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] HATS: High-Accuracy Triple-Set Watermarking for Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhiqing Hu, Chenxu Zhao, Jiazhong Lu, Xiaolei Liu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19378",children:"https://arxiv.org/pdf/2512.19378"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57616dcd6c2bbaa04b7fa3a0787e49d5a02a47c5a4535936d5c9f62643957b10_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57616dcd6c2bbaa04b7fa3a0787e49d5a02a47c5a4535936d5c9f62643957b10_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," HATS: High-Accuracy Triple-Set Watermarking for Large Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yacouba Diarra, Panga Azazia Kamate, Nouhoum Souleymane Coulibaly, Michael Leventhal"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19400",children:"https://arxiv.org/pdf/2512.19400"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b8ba0dabc6aa24a9c91f88fe9dbdb71fe95fe660fb7d092170d584e8ca01617_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b8ba0dabc6aa24a9c91f88fe9dbdb71fe95fe660fb7d092170d584e8ca01617_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jiaren Peng, Hongda Sun, Xuan Tian, Cheng Huang, Zeqing Li, Rui Yan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19414",children:"https://arxiv.org/pdf/2512.19414"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/553b6290892a9990d1a3b4b102abcd09ff7265499b12fe86a581ef08a388f0b6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/553b6290892a9990d1a3b4b102abcd09ff7265499b12fe86a581ef08a388f0b6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] CodeSimpleQA: Scaling Factuality in Code Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19424",children:"https://arxiv.org/pdf/2512.19424"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91e94588098b016a931e564501e220f391d8186f55b4c39b93da715a899afb11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91e94588098b016a931e564501e220f391d8186f55b4c39b93da715a899afb11_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," CodeSimpleQA: Scaling Factuality in Code Large Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, Zhidan Liu, Steven Hoi, Yue Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19432",children:"https://arxiv.org/pdf/2512.19432"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f9de7d24f3262309b81c73b75271d399f4875308f21feaf79b5ca283f85311c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f9de7d24f3262309b81c73b75271d399f4875308f21feaf79b5ca283f85311c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jinwei Chi, Ke Wang, Yu Chen, Xuanye Lin, Qiang Xu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19456",children:"https://arxiv.org/pdf/2512.19456"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44e2303feb430ac2c66a05d707fa59f0184a8efed477dd24968816daffaaf4a2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44e2303feb430ac2c66a05d707fa59f0184a8efed477dd24968816daffaaf4a2_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Epistemological Fault Lines Between Human and Artificial Intelligence"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Walter Quattrociocchi, Valerio Capraro, Matja\u017e Perc"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19466",children:"https://arxiv.org/pdf/2512.19466"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Epistemological Fault Lines Between Human and Artificial Intelligence"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Thittipat Pairatsuppawat, Abhibhu Tachaapornchai, Paweekorn Kusolsomboon, Chutikan Chaiwong, Thodsaporn Chay-intr, Kobkrit Viriyayudhakorn, Nongnuch Ketui, Aslan B. Wong"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19455",children:"https://arxiv.org/pdf/2512.19455"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa063ad12c4a8d092558907067bccfddb046268125d4e779376dab9a3d77d1e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa063ad12c4a8d092558907067bccfddb046268125d4e779376dab9a3d77d1e0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] A Large-Language-Model Framework for Automated Humanitarian Situation Reporting"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ivan Decostanzi, Yelena Mejova, Kyriaki Kalimeri"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19475",children:"https://arxiv.org/pdf/2512.19475"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a54fad5edc889b5ad9104209f3afb0330725100e661de009b823a7db7e07d9ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a54fad5edc889b5ad9104209f3afb0330725100e661de009b823a7db7e07d9ec_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Large-Language-Model Framework for Automated Humanitarian Situation Reporting"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Algerian Dialect"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zakaria Benmounah, Abdennour Boulesnane"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19543",children:"https://arxiv.org/pdf/2512.19543"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b20aa2b1c53242ea58da6106d340fde3ef7bc07b9c5b254164c44ae1890074a7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b20aa2b1c53242ea58da6106d340fde3ef7bc07b9c5b254164c44ae1890074a7_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Algerian Dialect"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Event Extraction in Large Language Model"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Bobo Li, Xudong Han, Jiang Liu, Yuzhe Ding, Liqiang Jing, Zhaoqi Zhang, Jinheng Li, Xinya Du, Fei Li, Meishan Zhang, Min Zhang, Aixin Sun, Philip S. Yu, Hao Fei"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19537",children:"https://arxiv.org/pdf/2512.19537"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd9b5f9c26e59b2ed6103844101a774216e9cbe68f45ba35eb88ca2b32f24ec6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd9b5f9c26e59b2ed6103844101a774216e9cbe68f45ba35eb88ca2b32f24ec6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Event Extraction in Large Language Model"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Angelo Ortiz Tandazo, Manel Khentout, Youssef Benchekroun, Thomas Hueber, Emmanuel Dupoux"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19612",children:"https://arxiv.org/pdf/2512.19612"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d640e4e40b2a6b04fca6e826cd560376d85f3c26277639aacff733108db2cc6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d640e4e40b2a6b04fca6e826cd560376d85f3c26277639aacff733108db2cc6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Increasing the Thinking Budget is Not All You Need"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ignacio Iacobacci, Zhaozhi Qian, Faroq AL-Tam, Muhammad AL-Qurishi, Riad Souissi"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19585",children:"https://arxiv.org/pdf/2512.19585"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1349874483030f6b12e0c38e99a95b5753e35155a3ffc245d9bfc23d426e906f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1349874483030f6b12e0c38e99a95b5753e35155a3ffc245d9bfc23d426e906f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Increasing the Thinking Budget is Not All You Need"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Exploring the features used for summary evaluation by Human and GPT"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zahra Sadeghi, Evangelos Milios, Frank Rudzicz"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19620",children:"https://arxiv.org/pdf/2512.19620"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e154f176f186b8dabd43d09d2a96579db8d3bbe3ccbf6debeb1b756642ffa2a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e154f176f186b8dabd43d09d2a96579db8d3bbe3ccbf6debeb1b756642ffa2a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Exploring the features used for summary evaluation by Human and GPT"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands M\u0101ori"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Rolando Coto-Solano, Daisy Li, Manoela Teleginski Ferraz, Olivia Sasse, Cha Krupka, Sharid Lo\xe1iciga, Sally Akevai Tenamu Nicholas"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19630",children:"https://arxiv.org/pdf/2512.19630"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2b93b8d306871e2d8f350baf0bfd0f0d6c6c700437457c6697698611b042a41_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2b93b8d306871e2d8f350baf0bfd0f0d6c6c700437457c6697698611b042a41_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands M\u0101ori"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Filippos Ventirozos, Peter Appleby, Matthew Shardlow"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19651",children:"https://arxiv.org/pdf/2512.19651"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48528a8f2854ae05c55d577072800564793ed86ef6169c7375b1685f01ca86_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48528a8f2854ae05c55d577072800564793ed86ef6169c7375b1685f01ca86_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19673",children:"https://arxiv.org/pdf/2512.19673"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19682",children:"https://arxiv.org/pdf/2512.19682"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f15c4d0f8b9a87e3d2373c2b35ef0faccb02043a099a932d6e9b7afa805adea_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f15c4d0f8b9a87e3d2373c2b35ef0faccb02043a099a932d6e9b7afa805adea_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ravi Prasad"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17968",children:"https://arxiv.org/pdf/2512.17968"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7665945961b2f7304f7c1df5cbe15f902828795ce6193b462edbd1a98af56880_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7665945961b2f7304f7c1df5cbe15f902828795ce6193b462edbd1a98af56880_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kohei Watanabe"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18119",children:"https://arxiv.org/pdf/2512.18119"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67542bc330fbf0441e0930413b5ebeee7ea4fb111df4bff5fba61c49280b9f8a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67542bc330fbf0441e0930413b5ebeee7ea4fb111df4bff5fba61c49280b9f8a_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18263",children:"https://arxiv.org/pdf/2512.18263"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251223] Merge on workspaces as Hopf algebra Markov chain"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Matilde Marcolli, David Skigin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18861",children:"https://arxiv.org/pdf/2512.18861"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0e865fcb929853efc360d95d314faf7e223840ff014d953cb55dcb92aadbc95_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0e865fcb929853efc360d95d314faf7e223840ff014d953cb55dcb92aadbc95_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Merge on workspaces as Hopf algebra Markov chain"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-24",children:"2025-12-24"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shashi Kant Gupta, Arijeet Pramanik, Jerrin John Thomas, Regina Schwind, Lauren Wiener, Avi Raju, Jeremy Kornbluth, Yanshan Wang, Zhaohui Su, Hrituraj Singh"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19864",children:"https://arxiv.org/pdf/2512.19864"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0cfa9ec043e8a27718dd14bb89bf3c4ceafb97fb48a8a0b61d661ec9d34b09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0cfa9ec043e8a27718dd14bb89bf3c4ceafb97fb48a8a0b61d661ec9d34b09_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kirk Vanacore, Rene F. Kizilcec"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19903",children:"https://arxiv.org/pdf/2512.19903"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcd1ac8680cb9e68b32ea021e524b8dddd82c5081672c081d9ac46ec2f2c6180_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcd1ac8680cb9e68b32ea021e524b8dddd82c5081672c081d9ac46ec2f2c6180_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Counterfactual LLM-based Framework for Measuring Rhetorical Style"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jingyi Qiu, Hong Chen, Zongyi Li"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19908",children:"https://arxiv.org/pdf/2512.19908"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6db39310497bbc29a6efe3c72529c4c231f4c45f6ccb6856571045197a2f074d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6db39310497bbc29a6efe3c72529c4c231f4c45f6ccb6856571045197a2f074d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Counterfactual LLM-based Framework for Measuring Rhetorical Style"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhixiang Lu, Xueyuan Deng, Yiran Liu, Yulong Li, Qiang Yan, Imran Razzak, Jionglong Su"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19933",children:"https://arxiv.org/pdf/2512.19933"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acfc234a0a131551ac80194bbad4cea0eac21fb397065d4acf70449da5327f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acfc234a0a131551ac80194bbad4cea0eac21fb397065d4acf70449da5327f6_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Heet Bodara, Md Masum Mushfiq, Isma Farah Siddiqui"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19950",children:"https://arxiv.org/pdf/2512.19950"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25282dd436ce0f7a5fabd0438ec9d8be57567585d626e71c9a9af6d0ac8451b9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25282dd436ce0f7a5fabd0438ec9d8be57567585d626e71c9a9af6d0ac8451b9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Schoenfeld's Anatomy of Mathematical Reasoning by Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19995",children:"https://arxiv.org/pdf/2512.19995"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Schoenfeld's Anatomy of Mathematical Reasoning by Language Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Reason2Decide: Rationale-Driven Multi-Task Learning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20074",children:"https://arxiv.org/pdf/2512.20074"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/187b806c979650defdb64bdb9ce297598ef473654b93625ec2257af228dbd0da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/187b806c979650defdb64bdb9ce297598ef473654b93625ec2257af228dbd0da_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Reason2Decide: Rationale-Driven Multi-Task Learning"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20092",children:"https://arxiv.org/pdf/2512.20092"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f21f9409d7ddcd534ada400fa2d7b093de0f8e8672067e512129d84ead74883_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f21f9409d7ddcd534ada400fa2d7b093de0f8e8672067e512129d84ead74883_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] A Novel Graph-Sequence Learning Model for Inductive Text Classification"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zuo Wang, Ye Yuan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20097",children:"https://arxiv.org/pdf/2512.20097"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d452912781840b7b95df9d1137178809626a19d0d789bf4b893df1553a7c677d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d452912781840b7b95df9d1137178809626a19d0d789bf4b893df1553a7c677d_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," A Novel Graph-Sequence Learning Model for Inductive Text Classification"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20111",children:"https://arxiv.org/pdf/2512.20111"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Multi-hop Reasoning via Early Knowledge Alignment"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yuxin Wang, Shicheng Fang, Bo Wang, Qi Luo, Xuanjing Huang, Yining Zheng, Xipeng Qiu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20144",children:"https://arxiv.org/pdf/2512.20144"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d88b182b244d4a4ceb1d9822c08a4a4d748f40278f40b510b9dc474ab390f2c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d88b182b244d4a4ceb1d9822c08a4a4d748f40278f40b510b9dc474ab390f2c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Multi-hop Reasoning via Early Knowledge Alignment"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Retrieval-augmented Prompt Learning for Pre-trained Foundation Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20145",children:"https://arxiv.org/pdf/2512.20145"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Retrieval-augmented Prompt Learning for Pre-trained Foundation Models"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsxs)(e.strong,{children:["[arXiv251224] M",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsx)(e.mrow,{children:(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mn,{children:"3"})]})}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^3"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mord mtight",children:"3"})})]})})})})})]})]})})]}),"KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation"]})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20136",children:"https://arxiv.org/pdf/2512.20136"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c211a1c8d3a17c264fe9655b35305f3ece6ef329a1cb2344fb1a18976f11017_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c211a1c8d3a17c264fe9655b35305f3ece6ef329a1cb2344fb1a18976f11017_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," M",(0,a.jsxs)(e.span,{className:"katex",children:[(0,a.jsx)(e.span,{className:"katex-mathml",children:(0,a.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,a.jsxs)(e.semantics,{children:[(0,a.jsx)(e.mrow,{children:(0,a.jsxs)(e.msup,{children:[(0,a.jsx)(e.mrow,{}),(0,a.jsx)(e.mn,{children:"3"})]})}),(0,a.jsx)(e.annotation,{encoding:"application/x-tex",children:"^3"})]})})}),(0,a.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,a.jsxs)(e.span,{className:"base",children:[(0,a.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,a.jsxs)(e.span,{className:"mord",children:[(0,a.jsx)(e.span,{}),(0,a.jsx)(e.span,{className:"msupsub",children:(0,a.jsx)(e.span,{className:"vlist-t",children:(0,a.jsx)(e.span,{className:"vlist-r",children:(0,a.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,a.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,a.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,a.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,a.jsx)(e.span,{className:"mord mtight",children:"3"})})]})})})})})]})]})})]}),"KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Fun-Audio-Chat Technical Report"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20156",children:"https://arxiv.org/pdf/2512.20156"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Fun-Audio-Chat Technical Report"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Honglin Mu, Jinghao Liu, Kaiyang Wan, Rui Xing, Xiuying Chen, Timothy Baldwin, Wanxiang Che"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20164",children:"https://arxiv.org/pdf/2512.20164"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be09765889dd9516b45fd948d07346b8b6487f6dc02927a597c1cab7861bfb7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be09765889dd9516b45fd948d07346b8b6487f6dc02927a597c1cab7861bfb7_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Learning to Reason in LLMs by Expectation Maximization"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Junghyun Lee, Branislav Kveton, Sunav Choudhary, Subhojyoti Mukherjee, Anup Rao, Ryan A. Rossi, Alexa Siu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20169",children:"https://arxiv.org/pdf/2512.20169"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fc3f6af733f26b26d15d5332cff89ae665d865f5359eb651dbf107c200c4794_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Learning to Reason in LLMs by Expectation Maximization"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20174",children:"https://arxiv.org/pdf/2512.20174"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] FaithLens: Detecting and Explaining Faithfulness Hallucination"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20182",children:"https://arxiv.org/pdf/2512.20182"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," FaithLens: Detecting and Explaining Faithfulness Hallucination"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Marko \u010cechovi\u010d, Nat\xe1lia Komorn\xedkov\xe1, Dominik Mach\xe1\u010dek, Ond\u0159ej Bojar"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20204",children:"https://arxiv.org/pdf/2512.20204"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d50c0d1f767aca0e832de0ef650261ca473b1bbf6f1ac37ad18132605e13bc77_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d50c0d1f767aca0e832de0ef650261ca473b1bbf6f1ac37ad18132605e13bc77_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] AprielGuard"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jaykumar Kasundra, Anjaneya Praharaj, Sourabh Surana, Lakshmi Sirisha Chodisetty, Sourav Sharma, Abhigya Verma, Abhishek Bhardwaj, Debasish Kanhar, Aakash Bhagat, Khalil Slimi, Seganrasan Subramanian, Sathwik Tejaswi Madhusudhan, Ranga Prasad Chenna, Srinivas Sunkara"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20293",children:"https://arxiv.org/pdf/2512.20293"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df72be31938b1703d3c0991c388a31ae15e5ab7e811b6868c1d2b87ce212408_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df72be31938b1703d3c0991c388a31ae15e5ab7e811b6868c1d2b87ce212408_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," AprielGuard"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Karolina Dro\u017cd\u017c, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20298",children:"https://arxiv.org/pdf/2512.20298"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20292",children:"https://arxiv.org/pdf/2512.20292"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20308",children:"https://arxiv.org/pdf/2512.20308"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6351d1c5218be825df7f7572c20ff77011eb6baf9648cff6ea5fd370a23fda1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6351d1c5218be825df7f7572c20ff77011eb6baf9648cff6ea5fd370a23fda1_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20324",children:"https://arxiv.org/pdf/2512.20324"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76de34d24836232b3e2c53b464d6001663ee4428a6ebe0f1d009c4d37f298fc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76de34d24836232b3e2c53b464d6001663ee4428a6ebe0f1d009c4d37f298fc_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Nilesh Jain, Seyi Adeyinka, Leor Roseman, Aza Allsop"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20352",children:"https://arxiv.org/pdf/2512.20352"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2047f0800a6a91a372fd66013fa3526084396a7513879598fb459814e55b18c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2047f0800a6a91a372fd66013fa3526084396a7513879598fb459814e55b18c_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20387",children:"https://arxiv.org/pdf/2512.20387"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Junyi Liu, Stanley Kok"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20404",children:"https://arxiv.org/pdf/2512.20404"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cbe3d2a9e0dbffa2ba1d89f2d2d61bcc3900bbeb02faaa7db06ace8b2d8e09b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cbe3d2a9e0dbffa2ba1d89f2d2d61bcc3900bbeb02faaa7db06ace8b2d8e09b_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Step-DeepResearch Technical Report"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Chen Hu, Haikuo Du, Heng Wang, Lin Lin, Mingrui Chen, Peng Liu, Ruihang Miao, Tianchi Yue, Wang You, Wei Ji, Wei Yuan, Wenjin Deng, Xiaojian Yuan, Xiaoyun Zhang, Xiangyu Liu, Xikai Liu, Yanming Xu, Yicheng Cao, Yifei Zhang, Yongyao Wang, Yubo Shu, Yurong Zhang, Yuxiang Zhang, Zheng Gong, Zhichao Chang, Binyan Li, Dan Ma, Furong Jia, Hongyuan Wang, Jiayu Liu, Jing Bai, Junlan Liu, Manjiao Liu, Na Wang, Qiuping Wu, Qinxin Du, Shiwei Li, Wen Sun, Yifeng Gong, Yonglin Chen, Yuling Zhao, Yuxuan Lin, Ziqi Ren, Zixuan Wang, Aihu Zhang, Brian Li, Buyun Ma, Kang An, Li Xie, Mingliang Li, Pan Li, Shidong Yang, Xi Chen, Xiaojia Liu, Yuchu Luo, Yuan Song, YuanHao Ding, Yuanwei Liang, Zexi Li, Zhaoning Zhang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20491",children:"https://arxiv.org/pdf/2512.20491"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e31009744d096e0b219624a8cb385cd29ef47c699069fff12401b051dd695f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e31009744d096e0b219624a8cb385cd29ef47c699069fff12401b051dd695f_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Step-DeepResearch Technical Report"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Distilling to Hybrid Attention Models via KL-Guided Layer Selection"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda, Jiawei Zhou, Yoon Kim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20569",children:"https://arxiv.org/pdf/2512.20569"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36c08fad9c560eeacd24d61bbc8fc4ace2f57a4dda4d1eaeb59a63b10f01d2e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36c08fad9c560eeacd24d61bbc8fc4ace2f57a4dda4d1eaeb59a63b10f01d2e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Distilling to Hybrid Attention Models via KL-Guided Layer Selection"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Amirhosein Ghasemabadi, Di Niu"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20578",children:"https://arxiv.org/pdf/2512.20578"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0ae4be0acd6ad133b6649afc37037f398f8e5753e19ae55612dfc2522618af9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0ae4be0acd6ad133b6649afc37037f398f8e5753e19ae55612dfc2522618af9_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20586",children:"https://arxiv.org/pdf/2512.20586"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Dhruv Anand, Ehsan Shareghi"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20595",children:"https://arxiv.org/pdf/2512.20595"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Making Large Language Models Efficient Dense Retrievers"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yibin Lei, Shwai He, Ang Li, Andrew Yates"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20612",children:"https://arxiv.org/pdf/2512.20612"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40238eef858f9a1a1327758d04b0c4c31e71fbbf6df6898a51ccf0f7ff9a8f36_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40238eef858f9a1a1327758d04b0c4c31e71fbbf6df6898a51ccf0f7ff9a8f36_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Making Large Language Models Efficient Dense Retrievers"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Alexandros Christoforos, Chadbourne Davis"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20604",children:"https://arxiv.org/pdf/2512.20604"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/740b7502b49c1387c998a9fd8b95bff7878c9c8ecb80d21efccf1c64db303459_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/740b7502b49c1387c998a9fd8b95bff7878c9c8ecb80d21efccf1c64db303459_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251224] Coherence in the brain unfolds across separable temporal regimes"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Davide Stauba, Finn Rabe, Akhil Misra, Yves Pauli, Roya H\xfcppi, Nils Lang, Lars Michels, Victoria Edkins, Sascha Fr\xfchholz, Iris Sommer, Wolfram Hinzen, Philipp Homan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TBD"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20481",children:"https://arxiv.org/pdf/2512.20481"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ad4bde9d11d688dda33f0f878fe578450d6097fcbcbf36ba57e75d54765a25e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ad4bde9d11d688dda33f0f878fe578450d6097fcbcbf36ba57e75d54765a25e_w640_q70.webp"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," Coherence in the brain unfolds across separable temporal regimes"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"2025-12-25",children:"2025-12-25"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [multimodal knowledge graph, cross-modal reasoning, visual document understanding, retrieval-augmented generation, entity-centric structure]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20626",children:"https://arxiv.org/pdf/2512.20626"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[MegaRAG: \u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u589e\u5f3a\u751f\u6210 / MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709RAG\u65b9\u6cd5\u5728\u957f\u6587\u6863\u3001\u591a\u6a21\u6001\u5185\u5bb9\u4e0a\u7406\u89e3\u4e0d\u8db3 / Existing RAG struggles with long-form, multimodal document understanding]\n    C --\x3e C1[\u6784\u5efa\u878d\u5408\u89c6\u89c9\u7ebf\u7d22\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31 / Construct multimodal KG incorporating visual cues]\n    C --\x3e C2[\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\u4e2d\u5229\u7528\u56fe\u8c31 / Utilize KG in multimodal retrieval & generation]\n    D --\x3e D1[\u5728\u5168\u5c40\u4e0e\u7ec6\u7c92\u5ea6QA\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5 / Outperforms existing methods on global & fine-grained QA]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [sentiment analysis], [temporal drift, zero-training detection, transformer models, social media streams, model instability]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Aayam Bansal, Ishaan Gangwani"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," IEEE"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20631",children:"https://arxiv.org/pdf/2512.20631"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"}),"  1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u6a21\u578b\u5728\u52a8\u6001\u4e8b\u4ef6\u671f\u95f4\u7684\u884c\u4e3a\u4e0d\u7a33\u5b9a/Transformer model instability during dynamic events]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u96f6\u8bad\u7ec3\u68c0\u6d4b\u6846\u67b6\u4e0e\u56db\u4e2a\u65b0\u6307\u6807/Zero-training detection framework with four novel metrics]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u8fbe23.4%\uff0c\u68c0\u6d4b\u80fd\u529b\u5f3a/Validated on authentic data, 23.4% accuracy drop, strong detection capability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [catastrophic forgetting, spurious forgetting, shallow alignment, deep alignment, task alignment depth]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Weiwei Wang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Shenzhen Sunline Tech Co., Ltd."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20634",children:"https://arxiv.org/pdf/2512.20634"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to "spurious forgetting" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>\u865a\u5047\u9057\u5fd8\u7684\u5b9e\u65f6\u68c0\u6d4b\u4e0e\u5b9a\u91cf\u5206\u6790] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Catastrophic forgetting from shallow task alignment<br/>\u7531\u6d45\u5c42\u4efb\u52a1\u5bf9\u9f50\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Quantitative metrics & real-time detection for alignment depth<br/>\u5bf9\u9f50\u6df1\u5ea6\u7684\u91cf\u5316\u6307\u6807\u4e0e\u5b9e\u65f6\u68c0\u6d4b]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High identification accuracy & improved robustness<br/>\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\u4e0e\u63d0\u5347\u7684\u9c81\u68d2\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Uncovering Competency Gaps in Large Language Models and Their Benchmarks"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [llm evaluation], [sparse autoencoders, benchmark gaps, model gaps, concept activations, competency gaps]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Stanford University, Google DeepMind"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20638",children:"https://arxiv.org/pdf/2512.20638"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," competency-gaps.github.io"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Uncovering Competency Gaps<br/>\u63ed\u793a\u80fd\u529b\u5dee\u8ddd] --\x3e B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>\u95ee\u9898\uff1a\u805a\u5408\u6307\u6807\u63a9\u76d6\u6a21\u578b/\u57fa\u51c6\u5dee\u8ddd]\n    A --\x3e C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>\u65b9\u6cd5\uff1a\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u6982\u5ff5\u7ea7\u5206\u89e3]\n    A --\x3e D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>\u7ed3\u679c\uff1a\u53d1\u73b0\u975e\u8c04\u5a9a\u3001\u5b89\u5168\u65b9\u9762\u7684\u5dee\u8ddd\uff1b\u57fa\u51c6\u8fc7\u5ea6\u4ee3\u8868\u670d\u4ece\u6027]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [sec], [llm security assessment], [automated red-teaming, adversarial prompts, meta-prompting, vulnerability detection, alignment robustness]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhang Wei, Peilu Hu, Shengning Lang, Hao Yan, Li Mei, Yichao Zhang, Chen Yang, Junfeng Hao, Zhimo Han"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Stevens Institute of Technology, The University of Texas at Dallas, AI Safety Research Lab (Institute of Advanced Computing, Shenzhen), Zheng Zhou University of Light Industry, Affiliated Hospital of Guangdong Medical University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20677",children:"https://arxiv.org/pdf/2512.20677"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces an automated framework for systematic generation, execution, and evaluation of adversarial prompts against LLMs. 2. Integrates meta-prompting-based attack synthesis and multi-modal detection across six major threat categories (e.g., reward hacking, deceptive alignment). 3. Demonstrates significant improvement in vulnerability discovery rate (3.9x over manual testing) with high detection accuracy (89%) on a target model."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35d6a91284e231b153dfe35185eb27aafd1549063ba05304240c7a14c0b8f78d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35d6a91284e231b153dfe35185eb27aafd1549063ba05304240c7a14c0b8f78d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an automated red-teaming framework to systematically find security vulnerabilities in large language models. The framework uses meta-prompting to generate attacks and multi-modal detection to evaluate them across six threat categories. Experiments show it discovers vulnerabilities much faster than manual testing while maintaining high accuracy, enabling scalable AI safety evaluations."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Automated Red-Teaming Framework for LLM Security] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Manual red-teaming is not scalable for comprehensive LLM security assessment]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Automated framework with meta-prompting attack synthesis & multi-modal vulnerability detection]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 3.9x faster vulnerability discovery, 89% detection accuracy, 47 vulnerabilities found]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20687",children:"https://arxiv.org/pdf/2512.20687"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[PHOTON: Hierarchical Autoregressive Modeling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Transformer\u6c34\u5e73\u626b\u63cf\u5bfc\u81f4KV\u7f13\u5b58\u8bfb\u5199\u6210\u4e3a\u5185\u5b58\u74f6\u9888/Horizontal scanning causes memory-bound KV-cache bottleneck]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7528\u5782\u76f4\u591a\u5206\u8fa8\u7387\u5c42\u6b21\u6a21\u578b\u66ff\u4ee3/Replace with vertical multi-resolution hierarchical model]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5185\u5b58\u6548\u7387\u4e0e\u541e\u5410\u91cf\u5927\u5e45\u63d0\u5347/Significant improvement in memory efficiency & throughput]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion models], [sparse attention, diffusion models, long-text generation, soft absorbing state, computational complexity]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Alexandros Christoforos, Chadbourne Davis"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Suffolk University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20724",children:"https://arxiv.org/pdf/2512.20724"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces SA-DiffuSeq, a diffusion framework that integrates sparse attention to improve scalability for long-document modeling. 2. Proposes a novel soft absorbing state tailored to sparse attention dynamics to stabilize diffusion trajectories and accelerate sequence reconstruction. 3. Demonstrates superior training efficiency and sampling speed compared to state-of-the-art diffusion baselines, especially on extended sequences."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost of diffusion models for long-text generation by proposing SA-DiffuSeq, which integrates sparse attention and a novel soft absorbing state. This method reduces complexity while maintaining generation quality, making it suitable for applications like scientific writing and code generation. The results show that incorporating structured sparsity is a promising direction for efficient long-text generation."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[SA-DiffuSeq] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>Computational Cost & Scalability];\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Sparse Attention & Soft Absorbing State];\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Improved Efficiency & Quality];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [agent system], [tool-augmented agent, agentic reinforcement learning, supervised fine-tuning (SFT), request-level asynchronous rollout, prefix-aware load balancing]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Tencent Hunyuan"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20745",children:"https://arxiv.org/pdf/2512.20745"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[AgentMath] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LRMs are inefficient and inaccurate for complex math]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [tokenization], [tokenizer, language models, benchmark, subword segmentation, BPE]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," G\xfcl Sena Alt\u0131nta\u015f, Malikeh Ehghaghi, Brian Lester, Fengyuan Liu, Wanru Zhao, Marco Ciccone, Colin Raffel"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Toronto, Vector Institute, Google DeepMind, McGill University, Mila - Quebec AI Institute, University of Cambridge, Hugging Face"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20757",children:"https://arxiv.org/pdf/2512.20757"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/r-three/Tokenizers",children:"https://github.com/r-three/Tokenizers"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces TokSuite, a collection of fourteen language models that are identical except for their tokenizers, enabling isolated study of tokenizer impact. 2. Curates and releases a new benchmark designed to measure model performance under real-world text perturbations that affect tokenization. 3. Provides a robust framework that supports novel findings on the benefits and shortcomings of various popular tokenizers."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48b6cd3c25dd384b02a8b1601f98df302b0d84a5c6e3b043841ea275f5ffdcbd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of isolating the impact of tokenizer choice on language model behavior. It proposes TokSuite, a suite of models with different tokenizers but identical other components, along with a specialized benchmark. The work enables systematic analysis and reveals new insights into how different tokenizers affect model performance."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[TokSuite: Measuring Tokenizer Impact] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Tokenization's role in LM performance is poorly understood]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: TokSuite - Identical models with different tokenizers + new benchmark]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Novel findings on tokenizer benefits and shortcomings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [reinforcement learning], [RLVR, causal reasoning, generalization, supervised fine-tuning, large language models]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20760",children:"https://arxiv.org/pdf/2512.20760"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:'graph LR\nA["Generalization of RLVR Using Causal Reasoning as a Testbed<br>\u4ee5\u56e0\u679c\u63a8\u7406\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u7684RLVR\u6cdb\u5316\u7814\u7a76"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem<br>RLVR\u4f55\u65f6\u80fd\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\uff1f<br>When does RLVR yield robust generalization?"]\nA --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5728\u56e0\u679c\u56fe\u6a21\u578b\u4e0a\u5b9e\u8bc1\u7814\u7a76RLVR\u4e0eSFT<br>Empirical study of RLVR vs SFT on causal graphical models"]\nA --\x3e D["\u5173\u952e\u7ed3\u679c/Results<br>RLVR\u6cdb\u5316\u66f4\u5f3a\uff0c\u4f46\u4f9d\u8d56\u6a21\u578b\u89c4\u6a21\u4e0e\u521d\u59cb\u80fd\u529b<br>RLVR yields stronger generalization but depends on model size & initial competence"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [dialogue systems], [adversarial training, user simulation, task-oriented dialogue, mental health chatbots, direct preference optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ziyi Zhu, Olivier Tieleman, Caitlin A. Stamatis, Luka Smyth, Thomas D. Hull, Daniel R. Cahn, Matteo Malgaroli"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Slingshot AI, NYU School of Medicine"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20773",children:"https://arxiv.org/pdf/2512.20773"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes an adversarial training framework for improving user simulator realism in task-oriented dialogue systems. 2. Applies the framework to mental health support chatbots, demonstrating enhanced ability to surface system failure modes. 3. Shows that the fine-tuned and adversarially trained simulator achieves strong correlation between simulated and real failure rates while maintaining low distributional divergence."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f72756c707df5d24dd551e3e95152618e1b00d48b0aae580dd4f6ebbf8dc335_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f72756c707df5d24dd551e3e95152618e1b00d48b0aae580dd4f6ebbf8dc335_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of creating realistic user simulators for evaluating task-oriented dialogue systems. It proposes an adversarial training framework where a user simulator (generator) is refined against a discriminator to improve realism, specifically applied to mental health chatbots. The results show this approach creates simulators that effectively expose system failures, enabling more reliable and cost-effective evaluation before deployment."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Adversarial Training for Failure-Sensitive User Simulation<br>\u5bf9\u6297\u6027\u8bad\u7ec3\u7528\u4e8e\u6545\u969c\u654f\u611f\u7684\u7528\u6237\u6a21\u62df] --\x3e B(Problem: Realistic user simulation is challenging<br>\u6838\u5fc3\u95ee\u9898\uff1a\u771f\u5b9e\u7684\u7528\u6237\u6a21\u62df\u5177\u6709\u6311\u6218\u6027)\n    A --\x3e C(Method: Adversarial training framework<br>\u4e3b\u8981\u65b9\u6cd5\uff1a\u5bf9\u6297\u6027\u8bad\u7ec3\u6846\u67b6)\n    A --\x3e D(Results: Enhanced failure exposure & realism<br>\u5173\u952e\u7ed3\u679c\uff1a\u589e\u5f3a\u7684\u6545\u969c\u66b4\u9732\u4e0e\u771f\u5b9e\u6027)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [educational technology / intelligent tutoring systems], [large language models, pedagogical quality, instructional strategies, linguistic analysis, math tutoring]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ramatu Oiza Abdulsalam, Segun Aroyehun"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," African University of Science and Technology, University of Konstanz"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20780",children:"https://arxiv.org/pdf/2512.20780"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Conducted a controlled, turn-level comparison of tutoring responses between expert human tutors, novice human tutors, and multiple large language models (LLMs) in math remediation. 2. Identified systematic differences in instructional and linguistic profiles, finding that LLMs underuse restating/revoicing strategies but produce longer, more lexically diverse, and more polite responses compared to human tutors. 3. Established statistical associations between specific instructional/linguistic features (e.g., restating, lexical diversity) and perceived pedagogical quality, showing LLMs can achieve comparable quality using different strategies."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4c157f4475efaa64bb039e61eccf65a1facd8888dde9576734865854a42e878_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4c157f4475efaa64bb039e61eccf65a1facd8888dde9576734865854a42e878_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates how closely the instructional behavior of large language models (LLMs) aligns with expert human tutors in math tutoring. By comparing responses from experts, novices, and LLMs to the same conversation turns, the study analyzes instructional strategies and linguistic features. It finds that LLMs approach expert-level pedagogical quality on average but rely on systematically different strategies, such as underusing restating/revoicing while being more verbose and polite."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLM\u6559\u5b66\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u4e00\u81f4\u6027/Alignment of LLM instructional behavior with expert human tutors)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u63a7\u5236\u6027\u5bf9\u8bdd\u8f6e\u6bd4\u8f83/Controlled turn-level comparison of expert, novice, and LLM responses)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: LLM\u63a5\u8fd1\u4e13\u5bb6\u6559\u5b66\u6c34\u5e73\u4f46\u7b56\u7565\u4e0d\u540c/LLMs approach expert quality but use different instructional & linguistic strategies)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Investigating Model Editing for Unlearning in Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [machine unlearning], [model editing, ROME, IKE, WISE, TOFU benchmark]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shariqah Hossain, Lalana Kagal"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Massachusetts Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20794",children:"https://arxiv.org/pdf/2512.20794"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"}),' 1. Investigated the application of existing model editing algorithms (ROME, IKE, WISE) to the problem of machine unlearning in LLMs. 2. Designed new editing targets, including a novel "Avoidant" target, specifically formulated for the goal of information removal rather than alteration. 3. Demonstrated that model editing approaches can surpass traditional unlearning baselines in the quality of forgetting, while also highlighting the persistent trade-off between effective unlearning and preserving general model performance.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/841879fe7d15bfc601ea216e8680f83f41000cc4f51b756525680ffd81869551_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/841879fe7d15bfc601ea216e8680f83f41000cc4f51b756525680ffd81869551_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper investigates using model editing techniques for machine unlearning in Large Language Models. It applies editing algorithms like ROME, IKE, and WISE with new "unlearning" targets and evaluates them on the TOFU benchmark. The results show that model editing can outperform baseline unlearning methods in some settings but still struggles to fully remove information without harming overall model performance.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Investigating Model Editing for Unlearning in LLMs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Inefficient or damaging unlearning in LLMs)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Apply model editing algorithms ROME/IKE/WISE with new unlearning targets)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Can exceed baselines but trade-off with performance remains)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [bias mitigation & interpretability], [sparse autoencoder, feature ablation, mechanistic interpretability, demographic bias, causal influence]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhengyang Shan, Aaron Mueller"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Boston University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20796",children:"https://arxiv.org/pdf/2512.20796"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces a multi-task evaluation framework to measure the independence of demographic bias mechanisms from general demographic recognition in language models. 2. Compares attribution-based and correlation-based methods for locating bias features and demonstrates their differential effectiveness across bias dimensions (race/gender vs. education). 3. Shows that targeted sparse autoencoder feature ablations can enable surgical debiasing, reducing stereotypes without erasing legitimate demographic detection, indicating bias arises from task-specific mechanisms."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fb5fdb2f139d53a61fbac99afbf6f9d9a8061df4ee47ccc35f3e788fe619760_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fb5fdb2f139d53a61fbac99afbf6f9d9a8061df4ee47ccc35f3e788fe619760_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates whether demographic bias in language models can be removed without harming the model's ability to recognize demographics. The authors use a multi-task setup and compare attribution-based and correlation-based methods to locate bias features in a sparse autoencoder, performing targeted ablations on the Gemma-2-9B model. They find that such mechanistic interventions can reduce specific stereotypes (e.g., race/gender in professions) while preserving name recognition, but the effectiveness depends on the bias dimension, highlighting the need for task-specific debiasing strategies."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Measuring Mechanistic Independence<br/>\u673a\u5236\u72ec\u7acb\u6027\u6d4b\u91cf] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Can bias be removed without erasing demographics?<br/>\u80fd\u5426\u53bb\u9664\u504f\u89c1\u800c\u4e0d\u6d88\u9664\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u8bc6\u522b\u80fd\u529b\uff1f)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Multi-task evaluation & Sparse Autoencoder feature ablation<br/>\u591a\u4efb\u52a1\u8bc4\u4f30\u4e0e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u6d88\u878d)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Attribution ablation works for race/gender; Correlation ablation works for education<br/>\u5f52\u56e0\u6d88\u878d\u5bf9\u79cd\u65cf/\u6027\u522b\u6709\u6548\uff1b\u76f8\u5173\u6d88\u878d\u5bf9\u6559\u80b2\u504f\u89c1\u6709\u6548)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Semantic Deception: When Reasoning Models Can't Compute an Addition"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [large language model evaluation], [semantic deception, symbolic reasoning, abstraction, chain-of-thought, evaluation framework]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Nathani\xebl de Leeuw, Marceau Nahon, Mathis Reymond, Raja Chatila, Mehdi Khamassi"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Institute of Intelligent Systems and Robotics (CNRS, Sorbonne University), Paris Cit\xe9 University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20812",children:"https://arxiv.org/pdf/2512.20812"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces the concept of \"semantic deceptions\" as a novel experimental framework to test LLMs' symbolic abstraction capabilities. 2. Demonstrates that misleading semantic cues can significantly degrade the performance of reasoning models on simple arithmetic tasks. 3. Reveals a critical limitation in LLMs' ability to perform robust symbolic manipulation, highlighting their over-reliance on surface-level semantics and statistical correlations."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51eafa5ff63a45ff90df02eb9c82e60fc34432de99dc2dca39fb261430db4031_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51eafa5ff63a45ff90df02eb9c82e60fc34432de99dc2dca39fb261430db4031_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper investigates the symbolic reasoning capabilities of large language models by introducing "semantic deceptions," where digits and operators are replaced with novel symbols carrying misleading associations. Experiments show that these semantic cues severely impair model performance on simple calculations, revealing a fundamental weakness in abstraction and a tendency to rely on learned correlations rather than true symbolic logic.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Semantic Deception: When Reasoning Models Can\u2019t Compute an Addition] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs' symbolic reasoning and abstraction capabilities under misleading semantic cues]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Introduce semantic deceptions by redefining digits/operators with novel symbols]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Semantic cues deteriorate performance, revealing over-reliance on surface semantics and limitations in symbolic manipulation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [automated essay scoring], [Concept Bottleneck Models, Explainable AI, Human-in-the-loop, Rubric-Aligned, Interpretability]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kumar Satvik Chaudhary, Chengshuai Zhao, Fan Zhang, Yung Hin Tse, Garima Agrawal, Yuli Deng, Huan Liu"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20817",children:"https://arxiv.org/pdf/2512.20817"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/scott-f-zhang/CBM-Demo",children:"https://github.com/scott-f-zhang/CBM-Demo"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes EssayCBM, a rubric-aligned framework that uses dedicated prediction heads to evaluate specific writing concepts (e.g., Thesis Clarity, Evidence Use) instead of predicting grades directly from text. 2. Introduces a transparent bottleneck where a lightweight network computes the final grade using only the concept scores, enabling interpretability and human-in-the-loop adjustments. 3. Demonstrates that the system matches black-box performance while providing actionable, concept-level feedback through an intuitive web interface."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b05be6579e2ddbef36e15a910aaffe552adbb13cce68a7840f6e46806eb13aa0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b05be6579e2ddbef36e15a910aaffe552adbb13cce68a7840f6e46806eb13aa0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of transparency in automated essay grading systems by introducing EssayCBM, a framework that first predicts scores for specific writing concepts aligned with a rubric and then uses these scores to compute a final grade. This approach provides interpretable, concept-level feedback and allows instructors to adjust predictions in a human-in-the-loop manner. The proposed method achieves performance comparable to black-box models while offering greater accountability and actionable insights for educators and students."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[EssayCBM: Rubric-Aligned Concept Bottleneck Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6/Black-box grading lacks transparency]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u6982\u5ff5\u74f6\u9888\u548c\u89c4\u5219\u5bf9\u9f50\u7684\u8bc4\u4f30/Rubric-aligned concept bottleneck evaluation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u5339\u914d\u9ed1\u76d2\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u53cd\u9988/Matches performance & provides interpretable feedback]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [medical nlp / llm evaluation], [medical benchmark, electronic health records (EHR), knowledge grounding, counterfactual reasoning, DPO fine-tuning]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhan Qu, Michael F\xe4rber"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TU Dresden, ScaDS.AI"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20822",children:"https://arxiv.org/pdf/2512.20822"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces MediEval, a unified benchmark linking real EHRs (MIMIC-IV) to a biomedical knowledge base for evaluating LLMs on patient-contextual and knowledge-grounded reasoning. 2. Proposes a 4-quadrant evaluation framework to systematically assess models on both factual correctness and contextual consistency, identifying critical failure modes like hallucinated support and truth inversion. 3. Proposes Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty, which significantly improves model accuracy and safety by eliminating truth inversion errors."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59c4d88b1ecf7256d86a2c1dd12f74897d1a42b0c88d272ea8cf058355f013cd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59c4d88b1ecf7256d86a2c1dd12f74897d1a42b0c88d272ea8cf058355f013cd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies a gap in evaluating LLMs for medical applications, where existing benchmarks either test isolated knowledge or patient reasoning without verifying correctness. To address this, the authors introduce the MediEval benchmark and a 4-quadrant evaluation framework to systematically assess LLMs, and propose a novel fine-tuning method called CoRFu. The results show that CoRFu significantly improves model performance and safety by eliminating dangerous error types like truth inversion."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[MediEval] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs in medicine lack reliable evaluation combining knowledge and patient context];\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified benchmark (EHR + KB) & 4-quadrant framework & CoRFu fine-tuning];\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Identifies failure modes; CoRFu improves accuracy and safety];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] How important is Recall for Measuring Retrieval Quality?"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [retrieval quality, recall estimation, LLM-based evaluation, nDCG, RAG]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shelly Schwartz, Oleg Vasilyev, Randy Sawaya"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Primer Technologies Inc."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20854",children:"https://arxiv.org/pdf/2512.20854"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Evaluates established strategies for measuring retrieval quality when the total number of relevant documents (and thus recall) is unknown, by correlating metrics with LLM-based judgments of response quality. 2. Conducts experiments across multiple datasets with a low number of relevant documents (2-15) to assess these strategies. 3. Introduces a new, simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8385a132c621f6e8d9e71274fc95535412aa2b9f0d0f40e834705d3548f0a601_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8385a132c621f6e8d9e71274fc95535412aa2b9f0d0f40e834705d3548f0a601_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of evaluating retrieval quality in realistic settings where the total number of relevant documents is unknown, making recall uncomputable. It evaluates existing strategies and proposes a new simple metric by measuring their correlation with LLM-generated response quality. The main conclusion is that a simple measure can perform effectively without needing to know the total relevant documents, offering a practical solution for dynamic knowledge bases."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[How important is Recall for Measuring Retrieval Quality?] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Realistic retrieval with unknown total relevant docs]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Correlate metrics with LLM response quality; propose new measure]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Simple measure works well without recall]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, agentic reasoning, sparse activation, long context]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," NVIDIA"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20848",children:"https://arxiv.org/pdf/2512.20848"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Nemotron 3 Nano<br>\u8bba\u6587\u6807\u9898/Paper Title] --\x3e B[\u6784\u5efa\u9ad8\u6548\u3001\u80fd\u8fdb\u884c\u667a\u80fd\u4f53\u63a8\u7406\u7684\u5927\u6a21\u578b<br>\u6838\u5fc3\u95ee\u9898/Problem];\nA --\x3e C[\u6df7\u5408MoE\u4e0eMamba-Transformer\u67b6\u6784\uff0c\u7a00\u758f\u6fc0\u6d3b\u53c2\u6570<br>\u4e3b\u8981\u65b9\u6cd5/Method];\nA --\x3e D[\u66f4\u9ad8\u63a8\u7406\u541e\u5410\u4e0e\u7cbe\u5ea6\uff0c\u652f\u6301100\u4e07\u4ee4\u724c\u4e0a\u4e0b\u6587<br>\u5173\u952e\u7ed3\u679c/Results];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, LatentMoE, NVFP4, multi-environment reinforcement learning]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," NVIDIA"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20856",children:"https://arxiv.org/pdf/2512.20856"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[NVIDIA Nemotron 3] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Efficient and open intelligence for agentic applications]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Architectural Trade-offs in Small Language Models Under Compute Constraints"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [language modeling], [small language models, compute constraints, architectural trade-offs, rotary positional embeddings, transformer]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shivraj Singh Bhatti"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Massachusetts Amherst"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20877",children:"https://arxiv.org/pdf/2512.20877"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. A systematic empirical study of architectural choices (from linear predictors to transformers) for small language models under strict compute constraints. 2. An analysis showing attention-based models are more FLOP-efficient than MLPs even at small scale, and that increasing depth/context without sufficient optimization can hurt performance. 3. An investigation revealing that techniques like Rotary Positional Embeddings (RoPE), successful in large models, do not necessarily transfer effectively to the small-model regime."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b54dee144c67bdb877818e8171163f9556734c09b3a1d7d29fe8464aee558b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically studies how architectural choices affect small language model performance under limited compute. The method involves progressively building from linear predictors to multi-layer transformers and evaluating them on character and word-level datasets. The main conclusion is that attention is more efficient than MLPs per FLOP at small scales, but scaling depth or applying large-model techniques like RoPE can be detrimental without careful optimization."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Architectural Trade-offs in Small Language Models<br>\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\u6743\u8861] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>How do architectural choices affect performance under compute constraints?<br>\u8ba1\u7b97\u7ea6\u675f\u4e0b\u67b6\u6784\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u6027\u80fd\uff1f]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Progressive architectural study from linear to transformer models<br>\u4ece\u7ebf\u6027\u5230Transformer\u6a21\u578b\u7684\u6e10\u8fdb\u5f0f\u67b6\u6784\u7814\u7a76]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>Attention > MLPs in per-FLOP efficiency; RoPE may not transfer<br>\u6ce8\u610f\u529b\u673a\u5236\u5355\u4f4dFLOP\u6548\u7387\u4f18\u4e8eMLP\uff1bRoPE\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5c0f\u6a21\u578b]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [reasoning distillation, provenance tracing, teacher-guided data selection, model generalization, knowledge transfer]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Kaiyuan Liu, Shaotian Yan, Rui Miao, Bing Wang, Chen Shen, Jun Zhang, Jieping Ye"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Alibaba Cloud Computing, Jilin University, University of Michigan"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20908",children:"https://arxiv.org/pdf/2512.20908"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces a cross-model Reasoning Distillation Provenance Tracing framework to classify the origin of a distilled model's outputs into four categories. 2. Empirically demonstrates that teacher-originated actions in the distilled model correlate with its performance, providing an explanatory analysis for distillation. 3. Proposes a principled, teacher-guided data selection method based on teacher-student divergence, validated across multiple models."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d19fc56be8917fa897a8772f9f0c999250e917eda937c0e1b5176333830faf5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d19fc56be8917fa897a8772f9f0c999250e917eda937c0e1b5176333830faf5_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of analysis on the origins of capabilities in reasoning-distilled models by introducing a provenance tracing framework. The method classifies model outputs by comparing probabilities from teacher, student, and distilled models, showing that teacher-originated actions explain performance. Based on this, a teacher-guided data selection method is proposed and validated to improve distillation."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[\u8bba\u6587\u6807\u9898 / Paper Title<br>Where Did This Sentence Come From?] --\x3e B{\u6838\u5fc3\u95ee\u9898 / Problem};\nA --\x3e C{\u4e3b\u8981\u65b9\u6cd5 / Method};\nA --\x3e D{\u5173\u952e\u7ed3\u679c / Results};\nB --\x3e B1[\u84b8\u998f\u6a21\u578b\u80fd\u529b\u6765\u6e90\u4e0d\u660e / Unclear provenance of distilled model capabilities];\nB --\x3e B2[\u6cdb\u5316\u80fd\u529b\u5b58\u7591 / Concerns about generalization];\nC --\x3e C1[\u6eaf\u6e90\u6846\u67b6 / Provenance Tracing Framework];\nC --\x3e C2[\u6982\u7387\u6bd4\u8f83\u5206\u7c7b / Classify by comparing probabilities];\nC --\x3e C3[\u6559\u5e08\u5f15\u5bfc\u6570\u636e\u9009\u62e9 / Teacher-guided data selection];\nD --\x3e D1[\u6559\u5e08\u884c\u4e3a\u53ef\u88ab\u7ee7\u627f / Teacher-originated actions are generated];\nD --\x3e D2[\u884c\u4e3a\u4e0e\u6027\u80fd\u76f8\u5173 / Actions correlate with performance];\nD --\x3e D3[\u65b0\u65b9\u6cd5\u6709\u6548 / New selection method is effective];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [cv], [visual reasoning], [visual programming, spatial reasoning, tool induction, transductive learning, 3D scene understanding]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20934",children:"https://arxiv.org/pdf/2512.20934"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://transductive-visualprogram.github.io/",children:"https://transductive-visualprogram.github.io/"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Transductive Visual Programming] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br>Spatial reasoning is challenging for VLMs]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Build tools from experience, not speculation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br>SOTA performance, better tool reuse & generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [multi-modal learning], [foundation models, multi-modal fusion, cross-corpus evaluation, neuropsychiatric disorders, multi-lingual datasets]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Hunan University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20948",children:"https://arxiv.org/pdf/2512.20948"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposed FEND, a comprehensive multi-modal framework using foundation models for evaluating neuropsychiatric disorders across the lifespan. 2. Conducted a systematic evaluation using 13 multi-lingual datasets, identifying strengths and limitations of multi-modal fusion for different disorders. 3. Provided extensive benchmarks and analysis of performance-influencing factors (e.g., modality imbalance, dataset heterogeneity) to advance reproducible research in the field."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18e74bbfe865915192b7a6c5c53058f33d0688ed82ef83023913d356622a3899_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18e74bbfe865915192b7a6c5c53058f33d0688ed82ef83023913d356622a3899_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FEND, a foundation model-based multi-modal framework for detecting neuropsychiatric disorders like Alzheimer's, depression, and autism from speech and text. It evaluates the framework on 13 multi-lingual datasets, finding that multi-modal fusion works well for Alzheimer's and depression but underperforms for autism due to dataset heterogeneity, and identifies modality imbalance as a key challenge."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Foundation Model-based Evaluation of Neuropsychiatric Disorders] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Multi-lingual generalization & lack of unified framework)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: FEND multi-modal framework using speech & text)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Multi-modal fusion excels for AD/depression, underperforms for ASD)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Neural Probe-Based Hallucination Detection for Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [hallucination detection], [MLP probes, token-level detection, Bayesian optimization, hidden states, multi-objective loss]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Shize Liang, Hongzhi Wang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Harbin Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20949",children:"https://arxiv.org/pdf/2512.20949"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposed a neural network-based framework using lightweight MLP probes for token-level hallucination detection, enabling nonlinear modeling of hidden states. 2. Designed a multi-objective joint loss function to improve detection stability and semantic disambiguation. 3. Established a layer position-probe performance response model and used Bayesian optimization to automatically search for optimal probe insertion layers."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/256e2b7c6550072fc0e643c4045a4a592ba6b2241cd12656b7dd16ad27bf89b0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/256e2b7c6550072fc0e643c4045a4a592ba6b2241cd12656b7dd16ad27bf89b0_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of hallucination in large language models by proposing a real-time, token-level detection method. The method uses lightweight MLP probes on frozen model hidden states and a Bayesian-optimized layer search. Experiments show it outperforms existing methods in accuracy and recall under low false-positive conditions."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Neural Probe-Based Hallucination Detection for Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLMs\u751f\u6210\u5e7b\u89c9\u5185\u5bb9/LLMs generate hallucinations)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: MLP\u63a2\u9488 & \u8d1d\u53f6\u65af\u4f18\u5316/MLP probes & Bayesian optimization)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02/Outperforms SOTA on multiple datasets)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [crosslingual information retrieval], [dual-encoder, contrastive learning, hard negative sampling, data augmentation, multi-source alignment]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"}),' Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is "MultiMind".']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20950",children:"https://arxiv.org/pdf/2512.20950"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Rapid spread of multilingual misinformation);\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved retrieval accuracy on benchmarks);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [protein language models], [reflection pretraining, chain-of-thought, language expressiveness, self-correction, biological sequences]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Fudan University, Shanghai Artificial Intelligence Laboratory, University of British Columbia, Zhejiang University, The Chinese University of Hong Kong, Stony Brook University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20954",children:"https://arxiv.org/pdf/2512.20954"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"}),' 1. Proposed and defined the concept of "language expressiveness" to explain the difficulty of applying Chain-of-Thought reasoning to biological sequence models. 2. Introduced reflection pretraining for biological sequence models, enabling intermediate reasoning through auxiliary "thinking tokens". 3. Demonstrated that this approach enables self-correction, improves performance, and offers benefits like counter-memorization and enhanced human steerability.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c51a6a0ca0e6bf5774254f47e4581544610c262b22a0cc12fe84b840bda40a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c51a6a0ca0e6bf5774254f47e4581544610c262b22a0cc12fe84b840bda40a_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of applying Chain-of-Thought reasoning to biological sequence models like protein language models, which have limited token expressiveness. The authors propose reflection pretraining, which augments the model with auxiliary "thinking tokens" to enable intermediate reasoning and self-correction. The method theoretically enhances language expressiveness and experimentally leads to substantial performance gains compared to standard pretraining.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:'graph LR\nA[Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Limited expressiveness of protein language restricts CoT reasoning)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Reflection pretraining with auxiliary "thinking tokens")\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Enhanced expressiveness, self-correction, performance gains)'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Automatic Replication of LLM Mistakes in Medical Conversations"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [llm evaluation], [medical conversation, mistake replication, benchmark creation, llm judges, single-shot qa]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Lumos AI"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20983",children:"https://arxiv.org/pdf/2512.20983"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Automatic Replication of LLM Mistakes in Medical Conversations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLM\u9519\u8bef\u96be\u4ee5\u5728\u5176\u4ed6\u6a21\u578b\u4e2d\u590d\u73b0/Mistakes hard to replicate across LLMs)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: MedMistake\u81ea\u52a8\u7ba1\u9053/MedMistake automatic pipeline)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u53d1\u5e03\u57fa\u51c6\u5e76\u8bc4\u4f3012\u4e2aLLM/Released benchmark & evaluated 12 LLMs)\nC --\x3e C1(\u751f\u6210\u5bf9\u8bdd/Generate conversations)\nC --\x3e C2(LLM\u59d4\u5458\u4f1a\u8bc4\u4f30/LLM committee evaluation)\nC --\x3e C3(\u521b\u5efa\u5355\u8f6eQA\u5bf9/Create single-shot QA pairs)\nD --\x3e D1(MedMistake-All\u6570\u636e\u96c6/MedMistake-All dataset)\nD --\x3e D2(MedMistake-Bench\u9a8c\u8bc1\u5b50\u96c6/MedMistake-Bench validated subset)\nD --\x3e D3(GPT/Claude/Grok\u8868\u73b0\u6700\u4f73/GPT/Claude/Grok performed best)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [knowledge distillation, chain-of-thought, sequence truncation, training efficiency, reasoning models]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Wei-Rui Chen, Vignesh Kothapalli, Ata Fatahibaarzi, Hejian Sang, Shao Tang, Qingquan Song, Zhipeng Wang, Muhammad Abdul-Mageed"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," The University of British Columbia, LinkedIn"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21002",children:"https://arxiv.org/pdf/2512.21002"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/weiruichen01/distilling-the-essence",children:"https://github.com/weiruichen01/distilling-the-essence"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Analysis of supervision allocation in reasoning distillation, showing the CoT segment is the dominant factor for transferring reasoning capability. 2. Establishment of a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. 3. Empirical demonstration that training on only the first 50% of tokens retains ~94% of performance while halving computational costs."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a99e2da19bbc9bacf5104e37b4afd860b26a5285dd752f9a6e025702d930839_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a99e2da19bbc9bacf5104e37b4afd860b26a5285dd752f9a6e025702d930839_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational expense of distilling reasoning capabilities from large to small models over long sequences. It proposes a method of selective distillation and sequence truncation, focusing on early reasoning tokens. The key finding is that training on just the first half of tokens can preserve most performance while significantly reducing training time, memory, and FLOPs."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Distilling the Essence<br>\u9ad8\u6548\u63a8\u7406\u84b8\u998f] --\x3e B{\u6838\u5fc3\u95ee\u9898/Problem};\n    A --\x3e C{\u4e3b\u8981\u65b9\u6cd5/Method};\n    A --\x3e D{\u5173\u952e\u7ed3\u679c/Results};\n    B --\x3e B1[\u957f\u5e8f\u5217\u63a8\u7406\u84b8\u998f\u8ba1\u7b97\u6602\u8d35<br>Long-Sequence Reasoning Distillation is Expensive];\n    C --\x3e C1[\u9009\u62e9\u6027\u76d1\u7763\u4e0e\u5e8f\u5217\u622a\u65ad<br>Selective Supervision & Sequence Truncation];\n    D --\x3e D1[\u4fdd\u755994%\u6027\u80fd\uff0c\u51cf\u5c1150%\u6210\u672c<br>Retain 94% Performance, Reduce 50% Cost];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Supervised Fine-Tuning, Chain-of-Thought, Two-Stage Training, Attention Imbalance, Key Answer Tokens]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Xiaofeng Shi, Qian Kou, Yuduo Li, Hua Zhou"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Beijing Academy of Artificial Intelligence (BAAI), Beijing Jiaotong University (BJTU)"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21017",children:"https://arxiv.org/pdf/2512.21017"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Identifies a key limitation in conventional SFT where models over-attend to lengthy Chain-of-Thought reasoning sequences at the expense of the shorter, critical final answer tokens. 2. Proposes SFTKey, a novel two-stage fine-tuning scheme that first applies conventional SFT for format learning, then fine-tunes only on the Key (final answer) portion to boost accuracy. 3. Demonstrates through extensive experiments that SFTKey achieves an average accuracy improvement of over 5% compared to standard SFT while maintaining correct output formatting."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eab786d7e6ac187d45153b771fdc458d7c8434c0e133a9bcdb70a2afa441a73_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eab786d7e6ac187d45153b771fdc458d7c8434c0e133a9bcdb70a2afa441a73_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies that standard Supervised Fine-Tuning (SFT) for LLMs can cause an attention imbalance, where models focus too much on long reasoning chains (CoT) and not enough on the final answer. To solve this, the authors propose SFTKey, a two-stage method that first does standard SFT for formatting, then fine-tunes only on the key answer tokens. Experiments show this approach improves average accuracy by over 5% without harming output format correctness."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[\u8bba\u6587\u6807\u9898 / Paper Title<br>Rethinking Supervised Fine-Tuning] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem<br>\u6ce8\u610f\u529b\u5931\u8861\u4e8e\u957f\u63a8\u7406\u94fe / Attention Imbalance on Long CoT]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method<br>\u4e24\u9636\u6bb5\u8bad\u7ec3 SFTKey / Two-Stage Training SFTKey]\nA --\x3e D[\u5173\u952e\u7ed3\u679c / Results<br>\u51c6\u786e\u7387\u63d0\u5347>5% / Accuracy Improvement >5%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Semi-Supervised Learning for Large Language Models Safety and Content Moderation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [content moderation], [semi-supervised learning, data augmentation, safety classifiers, LLM safety, prompt harmfulness]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21107",children:"https://arxiv.org/pdf/2512.21107"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[\u8bba\u6587\u6807\u9898 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem<br>\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e / Reliance on large labeled data]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method<br>\u534a\u76d1\u7763\u5b66\u4e60\u4e0e\u4efb\u52a1\u7279\u5b9a\u589e\u5f3a / SSL & Task-Specific Augmentation]\nA --\x3e D[\u5173\u952e\u7ed3\u679c / Results<br>\u6027\u80fd\u663e\u8457\u63d0\u5347 / Significant Performance Improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Semantic Refinement with LLMs for Graph Representations"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [graph representation learning], [graph neural network, large language model, semantic refinement, structure-semantics heterogeneity, data-centric adaptation]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of Connecticut, University of Notre Dame"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21106",children:"https://arxiv.org/pdf/2512.21106"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Semantic Refinement with LLMs for Graph Representations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Graph structure-semantics heterogeneity \u56fe\u7684\u7ed3\u6784-\u8bed\u4e49\u5f02\u8d28\u6027)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Data-Adaptive Semantic Refinement (DAS) framework \u6570\u636e\u81ea\u9002\u5e94\u8bed\u4e49\u7cbe\u70bc\u6846\u67b6)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs \u63d0\u5347\u7ed3\u6784\u4e3b\u5bfc\u56fe\u6027\u80fd\uff0c\u5728\u8bed\u4e49\u4e30\u5bcc\u56fe\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Beyond Context: Large Language Models Failure to Grasp Users Intent"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [ai safety], [intent recognition, contextual understanding, safety circumvention, prompt engineering, transformer architectures]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21110",children:"https://arxiv.org/pdf/2512.21110"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Identifies and empirically demonstrates a critical vulnerability in LLMs: their inability to understand user intent and context, which allows safety mechanisms to be circumvented. 2. Evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows that exploitation techniques like emotional framing and progressive revelation are effective, and that reasoning capabilities can amplify this risk. 3. Proposes a paradigmatic shift in AI safety design, arguing for contextual understanding and intent recognition to be core capabilities rather than post-hoc protective mechanisms."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper identifies a fundamental vulnerability in Large Language Models (LLMs): their lack of contextual understanding and intent recognition, which allows safety mechanisms to be systematically bypassed. The authors empirically evaluate several LLMs, showing they can be exploited through techniques like emotional framing, and find that reasoning capabilities often worsen the problem. They conclude that a paradigm shift is needed to build intent recognition directly into LLM architectures for safety."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Beyond Context: Large Language Models Failure to Grasp Users Intent] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b/LLMs lack contextual understanding & intent recognition]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5bf9\u591a\u79cdLLM\u8fdb\u884c\u7ecf\u9a8c\u6027\u8bc4\u4f30/Empirical evaluation of multiple LLMs]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5b89\u5168\u673a\u5236\u53ef\u88ab\u7cfb\u7edf\u89c4\u907f\uff0c\u9700\u8303\u5f0f\u8f6c\u53d8/Safety mechanisms can be systematically circumvented, requiring a paradigm shift]\nB --\x3e E[\u5bfc\u81f4\u53ef\u5229\u7528\u7684\u6f0f\u6d1e/Creates exploitable vulnerabilities]\nC --\x3e F[\u4f7f\u7528\u60c5\u611f\u6846\u67b6\u3001\u6e10\u8fdb\u63ed\u793a\u7b49\u6280\u672f/Using emotional framing, progressive revelation, etc.]\nD --\x3e G[Claude Opus 4.1\u90e8\u5206\u4f8b\u5916\uff0c\u63a8\u7406\u80fd\u529b\u52a0\u5267\u98ce\u9669/Claude Opus 4.1 partial exception, reasoning amplifies risk]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [conversational ai], [multi-turn clarification, ambiguity taxonomy, agentic approach]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sichun Luo, Yi Huang, Mukai Li, Shichang Meng, Fengyuan Liu, Zefa Hu, Junlan Feng, Qi Liu"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," The University of Hong Kong, JIUTIAN Research (China Mobile), City University of Hong Kong"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21120",children:"https://arxiv.org/pdf/2512.21120"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"}),' 1. Introduces ClarifyMT-Bench, a novel benchmark for multi-turn clarification featuring a five-dimensional ambiguity taxonomy and diverse simulated user personas. 2. Uncovers a consistent "under-clarification bias" in LLMs, where they answer prematurely and performance degrades with dialogue depth. 3. Proposes ClarifyAgent, an agentic framework that decomposes clarification into perception, forecasting, tracking, and planning to improve robustness.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c251a364522d772d4c0ccb3f8108c9a5bafb4d76316e5783c96374fcd1c4488_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c251a364522d772d4c0ccb3f8108c9a5bafb4d76316e5783c96374fcd1c4488_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that LLMs tend to answer ambiguous user queries prematurely in multi-turn conversations. To study this, the authors introduce ClarifyMT-Bench, a multi-turn clarification benchmark, and propose ClarifyAgent, an agentic method that improves clarification robustness. The main finding is that current LLMs have an under-clarification bias, which the proposed agentic approach helps mitigate."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[ClarifyMT-Bench] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs under-clarify in multi-turn dialogues]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Benchmark + ClarifyAgent]\nC --\x3e D[Benchmark: \u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6/Multi-turn Benchmark]\nC --\x3e E[Agent: \u4ee3\u7406\u65b9\u6cd5/Agentic Approach]\nA --\x3e F[\u5173\u952e\u7ed3\u679c/Results: Bias identified, Agent improves robustness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [speech representation learning], [meta-learning, bi-level optimization, few-shot adaptation, self-supervised learning, speech representation]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Meta AI, ENS-PSL, EHESS, CNRS"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21204",children:"https://arxiv.org/pdf/2512.21204"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/facebookresearch/spidr-adapt",children:"https://github.com/facebookresearch/spidr-adapt"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces the Multi-task Adaptive Pre-training (MAdaPT) protocol, framing few-shot speech representation learning as a bi-level optimization meta-learning problem. 2. Proposes a novel First-Order Bi-Level Optimization (FOBLO) heuristic to enable scalable meta-training by avoiding heavy computation costs. 3. Stabilizes meta-training with a robust initialization technique using interleaved supervision that alternates between self-supervised and supervised objectives."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff9692c36cbda26291fde2551256e229a90f6eb51f087d833a2d84cb4b10925b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff9692c36cbda26291fde2551256e229a90f6eb51f087d833a2d84cb4b10925b_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SpidR-Adapt, a method for rapid adaptation of speech representation models to new languages using minimal unlabeled data. It formulates the problem as meta-learning with a bi-level optimization framework (MAdaPT), proposes an efficient solver (FOBLO), and uses interleaved supervision for stable training. The model achieves significant gains in phonemic discrimination and language modeling after training on less than 1 hour of target-language audio, demonstrating over 100x greater data efficiency than standard methods."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[SpidR-Adapt] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u6570\u636e\u6548\u7387\u5dee\u8ddd/Data-Efficiency Gap]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5143\u5b66\u4e60\u4e0e\u53cc\u5c42\u4f18\u5316/Meta-Learning & Bi-Level Optimization]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 100\u500d\u6570\u636e\u6548\u7387/100x Data Efficiency]\n    B --\x3e B1[\u5a74\u513f\u9ad8\u6548 vs. \u6a21\u578b\u4f4e\u6548/Infant Efficiency vs. Model Inefficiency]\n    C --\x3e C1[MAdaPT\u534f\u8bae/MAdaPT Protocol]\n    C --\x3e C2[FOBLO\u4f18\u5316/FOBLO Optimization]\n    C --\x3e C3[\u4ea4\u9519\u76d1\u7763/Interleaved Supervision]\n    D --\x3e D1[<1h\u97f3\u9891/<1h Audio]\n    D --\x3e D2[\u97f3\u7d20\u53ef\u8fa8\u6027\u63d0\u5347/Improved Phonemic Discriminability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [agent system], [sequential modeling, chain-of-thought reasoning, diffusion large language models, multi-agent collaboration, world knowledge]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Chuan Wang, Gaoming Yang, Han Wu, Jiakai Tang, Jiahao Yu, Jian Wu, Jianwu Hu, Junjun Zheng, Shuwen Xiao, Yeqiu Yang, Yuning Jiang, Ahjol Nurlanbek, Binbin Cao, Bo Zheng, Fangmei Zhu, Gaoming Zhou, Huimin Yi, Huiping Chu, Jin Huang, Jinzhe Shan, Kenan Cui, Longbin Li, Silu Zhou, Wen Chen, Xia Ming, Xiang Gao, Xin Yao, Xingyu Wen, Yan Zhang, Yiwen Hu, Yulin Wang, Ziheng Bao, Zongyuan Wu"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," TaoRank Team (Alibaba Group / Taobao)"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21257",children:"https://arxiv.org/pdf/2512.21257"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes ReaSeq, a reasoning-enhanced framework that leverages LLM world knowledge to overcome limitations of log-driven recommender systems. 2. Introduces explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into enriched item representations. 3. Employs latent reasoning via Diffusion LLMs to infer plausible beyond-log user behaviors, enhancing interest modeling."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f98d5c26b79acf6e7c20219639988198127de51ba1227ceaeb063216243ba42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f98d5c26b79acf6e7c20219639988198127de51ba1227ceaeb063216243ba42_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces ReaSeq, a framework that uses Large Language Models' world knowledge for explicit and implicit reasoning to address knowledge poverty and systemic blindness in log-driven industrial recommender systems. It enhances item representations and infers beyond-log user behaviors. Deployed on Taobao, it achieved significant improvements in key business metrics like CTR and GMV."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u77e5\u8bc6\u8d2b\u4e4f/Knowledge Poverty in ID-based Representations]\n    B --\x3e B2[\u7cfb\u7edf\u76f2\u533a/Systemic Blindness to Beyond-Log Interests]\n    C --\x3e C1[\u663e\u5f0f\u63a8\u7406/Explicit Chain-of-Thought Reasoning via Multi-Agent]\n    C --\x3e C2[\u9690\u5f0f\u63a8\u7406/Latent Reasoning via Diffusion LLMs]\n    D --\x3e D1[IPV & CTR\u63d0\u5347 >6.0%/IPV & CTR Gain >6.0%]\n    D --\x3e D2[\u8ba2\u5355\u63d0\u5347 >2.9%/Orders Gain >2.9%]\n    D --\x3e D3[GMV\u63d0\u5347 >2.5%/GMV Gain >2.5%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [document question answering], [Tree-LSTM, Memory Augmented Neural Network (MANN), Retrieval Augmented Generation (RAG), Parameter Efficiency, Fact Extraction]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Divij Dudeja, Mayukha Pal"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," ABB Ability Innovation Center, Indian Institute of Information Technology, Nagpur"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21280",children:"https://arxiv.org/pdf/2512.21280"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Introduces a hierarchical, syntax-aware fact extractor (Grammarian Tree-LSTM) to parse engineering manuals into structured subject-relation-object triples., 2. Proposes a compact, indexed memory system (MANN) to store and retrieve extracted facts as vectors, enabling efficient knowledge access., 3. Designs a dual-mode inference system combining a fast path for known documents and a dynamic RAG-assisted path for new uploads, reducing hallucinations."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fe5f40637f711007d6bb6875182fa80072536380614c5e93c7c4f5cf8dc2232_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fe5f40637f711007d6bb6875182fa80072536380614c5e93c7c4f5cf8dc2232_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of accurately answering questions from dense engineering manuals, where standard small language models fail. It proposes SMART, a structured model that hierarchically extracts facts, stores them in an indexed memory, and uses a transformer to generate answers from retrieved facts. The result is a parameter-efficient model that achieves higher accuracy with fewer parameters and reduced hallucinations compared to baselines like GPT-2."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[SMART SLM] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5de5\u7a0b\u624b\u518c\u96be\u4ee5\u9605\u8bfb\uff0c\u73b0\u6709\u5c0f\u6a21\u578b\u5904\u7406\u4e3a\u6241\u5e73token\u6d41\uff0c\u5bfc\u81f4\u9519\u8bef\u7b54\u6848/Engineering manuals are hard to read; flat token processing leads to incorrect answers]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5206\u5c42\u5904\u7406\uff1a\u8bed\u6cd5\u611f\u77e5\u4e8b\u5b9e\u63d0\u53d6\u5668 + \u7d22\u5f15\u8bb0\u5fc6(MANN) + 6\u5c42Transformer/Hierarchical processing: Syntax-aware fact extractor + Indexed memory (MANN) + 6-layer Transformer]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u53c2\u6570\u51cf\u5c1164-69%\uff0c\u51c6\u786e\u7387\u63d0\u534721.3%\uff0c\u51cf\u5c11\u5e7b\u89c9/64-69% fewer parameters, 21.3% higher accuracy, reduced hallucinations]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Parallel Token Prediction for Language Models"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [parallel token prediction, speculative decoding, autoregressive decoding, transformer inference, latency optimization]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," University of California, Irvine, Chan-Zuckerberg Initiative, Pyramidal AI"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21323",children:"https://arxiv.org/pdf/2512.21323"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes Parallel Token Prediction (PTP), a universal framework for parallel sequence generation that jointly predicts multiple dependent tokens in a single transformer call. 2. Proves that PTP can represent arbitrary autoregressive sequence distributions, avoiding the restrictive independence assumptions of prior multi-token prediction methods. 3. Demonstrates state-of-the-art speculative decoding performance, accepting over four tokens per step on Spec-Bench with Vicuna-7B, showing parallel long-sequence generation is feasible without losing modeling power."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5681ed12f339fba2f3eb1e012a75a0b58e77b2c9c9aa20352d3b12fcba4f3c_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper addresses the high latency of autoregressive decoding in large language models by proposing Parallel Token Prediction (PTP), a framework that predicts multiple dependent tokens in parallel within a single transformer call. It proves PTP's universality in representing autoregressive distributions and shows it achieves superior speculative decoding performance, enabling faster text generation without sacrificing quality."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Parallel Token Prediction for Language Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Autoregressive decoding latency bottleneck]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Parallel Token Prediction (PTP), joint prediction of dependent tokens]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: State-of-the-art speculative decoding, >4 tokens/step, universal framework]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Measuring all the noises of LLM Evals"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [LLM evaluation, statistical noise, paired analysis, prediction variance, data variance]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sida Wang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," FAIR at Meta"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21326",children:"https://arxiv.org/pdf/2512.21326"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"}),' 1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the "all-pairs paired method" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an "all-pairs paired method" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.']}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Measuring all the noises of LLM Evals] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLM\u8bc4\u4f30\u4e2d\u7684\u7edf\u8ba1\u566a\u58f0/Separating signal from noise in LLM evals)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u5168\u914d\u5bf9\u5206\u6790\u6cd5/All-pairs paired method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u53ef\u9884\u6d4b\u7684\u603b\u566a\u58f0\u4e0e\u4e3b\u5bfc\u7684\u9884\u6d4b\u566a\u58f0/Predictable total noise & dominant prediction noise)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [reasoning evaluation], [abstract reasoning, perception bottleneck, vision-language models, inductive reasoning, evaluation protocol]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Xinhe Wang, Jin Huang, Xingjian Zhang, Tianhao Wang, Jiaqi W. Ma"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Carnegie Mellon University, University of Michigan, University of California San Diego, University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21329",children:"https://arxiv.org/pdf/2512.21329"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a two-stage experimental pipeline to explicitly separate perception and reasoning in abstract reasoning benchmarks. 2. Empirically demonstrates that perception capability, not reasoning, is the dominant factor in the performance gap for VLMs on ARC-style tasks. 3. Reveals through manual analysis that approximately 80% of model failures stem from perception errors, challenging the common interpretation of these benchmarks."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fcc43646a911ef2be3f5f5aea5201f8582d0ad2c6e3465915701cc2f7fb9f09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fcc43646a911ef2be3f5f5aea5201f8582d0ad2c6e3465915701cc2f7fb9f09_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper challenges the view that poor performance of vision-language models on abstract reasoning benchmarks like ARC indicates a reasoning deficiency. It introduces a two-stage pipeline that isolates perception (image-to-text description) from reasoning (rule induction on text) and shows that perception bottlenecks are the primary cause of failure, suggesting current benchmarks conflate these challenges."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\nA[Your Reasoning Benchmark May Not Test Reasoning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[VLMs\u5728\u62bd\u8c61\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u5e38\u5f52\u56e0\u4e8e\u63a8\u7406\u7f3a\u9677/VLMs perform poorly on abstract reasoning benchmarks, often attributed to reasoning deficits]\nC --\x3e C1[\u63d0\u51fa\u4e24\u9636\u6bb5\u5b9e\u9a8c\u6d41\u7a0b\uff1a\u611f\u77e5\uff08\u56fe\u50cf\u5230\u6587\u672c\uff09\u4e0e\u63a8\u7406\uff08\u57fa\u4e8e\u6587\u672c\u7684\u89c4\u5219\u5f52\u7eb3\uff09/Propose a two-stage pipeline: Perception (image-to-text) and Reasoning (text-based rule induction)]\nD --\x3e D1[\u611f\u77e5\u80fd\u529b\u662f\u6027\u80fd\u5dee\u8ddd\u7684\u4e3b\u5bfc\u56e0\u7d20\uff0c\u7ea680%\u7684\u5931\u8d25\u6e90\u4e8e\u611f\u77e5\u9519\u8bef/Perception is the dominant factor for the performance gap, ~80% of failures stem from perception errors]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [nlp], [code retrieval], [Pooling by Multihead Attention (PMA), contrastive learning, code embedding, MTEB-Code, Qwen-2.5-Coder]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Ant Group, Shanghai Jiao Tong University"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21332",children:"https://arxiv.org/pdf/2512.21332"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/codefuse-ai/CodeFuse-Embeddings",children:"https://github.com/codefuse-ai/CodeFuse-Embeddings"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Proposes a Pooling by Multihead Attention (PMA) module to generate sequence embeddings from token embeddings, effectively utilizing the LLM's causal representations. 2. The PMA module aggregates information from all tokens in a sequence, overcoming the information bottleneck of traditional EOS-based sequence embeddings. 3. The approach supports flexible adaptation of embedding dimensions, serving as an alternative to Multi-Representation Learning (MRL)."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27c6e6ad01eacb3bd759d1aafa323cd3f72410efd901b1df691e709d8fbe3a4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27c6e6ad01eacb3bd759d1aafa323cd3f72410efd901b1df691e709d8fbe3a4_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces C2LLM, a family of code embedding models built on Qwen-2.5-Coder backbones. It proposes a novel Pooling by Multihead Attention (PMA) module to create better sequence embeddings for code retrieval. The models, trained on three million data points, achieve state-of-the-art performance on the MTEB-Code benchmark, with the 7B version ranking first overall."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[C2LLM Technical Report] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u5e8f\u5217\u8868\u793a\u74f6\u9888/Sequence representation bottleneck in code retrieval]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u81ea\u9002\u5e94\u4ea4\u53c9\u6ce8\u610f\u529b\u6c60\u5316 (PMA) / Adaptive Cross-Attention Pooling (PMA)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728MTEB-Code\u4e0aSOTA / SOTA on MTEB-Code]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [mlsys], [diffusion models], [Denoising Entropy, Masked Diffusion Models, decoding path optimization, predictive uncertainty, non-autoregressive generation]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Westlake University, University of Chicago"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.21336",children:"https://arxiv.org/pdf/2512.21336"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"code:"})," ",(0,a.jsx)(e.a,{href:"https://github.com/LINs-lab/DenoisingEntropy",children:"https://github.com/LINs-lab/DenoisingEntropy"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u89e3\u7801\u8def\u5f84] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: MDMs\u751f\u6210\u8d28\u91cf\u5bf9\u89e3\u7801\u987a\u5e8f\u654f\u611f<br/>MDM output quality is sensitive to decoding order)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u53bb\u566a\u71b5\u548c\u8def\u5f84\u4f18\u5316\u7b97\u6cd5<br/>Propose Denoising Entropy & path optimization algorithms)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u71b5\u5f15\u5bfc\u65b9\u6cd5\u63d0\u5347\u751f\u6210\u8d28\u91cf<br/>Entropy-guided methods improve generation quality)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"[arXiv251225] Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"tags:"})," [ai], [computational neuroscience], [predictive coding, EEG, neural coherence, optical flow, entropy-based feature selection]"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"authors:"})," Sean C. Borneman, Julia Krebs, Ronnie B. Wilbur, Evie A. Malaia"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"institution:"})," Carnegie-Mellon University, University of Salzburg, Purdue University, University of Alabama"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"link:"})," ",(0,a.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.20929",children:"https://arxiv.org/pdf/2512.20929"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"contributions:"})," 1. A novel machine learning framework for decoding EEG responses to dynamic visual language (sign language) using spatiotemporal neural coherence. 2. The identification of frequency-specific neural signatures (distributed left-hemispheric and frontal low-frequency coherence) that differentiate linguistic from non-linguistic visual input. 3. Demonstration of experience-dependent neural signatures correlating with age, linking lifelong exposure to the shaping of internal generative models for visual language."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"thumbnail:"})," ",(0,a.jsx)(e.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d4252b4df430bb18f969d798aff092b793b8033ce3e2ca01f2a17d6aeff53f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d4252b4df430bb18f969d798aff092b793b8033ce3e2ca01f2a17d6aeff53f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning framework that uses coherence between EEG signals and optical flow features to decode predictive neural dynamics in Deaf signers watching sign language. The method identifies specific low-frequency neural signatures crucial for language comprehension and shows these signatures are experience-dependent. The work provides a novel multimodal approach for probing the brain's generative models of visual language perception."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Mindmap:"})}),"\n",(0,a.jsx)(e.mermaid,{value:"graph LR\n    A[Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence<br>\u901a\u8fc7\u65f6\u7a7a\u795e\u7ecf\u4e00\u81f4\u6027\u89e3\u7801\u89c6\u89c9\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u9884\u6d4b\u63a8\u7406] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How does the brain perform predictive inference during visual language (sign language) comprehension?<br>\u5927\u8111\u5982\u4f55\u5728\u89c6\u89c9\u8bed\u8a00\uff08\u624b\u8bed\uff09\u7406\u89e3\u4e2d\u8fdb\u884c\u9884\u6d4b\u63a8\u7406\uff1f)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: A machine learning framework using EEG-optical flow coherence & entropy-based feature selection.<br>\u4f7f\u7528EEG-\u5149\u6d41\u4e00\u81f4\u6027\u53ca\u57fa\u4e8e\u71b5\u7684\u7279\u5f81\u9009\u62e9\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Identified left-hemispheric/frontal low-frequency coherence as key; neural signatures are experience-dependent.<br>\u53d1\u73b0\u5de6\u534a\u7403/\u524d\u989d\u4f4e\u9891\u4e00\u81f4\u6027\u662f\u5173\u952e\uff1b\u795e\u7ecf\u7279\u5f81\u5177\u6709\u7ecf\u9a8c\u4f9d\u8d56\u6027\u3002)"}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function o(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>l});var s=i(6540);const a={},r=s.createContext(a);function t(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);