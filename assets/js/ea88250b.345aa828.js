"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5740],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(96540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}},85772:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"daily/cs_LG/20251229-20260104","title":"20251229-20260104 (cs.LG)","description":"2025-12-29","source":"@site/docs/daily/cs_LG/20251229-20260104.md","sourceDirName":"daily/cs_LG","slug":"/daily/cslg/20251229-20260104","permalink":"/ai_toutiao/daily/cslg/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767087759000,"frontMatter":{"slug":"/daily/cslg/20251229-20260104"},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228 (cs.LG)","permalink":"/ai_toutiao/daily/cslg/20251222-20251228"},"next":{"title":"cs.LO","permalink":"/ai_toutiao/category/cslo"}}');var r=i(74848),a=i(28453);const t={slug:"/daily/cslg/20251229-20260104"},o="20251229-20260104 (cs.LG)",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2},{value:"2025-12-30",id:"2025-12-30",level:2}];function c(e){const n={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mermaid:"mermaid",mi:"mi",mo:"mo",mover:"mover",mrow:"mrow",msqrt:"msqrt",p:"p",path:"path",semantics:"semantics",span:"span",strong:"strong",svg:"svg",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251229-20260104-cslg",children:"20251229-20260104 (cs.LG)"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [data spaces, cloud-edge continuum, containerized microservices, edge AI, intelligent infrastructure monitoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Spark Works Ltd., IONOS SE, Iquadrat Inform\xe1tica S.L."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21340",children:"https://arxiv.org/pdf/2512.21340"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Harnessing Data Spaces for Smart City Infrastructures] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [physics-informed machine learning], [physics-informed neural networks, Floquet-Bloch eigenvalue problem, honeycomb lattice, band structure, transfer learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Haaris Mian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Columbia University (Program in Applied Mathematics, Department of Applied Physics and Applied Mathematics)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21349",children:"https://arxiv.org/pdf/2512.21349"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem for particles in 2D periodic potentials, using neural networks to learn Bloch functions and eigenvalues simultaneously. 2. A mesh-free solver that enforces the Schr\xf6dinger equation, Bloch periodicity, and normalization via a composite loss function without supervision. 3. Demonstration of transfer learning to adapt the solver from nearly-free to strongly varying potentials, capturing changes in band topology."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18928753702626417cc700c2238d86a4711adfe422a6c1995a68fbcbf3401d4f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This thesis proposes a physics-informed neural network framework to solve quantum eigenproblems for particles in periodic potentials, specifically targeting the honeycomb lattice. The method learns Bloch wavefunctions and their energies by training over the Brillouin zone with a loss function that encodes the governing physics. It is validated against traditional methods and shows promise for exploring band structure topology through transfer learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6c42\u89e3\u4e8c\u7ef4\u5468\u671f\u52bf\u4e2d\u7684Floquet-Bloch\u672c\u5f81\u503c\u95ee\u9898/Solve Floquet-Bloch eigenvalue problem in 2D periodic potentials]\n    B --\x3e B2[\u5173\u6ce8\u77f3\u58a8\u70ef\u7b49\u6750\u6599\u7684\u8702\u7a9d\u6676\u683c\u548c\u80fd\u5e26\u62d3\u6251/Focus on honeycomb lattice & band topology for materials like graphene]\n    C --\x3e C1[\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u540c\u65f6\u5b66\u4e60\u5e03\u6d1b\u8d6b\u51fd\u6570\u548c\u672c\u5f81\u503c/Use neural networks to learn Bloch functions & eigenvalues]\n    C --\x3e C2[\u901a\u8fc7\u590d\u5408\u635f\u5931\u51fd\u6570\u5f3a\u5236\u6267\u884c\u859b\u5b9a\u8c14\u65b9\u7a0b\u548c\u5468\u671f\u6027/Enforce Schr\xf6dinger eq. & periodicity via composite loss]\n    C --\x3e C3[\u5728\u5e03\u91cc\u6e0a\u533a\u8bad\u7ec3\u5e76\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60/Train over Brillouin zone & explore transfer learning]\n    D --\x3e D1[\u6570\u503c\u9a8c\u8bc1\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e00\u81f4/Numerical validation against traditional methods]\n    D --\x3e D2[\u8fc1\u79fb\u5b66\u4e60\u6355\u6349\u80fd\u5e26\u62d3\u6251\u53d8\u5316/Transfer learning captures changes in band topology]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [synthetic data generation, reinforcement learning, proximal policy optimization, privacy, biomedical data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Natalia Espinosa-Dice, Nicholas J. Jackson, Chao Yan, Aaron Lee, Bradley A. Malin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University, Vanderbilt University Medical Center, Washington University in St. Louis"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21395",children:"https://arxiv.org/pdf/2512.21395"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Reframes synthetic data generation (SDG) as a reinforcement learning problem, introducing a novel perspective. 2. Proposes RLSyn, a framework that models the data generator as a stochastic policy optimized via Proximal Policy Optimization with discriminator-derived rewards for stable, data-efficient training. 3. Demonstrates the effectiveness of the RL approach, showing it performs comparably to or better than GANs and diffusion models, especially in data-scarce regimes on biomedical datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75d9e0c3d2cba88654d16f9652042680f0037508065d6d94fba27208a6199969_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes RLSyn, a reinforcement learning framework for generating synthetic biomedical data by modeling the generator as a policy optimized with PPO. It shows that this approach achieves performance comparable to or better than GANs and diffusion models, particularly when training data is limited, offering a stable and data-efficient alternative for privacy-preserving data sharing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Reinforcement Learning Approach to Synthetic Data Generation] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: State-of-the-art generative models need large datasets and complex training, limiting use in small-sample settings.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Reframe SDG as RL; introduce RLSyn (stochastic policy optimized via PPO with discriminator rewards).]\n    D[\u5173\u952e\u7ed3\u679c/Results: RLSyn performs comparably to/better than GANs & diffusion models, especially on smaller datasets.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [storage systems], [constrained coding, LOCO codes, linear programming, code reconfiguration, two-dimensional magnetic recording (TDMR)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Do\u011fukan \xd6zbayrak, Ahmed Hareedy"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Middle East Technical University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21396",children:"https://arxiv.org/pdf/2512.21396"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes offline and online learning methods to reconfigure constrained coding schemes based on actual device status, moving beyond predetermined time stamps. 2. Models the reconfiguration problem as a linear programming optimization to maximize storage capacity and/or minimize decoding complexity, proving global optimality. 3. Demonstrates the effectiveness of the approach through experimental results in Two-Dimensional Magnetic Recording (TDMR) systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/602c83173fa9054dd0b0d4fea2c1b1c7d235aa475323c43a09847363ee851c20_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of optimally reconfiguring constrained coding schemes in storage devices as they age. The authors propose offline and online learning methods that fit device status data to polynomial models and formulate the reconfiguration decision as a linear programming problem to maximize capacity or minimize complexity. Experimental results show the proposed method is effective for TDMR systems and can be extended to other storage or transmission systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u8bbe\u5907\u8001\u5316\u9700\u8981\u4e0d\u540c\u7b49\u7ea7\u7684\u6570\u636e\u4fdd\u62a4/Device aging requires different levels of data protection")\n    Problem --\x3e P2("\u57fa\u4e8e\u9884\u5b9a\u65f6\u95f4\u6233\u7684\u91cd\u914d\u7f6e\u5ffd\u7565\u5b9e\u9645\u72b6\u6001/Reconfiguration based on timestamps neglects actual device status")\n    Method --\x3e M1("\u63d0\u51fa\u79bb\u7ebf\u548c\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5/Propose offline and online learning methods")\n    Method --\x3e M2("\u5c06\u8bad\u7ec3\u6570\u636e\u62df\u5408\u4e3a\u591a\u9879\u5f0f\u65b9\u7a0b/Fit training data to polynomial equations")\n    Method --\x3e M3("\u5c06\u91cd\u914d\u7f6e\u5efa\u6a21\u4e3a\u7ebf\u6027\u89c4\u5212\u95ee\u9898/Model reconfiguration as a linear programming problem")\n    Results --\x3e R1("\u89e3\u51b3\u65b9\u6848\u662f\u5168\u5c40\u6700\u4f18\u7684/Solution is globally optimal")\n    Results --\x3e R2("\u5b9e\u9a8c\u8bc1\u660e\u5728TDMR\u7cfb\u7edf\u4e2d\u6709\u6548/Experiments demonstrate effectiveness in TDMR systems")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [dynamical systems learning], [Koopman operator, transfer operator, spectral decomposition, scikit-learn API, reduced-order models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Giacomo Turri, Gr\xe9goire Pacreau, Giacomo Meanti, Timoth\xe9e Devergne, Daniel Ordonez, Erfan Mirzaei, Bruno Belucci, Karim Lounici, Vladimir Kostic, Massimiliano Pontil, Pietro Novelli"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Italian Institute of Technology, \xc9cole Polytechnique, Inria, University College London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21409",children:"https://arxiv.org/pdf/2512.21409"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Machine-Learning-Dynamical-Systems/kooplearn",children:"https://github.com/Machine-Learning-Dynamical-Systems/kooplearn"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a unified library implementing linear, kernel, and deep-learning estimators for both discrete-time (Koopman/Transfer) and continuous-time evolution operators. 2. Offers a scikit-learn compatible API for easy integration into existing ML workflows. 3. Includes curated benchmark datasets to support experimentation, reproducibility, and fair algorithm comparison."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6abf4d9f1d18bb64fbf627b5a4cc8d5b0c12b9f7fda20d8f6811df3f96602779_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces kooplearn, a Python library for learning evolution operators (like Koopman and transfer operators) from dynamical systems data. It provides a suite of estimators and a scikit-learn compatible interface to enable spectral analysis, reduced-order modeling, and forecasting. The library aims to standardize and facilitate research in data-driven dynamical systems modeling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[kooplearn: A Scikit-Learn Compatible Library] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Need for a unified, easy-to-use library for learning evolution operators from dynamical systems data] --\x3e Problem_Sub1[\u5e94\u7528/Application: Analyze systems, forecast, reduce model dimension]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Implement linear, kernel, and deep-learning estimators] --\x3e Method_Sub1[\u63a5\u53e3/Interface: Scikit-learn compatible API]\n    Method --\x3e Method_Sub2[\u6a21\u578b/Model: Discrete-time (Koopman/Transfer) & continuous-time operators]\n    Results[\u5173\u952e\u7ed3\u679c/Results: kooplearn library released] --\x3e Results_Sub1[\u7279\u6027/Features: Includes benchmark datasets for fair comparison]\n    Results --\x3e Results_Sub2[\u76ee\u6807/Goal: Facilitate integration, experimentation, and reproducibility]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [tool-use framework, vision-language model, interpretability, data-efficiency, tool bottleneck model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," California Institute of Technology, Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21414",children:"https://arxiv.org/pdf/2512.21414"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/christinaliu2020/tool-bottleneck-framework",children:"https://github.com/christinaliu2020/tool-bottleneck-framework"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Tool Bottleneck Framework for Medical Image Understanding] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Text-based tool composition fails for medical images with localized features]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches or beats baselines, more interpretable, data-efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [wireless networking], [Age of Information (AoI), reinforcement learning, freshness optimization, wireless networks, multi-agent systems]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alimu Alibotaiken, Suyang Wang, Oluwaseun T. Ajayi, Yu Cheng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Illinois Institute of Technology, California State University, San Bernardino"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21412",children:"https://arxiv.org/pdf/2512.21412"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel taxonomy for Age of Information (AoI) and its variants, categorizing them into native, function-based, and application-oriented families to clarify freshness modeling for B5G/6G systems. 2. Introduces a policy-centric taxonomy for reinforcement learning in freshness-aware networks, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. 3. Synthesizes recent RL-driven freshness control progress and highlights open challenges like delayed decision processes and cross-layer design to establish a unified foundation for learning-based freshness optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b58e6fe193d4299ab0beca4eb949d8b13e21e8198c223c3dd6c0ca64896bbf7d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey addresses the gap between classical Age of Information (AoI) studies and broad reinforcement learning (RL) discussions in wireless networks by examining RL specifically for freshness optimization. It organizes AoI variants and introduces a policy-centric RL taxonomy to provide a coherent framework for freshness-aware decision-making in next-generation wireless systems. The paper aims to establish a unified foundation for learning-based freshness control and highlights key open challenges for future research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u7efc\u8ff0\u7684\u4e0d\u8db3: \u7ecf\u5178AoI\u4e0e\u6cdb\u5316RL\u7814\u7a76\u5206\u79bb / Gap: Classical AoI vs. Broad RL]\n    C --\x3e C1[\u63d0\u51fa\u4ee5AoI\u4e3a\u4e2d\u5fc3\u7684RL\u7efc\u8ff0\u6846\u67b6 / Propose AoI-centric RL Survey Framework]\n    C --\x3e C2[\u6784\u5efaAoI\u53d8\u4f53\u5206\u7c7b\u4e0e\u7b56\u7565\u4e2d\u5fc3\u5206\u7c7b\u6cd5 / Build AoI Variant & Policy-Centric Taxonomies]\n    D --\x3e D1[\u4e3aB5G/6G\u5efa\u7acb\u5b66\u4e60\u5f0f\u65b0\u9c9c\u5ea6\u4f18\u5316\u7684\u7edf\u4e00\u57fa\u7840 / Establish Unified Foundation for Learning-based Freshness Optimization]\n    D --\x3e D2[\u8bc6\u522b\u5f00\u653e\u6311\u6218: \u5ef6\u8fdf\u51b3\u7b56\u3001\u968f\u673a\u6027\u3001\u8de8\u5c42\u8bbe\u8ba1 / Identify Open Challenges: Delayed Decisions, Stochasticity, Cross-layer]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Scalable Deep Subspace Clustering Network"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [subspace clustering], [landmark-based approximation, self-expression, spectral clustering, linear complexity, convolutional auto-encoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Quebec at Montreal"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21434",children:"https://arxiv.org/pdf/2512.21434"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n\xd7n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Deep Subspace Clustering Network"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: O(n^3) \u8ba1\u7b97\u590d\u6742\u5ea6 / O(n^3) Computational Complexity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u5730\u6807\u8fd1\u4f3c\u4e0e\u8054\u5408\u4f18\u5316 / Landmark Approximation & Joint Optimization"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u7ebf\u6027\u590d\u6742\u5ea6\u4e0e\u53ef\u6bd4\u6027\u80fd / Linear Complexity & Comparable Performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [se], [software testing], [runtime error detection, coverage-guided testing, multi-agent reasoning, large language models, static analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hridya Dhulipala, Xiaokai Rong, Tien N. Nguyen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Dallas"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21431",children:"https://arxiv.org/pdf/2512.21431"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Cerberus, a novel predictive, execution-free coverage-guided testing framework that uses LLMs for input generation, coverage prediction, and error detection without code execution. 2. Introduces a two-phase feedback loop that first maximizes code coverage and detects errors, then focuses solely on error detection after coverage is maximized, improving performance over single-phase prompting. 3. Empirically demonstrates that Cerberus outperforms conventional and learning-based testing frameworks for both complete and incomplete code snippets by generating high-coverage test cases more efficiently and discovering more runtime errors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d41379a4c8476f7ed1a8a02b193c5fe427e6a274d56beccd85313ce47ba5e76_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Cerberus, a framework that uses Large Language Models (LLMs) to statically detect runtime errors in code snippets without execution. It employs a multi-agent reasoning approach with a two-phase, coverage-guided feedback loop to generate test inputs and predict errors. The evaluation shows Cerberus is more efficient and effective at finding runtime errors than existing testing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Detecting runtime errors in code snippets without execution is crucial for software safety.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Uses LLMs for execution-free, coverage-guided testing with a two-phase feedback loop.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms conventional and learning-based frameworks by generating high-coverage tests and finding more errors.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [lossy compression, quality prediction, deep-surrogate, mixture-of-experts, feature-extraction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Luki\u0107, Franck Cappello"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21433",children:"https://arxiv.org/pdf/2512.21433"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DeepCQ: \u901a\u7528\u6df1\u5ea6\u4ee3\u7406\u6846\u67b6\u7528\u4e8e\u6709\u635f\u538b\u7f29\u8d28\u91cf\u9884\u6d4b] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bc4\u4f30\u538b\u7f29\u540e\u6570\u636e\u8d28\u91cf\u7684\u8ba1\u7b97\u6210\u672c\u9ad8/Expensive to assess post-compression data quality]\n    C --\x3e C1[\u4e24\u9636\u6bb5\u8bbe\u8ba1: \u7279\u5f81\u63d0\u53d6 + \u8f7b\u91cf\u9884\u6d4b/Two-stage design: feature extraction + lightweight prediction]\n    C --\x3e C2[\u4e13\u5bb6\u6df7\u5408\u8bbe\u8ba1\u5904\u7406\u65f6\u53d8\u6570\u636e/Mixture-of-experts for time-evolving data]\n    D --\x3e D1[\u9884\u6d4b\u8bef\u5dee\u666e\u904d\u4f4e\u4e8e10%/Prediction errors generally under 10%]\n    D --\x3e D2[\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5/Significantly outperforms existing methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [computational ethics], [moral context, probabilistic clustering, LLM semantics, interpretable prediction, human judgment]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Intelligent Systems and Robotics, Sorbonne University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21439",children:"https://arxiv.org/pdf/2512.21439"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[COMETH: Learning Interpretable Moral Contexts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Moral judgments are context-dependent]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Probabilistic clustering + LLM semantics + Human judgments]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Doubles alignment with human judgments vs. LLM prompting]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [se], [fuzz testing], [initial corpus generation, large language models, multi-agent framework, predictive code coverage, mutation-based fuzzing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hridya Dhulipala, Xiaokai Rong, Aashish Yadavally, Tien N. Nguyen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Dallas"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21440",children:"https://arxiv.org/pdf/2512.21440"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FuzzWise, a novel method that integrates initial corpus generation and minimization into a single, streamlined process using an LLM-based multi-agent framework., 2. Introduces a predictive code coverage module (an LLM agent) that assesses new test cases without requiring actual program execution, saving computational resources., 3. Demonstrates empirically that FuzzWise generates a smaller, higher-quality initial corpus that achieves higher code coverage and triggers more runtime errors more efficiently than baseline methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcb8eafec283e39ae04e0274dc4688aced924346193560581e8469f1151507f6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of generating a high-quality initial seed corpus for mutation-based fuzzing. It proposes FuzzWise, a method that uses a multi-agent LLM framework to generate and intelligently select test cases based on predicted coverage without execution. The evaluation shows FuzzWise produces a smaller, more effective corpus that achieves higher coverage and finds more bugs efficiently."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FuzzWise: Intelligent Initial Corpus Generation for Fuzzing] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u4e3a\u6a21\u7cca\u6d4b\u8bd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u79cd\u5b50\u8bed\u6599\u5e93/Generating high-quality initial seed corpus for fuzzing]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u96c6\u6210\u751f\u6210\u4e0e\u9884\u6d4b\u6027\u8986\u76d6\u8bc4\u4f30/LLM-based multi-agent framework integrating generation and predictive coverage assessment]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u7528\u66f4\u5c11\u7684\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u73b0\u66f4\u9ad8\u7684\u4ee3\u7801\u8986\u76d6\u7387\u548c\u9519\u8bef\u53d1\u73b0\u7387/Achieves higher code coverage and bug detection with fewer test cases]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [masked diffusion language models, reinforcement learning, parallel decoding, on-policy optimization, unmasking planner]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Washington, University of California, Berkeley"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21446",children:"https://arxiv.org/pdf/2512.21446"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards "diffusion supremacy" over autoregressive models.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[MDLMs\u89e3\u7801\u6162\uff0c\u901f\u5ea6\u4f18\u52bf\u6709\u9650/MDLMs decode slowly, limiting speed advantage]\n    C --\x3e C1[\u57fa\u4e8eGRPO\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6/On-policy RL framework based on GRPO]\n    C --\x3e C2[\u8054\u5408\u4f18\u5316\u6269\u6563\u6a21\u578b\u4e0e\u89e3\u63a9\u7801\u89c4\u5212\u5668/Jointly optimize diffusion model & unmasking planner]\n    D --\x3e D1[\u63d0\u5347\u7cbe\u5ea6-\u6548\u7387\u6743\u8861/Improves accuracy-efficiency trade-off]\n    D --\x3e D2[\u8fc8\u5411"\u6269\u6563\u9738\u6743"/Moving towards "diffusion supremacy"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] An Equivariance Toolbox for Learning Dynamics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [learning theory], [equivariance, Noether's theorem, Hessian constraints, learning dynamics, symmetry]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yongyi Yang, Liu Ziyin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Michigan, Massachusetts Institute of Technology, NTT Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21447",children:"https://arxiv.org/pdf/2512.21447"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a general equivariance framework that unifies first-order constraints (conservation laws, implicit bias) into a single identity. 2. Extended the analysis to second-order, providing structural predictions about curvature, flat/sharp directions, and gradient-Hessian alignment. 3. Generalized classical symmetry analyses from continuous transformations to include general equivariance and discrete transformations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69f03ddf87a64feb67cd904eadbbfc83a393f0b282be28c74e5bfeac489db50b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper develops a unified equivariance toolbox for analyzing learning dynamics in neural networks. It extends classical symmetry-based analyses to derive coupled first- and second-order constraints, connecting transformation structure to loss landscape geometry. The framework recovers known results and provides new characterizations linking equivariance to modern optimization phenomena."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[An Equivariance Toolbox for Learning Dynamics] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u5206\u6790\u662f\u7279\u5b9a\u95ee\u9898\u7684/Existing analyses are problem-specific]\n    Problem --\x3e P2[\u4e8c\u9636\u7ed3\u6784\u7406\u89e3\u4e0d\u8db3/Second-order structure less understood]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u6784\u5efa\u7b49\u53d8\u6027\u5de5\u5177\u7bb1/Build general equivariance toolbox]\n    Method --\x3e M2[\u6269\u5c55\u8bfa\u7279\u5b9a\u7406\u5206\u6790/Extend Noether-type analyses]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u7edf\u4e00\u4e00\u9636\u7ea6\u675f/Unify first-order constraints]\n    Results --\x3e R2[\u63d0\u4f9b\u4e8c\u9636\u7ed3\u6784\u9884\u6d4b/Provide second-order structural predictions]\n    Results --\x3e R3[\u8fde\u63a5\u53d8\u6362\u7ed3\u6784\u4e0e\u51e0\u4f55/Connect transformation structure to geometry]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] RLLaVA: An RL-central Framework for Language and Vision Assistants"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [reinforcement learning, vision-language models, Markov decision process, resource-efficient training, modular framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lei Zhao, Zihao Ma, Boyu Lin, Yuhe Liu, Wenjun Wu, Lei Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beihang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21450",children:"https://arxiv.org/pdf/2512.21450"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/TinyLoopX/RLLaVA",children:"https://github.com/TinyLoopX/RLLaVA"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a modular RL framework (RLLaVA) that decouples algorithmic logic from model architecture and distributed execution, enabling easy implementation of new RL algorithms. 2. Formulates the joint visual-textual sequential decision of VLMs as a unified Markov Decision Process (MDP), providing a principled foundation for multi-modal RL. 3. Achieves resource-efficient training, enabling end-to-end full-parameter updates for up to 4B-scale models on a single 24GB GPU."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c1815245107c8b5c717a57c722eeb64e0b9b4b77c5a989f0d2b9f4353b36df_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces RLLaVA, a lightweight and modular reinforcement learning framework specifically designed for training vision-language models. It decouples RL logic from system components to simplify algorithm development and enables efficient training of large models on common GPUs. Experiments show that models trained with RLLaVA improve over base models and are competitive with other specialized RL frameworks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RLLaVA: An RL-central Framework for Language and Vision Assistants] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u7f3a\u4e4f\u4e13\u95e8\u7684\u591a\u6a21\u6001RL\u6846\u67b6 / Lack of specialized multi-modal RL framework]\n    B --\x3e B2[\u73b0\u6709\u6846\u67b6\u8d44\u6e90\u6d88\u8017\u5927 / Existing frameworks are resource-intensive]\n    C --\x3e C1[\u89e3\u8026RL\u903b\u8f91\u4e0e\u67b6\u6784 / Decouple RL logic from architecture & execution]\n    C --\x3e C2[\u7edf\u4e00MDP\u5efa\u6a21 / Unified MDP formulation for VLMs]\n    C --\x3e C3[\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u4e0e\u6a21\u578b / Supports broad RL methods & VLMs]\n    D --\x3e D1[\u8d44\u6e90\u9ad8\u6548\u8bad\u7ec3 / Resource-efficient training (e.g., 4B model on 24GB GPU)]\n    D --\x3e D2[\u6027\u80fd\u63d0\u5347 / Models show improved performance]\n    D --\x3e D3[\u4efb\u52a1\u53ef\u6269\u5c55\u6027 / Task extensibility demonstrated]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] CCAD: Compressed Global Feature Conditioned Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [global feature conditioning, adaptive compression, reconstruction-based anomaly detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21459",children:"https://arxiv.org/pdf/2512.21459"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/chloeqxq/CCAD",children:"https://github.com/chloeqxq/CCAD"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5f02\u5e38\u68c0\u6d4b\u5728\u6709\u9650\u5f02\u5e38\u6570\u636e\u4e0b\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u6548\u7387\u548c\u7ea6\u675f\u4e0a\u7684\u4e0d\u8db3]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faCCAD\uff0c\u878d\u5408\u91cd\u5efa\u4e0e\u8868\u5f81\u65b9\u6cd5\uff0c\u4f7f\u7528\u538b\u7f29\u7684\u5168\u5c40\u7279\u5f81\u4f5c\u4e3a\u91cd\u5efa\u6a21\u578b\u7684\u6761\u4ef6]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728AUC\u4e0a\u8d85\u8d8aSOTA\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u8d21\u732e\u4e86\u91cd\u65b0\u6807\u6ce8\u7684DAGM 2007\u6570\u636e\u96c6]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [LSTM, SARIMA, conformal prediction, counterfactual estimation, uncertainty quantification]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sukanya Krishna, Marie-Laure Charpignon, Maimuna Majumder"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harvard University, Boston Children's Hospital, Broad Institute of MIT and Harvard, Massachusetts Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21456",children:"https://arxiv.org/pdf/2512.21456"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a systematic empirical comparison showing LSTM outperforms SARIMA in point estimation and uncertainty calibration for counterfactual mortality projection under regime change. 2. Provided analysis that attention-based models (Seq2Seq, Transformer) underperform in this task due to overfitting to historical means. 3. Developed a reproducible pipeline incorporating conformal prediction intervals and extensive convergence analysis, providing an open-source framework deployable for public health planning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f7ae15bb7d75d1d53ce8df2d4924f6311b8bfce998ef81244521cd5679c4225_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares traditional statistical (SARIMA) and deep learning models (LSTM, Seq2Seq, Transformer) for estimating substance overdose excess mortality in the US during the COVID-19 pandemic. The study finds that LSTM provides more accurate and better-calibrated counterfactual estimates than SARIMA, establishing that carefully validated deep learning models can be more reliable for public health planning under structural disruptions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f30\u8ba1\u75ab\u60c5\u5bfc\u81f4\u7684\u836f\u7269\u8fc7\u91cf\u8d85\u989d\u6b7b\u4ea1\u7387/Estimating pandemic-attributable excess mortality]\n    B --\x3e B2[\u4f20\u7edf\u65b9\u6cd5\u5728\u7ed3\u6784\u6027\u53d8\u5316\u4e0b\u53ef\u80fd\u5931\u6548/Traditional methods may fail under structural change]\n    C --\x3e C1[\u7cfb\u7edf\u6bd4\u8f83SARIMA\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b/Systematic comparison of SARIMA vs. DL models]\n    C --\x3e C2[\u4f7f\u7528CDC\u6570\u636e(2015-2019)\u8fdb\u884c\u8bad\u7ec3/Using CDC data (2015-2019) for training]\n    C --\x3e C3[\u9884\u6d4b2020-2023\u5e74\u7684\u53cd\u4e8b\u5b9e\u8f68\u8ff9/Projecting counterfactual trajectories for 2020-2023]\n    D --\x3e D1[LSTM\u5728\u70b9\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u4e0a\u8868\u73b0\u6700\u4f73/LSTM achieves best point estimation & uncertainty calibration]\n    D --\x3e D2[\u6ce8\u610f\u529b\u6a21\u578b\u56e0\u8fc7\u62df\u5408\u5386\u53f2\u5747\u503c\u800c\u8868\u73b0\u4e0d\u4f73/Attention models underperform due to overfitting to historical means]\n    D --\x3e D3[\u63d0\u4f9b\u53ef\u90e8\u7f72\u7684\u5f00\u6e90\u6846\u67b6/Providing a deployable open-source framework]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [tensor decomposition], [Bayesian tensor completion, multioutput Gaussian processes, variational inference, rank learning, functional universality]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Siyuan Li, Shikai Fang, Lei Cheng, Feng Yin, Yik-Chung Wu, Peter Gerstoft, Sergios Theodoridis"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang University, The Chinese University of Hong Kong, Shenzhen, The University of Hong Kong, Technical University of Denmark, University of California, San Diego, Athena R.C."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21486",children:"https://arxiv.org/pdf/2512.21486"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/OceanSTARLab/RR-FBTC",children:"https://github.com/OceanSTARLab/RR-FBTC"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a rank-revealing functional Bayesian tensor completion (RR-FBTC) method that handles tensors with real-valued indices and automatically determines the tensor rank during inference. 2. Establishes the universal approximation property of the model, demonstrating its expressive power for continuous multi-dimensional signals. 3. Derives an efficient variational inference algorithm with closed-form updates for learning the model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac46f14d99db0300a24117fcc6a1f29a54c4ad4ae1586354da1e156d0b048995_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of determining the optimal rank in functional tensor decomposition, which is NP-hard. It proposes a new method called RR-FBTC that uses multioutput Gaussian processes to model latent functions, enabling automatic rank learning and functional universality. Experiments show the method is effective and superior to state-of-the-art approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u9700\u5df2\u77e5\u5f20\u91cf\u79e9/Existing methods require known tensor rank]\n    B --\x3e B2[\u786e\u5b9a\u6700\u4f18\u79e9\u662fNP\u96be\u95ee\u9898/Determining optimal rank is NP-hard]\n    C --\x3e C1[\u63d0\u51faRR-FBTC\u65b9\u6cd5/Propose RR-FBTC method]\n    C --\x3e C2[\u4f7f\u7528\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21/Model with multioutput Gaussian processes]\n    C --\x3e C3[\u53d8\u5206\u63a8\u65ad\u4e0e\u95ed\u5f0f\u66f4\u65b0/Variational inference with closed-form updates]\n    D --\x3e D1[\u8bc1\u660e\u6cdb\u51fd\u903c\u8fd1\u80fd\u529b/Prove functional universal approximation]\n    D --\x3e D2[\u5b9e\u73b0\u81ea\u52a8\u79e9\u5b66\u4e60/Achieve automatic rank learning]\n    D --\x3e D3[\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027/Experiments validate effectiveness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [wearable sensing, actigraphy encoder, projection module, frozen LLM, behavioral summarization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dartmouth College"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21506",children:"https://arxiv.org/pdf/2512.21506"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [incomplete multi-view clustering, missing pattern tree, decision ensemble, knowledge distillation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21510",children:"https://arxiv.org/pdf/2512.21510"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Inconsistent missing patterns in multi-view data limit clustering performance."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art IMVC performance and superior robustness."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [differential privacy, convergence guarantees, partial client participation, local updates, clipping bias]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Egor Shulgin, Grigory Malinovsky, Sarit Khirirat, Peter Richt\xe1rik"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," King Abdullah University of Science and Technology (KAUST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21521",children:"https://arxiv.org/pdf/2512.21521"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Fed-\u03b1-NormEC, the first DP FL framework with provable convergence under standard assumptions (without bounded gradients or heterogeneity). 2. Fully supports practical FL features like multiple local updates and, crucially, partial client participation. 3. Provides theoretical analysis showing how partial client participation is essential for real-world deployment and vital for privacy amplification."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffad28b47b2302842557d1ba4a799ee36a241e4ab2529611ee61b38dd671f76d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap between theoretical private federated learning methods, which rely on unrealistic assumptions, and practical deployment needs. It proposes Fed-\u03b1-NormEC, a new framework that provides provable differential privacy and convergence guarantees while supporting key practical features like local updates and partial client participation. Experiments on private deep learning tasks validate the theoretical findings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[First Provable Guarantees for Practical Private FL] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u73b0\u6709\u79c1\u6709FL\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u5047\u8bbe/Existing private FL relies on unrealistic assumptions]\n    Problem --\x3e P2[\u5ffd\u7565\u672c\u5730\u66f4\u65b0\u4e0e\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e/Neglects local updates & partial participation]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faFed-\u03b1-NormEC\u6846\u67b6/Propose Fed-\u03b1-NormEC framework]\n    Method --\x3e M2[\u96c6\u6210\u672c\u5730\u66f4\u65b0\u4e0e\u72ec\u7acb\u5b66\u4e60\u7387/Integrates local updates & separate stepsizes]\n    Method --\x3e M3[\u652f\u6301\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e/Supports partial client participation]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6536\u655b\u4e0eDP\u4fdd\u8bc1/Provides provable convergence & DP guarantees]\n    Results --\x3e R2[\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba/Experiments corroborate theory]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [continual pre-training, scaling laws, perplexity, data selection, knowledge gap]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lei Liu, Hao Zhu, Yue Shen, Zhixuan Chu, Jian Wang, Jinjie Gu, Kui Ren"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ant Group, Zhejiang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21515",children:"https://arxiv.org/pdf/2512.21515"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel perplexity-aware data scaling law that predicts model test loss from the perplexity landscape of domain data, moving beyond dataset size. 2. Introduces the concept of "perplexity landscapes" to quantify the informational value and knowledge gap of candidate training samples. 3. Enables adaptive selection of high-utility data subsets for Continual Pre-training, improving efficiency and performance by prioritizing informative content and reducing redundancy.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d01b1cad6e908309c7e813cb1d98f2c41345aa1e4d78cbdaf163996c5d111b4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the inefficiency of scaling data for Continual Pre-training (CPT) of LLMs, where simply adding more data yields diminishing returns. The authors propose a new scaling law that uses the model's perplexity on domain data as a proxy for the knowledge gap, allowing for the predictive selection of optimal training subsets. Experiments show this method consistently identifies high-utility data, leading to superior performance on domain-specific benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Perplexity-Aware Data Scaling Law<br>\u56f0\u60d1\u5ea6\u611f\u77e5\u6570\u636e\u7f29\u653e\u5b9a\u5f8b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[CPT\u4e2d\u5355\u7eaf\u589e\u52a0\u6570\u636e\u6536\u76ca\u9012\u51cf<br>Diminishing returns from scaling data in CPT]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u666f\u89c2\u7684\u7f29\u653e\u5b9a\u5f8b<br>Propose perplexity-landscape-based scaling law]\n    C1 --\x3e C2[\u5229\u7528\u56f0\u60d1\u5ea6\u91cf\u5316\u77e5\u8bc6\u5dee\u8ddd<br>Use perplexity to quantify knowledge gap]\n    C2 --\x3e C3[\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u4ef7\u503c\u6570\u636e\u5b50\u96c6<br>Adaptively select high-utility data subsets]\n    D --\x3e D1[\u8bc6\u522b\u63a5\u8fd1\u6700\u4f18\u7684\u8bad\u7ec3\u5b50\u96c6<br>Identifies near-optimal training subsets]\n    D1 --\x3e D2[\u5728\u9886\u57df\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd<br>Achieves superior performance on domain benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-view clustering], [contrastive learning, incomplete multi-view data, noise-robust clustering, graph-guided learning, imputation-free]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21516",children:"https://arxiv.org/pdf/2512.21516"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>\u5168\u5c40-\u5c40\u90e8\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>Rare-paired & Mis-paired Samples<br/>\u6837\u672c\u914d\u5bf9\u7a00\u5c11\u4e0e\u9519\u8bef] --\x3e B1[Incomplete & Noise Multi-View Data<br/>\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u591a\u89c6\u56fe\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Unified Contrastive Learning Framework<br/>\u7edf\u4e00\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6] --\x3e C1[Global-Graph Guided CL<br/>\u5168\u5c40\u56fe\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60]\n    C --\x3e C2[Local-Graph Weighted CL<br/>\u5c40\u90e8\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60]\n    C1 --\x3e C1a[Construct Global Affinity Graph<br/>\u6784\u5efa\u5168\u5c40\u4eb2\u548c\u529b\u56fe]\n    C2 --\x3e C2a[Generate Adaptive Weights<br/>\u751f\u6210\u81ea\u9002\u5e94\u6743\u91cd]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Superior Clustering Performance<br/>\u4f18\u8d8a\u7684\u805a\u7c7b\u6027\u80fd] --\x3e D1[Outperforms SOTA Methods<br/>\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5]\n    D --\x3e D2[Effective on Incomplete & Noise Data<br/>\u5728\u4e0d\u5b8c\u6574\u4e0e\u566a\u58f0\u6570\u636e\u4e0a\u6709\u6548]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Generative Actor Critic"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [generative modeling, policy evaluation, latent plan, offline-to-online, actor-critic]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aoyang Qin, Deqian Kong, Wei Wang, Ying Nian Wu, Song-Chun Zhu, Sirui Xie"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Beijing Institute of General Artificial Intelligence (BIGAI), UCLA, Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21527",children:"https://arxiv.org/pdf/2512.21527"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/qayqaq/Generative-Actor-Critic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Generative Actor Critic (GAC) framework that reframes policy evaluation as learning a generative model of the joint distribution over trajectories and returns, decoupling decision-making. 2. Introduces a specific instantiation using a latent variable model with continuous latent plan vectors and novel inference strategies for exploitation and exploration. 3. Demonstrates strong offline performance and significantly enhanced offline-to-online improvement on benchmarks, even without step-wise rewards."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62b697a3a1c4a1b559c19af7d65fd19d2eed0bb097eccecdd9008a509a0456c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Generative Actor Critic (GAC), a novel reinforcement learning framework that decouples sequential decision-making by learning a generative model of trajectories and returns and then performing inference on it. It shows strong performance in offline learning and significantly improves when fine-tuned online, even in sparse-reward environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Generative Actor Critic] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edfRL\u5728\u7ebf\u6539\u8fdb\u79bb\u7ebf\u9884\u8bad\u7ec3\u6a21\u578b\u5b58\u5728\u6311\u6218/Challenges in refining offline models online]\n    C --\x3e C1[\u5c06\u7b56\u7565\u8bc4\u4f30\u91cd\u6784\u4e3a\u5b66\u4e60\u8f68\u8ff9\u4e0e\u56de\u62a5\u7684\u8054\u5408\u751f\u6210\u6a21\u578b/Reframe policy evaluation as learning p(\u03c4, y)]\n    C --\x3e C2[\u5c06\u7b56\u7565\u6539\u8fdb\u91cd\u6784\u4e3a\u5728\u6a21\u578b\u4e0a\u8fdb\u884c\u591a\u6837\u5316\u63a8\u7406/Reframe policy improvement as versatile inference]\n    C --\x3e C3[\u57fa\u4e8e\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u5b9e\u4f8b\u5316\u4e0e\u65b0\u9896\u63a8\u7406\u7b56\u7565/Instantiation with latent plans & novel inference]\n    D --\x3e D1[\u79bb\u7ebf\u6027\u80fd\u5f3a\u5927/Strong offline performance]\n    D --\x3e D2[\u79bb\u7ebf\u5230\u5728\u7ebf\u6539\u8fdb\u663e\u8457/Enhanced offline-to-online improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [bioinformatics], [adaptive gating mechanism, contrastive learning, transfer learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xinru Wen, Weizhong Lin, Xuan Xiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," JCI (inferred from email domain ",(0,r.jsx)(n.code,{children:"jci.edu.cn"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21544",children:"https://arxiv.org/pdf/2512.21544"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a two-stage deep learning framework (AVP-Fusion) for antiviral peptide identification and subclass prediction. 2. Introduces an Adaptive Gating Mechanism to dynamically fuse local (CNN) and global (BiLSTM) sequence features. 3. Employs a contrastive learning strategy with OHEM and BLOSUM62-based data augmentation to sharpen decision boundaries and handle hard samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72c58b1c8ae5a53950bfc6d9e8fcca6dc27fc849f514d47d4a2cdff795cb10b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AVP-Fusion, a two-stage deep learning framework that integrates adaptive feature fusion and contrastive learning for identifying antiviral peptides (AVPs). The method dynamically fuses multi-modal sequence features and uses contrastive learning to improve classification, achieving state-of-the-art accuracy and enabling precise prediction of antiviral activity against specific viral families."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[AVP-Fusion: \u6297\u75c5\u6bd2\u80bd\u8bc6\u522b / Antiviral Peptide Identification] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898 / Problem] --\x3e P1[\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u5e8f\u5217\u4f9d\u8d56 / Current methods struggle with sequence dependencies]\n    Problem --\x3e P2[\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u6837\u672c / Hard to handle ambiguous samples]\n    Method[\u4e3b\u8981\u65b9\u6cd5 / Method] --\x3e M1[\u6784\u5efa\u5168\u666f\u7279\u5f81\u7a7a\u95f4 / Construct panoramic feature space]\n    Method --\x3e M2[\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u878d\u5408\u7279\u5f81 / Adaptive Gating Mechanism for feature fusion]\n    Method --\x3e M3[\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u6570\u636e\u589e\u5f3a / Contrastive learning & data augmentation]\n    Results[\u5173\u952e\u7ed3\u679c / Results] --\x3e R1[\u51c6\u786e\u73870.9531, MCC 0.9064 / Accuracy 0.9531, MCC 0.9064]\n    Results --\x3e R2[\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5 / Outperforms SOTA]\n    Results --\x3e R3[\u5b9e\u73b0\u75c5\u6bd2\u5bb6\u65cf\u4e9a\u7c7b\u9884\u6d4b / Enables viral family subclass prediction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [sparse recovery], [neural architecture search, meta-learning, iterative shrinkage thresholding algorithm, sparse optimization, algorithm discovery]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University, University of California, Riverside"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21563",children:"https://arxiv.org/pdf/2512.21563"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a meta-learning framework using Neural Architecture Search (NAS) for automated discovery of sparse recovery algorithms. 2. Demonstrates the framework's capability to rediscover key elements of ISTA and FISTA from a search space of over 50,000 variables. 3. Shows the framework's applicability to various data distributions and algorithms beyond ISTA/FISTA."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49f1d5d0a89c3d52b36f1e443675494175442f0930725fc8a763e525ca05243a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a meta-learning framework that uses Neural Architecture Search (NAS) to automatically discover sparse recovery algorithms. It successfully rediscovers components of ISTA and FISTA from a large search space and demonstrates generalizability to other algorithms and data distributions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Discovering Sparse Recovery Algorithms Using Neural Architecture Search] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Automated discovery of sparse optimization algorithms is difficult and heuristic-driven]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Meta-learning framework using Neural Architecture Search (NAS) for algorithm rediscovery]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Rediscovered ISTA/FISTA elements; framework applies to various data and algorithms]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatio-temporal kriging], [graph neural networks, incremental learning, data stratification, anchor locations, incomplete features]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiaobin Ren, Kaiqi Zhao, Katerina Ta\u0161kova, Patricia Riddle"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Auckland, Harbin Institute of Technology, Shenzhen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21569",children:"https://arxiv.org/pdf/2512.21569"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/xren451/Spatial-interpolation",children:"https://github.com/xren451/Spatial-interpolation"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an anchor-based stratification framework to handle sparse spatial distributions and heterogeneous feature availability in sensor networks. 2. Proposes a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information to learn stratum-specific representations. 3. Designs an incremental representation mechanism that systematically utilizes all available features across different strata to mitigate feature incompleteness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c757748db9cf59d7295a4cdf698c5e194dc4ae79ec767aa6f7c71c0e78243768_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes AnchorGK, a graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployment and incomplete features. It uses anchor-based stratification and a dual-view graph learning layer to model correlations and incrementally integrate features. Experiments show it outperforms existing state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Sparse sensor distribution and incomplete features hinder accurate spatio-temporal kriging)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Anchor-based stratification and dual-view graph learning for incremental feature integration)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms state-of-the-art baselines on multiple benchmark datasets)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [compiler & ir], [e-graph, term rewriting, phase ordering, NUMA abstraction, auto vectorize]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Canaan Inc."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21571",children:"https://arxiv.org/pdf/2512.21571"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/kendryte/nncase",children:"https://github.com/kendryte/nncase"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a "compile once, adapt everywhere" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLM\u90e8\u7f72\u53d7\u9650\u4e8e\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\uff0c\u4f20\u7edf\u7f16\u8bd1\u5668\u6d41\u7a0b\u788e\u7247\u5316/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8ee-graph\u7684\u9879\u91cd\u5199\u5f15\u64ce\uff0c\u7edf\u4e00NUMA\u62bd\u8c61\uff0c\u96c6\u6210\u81ea\u52a8\u5411\u91cf\u5316\u3001\u5206\u5e03\u3001\u8c03\u5ea6\u6a21\u5757/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6027\u80fd\u8d85\u8d8aMLC LLM\u548cIntel IPEX\uff0c\u63a5\u8fd1\u624b\u5de5\u4f18\u5316\u7684llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Unified Definition of Hallucination, Or: It's the World Model, Stupid"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection & evaluation], [hallucination, world modeling, knowledge conflict, benchmark, language model evaluation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21577",children:"https://arxiv.org/pdf/2512.21577"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root["A Unified Definition of Hallucination / \u5e7b\u89c9\u7684\u7edf\u4e00\u5b9a\u4e49"] --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["Hallucination persists in LLMs / \u5e7b\u89c9\u5728LLM\u4e2d\u6301\u7eed\u5b58\u5728"]\n    Method --\x3e M1["Unified definition: inaccurate world modeling / \u7edf\u4e00\u5b9a\u4e49\uff1a\u4e0d\u51c6\u786e\u7684\u4e16\u754c\u5efa\u6a21"]\n    Method --\x3e M2["Framework: reference world & conflict policy / \u6846\u67b6\uff1a\u53c2\u8003\u4e16\u754c\u4e0e\u51b2\u7a81\u7b56\u7565"]\n    Results --\x3e R1["Clarifies evaluation & terminology / \u6f84\u6e05\u8bc4\u4f30\u4e0e\u672f\u8bed"]\n    Results --\x3e R2["Proposes new benchmark plans / \u63d0\u51fa\u65b0\u57fa\u51c6\u8ba1\u5212"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [Schr\xf6dinger Bridge, Low-rank Adaptation, Time Series Foundation Models, Financial Forecasting, Generative Refinement]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anthony Bolton, Wuyang Zhou, Zehua Chen, Giorgos Iacovides, Danilo Mandic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Imperial College London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21572",children:"https://arxiv.org/pdf/2512.21572"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes RefineBridge, a novel post-processing module built on a tractable Schr\xf6dinger Bridge framework to refine TSFM forecasts. 2. Introduces a paradigm that treats TSFM predictions as a generative prior and learns context-conditioned stochastic transport maps to iteratively improve them. 3. Demonstrates consistent performance improvements over state-of-the-art TSFMs on multiple financial benchmarks, addressing challenges like non-stationarity and noise."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d984f91104f06a2169d1d04626078f4bd0698ade16094c7642b5b47c5a1a6198_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of applying Time Series Foundation Models (TSFMs) to financial forecasting, where data properties like non-stationarity degrade performance. It proposes RefineBridge, a generative refinement module based on Schr\xf6dinger Bridges that iteratively transports TSFM predictions toward ground truths. Experiments show RefineBridge consistently enhances TSFM forecasts across various financial benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[TSFMs\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73<br/>TSFMs underperform in financial forecasting]\n    B --\x3e B2[LoRA\u7b49\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u5c40\u9650<br/>Limitations of adaptation methods like LoRA]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u4e8e\u859b\u5b9a\u8c14\u6865\u7684RefineBridge\u6a21\u5757<br/>Propose RefineBridge based on Schr\xf6dinger Bridge]\n    C --\x3e C2[\u5c06TSFM\u9884\u6d4b\u4f5c\u4e3a\u5148\u9a8c\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316<br/>Iteratively refine TSFM predictions as prior]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u63d0\u5347TSFM\u6027\u80fd<br/>Improves TSFM performance on multiple benchmarks]\n    D --\x3e D2[\u5bf9\u4e0d\u540c\u9884\u6d4b\u8303\u56f4\u5747\u6709\u6548<br/>Effective across different prediction horizons]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [imitation learning], [behavior cloning, latent representation, self-supervised learning, sample efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xin Liu, Haoran Li, Dongbin Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21586",children:"https://arxiv.org/pdf/2512.21586"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel, unsupervised framework (BCV-LR) for imitation learning from videos (ILV) that learns latent actions from visual inputs. 2. Introduces an iterative policy improvement loop that aligns pre-trained latent actions with the real action space online, enabling highly sample-efficient learning. 3. Demonstrates state-of-the-art sample efficiency, outperforming existing ILV and RL methods on a wide range of visual control tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c96d71be701998ebf55ef6ed2a0c0a001a306b44f3555239559ced527b17559_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes BCV-LR, a framework for learning policies from videos without action labels. It uses self-supervised learning to extract latent actions and an iterative alignment process for sample-efficient behavior cloning. The method achieves expert-level performance on many tasks with minimal interaction, showing videos can be highly effective supervision for policy learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u4ece\u89c6\u9891\u6a21\u4eff\u5b66\u4e60\u7684\u6311\u6218 / Challenges of Imitation Learning from Videos"]\n    C --\x3e C1["BCV-LR\u6846\u67b6 / BCV-LR Framework"]\n    C1 --\x3e C2["\u81ea\u76d1\u7763\u63d0\u53d6\u6f5c\u5728\u7279\u5f81 / Self-supervised Latent Feature Extraction"]\n    C1 --\x3e C3["\u57fa\u4e8e\u52a8\u6001\u7684\u6f5c\u5728\u52a8\u4f5c\u9884\u6d4b / Dynamics-based Latent Action Prediction"]\n    C1 --\x3e C4["\u5728\u7ebf\u5bf9\u9f50\u4e0e\u8fed\u4ee3\u7b56\u7565\u6539\u8fdb / Online Alignment & Iterative Policy Improvement"]\n    D --\x3e D1["\u9ad8\u6837\u672c\u6548\u7387 / High Sample Efficiency"]\n    D --\x3e D2["\u8d85\u8d8aSOTA\u65b9\u6cd5 / Outperforms SOTA Baselines"]\n    D --\x3e D3["\u9996\u6b21\u8bc1\u660e\u89c6\u9891\u53ef\u4f5c\u4e3a\u9ad8\u6548\u76d1\u7763 / First to Show Videos as Efficient Supervision"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Quantitative Verification of Omega-regular Properties in Probabilistic Programming"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [probabilistic programming and verification], [temporal posterior inference, omega-regular properties, stochastic barrier certificates, Rabin automata, quantitative verification]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," East China Normal University, Nanyang Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21596",children:"https://arxiv.org/pdf/2512.21596"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Quantitative Verification of Omega-regular Properties in Probabilistic Programming") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u6807\u51c6\u540e\u9a8c\u63a8\u65ad\u7684\u5c40\u9650/Limitation of Standard Posterior Inference")\n    P1 --\x3e P2("\u65e0\u6cd5\u6355\u6349\u7a0b\u5e8f\u6267\u884c\u7684\u65f6\u95f4\u6f14\u5316/Fails to capture temporal evolution")\n    Method --\x3e M1("\u63d0\u51fa\u65f6\u95f4\u540e\u9a8c\u63a8\u65ad\u6846\u67b6/Propose Temporal Posterior Inference (TPI)")\n    M1 --\x3e M2("\u7edf\u4e00\u6982\u7387\u7f16\u7a0b\u4e0e\u65f6\u5e8f\u903b\u8f91/Unifies Probabilistic Programming & Temporal Logic")\n    M2 --\x3e M3("\u57fa\u4e8e\u968f\u673a\u5c4f\u969c\u8bc1\u4e66\u7684\u5b9a\u91cf\u9a8c\u8bc1\u65b9\u6cd5/Quantitative Verification via Stochastic Barrier Certificates")\n    Results --\x3e R1("\u5b9e\u73b0\u539f\u578b\u5de5\u5177 TPInfer/Implement Prototype Tool TPInfer")\n    Results --\x3e R2("\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u6709\u6548\u6027\u4e0e\u6548\u7387/Demonstrates Effectiveness & Efficiency on Benchmarks")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [imbalanced classification], [XGBoost, TabNet, TabResNet, Bayesian hyperparameter search, class imbalance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yusuf Brima, Marcellin Atemkeng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Osnabr\xfcck University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21602",children:"https://arxiv.org/pdf/2512.21602"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u4e0e\u53ef\u6269\u5c55\u6027] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Imbalanced clinical data in emergency/critical care<br>\u6025\u8bca/\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u4e0d\u5e73\u8861\u4e34\u5e8a\u6570\u636e]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>\u7cfb\u7edf\u8bc4\u4f30\u6811\u6a21\u578b\u3001TabNet\u53ca\u63d0\u51fa\u7684TabResNet]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost\u6700\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u4e0b\u6027\u80fd\u4e0b\u964d]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [predictive modeling], [XGBoost, SHAP analysis, K-Fold Cross-Validation, Isolation Forest, hyperparameter tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jagaran Chakma, Zhiguang Zhou, Jyoti Chakma, Cao YuSen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tongji University, University of Chittagong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21610",children:"https://arxiv.org/pdf/2512.21610"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a two-stage data-driven framework for UHPC property prediction, integrating model selection, data cleaning (multicollinearity removal, outlier detection), and feature importance analysis. 2. Demonstrated the superior performance of the XGBoost model after rigorous hyperparameter tuning and validation, achieving high accuracy across multiple objectives (mechanical performance, flowability, porosity). 3. Created a practical graphical user interface (GUI) to facilitate the application of the predictive model by material designers, reducing reliance on extensive experimental testing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988c9f68c148fb1ef37ff8e292bf9a2cab1878ea08a2f5baee1a1a47f095be23_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a data-driven, multi-objective machine learning framework to predict the mechanical performance, flowability, and porosity of Ultra-High-Performance Concrete (UHPC). The method involves testing 21 algorithms, selecting and tuning XGBoost, and refining the dataset through multicollinearity removal, outlier detection, and SHAP-based feature selection. The final model achieves high prediction accuracy, and a supporting GUI is developed to aid in UHPC mix design, reducing the need for experimental tests."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["A Data-Driven Multi-Objective Approach for Predicting UHPC Properties<br>\u9884\u6d4bUHPC\u6027\u80fd\u7684\u6570\u636e\u9a71\u52a8\u591a\u76ee\u6807\u65b9\u6cd5"] --\x3e B["Problem: Predicting UHPC mechanical performance, flowability, porosity<br>\u6838\u5fc3\u95ee\u9898: \u9884\u6d4bUHPC\u7684\u529b\u5b66\u6027\u80fd\u3001\u6d41\u52a8\u6027\u548c\u5b54\u9699\u7387"]\n    A --\x3e C["Method: Two-stage ML framework with XGBoost, data cleaning, SHAP<br>\u4e3b\u8981\u65b9\u6cd5: \u4e24\u9636\u6bb5ML\u6846\u67b6\uff0c\u4f7f\u7528XGBoost\u3001\u6570\u636e\u6e05\u6d17\u548cSHAP"]\n    A --\x3e D["Results: High prediction accuracy, developed GUI for designers<br>\u5173\u952e\u7ed3\u679c: \u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u5f00\u53d1\u4e86GUI"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [Neural Galerkin Method, meta-learning, parametric PDEs, space-time decoupling, randomized sparse updates]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hunan University, Capital Normal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21633",children:"https://arxiv.org/pdf/2512.21633"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework that enhances the Neural Galerkin Method by integrating the Meta-Auto-Decoder paradigm for solving parametric PDEs. 2. Introduces space-time decoupling to enable more stable and efficient time integration compared to full space-time approximations. 3. Employs meta-learning for rapid adaptation to new parameters and randomized sparse updates to reduce computational cost without sacrificing accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8574d601c121a7a53ef19693b53d019c5139f87d5ae2dd641aec204463aa59c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenges of generalization and efficiency in neural network-based solvers for parametric PDEs. It proposes MAD-NG, a framework that combines the Neural Galerkin Method with meta-learning and space-time decoupling to achieve stable, efficient, and accurate long-term predictions. Numerical experiments show the method performs well in accuracy, robustness, and adaptability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MAD-NG: Meta-Auto-Decoder Neural Galerkin Method<br>MAD-NG: \u5143\u81ea\u89e3\u7801\u5668\u795e\u7ecf\u4f3d\u8fbd\u91d1\u65b9\u6cd5] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u795e\u7ecf\u6c42\u89e3\u5668\u6cdb\u5316\u4e0e\u6548\u7387\u6311\u6218<br>Traditional Neural Solvers' Generalization & Efficiency Challenges]\n    C --\x3e C1[\u7a7a\u95f4-\u65f6\u95f4\u89e3\u8026\u4e0e\u5143\u5b66\u4e60<br>Space-Time Decoupling & Meta-Learning]\n    C --\x3e C2[\u968f\u673a\u7a00\u758f\u66f4\u65b0<br>Randomized Sparse Updates]\n    D --\x3e D1[\u7269\u7406\u4e00\u81f4\u7684\u957f\u65f6\u9884\u6d4b<br>Physically Consistent Long-Horizon Predictions]\n    D --\x3e D2[\u8f83\u4f4e\u8ba1\u7b97\u5f00\u9500<br>Lower Computational Overhead]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [materials informatics], [hybrid machine learning, SHAP analysis, uncertainty quantification, strength prediction, high-performance concrete]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jagaran Chakma, Zhiguang Zhou, Badhan Chakma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tongji University, Chongqing Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21638",children:"https://arxiv.org/pdf/2512.21638"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed and evaluated three novel hybrid machine learning model families (ET-XGB, RF-LGBM, Transformer-XGB) for predicting the mechanical properties of fiber-reinforced concrete. 2. Conducted a comprehensive evaluation including k-fold cross-validation, hyperparameter optimization, SHAP analysis for interpretability, and uncertainty quantification for robustness assessment. 3. Identified key influential material parameters (fiber aspect ratios, silica fume, steel fiber content) and negative factors (water content, water-binder ratio) on concrete strength through SHAP analysis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eefc8193c0d98df6f997d7ae3134d34a448f8aba255f28a39044233c1f3bb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This research develops hybrid machine learning models to predict the compressive, flexural, and tensile strength of steel-polypropylene fiber-reinforced high-performance concrete. The study compares three hybrid models (ET-XGB, RF-LGBM, Transformer-XGB) and finds that the ET-XGB model achieved the highest overall accuracy, while SHAP analysis reveals the most influential material factors. The findings confirm that these ML models provide accurate and interpretable tools for optimizing concrete mix design in engineering applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Predict mechanical properties (CS, FS, TS) of fiber-reinforced HPC]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Develop & evaluate hybrid ML models (ET-XGB, RF-LGBM, Transformer-XGB) with SHAP & uncertainty analysis]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>ET-XGB most accurate, RF-LGBM most stable for FS, key influential factors identified via SHAP]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Monte Carlo Tree Search, Upper Confidence Bound, Variance-Aware, Prior-Based Tree Policy, Inverse-RPO]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Maximilian Weichart"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Regensburg"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21648",children:"https://arxiv.org/pdf/2512.21648"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Max-We/inverse-rpo",children:"https://github.com/Max-We/inverse-rpo"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Variance-Aware Prior-Based Tree Policies for MCTS] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Extending prior-based UCTs from other UCBs is challenging]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]\n    D[\u5173\u952e\u7ed3\u679c/Results: New policies outperform PUCT without extra cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [anomaly detection], [multimodal fusion, causal modeling, hierarchical modulation, sensor guidance, unsupervised learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chongqing University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21650",children:"https://arxiv.org/pdf/2512.21650"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified multimodal UAD framework (Causal-HM) that explicitly models the physical Process\u2192Result dependency to address causal blindness. 2. Introduces a Sensor-Guided CHM Modulation mechanism that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction. 3. Designs a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies violating physical consistency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f0b7760ac76038dd08b0ccd2890cb2628906dcf233282246d08ee584093193_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of causal blindness and the heterogeneity gap in multimodal anomaly detection for industrial processes like welding. It proposes the Causal-HM framework, which models the physical generative logic from process to result modalities using sensor-guided modulation and a causal-hierarchical architecture. The method achieves state-of-the-art performance on a new Weld-4M benchmark, demonstrating its effectiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u56e0\u679c\u76f2\u533a/Causal Blindness]\n    B --\x3e B2[\u6a21\u6001\u5f02\u8d28\u6027/Modality Heterogeneity]\n    C --\x3e C1[\u4f20\u611f\u5668\u5f15\u5bfc\u8c03\u5236/Sensor-Guided CHM Modulation]\n    C --\x3e C2[\u56e0\u679c\u5206\u5c42\u67b6\u6784/Causal-Hierarchical Architecture]\n    D --\x3e D1[\u65b0\u57fa\u51c6/Weld-4M Benchmark]\n    D --\x3e D2[SOTA\u6027\u80fd/SOTA I-AUROC 90.7%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Semantic Codebooks as Effective Priors for Neural Speech Compression"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [speech compression], [semantic codebooks, residual vector quantization (RVQ), HuBERT, FiLM-conditioned decoder, neural audio codec]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Liuyang Bai, Weiyi Lu, Li Guo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," NYU Shanghai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21653",children:"https://arxiv.org/pdf/2512.21653"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Semantic Codebooks as Effective Priors for Neural Speech Compression"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps)."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [1-bit quantization, post-training quantization, output alignment, activation error, large language models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dung Anh Hoang, Cuong Pham, Cuong Nguyen, Trung le, Jianfei Cai, Thanh-Toan Do"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Monash University, University of Surrey"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21651",children:"https://arxiv.org/pdf/2512.21651"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Investigates the failure conditions of naive output-matching in 1-bit LLM quantization, 2. Proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation, 3. Demonstrates consistent performance improvements over existing 1-bit PTQ methods with minimal overhead"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0357717d29d733787933b581fbbb292b187b9fbc651fbc0f15bb04ef03e619eb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the significant performance degradation in 1-bit post-training quantization (PTQ) of Large Language Models. The authors propose a new data-aware PTQ method that efficiently accounts for activation error accumulation. Their solution is shown to consistently outperform existing 1-bit PTQ approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Rethinking Output Alignment For 1-bit PTQ of LLMs<br>\u91cd\u65b0\u601d\u8003\u5927\u8bed\u8a00\u6a21\u578b1\u6bd4\u7279\u8bad\u7ec3\u540e\u91cf\u5316\u7684\u8f93\u51fa\u5bf9\u9f50"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["1-bit PTQ causes significant performance drop<br>1\u6bd4\u7279\u8bad\u7ec3\u540e\u91cf\u5316\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d"] --\x3e P1["Focus on weight alignment, not output<br>\u5173\u6ce8\u6743\u91cd\u5bf9\u9f50\u800c\u975e\u8f93\u51fa"]\n    Problem --\x3e P2["Naive output-matching fails<br>\u7b80\u5355\u7684\u8f93\u51fa\u5339\u914d\u65b9\u6cd5\u5931\u8d25"]\n    Method["Propose a data-aware PTQ approach<br>\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u611f\u77e5\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u65b9\u6cd5"] --\x3e M1["Accounts for activation error accumulation<br>\u8003\u8651\u6fc0\u6d3b\u8bef\u5dee\u7d2f\u79ef"]\n    Method --\x3e M2["Keeps optimization efficient<br>\u4fdd\u6301\u4f18\u5316\u9ad8\u6548"]\n    Results["Consistently outperforms existing 1-bit PTQ methods<br>\u6301\u7eed\u4f18\u4e8e\u73b0\u67091\u6bd4\u7279\u8bad\u7ec3\u540e\u91cf\u5316\u65b9\u6cd5"] --\x3e R1["Minimal overhead<br>\u5f00\u9500\u6781\u5c0f"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [deepfake detection], [mechanistic interpretability, sparse autoencoder, forensic manifold analysis, feature selectivity, vision-language model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Subramanyam Sahoo, Jared Junkin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley, Johns Hopkins University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21670",children:"https://arxiv.org/pdf/2512.21670"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective",children:"https://github.com/SubramanyamSahoo/The-Deepfake-Detective"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u662f\u9ed1\u76d2\u6a21\u578b/Deepfake detectors are black boxes]\n    C --\x3e C1[\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790/Sparse Autoencoder (SAE) Analysis]\n    C --\x3e C2[\u6cd5\u8bc1\u6d41\u5f62\u5206\u6790/Forensic Manifold Analysis]\n    D --\x3e D1[\u6f5c\u5728\u7279\u5f81\u7a00\u758f\u4f7f\u7528/Latent features are sparsely used]\n    D --\x3e D2[\u6d41\u5f62\u51e0\u4f55\u7279\u6027\u63ed\u793a\u4f2a\u5f71/Manifold geometry reveals artifacts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [YOLO-NAS, YOLOv8, perception, autonomous vehicles, custom dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jalal Khan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," United Arab Emirates University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21673",children:"https://arxiv.org/pdf/2512.21673"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd/Evaluate DL model performance for AV perception]\nC --\x3e C1[\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6bd4\u8f83YOLO-NAS\u4e0eYOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\nD --\x3e D1[YOLOv8s\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1175%/YOLOv8s saves 75% training time]\nD --\x3e D2[YOLOv8s\u51c6\u786e\u7387\u66f4\u9ad8(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Dictionary-Transform Generative Adversarial Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative adversarial networks], [dictionary learning, transform learning, sparse modeling, adversarial learning, nash equilibrium]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Angshul Majumdar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indraprastha Institute of Information Technology Delhi (IIIT-D)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21677",children:"https://arxiv.org/pdf/2512.21677"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DT-GAN, a fully model-based adversarial framework with a sparse synthesis dictionary generator and an analysis transform discriminator, enabling rigorous theoretical analysis. 2. Proves the adversarial game is well-posed, admits at least one Nash equilibrium, and that equilibrium solutions are identifiable under a sparse generative model. 3. Establishes finite-sample stability and consistency of empirical equilibria, demonstrating reliable convergence and robustness, validated on synthetic data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e368966cab749cb3ef0799abfb9a31d801a73291e7a5d2b4a5b81fd185a9d25_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the theoretical fragility and instability of classical GANs by proposing DT-GAN, a model-based adversarial framework where the generator and discriminator are constrained linear operators (a sparse synthesis dictionary and an analysis transform). The authors prove the framework is well-posed, has identifiable equilibria, and converges reliably, demonstrating that adversarial learning can be made interpretable and provably correct for data with sparse structure."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Dictionary-Transform GANs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Classical GANs are theoretically fragile, with ill-posed objectives and unstable training.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose DT-GAN, a model-based framework with a sparse synthesis dictionary generator and an analysis transform discriminator.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Game is well-posed with Nash equilibrium; solutions are identifiable and stable; framework is interpretable and provably correct.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatiotemporal forecasting], [probabilistic forecasting, uncertainty estimation, principal component analysis, road impedance, traffic flow]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beijing Jiaotong University, Aalborg University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21685",children:"https://arxiv.org/pdf/2512.21685"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(\u5982\u4f55\u5efa\u6a21\u4ea4\u901a\u6d41\u4e0d\u786e\u5b9a\u6027\u7684\u6210\u56e0? / How to model the causes of traffic flow uncertainty?)\n    B --\x3e B2(\u5982\u4f55\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u7684\u65f6\u7a7a\u76f8\u5173\u6027? / How to capture spatiotemporal correlations of uncertainty?)\n    C --\x3e C1(\u52a8\u6001\u963b\u6297\u6f14\u5316\u7f51\u7edc / Dynamic Impedance Evolution Network)\n    C --\x3e C2(\u4e3b\u6210\u5206\u7f51\u7edc / Principal Component Network)\n    D --\x3e D1(\u8d85\u8d8a\u73b0\u6709\u6982\u7387\u9884\u6d4b\u65b9\u6cd5 / Outperforms existing probabilistic forecasting methods)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [communication & networking], [multiconnectivity, SAGIN, resource allocation, agentic reinforcement learning, heterogeneous networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kyung Hee University, Ghent University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21717",children:"https://arxiv.org/pdf/2512.21717"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: Use AI-driven approaches, specifically agentic reinforcement learning"]\n    Results["\u5173\u952e\u7ed3\u679c/Results: Enhanced network performance (latency, capacity) with moderate power trade-off"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] An Information Theoretic Perspective on Agentic System Design"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [mutual information, noisy channel, compressor-predictor, on-device AI, information-theoretic]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher R\xe9, Dan Biderman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21720",children:"https://arxiv.org/pdf/2512.21720"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[An Information Theoretic Perspective on Agentic System Design] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1("Agentic\u7cfb\u7edf\u8bbe\u8ba1\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc<br/>Agentic system design lacks theoretical guidance")\n    C --\x3e C1("\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6\u4e0e\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668<br/>Propose information-theoretic framework & mutual information estimator")\n    D --\x3e D1("\u66f4\u5927\u538b\u7f29\u5668\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e<br/>Larger compressors are more efficient and accurate")\n    D --\x3e D2("\u6269\u5c55\u538b\u7f29\u5668\u4f18\u4e8e\u6269\u5c55\u9884\u6d4b\u5668<br/>Scaling compressors outperforms scaling predictors")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] HELP: Hierarchical Embodied Language Planner for Household Tasks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [embodied agent, hierarchical planning, large language model, household tasks, open source LLM]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," MIRAI, Cognitive AI Systems Lab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21723",children:"https://arxiv.org/pdf/2512.21723"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Model of Causal Explanation on Neural Networks for Tabular Data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [explainable ai], [CENNET, structural causal models, entropy, causal explanation, tabular data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sony Computer Science Laboratories, Inc., Sony Corporation of America"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21746",children:"https://arxiv.org/pdf/2512.21746"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Model of Causal Explanation on Neural Networks for Tabular Data] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: CENNET provides causal explanations, validated via comparative experiments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [continual learning], [entropy scaling, catastrophic forgetting, stability-plasticity dilemma, dynamic feedback, layer-wise control]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hengyi Wu, Zhenyi Wang, Heng Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Maryland, College Park, University of Central Florida"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21743",children:"https://arxiv.org/pdf/2512.21743"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --\x3e P1[\u9ad8\u71b5\u5c42\u6b20\u62df\u5408/High-entropy layers underfit]\n    Problem --\x3e P2[\u4f4e\u71b5\u5c42\u8fc7\u62df\u5408/Low-entropy layers overfit]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Entropy-aware dynamic feedback for layer-wise control] --\x3e M1[\u51cf\u5c11\u9ad8\u71b5\u5c42\u71b5\u503c/Reduce entropy in high-entropy layers]\n    Method --\x3e M2[\u589e\u52a0\u4f4e\u71b5\u5c42\u71b5\u503c/Increase entropy in low-entropy layers]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Improved generalization and performance] --\x3e R1[\u6536\u655b\u5230\u66f4\u5bbd\u7684\u5c40\u90e8\u6781\u5c0f\u503c/Converge to wider local minima]\n    Results --\x3e R2[\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5/Outperforms state-of-the-art baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Approximation Capabilities of Feedforward Neural Networks with GELU Activations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural network approximation theory], [GELU activation, feedforward neural networks, approximation error bounds, derivative approximation, constructive approximation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Konstantin Yakovlev, Nikita Puchkin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HSE University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21749",children:"https://arxiv.org/pdf/2512.21749"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Derivation of simultaneous approximation error bounds for a function and all its derivatives up to any prescribed order using GELU networks. 2. Constructive approximation guarantees for elementary operations (multiplication, division, exponential) with control over network size, weight magnitudes, and behavior at infinity. 3. Extension of approximation results to multivariate polynomials and other elementary functions, ensuring global boundedness of higher-order derivatives of the approximators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af63acd2ab9f3e42763901f23d1762f6658fd25b72daf9e1f1f73e7b30ce1173_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the approximation capabilities of feedforward neural networks using the GELU activation function. The authors provide constructive methods to approximate elementary functions and their derivatives, proving simultaneous error bounds for the function and all its higher-order derivatives. The main conclusion is that GELU networks can effectively approximate key operations like multiplication, division, and exponentiation with controlled network complexity and globally bounded derivatives."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\nRoot("Approximation Capabilities of Feedforward Neural Networks with GELU Activations<br>GELU\u6fc0\u6d3b\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u903c\u8fd1\u80fd\u529b") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\nRoot --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\nRoot --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\nProblem --\x3e P1("\u903c\u8fd1\u51fd\u6570\u53ca\u5176\u5bfc\u6570<br>Approximating functions and their derivatives")\nMethod --\x3e M1("\u6784\u9020\u6027\u4e58\u6cd5\u903c\u8fd1<br>Constructive multiplication approximation")\nMethod --\x3e M2("\u6269\u5c55\u5230\u9664\u6cd5\u548c\u6307\u6570<br>Extension to division and exponent")\nResults --\x3e R1("\u540c\u65f6\u8bef\u5dee\u754c<br>Simultaneous error bounds")\nResults --\x3e R2("\u5168\u5c40\u6709\u754c\u5bfc\u6570<br>Globally bounded derivatives")\nResults --\x3e R3("\u7f51\u7edc\u89c4\u6a21\u63a7\u5236<br>Network size control")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Assessing the Effectiveness of Membership Inference on Generative Music"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [membership inference attacks], [membership inference attack (MIA), generative music, MuseGAN, privacy, copyright]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kurtis Chow, Omar Samiullah, Vinesh Sridhar, Hewen Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Irvine"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21762",children:"https://arxiv.org/pdf/2512.21762"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducts the first preliminary study on the effectiveness of membership inference attacks (MIAs) specifically on generative music models., 2. Evaluates several existing MIA techniques on the popular MuseGAN model to assess their performance in this domain., 3. Provides empirical evidence suggesting that generative music data is relatively resilient to known membership inference techniques, aligning with prior findings in generative audio."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b7176fccbb69c4f0a15bfa6bcbb6ff59b09cf6ac02e0f524334f306ce4041f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether membership inference attacks (MIAs) are effective against generative music models. The authors conduct a preliminary study by applying several existing MIA techniques to the MuseGAN model. Their findings indicate that generative music data is fairly resilient to these known attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Assessing the Effectiveness of Membership Inference on Generative Music"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Lack of MIA study on generative music, privacy & copyright concerns"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Apply existing MIAs to MuseGAN model"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Music data is resilient to known MIAs"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3d medical image analysis], [Masked Autoencoder, Swin Transformer, Self-Supervised Learning, 3D Vision Transformer, Structural Priority Loss]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Evgeny Alves Limarenko, Anastasiia Studenikina"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Moscow Institute of Physics and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21769",children:"https://arxiv.org/pdf/2512.21769"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("BertsWin: 3D MAE\u4f18\u5316") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("3D MAE\u62d3\u6251\u7a00\u758f\u6027/Topological Sparsity in 3D MAE")\n    Problem --\x3e P2("\u7834\u574f\u7a7a\u95f4\u5173\u7cfb/Destroys Spatial Context")\n    Method --\x3e M1("BertsWin\u6df7\u5408\u67b6\u6784/BertsWin Hybrid Architecture")\n    Method --\x3e M2("\u5b8c\u65743D\u4ee4\u724c\u7f51\u683c/Full 3D Token Grid")\n    Method --\x3e M3("Swin\u7a97\u53e3 & \u7ed3\u6784\u635f\u5931/Swin Windows & Structural Loss")\n    Results --\x3e R1("5.8x\u8bed\u4e49\u6536\u655b\u52a0\u901f/5.8x Faster Convergence")\n    Results --\x3e R2("15\u500d\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c11/15x Fewer Epochs")\n    Results --\x3e R3("FLOPs\u6301\u5e73\uff0c\u603b\u8d44\u6e90\u51cf\u5c11/FLOP Parity, Net Resource Reduction")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Accelerating Scientific Discovery with Autonomous Goal-evolving Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific discovery agents], [autonomous goal evolution, bi-level optimization, LLM agents, objective function design, scientific discovery]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cornell University, The Ohio State University, Yale University, Simon Fraser University, \xc9cole Polytechnique F\xe9d\xe9rale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21782",children:"https://arxiv.org/pdf/2512.21782"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[Fixed objectives are imperfect proxies for grand scientific challenges / \u56fa\u5b9a\u7684\u76ee\u6807\u51fd\u6570\u662f\u79d1\u5b66\u91cd\u5927\u6311\u6218\u7684\u4e0d\u5b8c\u7f8e\u4ee3\u7406]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / \u63d0\u51faSAGA: \u79d1\u5b66\u81ea\u4e3b\u76ee\u6807\u6f14\u5316\u667a\u80fd\u4f53]\n    M1 --\x3e M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / \u53cc\u5c42\u67b6\u6784: LLM\u5916\u5faa\u73af\u6f14\u5316\u76ee\u6807\uff0c\u5185\u5faa\u73af\u4f18\u5316\u89e3]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Applied to antibiotic, materials, DNA, chemical process design / \u5e94\u7528\u4e8e\u6297\u751f\u7d20\u3001\u6750\u6599\u3001DNA\u3001\u5316\u5de5\u8fc7\u7a0b\u8bbe\u8ba1]\n    R1 --\x3e R2[Automating objective formulation improves discovery effectiveness / \u81ea\u52a8\u5316\u76ee\u6807\u5236\u5b9a\u63d0\u5347\u4e86\u53d1\u73b0\u6548\u80fd]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Synthetic Financial Data Generation for Enhanced Financial Modelling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [synthetic data generation], [synthetic financial data, TimeGAN, ARIMA-GARCH, VAE, Maximum Mean Discrepancy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Christophe D. Hounwanou, Yae Ulrich Gaba, Pierre Ntakirutimana"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," AIMS Rwanda, Carnegie Mellon University, Sefako Makgatho Health Sciences University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21791",children:"https://arxiv.org/pdf/2512.21791"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a unified multi-criteria evaluation framework for synthetic financial data. 2. Empirically compares three generative paradigms (ARIMA-GARCH, VAE, TimeGAN) on fidelity, temporal structure, and downstream utility. 3. Provides practical guidelines for model selection and releases a reproducible codebase to standardize benchmarking."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f9e4bbcf426a2c32c05089e0964e9937c0becca521cac8a83b48c652d0d86c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses data scarcity in finance by proposing a unified evaluation framework for synthetic financial data generation. It compares ARIMA-GARCH, VAE, and TimeGAN models using S&P 500 data, finding that TimeGAN offers the best trade-off between realism and temporal coherence. The work concludes with practical guidelines and aims to standardize benchmarking in the field."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Synthetic Financial Data Generation for Enhanced Financial Modelling"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6570\u636e\u7a00\u7f3a\u4e0e\u4fdd\u5bc6\u6027<br/>Data Scarcity & Confidentiality"]\n    Method["\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u4e0e\u4e09\u79cd\u751f\u6210\u6a21\u578b<br/>Unified Evaluation Framework & Three Generative Models"]\n    Results["TimeGAN\u6700\u4f73\u6743\u8861\u4e0e\u5b9e\u7528\u6307\u5357<br/>TimeGAN Best Trade-off & Practical Guidelines"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Multi-agent Adaptive Mechanism Design"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [mechanism design], [distributionally robust optimization, online learning, incentive compatibility, adaptive mechanism, regret analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Massachusetts Institute of Technology, University of Illinois Urbana-Champaign"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21794",children:"https://arxiv.org/pdf/2512.21794"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal ",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsxs)(n.mrow,{children:[(0,r.jsxs)(n.mover,{accent:"true",children:[(0,r.jsx)(n.mo,{stretchy:"false",children:"{"}),(0,r.jsx)(n.mo,{children:"~"})]}),(0,r.jsx)(n.mi,{children:"O"}),(0,r.jsx)(n.mo,{stretchy:"false",children:"}"}),(0,r.jsx)(n.mo,{stretchy:"false",children:"("}),(0,r.jsx)(n.msqrt,{children:(0,r.jsx)(n.mo,{stretchy:"false",children:"{"})}),(0,r.jsx)(n.mi,{children:"T"}),(0,r.jsx)(n.mo,{stretchy:"false",children:"}"}),(0,r.jsx)(n.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"\\tilde\\{O\\}(\\sqrt\\{T\\})"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"1.2919em",verticalAlign:"-0.305em"}}),(0,r.jsx)(n.span,{className:"mord accent",children:(0,r.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,r.jsxs)(n.span,{className:"vlist-r",children:[(0,r.jsxs)(n.span,{className:"vlist",style:{height:"0.9869em"},children:[(0,r.jsxs)(n.span,{style:{top:"-3em"},children:[(0,r.jsx)(n.span,{className:"pstrut",style:{height:"3em"}}),(0,r.jsx)(n.span,{className:"mopen",children:"{"})]}),(0,r.jsxs)(n.span,{style:{top:"-3.669em"},children:[(0,r.jsx)(n.span,{className:"pstrut",style:{height:"3em"}}),(0,r.jsx)(n.span,{className:"accent-body",style:{left:"-0.25em"},children:(0,r.jsx)(n.span,{className:"mord",children:"~"})})]})]}),(0,r.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,r.jsx)(n.span,{className:"vlist-r",children:(0,r.jsx)(n.span,{className:"vlist",style:{height:"0.25em"},children:(0,r.jsx)(n.span,{})})})]})}),(0,r.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"O"}),(0,r.jsx)(n.span,{className:"mclose",children:"}"}),(0,r.jsx)(n.span,{className:"mopen",children:"("}),(0,r.jsx)(n.span,{className:"mord sqrt",children:(0,r.jsxs)(n.span,{className:"vlist-t vlist-t2",children:[(0,r.jsxs)(n.span,{className:"vlist-r",children:[(0,r.jsxs)(n.span,{className:"vlist",style:{height:"0.935em"},children:[(0,r.jsxs)(n.span,{className:"svg-align",style:{top:"-3.2em"},children:[(0,r.jsx)(n.span,{className:"pstrut",style:{height:"3.2em"}}),(0,r.jsx)(n.span,{className:"mopen",style:{paddingLeft:"1em"},children:"{"})]}),(0,r.jsxs)(n.span,{style:{top:"-2.895em"},children:[(0,r.jsx)(n.span,{className:"pstrut",style:{height:"3.2em"}}),(0,r.jsx)(n.span,{className:"hide-tail",style:{minWidth:"1.02em",height:"1.28em"},children:(0,r.jsx)(n.svg,{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.28em",viewBox:"0 0 400000 1296",preserveAspectRatio:"xMinYMin slice",children:(0,r.jsx)(n.path,{d:"M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z"})})})]})]}),(0,r.jsx)(n.span,{className:"vlist-s",children:"\u200b"})]}),(0,r.jsx)(n.span,{className:"vlist-r",children:(0,r.jsx)(n.span,{className:"vlist",style:{height:"0.305em"},children:(0,r.jsx)(n.span,{})})})]})}),(0,r.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"T"}),(0,r.jsx)(n.span,{className:"mclose",children:"})"})]})})]})," cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-agent Adaptive Mechanism Design] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Distributionally Robust Adaptive Mechanism (DRAM)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Guaranteed truthfulness & optimal $\\tilde{O}(\\sqrt{T})$ regret]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [bioinformatics], [Set Attention Transformer, 1D-CNN, Multi-Path Network, Interpretable Machine Learning, Genomic Variant Analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aicha Boutorh, Kamar Hibatallah Baghdadi, Anais Daoud"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National School of Artificial Intelligence (ENSIA)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21786",children:"https://arxiv.org/pdf/2512.21786"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel multi-path deep learning architecture (VAMP-Net) that combines a permutation-invariant Set Attention Transformer for capturing epistatic interactions and a 1D-CNN for analyzing sequencing quality metrics. 2. Introduces a comparative evaluation of unmasked vs. padding-masked Set Attention Blocks within the architecture. 3. Provides dual-layer interpretability through attention weight analysis for epistatic networks and gradient-based methods for feature importance on both genetic variants and quality metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf587b5249dfee9f3a9866a887e3d791367588c15a447d0c86619ea9c8df8a45_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces VAMP-Net, a multi-path deep learning framework designed to predict drug resistance in Mycobacterium tuberculosis. It combines a Set Attention Transformer to model genetic interactions and a 1D-CNN to assess data quality, achieving high accuracy and AUC. The work concludes that this approach offers superior predictive performance and dual-layer interpretability for clinical genomics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[VAMP-Net: Interpretable Multi-Path Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Challenges in MTB Drug Resistance Prediction] --\x3e P1[\u590d\u6742\u7684\u4e0a\u4f4d\u6027\u76f8\u4e92\u4f5c\u7528/Complex Epistatic Interactions]\n    Problem --\x3e P2[\u6d4b\u5e8f\u6570\u636e\u8d28\u91cf\u591a\u53d8/Variable Sequencing Data Quality]\n\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>VAMP-Net Multi-Path Architecture] --\x3e M1[\u8def\u5f841: \u96c6\u5408\u6ce8\u610f\u529b\u53d8\u6362\u5668/Path-1: Set Attention Transformer]\n    Method --\x3e M2[\u8def\u5f842: \u8d28\u91cf\u611f\u77e51D-CNN/Path-2: Quality-Aware 1D-CNN]\n    Method --\x3e M3[\u878d\u5408\u6a21\u5757/Fusion Module]\n    M1 --\x3e M1_Detail[\u5904\u7406\u53d8\u5f02\u96c6\u5408/Processes Variant Sets]\n    M2 --\x3e M2_Detail[\u5206\u6790\u8d28\u91cf\u6307\u6807/Analyzes Quality Metrics]\n\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>Superior Performance & Interpretability] --\x3e R1[\u6027\u80fd: >95% \u51c6\u786e\u7387, ~97% AUC/Performance: >95% Acc, ~97% AUC]\n    Results --\x3e R2[\u53ef\u89e3\u91ca\u6027: \u53cc\u5c42\u9762\u5206\u6790/Interpretability: Dual-Layer Analysis]\n    R2 --\x3e R2_1[\u6ce8\u610f\u529b\u6743\u91cd\u63ed\u793a\u4e0a\u4f4d\u7f51\u7edc/Attention Weights Reveal Epistatic Networks]\n    R2 --\x3e R2_2[\u68af\u5ea6\u5206\u6790\u8bc6\u522b\u5173\u952e\u4f4d\u70b9\u4e0e\u8d28\u91cf\u6307\u6807/Gradient Analysis Identifies Key Loci & Quality Metrics]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Krishna Chaitanya Sunkara, Rambabu Konakanchi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Oracle, Charles Schwab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21801",children:"https://arxiv.org/pdf/2512.21801"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Probabilistic LSTM forecasting validated within \xb130-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Smart IoT-Based Leak Forecasting and Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Coolant leaks cause energy loss in AI data centers]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [adversarial attacks], [entropy-guided attacks, vision-language models, adversarial robustness, harmful content generation, transferability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Australian National University, The University of Queensland, GE Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21815",children:"https://arxiv.org/pdf/2512.21815"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[VLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb/VLMs are vulnerable to adversarial attacks]\n    Problem --\x3e P2[\u5148\u9a8c\u653b\u51fb\u5047\u8bbe\u6240\u6709token\u540c\u7b49\u91cd\u8981/Prior attacks assume all tokens are equally important]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u8bc6\u522b\u9ad8\u71b5\u5173\u952e\u51b3\u7b56\u70b9/Identify high-entropy critical decision points]\n    Method --\x3e M2[\u63d0\u51fa\u71b5\u5e93\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u9ad8\u6548\u653b\u51fb:\u5c0f\u9884\u7b97\u5b9e\u73b0\u5f3a\u8bed\u4e49\u9000\u5316/Efficient attack: strong degradation with small budget]\n    Results --\x3e R2[\u9ad8\u6709\u5bb3\u8f6c\u5316\u7387:35-49%/High harmful conversion: 35-49%]\n    Results --\x3e R3[\u53ef\u884c\u8fc1\u79fb\u6027:17-26%/Feasible transferability: 17-26%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Scalable Class-Incremental Learning Based on Parametric Neural Collapse"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [class-incremental learning], [parametric neural collapse, equiangular tight frame, knowledge distillation, adaptive expansion, feature alignment]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content. Affiliation information is not included."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21845",children:"https://arxiv.org/pdf/2512.21845"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," this https URLETF2"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Scalable Class-Incremental Learning Based on Parametric Neural Collapse"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898 / Problem"] --\x3e P1["\u8fc7\u62df\u5408\u65b0\u6570\u636e / Overfitting to new data"]\n    Problem --\x3e P2["\u707e\u96be\u6027\u9057\u5fd8\u65e7\u6570\u636e / Catastrophic forgetting of old data"]\n    Problem --\x3e P3["\u7279\u5f81\u5dee\u5f02\u4e0e\u7c7b\u522b\u9519\u4f4d / Feature difference & Class misalignment"]\n\n    Method["\u4e3b\u8981\u65b9\u6cd5 / Method"] --\x3e M1["SCL-PNC\u65b9\u6cd5 / SCL-PNC Method"]\n    M1 --\x3e M1_1["\u81ea\u9002\u5e94\u5c42\u6269\u5c55\u4e3b\u5e72 / Adapt-layer for backbone expansion"]\n    M1 --\x3e M1_2["\u52a8\u6001\u53c2\u6570\u5316ETF\u5206\u7c7b\u5668 / Dynamic Parametric ETF Classifier"]\n    M1 --\x3e M1_3["\u5e76\u884c\u6269\u5c55\u4e0e\u77e5\u8bc6\u84b8\u998f / Parallel expansion & Knowledge distillation"]\n\n    Results["\u5173\u952e\u7ed3\u679c / Results"] --\x3e R1["\u9ad8\u6548\u5904\u7406\u7c7b\u522b\u589e\u957f / Efficiently handles increasing categories"]\n    Results --\x3e R2["\u89e3\u51b3\u7c7b\u522b\u9519\u4f4d / Addresses class misalignment"]\n    Results --\x3e R3["\u786e\u4fdd\u7279\u5f81\u4e00\u81f4\u6027 / Ensures feature consistency"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [KL divergence, policy gradient, on-policy sampling, off-policy training, gradient bias]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mila \u2013 Quebec AI Institute, Universit\xe9 de Montr\xe9al, McGill University, LLNL, University of Edinburgh, CIFAR"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21852",children:"https://arxiv.org/pdf/2512.21852"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>\u8bba\u6587\u6807\u9898"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>KL\u6b63\u5219\u5316\u4f30\u8ba1\u5668\u914d\u7f6e\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u68af\u5ea6\u5b58\u5728\u504f\u5dee"] --\x3e P1["\u5b9e\u8df5\u95ee\u9898/Practical Issue<br>\u5e7f\u6cdb\u4f7f\u7528\u4f46\u5b9e\u73b0\u4e0e\u76ee\u6807\u4e0d\u4e00\u81f4"]\n    Problem --\x3e P2["\u7406\u8bba\u95ee\u9898/Theoretical Issue<br>\u68af\u5ea6\u504f\u5dee\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5206\u6790\u68af\u5ea6\u504f\u5dee\u5e76\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1"] --\x3e M1["\u5206\u6790/Analysis<br>\u7814\u7a76\u591a\u79cd\u4f30\u8ba1\u5668\u914d\u7f6e\u7684\u68af\u5ea6"]\n    Method --\x3e M2["\u5b9e\u9a8c/Experiments<br>RL\u5fae\u8c03\u591a\u4e2aLLM\u5e76\u8bc4\u4f30\u6027\u80fd"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u65e0\u504f\u68af\u5ea6\u914d\u7f6e\u5e26\u6765\u66f4\u597d\u6027\u80fd"] --\x3e R1["\u5728\u7ebf\u7b56\u7565/On-Policy<br>\u65e0\u504f\u68af\u5ea6\u914d\u7f6e\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6027\u80fd"]\n    Results --\x3e R2["\u79bb\u7ebf\u7b56\u7565/Off-Policy<br>KL\u6b63\u5219\u5316\u6709\u52a9\u4e8e\u7a33\u5b9a\u5f02\u6b65\u8bad\u7ec3"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [feature-level fusion, convolutional neural networks, diabetic retinopathy screening, EfficientNet, DenseNet]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," North South University, Korea Institute of Oriental Medicine, American International University\u2013Bangladesh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21861",children:"https://arxiv.org/pdf/2512.21861"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [Privacy-preserving machine learning], [dataset distillation, random forest, synthetic data generation, explainable AI, membership-inference attack]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi\u2019an Jiaotong\u2013Liverpool University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21866",children:"https://arxiv.org/pdf/2512.21866"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5f0f\u6b3a\u8bc8\u68c0\u6d4b/Need for privacy-preserving collaborative fraud detection")\n    Problem --\x3e P2("\u6a21\u578b\u9700\u8981\u53ef\u89e3\u91ca\u6027/Model needs explainability")\n    Method --\x3e M1("\u5c06\u968f\u673a\u68ee\u6797\u8f6c\u6362\u4e3a\u89c4\u5219\u533a\u57df/Convert random forest to rule regions")\n    Method --\x3e M2("\u5728\u533a\u57df\u5185\u5747\u5300\u91c7\u6837\u751f\u6210\u5408\u6210\u6570\u636e/Uniformly sample within regions to generate synthetic data")\n    Results --\x3e R1("\u6570\u636e\u91cf\u51cf\u5c1185-93%/Data volume reduced by 85-93%")\n    Results --\x3e R2("\u4fdd\u6301\u7ade\u4e89\u6027\u6027\u80fd/Maintains competitive performance")\n    Results --\x3e R3("\u62b5\u6297\u6210\u5458\u63a8\u7406\u653b\u51fb/Resists membership-inference attacks")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [multimodal fusion, sparse mixture-of-experts, schema-guided textualization, clinical trial prediction, temperature scaling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Carolina Apar\xedcio, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21897",children:"https://arxiv.org/pdf/2512.21897"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[MMCTOP: \u591a\u6a21\u6001\u6587\u672c\u5316\u4e0e\u4e13\u5bb6\u6df7\u5408\u6846\u67b6<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6311\u6218<br>Multimodal Data Fusion Challenge] --\x3e P1[\u9ad8\u7ef4\u751f\u7269\u533b\u5b66\u4fe1\u606f\u5b66<br>High-Dim Biomedical Informatics]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u591a\u6a21\u6001\u6846\u67b6<br>Multimodal Framework] --\x3e M1[\u6a21\u5f0f\u611f\u77e5\u8868\u5f81\u5b66\u4e60<br>Modality-Aware Representation Learning]\n    Method --\x3e M2[\u67b6\u6784\u8bbe\u8ba1/Architecture Design]\n    M1 --\x3e M1_1[\u9886\u57df\u7279\u5b9a\u7f16\u7801\u5668<br>Domain-Specific Encoders]\n    M2 --\x3e M2_1[\u6a21\u5f0f\u611f\u77e5\u8868\u5f81\u5b66\u4e60<br>Modality-Aware Representation Learning]\n    M2 --\x3e M2_2[\u7a00\u758f\u4e13\u5bb6\u6df7\u5408<br>Sparse Mixture-of-Experts (SMoE)]\n    M2 --\x3e M2_3[\u6a21\u5f0f\u611f\u77e5\u8868\u5f81\u5b66\u4e60<br>Modality-Aware Representation Learning]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u6027\u80fd\u63d0\u5347\u4e0e\u6821\u51c6<br>Performance & Calibration] --\x3e R1[\u6307\u6807\u6539\u8fdb<br>Metric Improvements]\n    Results --\x3e R2[\u6d88\u878d\u7814\u7a76<br>Ablation Studies]\n    Results --\x3e R3[\u6982\u7387\u6821\u51c6<br>Probability Calibration]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] GQ-VAE: A gated quantized VAE for learning variable length tokens"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [tokenization], [GQ-VAE, variable-length tokens, VQ-VAE, neural tokenizer, byte-pair encoding]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Theo Datta, Kayla Huang, Sham Kakade, David Brandfonbrener"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kempner Institute, Harvard University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21913",children:"https://arxiv.org/pdf/2512.21913"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Theo-Datta-115/gq-vae",children:"https://github.com/Theo-Datta-115/gq-vae"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GQ-VAE, a novel gated quantized variational autoencoder architecture for learning discrete, variable-length tokens. 2. Demonstrates the model can serve as a standalone, pre-trained drop-in replacement for existing tokenizers, improving over fixed-chunk baselines. 3. Shows that with equivalent compression, GQ-VAE improves downstream language model learning compared to BPE."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49412452efda1a18023f64f968f3406b217d747381b47a09694b7d37c61c918a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limitations of traditional tokenizers like BPE and complex neural tokenizers by proposing GQ-VAE, a novel architecture that learns variable-length discrete tokens. GQ-VAE can be independently pre-trained as a drop-in replacement, approaching BPE's performance and, under equivalent compression, improving downstream language model learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[GQ-VAE: A gated quantized VAE for learning variable length tokens] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u4f20\u7edf\u5206\u8bcd\u5668\u5982BPE\u662f\u786e\u5b9a\u6027\u7684/Traditional tokenizers like BPE are deterministic]\n    Problem --\x3e P2[\u795e\u7ecf\u5206\u8bcd\u5668\u590d\u6742\u4e14\u96be\u96c6\u6210/Neural tokenizers are complex and hard to integrate]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u63d0\u51faGQ-VAE\u67b6\u6784/Propose GQ-VAE architecture]\n    Method --\x3e M2[\u5b66\u4e60\u53d8\u957f\u79bb\u6563\u4ee4\u724c/Learn variable-length discrete tokens]\n    Method --\x3e M3[\u53ef\u72ec\u7acb\u9884\u8bad\u7ec3/Can be independently pre-trained]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u538b\u7f29\u548c\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u63a5\u8fd1BPE/Compression & LM performance approaches BPE]\n    Results --\x3e R2[\u5728\u540c\u7b49\u538b\u7f29\u4e0b\u63d0\u5347\u4e0b\u6e38LM\u5b66\u4e60/Improves downstream LM learning at equivalent compression]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Tabular Data Generation, Large Language Models, Multi-Arm Bandit, Data Diversity, In-context Learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yafeng Tang, Xiaoou Ding, Jianzhuo Du, Zishuo Yan, Zhuang Ma, Zheng Liang, Zekai Qian, Hongzhi Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21915",children:"https://arxiv.org/pdf/2512.21915"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/windblow32/DATE",children:"https://github.com/windblow32/DATE"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DATE, a framework that partitions heterogeneous tabular data into diverse subsets to prepare high-quality examples for in-context learning. 2. Proves the selection problem in heterogeneous data generation lacks the greedy-choice property and designs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality. 3. Demonstrates that DATE-generated data improves downstream tasks like Direct Preference Optimization (DPO) and enhances LLM reasoning on target data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d8fa8039f8a5fa0717fc5e4a9c7ba2cf1fd158e44821059f4a767bc88790eaf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of generating high-quality synthetic tabular data from heterogeneous distributions. It proposes DATE, a framework that uses LLMs with decision tree feedback for subset-specific generation and a Multi-Arm Bandit algorithm for data selection. Experiments show DATE outperforms existing methods, reducing error rates and improving downstream model performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Real-world tabular data is heterogeneous, making universal generation models challenging"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: DATE framework partitions data, uses LLMs with decision tree feedback, and applies Multi-Arm Bandit for selection"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA methods, reduces error rate by 23.75%, improves DPO and LLM reasoning"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, single-index model, semiparametric, link function, policy learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nathan Kallus"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Netflix, Cornell University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21917",children:"https://arxiv.org/pdf/2512.21917"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Semiparametric Preference Optimization<br>\u4f60\u7684\u8bed\u8a00\u6a21\u578b\u662f\u4e00\u4e2a\u5355\u6307\u6807\u6a21\u578b"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>\u5df2\u77e5\u94fe\u63a5\u51fd\u6570\u9519\u8bef\u5bfc\u81f4\u7b56\u7565\u504f\u5dee<br>Misspecified link function causes policy misalignment"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5c06\u94fe\u63a5\u51fd\u6570\u89c6\u4e3a\u672a\u77e5\u7684\u534a\u53c2\u6570\u5355\u6307\u6807\u6a21\u578b<br>Treat link as unknown semiparametric single-index model"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u5f00\u53d1\u9c81\u68d2\u7684\u7b56\u7565\u5b66\u4e60\u5668\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1<br>Develop robust policy learners with theoretical guarantees"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] AutoPP: Towards Automated Product Poster Generation and Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image generation], [product poster generation, click-through rate optimization, isolated direct preference optimization, AutoPP1M dataset, unified design module]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," JD.COM"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21921",children:"https://arxiv.org/pdf/2512.21921"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/JD-GenX/AutoPP",children:"https://github.com/JD-GenX/AutoPP"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AutoPP: Towards Automated Product Poster Generation and Optimization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4eba\u5de5\u5236\u4f5c\u4e0e\u4f18\u5316\u6d77\u62a5\u8017\u65f6\u8017\u529b/Manual poster creation and optimization is laborious]\n    C --\x3e C1[\u81ea\u52a8\u5316\u751f\u6210\u4e0e\u4f18\u5316\u7ba1\u9053/Automated generation and optimization pipeline]\n    C1 --\x3e C1_1[\u751f\u6210\u5668: \u7edf\u4e00\u8bbe\u8ba1\u6a21\u5757\u4e0e\u5143\u7d20\u6e32\u67d3/Generator: Unified design & element rendering]\n    C1 --\x3e C1_2[\u4f18\u5316\u5668: \u5143\u7d20\u66ff\u6362\u4e0eIDPO/Optimizer: Element replacement & IDPO]\n    C --\x3e C2[\u6570\u636e\u96c6: AutoPP1M/Dataset: AutoPP1M]\n    D --\x3e D1[\u79bb\u7ebf\u548c\u5728\u7ebfSOTA\u7ed3\u679c/Offline and online SOTA results]\n    D --\x3e D2[\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u516c\u5f00/Code & dataset released]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Data relativistic uncertainty framework for low-illumination anime scenery image enhancement"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [low-light image enhancement], [Data Relativistic Uncertainty, Unsupervised Learning, EnlightenGAN, Anime Scenery, Domain Gap]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yiquan Gao, John See"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Heriot-Watt University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21944",children:"https://arxiv.org/pdf/2512.21944"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Data relativistic uncertainty framework for low-illumination anime scenery image enhancement") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.")\n    Results("\u5173\u952e\u7ed3\u679c/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-armed bandits], [combinatorial multi-armed bandits, probabilistically triggered arms, hybrid learning, offline data, online interaction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kongchang Zhou, Tingyu Zhang, Wei Chen, Fang Kong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Southern University of Science and Technology, Microsoft Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21925",children:"https://arxiv.org/pdf/2512.21925"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new hybrid CMAB-T framework that integrates offline data with online interaction to address the complementary weaknesses of purely online or offline methods. 2. Introduces the hybrid CUCB algorithm, which leverages offline data to guide exploration and strategically uses online interactions to correct dataset bias. 3. Provides theoretical regret guarantees and empirical results demonstrating the algorithm's consistent advantage over purely online or offline baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e97b8cf09a0691d48238a8272fecd32791ddc20b9ba00115a757d778b1e2017f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid framework for combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) that combines offline data with online interaction. The core method is the hybrid CUCB algorithm, which uses offline data to accelerate learning and online interaction to correct for dataset limitations. Theoretical and empirical results show this hybrid approach outperforms purely online or offline methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Hybrid CMAB-T<br>\u6df7\u5408\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u5728\u7ebf\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6162<br>Online: High Cost, Slow")\n    Problem --\x3e P2("\u79bb\u7ebf\u65b9\u6cd5\u53d7\u6570\u636e\u8d28\u91cf\u9650\u5236<br>Offline: Data Quality Limits")\n    Method --\x3e M1("\u63d0\u51fa\u6df7\u5408CMAB-T\u6846\u67b6<br>Propose Hybrid CMAB-T Framework")\n    Method --\x3e M2("\u8bbe\u8ba1\u6df7\u5408CUCB\u7b97\u6cd5<br>Design Hybrid CUCB Algorithm")\n    M2 --\x3e M2a("\u5229\u7528\u79bb\u7ebf\u6570\u636e\u5f15\u5bfc\u63a2\u7d22<br>Use Offline Data to Guide")\n    M2 --\x3e M2b("\u7ed3\u5408\u5728\u7ebf\u4ea4\u4e92\u7ea0\u6b63\u504f\u5dee<br>Use Online to Correct Bias")\n    Results --\x3e R1("\u7406\u8bba\u6094\u6068\u754c\u4fdd\u8bc1<br>Theoretical Regret Guarantee")\n    Results --\x3e R2("\u5b9e\u9a8c\u663e\u793a\u4e00\u81f4\u4f18\u52bf<br>Empirical Consistent Advantage")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [hallucination mitigation, adversarial parametric editing, parameter clustering, visual-language models, activation dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Chongqing University, Xinjiang University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21999",children:"https://arxiv.org/pdf/2512.21999"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/hujiayu1223/ALEAHallu",children:"https://github.com/hujiayu1223/ALEAHallu"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: VLM\u5e7b\u89c9\u95ee\u9898/VLM Hallucination Issue]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ALEAHallu\u6846\u67b6/ALEAHallu Framework]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u6709\u6548\u7f13\u89e3\u5e7b\u89c9/Effectively Mitigates Hallucinations]\n    C --\x3e C1[\u6fc0\u6d3b\u6570\u636e\u96c6/Activation Dataset]\n    C --\x3e C2[\u5b9a\u4f4d\u5173\u952e\u53c2\u6570/Locate Critical Parameters]\n    C --\x3e C3[\u5bf9\u6297\u6027\u7f16\u8f91/Adversarial Editing]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [computational biology], [protein language model, ESM-2, dual-stream architecture, 1D CNN, transformer encoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aicha Boutorh, Soumia Bouyahiaoui, Sara Belhadj, Nour El Yakine Guendouz, Manel Kara Laouar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National School of Artificial Intelligence (ENSIA)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22007",children:"https://arxiv.org/pdf/2512.22007"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DuaDeep-SeqAffinity, a novel sequence-only deep learning framework for antigen-antibody affinity prediction using a dual-stream hybrid architecture. 2. Integrates pre-trained ESM-2 embeddings with 1D CNNs for local motifs and Transformer encoders for global context, followed by a fusion module. 3. Demonstrates superior performance over single-branch models and existing SOTA methods, even surpassing some structure-sequence hybrid models, proving the efficacy of sequence-only high-fidelity embeddings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1985304be62407b10b5e5be53aea80fe4ff1468be03e261847b49774ddd937a8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces DuaDeep-SeqAffinity, a deep learning framework that predicts antigen-antibody binding affinity using only amino acid sequences. It combines ESM-2 embeddings with a dual-stream architecture of 1D CNNs and Transformers to capture local and global features. The model outperforms existing methods, showing that sequence-only models can effectively capture binding patterns and accelerate therapeutic discovery."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DuaDeep-SeqAffinity: \u5e8f\u5217\u6297\u539f-\u6297\u4f53\u4eb2\u548c\u529b\u9884\u6d4b / Sequence-Only Antigen-Antibody Affinity Prediction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7a00\u7f3a\u76843D\u7ed3\u6784 / Traditional methods rely on scarce 3D structures]\n    C --\x3e C1[\u53cc\u6d41\u6df7\u5408\u67b6\u6784 / Dual-Stream Hybrid Architecture]\n    C1 --\x3e C2[\u4f7f\u7528ESM-2\u5d4c\u5165 / Uses ESM-2 Embeddings]\n    C1 --\x3e C3[1D CNN\u68c0\u6d4b\u5c40\u90e8\u6a21\u5f0f / 1D CNN for Local Motifs]\n    C1 --\x3e C4[Transformer\u7f16\u7801\u5168\u5c40\u4e0a\u4e0b\u6587 / Transformer for Global Context]\n    C1 --\x3e C5[\u878d\u5408\u6a21\u5757\u6574\u5408\u7279\u5f81 / Fusion Module Integrates Features]\n    D --\x3e D1[\u6027\u80fd\u8d85\u8d8aSOTA / Outperforms SOTA]\n    D --\x3e D2[\u76ae\u5c14\u900a\u76f8\u5173: 0.688 / Pearson: 0.688]\n    D --\x3e D3[AUC: 0.890]\n    D --\x3e D4[\u8bc1\u660e\u5e8f\u5217\u5d4c\u5165\u7684\u6709\u6548\u6027 / Proves Efficacy of Sequence Embeddings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [hypergraph isomorphism network, hypergraph weisfeiler-lehman test, higher-order network robustness, hypergraph neural networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chengyu Tian, Wenbin Pei"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in the provided content."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22014",children:"https://arxiv.org/pdf/2512.22014"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN) framework for hypergraph learning. 2. Theoretically proves the model's expressive power is strictly equivalent to the Hypergraph Weisfeiler-Lehman test. 3. Successfully applies the model to predict hypergraph robustness, demonstrating superior performance and efficiency over existing graph-based and conventional HGNN models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3b228ae57b3d7937d8586b0c60419df83b49466ba7ca3b946109dd8186f827c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational overhead of traditional robustness assessment and the limited expressive power of existing hypergraph neural networks by proposing a hypergraph-level Hypergraph Isomorphism Network (HWL-HIN). The method is proven theoretically to be as powerful as the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experiments show it outperforms existing graph-based models and conventional HGNNs in tasks prioritizing topological structure while maintaining high efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HWL-HIN: Hypergraph-Level Hypergraph Isomorphism Network] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: High computational cost of robustness assessment; Limited expressive power of HGNNs)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Propose HWL-HIN framework inspired by GIN; Prove expressive power equivalent to Hypergraph WL test)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms graph-based models and HGNNs; Maintains superior efficiency)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models for drug discovery], [hit-like molecule generation, autoregressive models, diffusion models, docking scores, multi-stage filtering]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University College London, University of Urbino Carlo Bo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22031",children:"https://arxiv.org/pdf/2512.22031"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3\u03b2 hits confirmed active in vitro."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Hit identification is resource-intensive/\u547d\u4e2d\u8bc6\u522b\u8d44\u6e90\u5bc6\u96c6")\n    Method --\x3e M1("Propose tailored evaluation framework/\u63d0\u51fa\u5b9a\u5236\u8bc4\u4f30\u6846\u67b6")\n    Method --\x3e M2("Benchmark autoregressive & diffusion models/\u57fa\u51c6\u6d4b\u8bd5\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b")\n    Results --\x3e R1("Models generate valid, diverse, bioactive compounds/\u6a21\u578b\u751f\u6210\u6709\u6548\u3001\u591a\u6837\u3001\u6709\u751f\u7269\u6d3b\u6027\u7684\u5316\u5408\u7269")\n    Results --\x3e R2("Selected hits synthesized & confirmed active/\u9009\u5b9a\u547d\u4e2d\u7269\u88ab\u5408\u6210\u5e76\u786e\u8ba4\u6709\u6548")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [signal processing], [DOA estimation, sparse arrays, coarrays, spatial smoothing, MUSIC]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wesley S. Leite, Rodrigo C. de Lamare, Yuriy Zakharov, Wei Liu, Martin Haardt"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of York, Pontifical Catholic University of Rio de Janeiro, University of York, University of York, Ilmenau University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22024",children:"https://arxiv.org/pdf/2512.22024"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a variable window size (VWS) spatial smoothing framework for coarray-based DOA estimation with sparse linear arrays. 2. Proposes the VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms that replace perturbed data terms with unperturbed ones to improve signal-noise subspace separation. 3. Derives theoretical bounds for the compression parameter to guarantee identifiability and demonstrates performance improvements and complexity savings over fixed-window methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20bb39a95f66255cc62bbe7ee6e002de101d9a346c703b72e27fc3d3b08e1d30_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of direction-of-arrival (DOA) estimation for correlated sources using sparse linear arrays. It proposes a variable window size spatial smoothing framework and new VWS-CA-MUSIC algorithms that enhance estimation accuracy by compressing the smoothing aperture. Simulations show the method provides significant performance gains and reduced computational complexity compared to standard fixed-window coarray MUSIC."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[DOA\u4f30\u8ba1\u7cbe\u5ea6\u4e0b\u964d/DOA estimation accuracy degrades for correlated/coherent sources]\n    C --\x3e C1[\u53ef\u53d8\u7a97\u53e3\u7a7a\u95f4\u5e73\u6ed1/Variable Window Size Spatial Smoothing]\n    C --\x3e C2[\u538b\u7f29\u5e73\u6ed1\u5b54\u5f84/Compressing the smoothing aperture]\n    C --\x3e C3[VWS-CA-MUSIC\u7b97\u6cd5/VWS-CA-MUSIC algorithm]\n    D --\x3e D1[\u63d0\u9ad8\u4fe1\u566a\u5b50\u7a7a\u95f4\u5206\u79bb/Increased signal-noise subspace separation]\n    D --\x3e D2[\u6027\u80fd\u63d0\u5347\u4e0e\u590d\u6742\u5ea6\u964d\u4f4e/Performance improvements and complexity savings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] LibContinual: A Comprehensive Library towards Realistic Continual Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [catastrophic forgetting, stability-plasticity dilemma, modular architecture, memory budget, online continual learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanjing University, University of Wollongong, University of Rochester"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22029",children:"https://arxiv.org/pdf/2512.22029"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/RL-VIG/LibContinual",children:"https://github.com/RL-VIG/LibContinual"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u7814\u7a76\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6/Fragmented research landscape, lack of unified framework]\n    B --\x3e B2[\u8bc4\u4f30\u5b58\u5728\u4e0d\u73b0\u5b9e\u7684\u9690\u542b\u5047\u8bbe/Unrealistic implicit assumptions in evaluation]\n    C --\x3e C1[\u6784\u5efa\u6a21\u5757\u5316\u3001\u53ef\u590d\u73b0\u7684\u5e93/Build a modular, reproducible library]\n    C --\x3e C2[\u96c6\u621019\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5/Integrate 19 representative algorithms]\n    C --\x3e C3[\u5728\u66f4\u73b0\u5b9e\u7684\u8bbe\u5b9a\u4e0b\u7cfb\u7edf\u8bc4\u4f30/Systematically evaluate under more realistic settings]\n    D --\x3e D1[\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d/Existing methods show significant performance drop under realistic constraints]\n    D --\x3e D2[\u5f3a\u8c03\u8d44\u6e90\u611f\u77e5\u548c\u8bed\u4e49\u9c81\u68d2\u7b56\u7565\u7684\u5fc5\u8981\u6027/Highlight the necessity of resource-aware and semantically robust strategies]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Why Smooth Stability Assumptions Fail for ReLU Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [optimization theory], [ReLU networks, nonsmooth optimization, stability analysis, generalized derivatives, learning dynamics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ronald Katende"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kabale University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22055",children:"https://arxiv.org/pdf/2512.22055"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that no uniform smoothness-based stability proxy (e.g., gradient Lipschitzness) can hold globally for ReLU networks, even in simple, empirically stable settings. 2. Provides a concrete counterexample showing the failure of classical stability bounds for ReLU learning dynamics. 3. Identifies a minimal generalized derivative condition under which stability statements can be meaningfully restored for nonsmooth learning systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3150186f6ab289f5e6db87d8ca89851af409290b0b4c37667d493ee4b7e894b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper shows that smoothness assumptions like gradient Lipschitzness, commonly used in stability analyses, fail globally for ReLU networks due to their inherent nonsmoothness. It provides a counterexample to classical bounds and proposes a minimal condition based on generalized derivatives to restore meaningful stability analysis. This clarifies the limitations of smooth approximations and motivates nonsmooth-aware frameworks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Why Smooth Stability Assumptions Fail for ReLU Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Smooth stability assumptions are violated by ReLU networks.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Provide counterexample and identify minimal generalized derivative condition.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Classical bounds fail; stability can be restored under nonsmooth-aware condition.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Scaling Adversarial Training via Data Selection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [adversarial robustness], [adversarial training, PGD, sample selection, gradient matching, margin-based sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Youran Ye, Dejin Wang, Ajinkya Bhandare"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22069",children:"https://arxiv.org/pdf/2512.22069"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/youranye/Selective-Adversarial-Training",children:"https://github.com/youranye/Selective-Adversarial-Training"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Selective Adversarial Training, which perturbs only a critical subset of samples per minibatch to reduce computational cost., 2. Introduces two novel sample selection criteria: margin-based sampling and gradient-matching sampling., 3. Demonstrates that the method achieves comparable or superior robustness to full PGD adversarial training while reducing adversarial computation by up to 50%."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad822d1db3ef050d2caa84f51828c7b48f93eee442f9a9f954f4f36b07929f44_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high computational cost of Projected Gradient Descent (PGD) in adversarial training. It proposes Selective Adversarial Training, which uses principled criteria to select and perturb only critical samples in each batch. Experiments show the method maintains robustness while cutting adversarial computation by up to 50%."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Scaling Adversarial Training via Data Selection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: PGD\u8ba1\u7b97\u6210\u672c\u9ad8/High PGD Computational Cost]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u9009\u62e9\u6027\u5bf9\u6297\u8bad\u7ec3/Selective Adversarial Training]\n    C --\x3e D[\u9009\u62e9\u6807\u51c61: \u57fa\u4e8e\u8fb9\u754c\u7684\u91c7\u6837/Margin-based Sampling]\n    C --\x3e E[\u9009\u62e9\u6807\u51c62: \u68af\u5ea6\u5339\u914d\u91c7\u6837/Gradient-matching Sampling]\n    A --\x3e F[\u5173\u952e\u7ed3\u679c/Results: \u9c81\u68d2\u6027\u76f8\u5f53\uff0c\u8ba1\u7b97\u51cf\u5c1150%/Comparable Robustness, 50% Computation Reduction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [SRAM, frequency scaling, energy-delay product, systolic array, memory bandwidth]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Uppsala University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22066",children:"https://arxiv.org/pdf/2512.22066"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>LLM\u63a8\u7406\u80fd\u8017\u9ad8\uff0cPrefill\u4e0eDecode\u9636\u6bb5\u74f6\u9888\u4e0d\u540c"] --\x3e Problem_Sub1["SRAM\u5927\u5c0f\u4e0e\u9891\u7387\u5982\u4f55\u5f71\u54cd\u80fd\u6548\uff1f"]\n    Problem --\x3e Problem_Sub2["\u5185\u5b58\u5e26\u5bbd\u5982\u4f55\u9650\u5236\u6027\u80fd\uff1f"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u7ed3\u5408OpenRAM, LLMCompass, ScaleSIM\u7684\u6a21\u62df\u65b9\u6cd5"] --\x3e Method_Sub1["\u80fd\u8017\u5efa\u6a21/Energy Modeling"]\n    Method --\x3e Method_Sub2["\u5ef6\u8fdf\u6a21\u62df/Latency Simulation"]\n    Method --\x3e Method_Sub3["\u64cd\u4f5c\u5f3a\u5ea6\u5206\u6790/Operational Intensity"]\n    Results["\u5173\u952e\u7ed3\u679c/Results"] --\x3e Results_Sub1["\u603b\u80fd\u8017\u4e3b\u8981\u7531SRAM\u5927\u5c0f\u51b3\u5b9a<br>\u5927\u7f13\u5b58\u589e\u52a0\u9759\u6001\u80fd\u8017"]\n    Results --\x3e Results_Sub2["\u9ad8\u9891\u53ef\u964d\u4f4e\u603b\u80fd\u8017<br>\uff08\u51cf\u5c11\u9759\u6001\u80fd\u8017\uff09"]\n    Results --\x3e Results_Sub3["\u6700\u4f18\u914d\u7f6e\uff1a\u9ad8\u9891(1200-1400MHz) + \u5c0f\u7f13\u5b58(32-64KB)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Unifying Learning Dynamics and Generalization in Transformers Scaling Law"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [learning theory], [scaling law, learning dynamics, generalization error, transformer, stochastic gradient descent]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chiwun Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22088",children:"https://arxiv.org/pdf/2512.22088"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of \u0398(C^{-1/6}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Scaling Law\u7406\u8bba\u539f\u7406\u4e0d\u6e05 / Poorly understood theoretical underpinnings of scaling laws]\n    C --\x3e C1[\u5f62\u5f0f\u5316\u5b66\u4e60\u52a8\u6001\u4e3aODE\u7cfb\u7edf / Formalize learning dynamics as ODE system]\n    C --\x3e C2[\u8fd1\u4f3c\u4e3a\u6838\u884c\u4e3a / Approximate to kernel behaviors]\n    C --\x3e C3[\u5206\u6790SGD\u8bad\u7ec3\u771f\u5b9eTransformer / Analyze SGD training for real transformers]\n    D --\x3e D1[\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\u4e0e\u76f8\u53d8 / Upper bound on excess risk with phase transition]\n    D --\x3e D2[\u4f18\u5316\u76f8:\u6307\u6570\u8870\u51cf / Optimization phase: Exponential decay]\n    D --\x3e D3[\u7edf\u8ba1\u76f8:\u5e42\u5f8b\u8870\u51cf \u0398(C^{-1/6}) / Statistical phase: Power-law decay \u0398(C^{-1/6})]\n    D --\x3e D4[\u5206\u79bb\u7684\u89c4\u6a21\u5b9a\u5f8b / Isolated scaling laws for model size, time, data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent pipeline, automated data analysis, insight generation, report synthesis, visual analytics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Minnesota"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22101",children:"https://arxiv.org/pdf/2512.22101"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A2P-Vis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u74f6\u9888/Gaps in automating data science]\n    B1 --\x3e B2[\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u53ef\u89c6\u5316/Generating insightful visual evidence]\n    B1 --\x3e B3[\u7ec4\u88c5\u6210\u4e13\u4e1a\u62a5\u544a/Assembling coherent professional report]\n    C --\x3e C1[\u4e24\u90e8\u5206\u591a\u667a\u80fd\u4f53\u7ba1\u9053/Two-part multi-agent pipeline]\n    C1 --\x3e C2[\u6570\u636e\u5206\u6790\u5668/Data Analyzer]\n    C2 --\x3e C3[\u751f\u6210\u5e76\u8bc4\u4f30\u56fe\u8868\u4e0e\u6d1e\u5bdf/Generates & evaluates charts & insights]\n    C1 --\x3e C4[\u62a5\u544a\u5448\u73b0\u5668/Presenter]\n    C4 --\x3e C5[\u7f16\u6392\u4e3b\u9898\u5e76\u64b0\u5199\u53d9\u8ff0/Orders topics & composes narrative]\n    D --\x3e D1[\u7aef\u5230\u7aef\u534f\u540c\u5206\u6790/End-to-end co-analysis]\n    D1 --\x3e D2[\u63d0\u9ad8\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\u7684\u5b9e\u7528\u6027/Improves usefulness of automated analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Explainable Multimodal Regression via Information Decomposition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal machine learning], [Partial Information Decomposition (PID), multimodal regression, interpretability, Gaussianity assumption, conditional independence regularizer]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaozhao Ma, Shujian Yu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Georgia Institute of Technology, Vrije Universiteit Amsterdam, UiT - The Arctic University of Norway"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22102",children:"https://arxiv.org/pdf/2512.22102"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/xxx/PIDReg",children:"https://github.com/xxx/PIDReg"})," (URL placeholder from abstract)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel multimodal regression framework based on Partial Information Decomposition (PID) to quantify unique, redundant, and synergistic information from modalities. 2. Introduction of a Gaussianity assumption to resolve the underdetermined nature of PID, enabling analytical computation of its terms. 3. Derivation of a closed-form conditional independence regularizer to isolate unique information within each modality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a07cab8b6bb765060f8a015e46e79e686a9acb69bac3aa8045cf96acec9f61e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of interpretability in multimodal regression by proposing a new framework based on Partial Information Decomposition (PID). The method resolves PID's underdetermination by enforcing Gaussianity and uses a conditional independence regularizer to isolate unique modality information. Experiments on six datasets, including brain age prediction, show the framework outperforms state-of-the-art methods in both accuracy and interpretability, while aiding modality selection."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Explainable Multimodal Regression via Information Decomposition<br/>\u53ef\u89e3\u91ca\u591a\u6a21\u6001\u56de\u5f52\u4e0e\u4fe1\u606f\u5206\u89e3] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u91cf\u5316\u6a21\u6001\u8d21\u732e\u4e0e\u4ea4\u4e92\u7684\u5de5\u5177<br/>Existing methods lack tools to quantify modality contributions & interactions]\n    C --\x3e C1[\u57fa\u4e8ePID\u5206\u89e3\u6a21\u6001\u4fe1\u606f<br/>Decompose modality info via PID]\n    C --\x3e C2[\u5f15\u5165\u9ad8\u65af\u6027\u5047\u8bbe\u4e0e\u6b63\u5219\u5316<br/>Introduce Gaussianity & regularizer]\n    D --\x3e D1[\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA<br/>Outperforms SOTA on 6 datasets]\n    D --\x3e D2[\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027<br/>Improves predictive accuracy & interpretability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Sensitivity Analysis of the Consistency Assumption"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [causal inference], [consistency assumption, sensitivity analysis, hidden versions of treatment, partial identification, stable unit treatment value assumption (SUTVA)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Utah Valley University, University of Gothenburg, University of Minnesota, Michigan State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21379",children:"https://arxiv.org/pdf/2512.21379"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A formal mathematical framework for analyzing violations of the consistency assumption. 2. A novel sensitivity parameter to quantify bias induced by hidden versions of treatment. 3. Methods for specifying bounds on this parameter to enable partial identification of causal effects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0816c25e8631081977ffb91ecadbb3f527cfd99c7daf15f81c3f18a332125fcc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses violations of the consistency assumption in causal inference, which posits no hidden versions of treatment. It proposes a new sensitivity analysis method focused on confounding by hidden treatment versions, introducing a formal framework and a novel sensitivity parameter. The work enables researchers to assess and bound the bias from consistency violations when interpreting causal estimates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Sensitivity Analysis of the Consistency Assumption] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e00\u81f4\u6027\u5047\u8bbe\u53ef\u80fd\u88ab\u8fdd\u53cd/Consistency Assumption May Be Violated]\n    B1 --\x3e B2[\u5b58\u5728\u9690\u85cf\u7684\u6cbb\u7597\u7248\u672c/Hidden Versions of Treatment Exist]\n    C --\x3e C1[\u65b0\u9896\u7684\u654f\u611f\u6027\u5206\u6790\u65b9\u6cd5/Novel Sensitivity Analysis Method]\n    C1 --\x3e C2[\u4e13\u6ce8\u4e8e\u9690\u85cf\u7248\u672c\u5bfc\u81f4\u7684\u6df7\u6742/Focus on Confounding by Hidden Versions]\n    C2 --\x3e C3[\u5f15\u5165\u65b0\u7684\u6570\u5b66\u7b26\u53f7/Introduces New Mathematical Notation]\n    D --\x3e D1[\u63d0\u51fa\u65b0\u7684\u654f\u611f\u6027\u53c2\u6570/Proposes New Sensitivity Parameter]\n    D1 --\x3e D2[\u4fbf\u4e8e\u90e8\u5206\u8bc6\u522b\u56e0\u679c\u4f30\u8ba1\u91cf/Facilitates Partial Identification of Causal Estimands]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatio-temporal forecasting], [dynamic attention, zero-inflated negative binomial, elastic-net gating]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Stefano M. Iacus, Haodong Qi, Marcello Carammia, Thomas Juneau"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harvard University, Stockholm University, Malm\xf6 University, University of Catania, University of Toronto"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21435",children:"https://arxiv.org/pdf/2512.21435"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces DynAttn, an interpretable dynamic-attention framework for high-dimensional spatio-temporal count processes. 2. Combines rolling-window estimation, shared elastic-net feature gating, a compact self-attention encoder, and a ZINB likelihood for calibrated forecasts. 3. Demonstrates superior predictive accuracy and enables structured interpretation of regional conflict dynamics, showing the roles of persistence, diffusion, and climate stress."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad96b82177be36f56ddfe0b4e4d6be51396aa49723cdabef2c85f538301f2e0e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces DynAttn, an interpretable forecasting framework for sparse, high-dimensional spatio-temporal data like conflict fatalities. It combines dynamic attention, feature gating, and a specialized likelihood to produce accurate, multi-horizon forecasts and probabilities. The method outperforms benchmarks, particularly in sparse settings, and provides interpretable insights into conflict drivers such as persistence, spatial diffusion, and conditional climate effects."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DynAttn: Interpretable Spatio-Temporal Forecasting] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Forecasting sparse, bursty conflict fatalities)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Dynamic attention, elastic-net gating, ZINB likelihood)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Higher accuracy, interpretable regional dynamics)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [physics-informed machine learning], [molecular dynamics, convolutional neural network, friction stir welding, explainable AI, LAMMPS]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Akshansh Mishra"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Politecnico di Milano, AI Fab Lab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21344",children:"https://arxiv.org/pdf/2512.21344"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R\xb2=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / \u539f\u5b50\u6a21\u62df\u5f15\u5bfc\u7684CNN\u7528\u4e8e\u6405\u62cc\u6469\u64e6\u710a\u70ed\u5efa\u6a21"]\n    Root --\x3e Problem["\u51c6\u786e\u9884\u6d4b\u6e29\u5ea6\u6f14\u5316\u5bf9\u4e8e\u7406\u89e3\u6405\u62cc\u6469\u64e6\u710a\u7684\u70ed\u673a\u68b0\u884c\u4e3a\u81f3\u5173\u91cd\u8981 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW"]\n    Root --\x3e Method["\u4f7f\u7528LAMMPS\u8fdb\u884c\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u5c06\u539f\u5b50\u6570\u636e\u8f6c\u6362\u4e3a\u7269\u7406\u4e8c\u7ef4\u7a7a\u95f4\u7f51\u683c\uff0c\u5e76\u5f00\u53d12D CNN\u8fdb\u884c\u9884\u6d4b / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction"]\n    Root --\x3e Results["\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff08R\xb2=0.94\uff09\uff0cCAM\u5206\u6790\u8868\u660e\u6a21\u578b\u5173\u6ce8\u4e0e\u5267\u70c8\u53d8\u5f62\u548c\u751f\u70ed\u76f8\u5173\u7684\u533a\u57df / Model achieves high predictive accuracy (R\xb2=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [information geometry], [Fisher-Rao metric, non-parametric, G-entropy, Covariate Fisher Information Matrix (cFIM), intrinsic dimensionality]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bing Cheng, Howell Tong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content. Affiliation likely inferred from author names or arXiv metadata, but not present in the given text."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21451",children:"https://arxiv.org/pdf/2512.21451"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an orthogonal decomposition of the tangent space to derive a finite-dimensional, computable Covariate Fisher Information Matrix (cFIM) from the infinite-dimensional Fisher-Rao metric. 2. Establishes the geometric foundation of G-entropy via a Trace Theorem, linking it to total explainable statistical information. 3. Connects the cFIM to the Cram\xe9r-Rao Lower Bound and the Manifold Hypothesis, providing a method for estimating intrinsic dimensionality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a8f179358a2d0d118621b506d45d051ad949b1cc0acfe723d3c3268c4390e5a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles the intractability of the infinite-dimensional Fisher-Rao metric in non-parametric information geometry by proposing an orthogonal decomposition of the tangent space. This decomposition yields a finite-dimensional Covariate Fisher Information Matrix (cFIM), which serves as a computable geometric object for analyzing statistical information and model efficiency. The framework rigorously grounds G-entropy, links to fundamental statistical bounds, and provides a testable condition for the Manifold Hypothesis, bridging abstract geometry with explainable AI."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[An approach to Fisher-Rao metric for infinite dimensional non-parametric information geometry] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Intractability of infinite-dimensional Fisher-Rao metric)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Orthogonal decomposition of tangent space to derive Covariate Fisher Information Matrix (cFIM))\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Trace Theorem for G-entropy, link to CRLB, testable Manifold Hypothesis via cFIM rank)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [biomedical sensing and diagnostics], [vertical flow assay, dual-mode detection, neural network-based quantification, multiplexed optical sensor, point-of-care diagnostics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye, Aoi Tomoeda, Emily Ngo, Margherita Scussat, Xiao Wang, Zixiang Ji, Max Zhang, Jeffrey J. Hsu, Omai B. Garner, Dino Di Carlo, Aydogan Ozcan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21389",children:"https://arxiv.org/pdf/2512.21389"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a dual-mode (colorimetric and chemiluminescent) multiplexed vertical flow assay (xVFA) for simultaneous detection of three cardiac biomarkers, achieving a wide dynamic range of ~6 orders of magnitude. 2. Integrated a deep learning-based quantification pipeline with a portable optical reader to automate analysis and enhance accuracy, achieving high correlation (Pearson's r > 0.96) with reference assays. 3. Created a compact, cost-effective, and rapid (23 min) point-of-care diagnostic system for cardiovascular diseases using only 50 \xb5L of serum."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6eb23e46dc346744879ce56ec8c667b82f8cab434c54e4a4922a2eb842bf77d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a deep learning-enhanced, dual-mode optical sensor for point-of-care cardiovascular diagnostics. The system uses a multiplexed vertical flow assay with colorimetric and chemiluminescent detection to simultaneously measure three key biomarkers with high sensitivity across a wide dynamic range, and a neural network pipeline for accurate quantification. The result is a rapid, portable, and automated diagnostic tool validated on patient samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Deep learning-enhanced dual-mode multiplexed optical sensor<br>\u6df1\u5ea6\u5b66\u4e60\u589e\u5f3a\u7684\u53cc\u6a21\u5f0f\u591a\u91cd\u5149\u5b66\u4f20\u611f\u5668] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Current POC tests: long turnaround, narrow range, single-analyte<br>\u5f53\u524dPOC\u6d4b\u8bd5\uff1a\u8017\u65f6\u957f\u3001\u8303\u56f4\u7a84\u3001\u5355\u5206\u6790\u7269]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Dual-mode (colorimetric+chemiluminescent) xVFA + Neural Network<br>\u53cc\u6a21\u5f0f(\u6bd4\u8272+\u5316\u5b66\u53d1\u5149)xVFA + \u795e\u7ecf\u7f51\u7edc]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Simultaneous 3-analyte quant in 23 min, wide dynamic range, r>0.96<br>23\u5206\u949f\u540c\u6b653\u5206\u6790\u7269\u5b9a\u91cf\uff0c\u5bbd\u52a8\u6001\u8303\u56f4\uff0cr>0.96]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Quantum Nondecimated Wavelet Transform: Theory, Circuits, and Applications"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [quantum signal processing], [quantum nondecimated wavelet transform, epsilon decimation, Hadamard test, quantum wavelet shrinkage, shift invariance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Brani Vidakovic"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Texas A&M University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21478",children:"https://arxiv.org/pdf/2512.21478"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a quantum formulation of the Nondecimated Wavelet Transform (NDWT) based on epsilon-decimation, using a quantum register for shift indices and controlled circular shifts to realize all shifted transforms simultaneously. 2. Proposed an alternative quantum NDWT formulation using the Hadamard test and diagonal phase operators to directly access shift-invariant energy scalograms without full coefficient reconstruction. 3. Demonstrated that quantum redundancy and translation invariance can be exploited for applications like denoising and feature extraction, providing a foundation for multiscale quantum signal processing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01489d687d6b4e38b42c82e3054e553beab66883e073aec7483d40c87388a47a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces two quantum formulations of the classical Nondecimated Wavelet Transform (NDWT) to achieve shift invariance and redundancy in quantum computation. The first uses controlled shifts and a wavelet analysis unitary, while the second employs the Hadamard test with phase operators for direct energy measurement. The work shows these quantum NDWTs enable coherent multiscale signal processing tasks like denoising and feature extraction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Quantum Nondecimated Wavelet Transform<br/>\u91cf\u5b50\u975e\u62bd\u53d6\u5c0f\u6ce2\u53d8\u6362] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How to embed classical NDWT's redundancy and shift invariance into quantum computation?<br/>\u5982\u4f55\u5c06\u7ecf\u5178NDWT\u7684\u5197\u4f59\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u5d4c\u5165\u91cf\u5b50\u8ba1\u7b97\uff1f)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Two complementary quantum formulations.<br/>\u4e24\u79cd\u4e92\u8865\u7684\u91cf\u5b50\u5f62\u5f0f\u3002)\n    C --\x3e C1(Formulation 1: Epsilon-decimated, uses controlled circular shifts & wavelet unitary.<br/>\u65b9\u6cd5\u4e00\uff1a\u57fa\u4e8e\u03b5\u62bd\u53d6\uff0c\u4f7f\u7528\u53d7\u63a7\u5faa\u73af\u79fb\u4f4d\u548c\u5c0f\u6ce2\u9149\u53d8\u6362\u3002)\n    C --\x3e C2(Formulation 2: Hadamard test, uses diagonal phase operators for interference.<br/>\u65b9\u6cd5\u4e8c\uff1a\u57fa\u4e8eHadamard\u6d4b\u8bd5\uff0c\u4f7f\u7528\u5bf9\u89d2\u76f8\u4f4d\u7b97\u5b50\u8fdb\u884c\u5e72\u6d89\u3002)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Quantum NDWTs enable coherent postprocessing (e.g., shrinkage) and direct access to scalograms/spectra for applications like denoising.<br/>\u91cf\u5b50NDWT\u652f\u6301\u76f8\u5e72\u540e\u5904\u7406\u5e76\u53ef\u76f4\u63a5\u83b7\u53d6\u5c3a\u5ea6\u56fe/\u9891\u8c31\uff0c\u7528\u4e8e\u53bb\u566a\u7b49\u5e94\u7528\u3002)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [diffusion models, generative modeling, evidence lower bound, residual learning, two-stage framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Takuro Kutsuna"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Toyota Central R&D Labs., Inc."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21593",children:"https://arxiv.org/pdf/2512.21593"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Residual Prior Diffusion (RPD) / \u6b8b\u5dee\u5148\u9a8c\u6269\u6563\u6a21\u578b"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["\u5355\u4e00\u6269\u6563\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u7ec6\u8282 / Single diffusion model struggles with global structure and local details"]\n    Method --\x3e M1["\u4e24\u9636\u6bb5\u6846\u67b6: \u7c97\u7c92\u5ea6\u5148\u9a8c + \u6b8b\u5dee\u6269\u6563\u6a21\u578b / Two-stage framework: coarse prior + residual diffusion model"]\n    Method --\x3e M2["\u6982\u7387\u6a21\u578b\u4e0e\u53ef\u5904\u7406ELBO / Probabilistic model with tractable ELBO"]\n    Results --\x3e R1["\u5728\u5408\u6210\u6570\u636e\u4e0a\u51c6\u786e\u6355\u6349\u7ec6\u8282 / Accurately captures details on synthetic data"]\n    Results --\x3e R2["\u81ea\u7136\u56fe\u50cf\u751f\u6210\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf / Natural image generation matches or exceeds baselines"]\n    Results --\x3e R3["\u5c11\u6b65\u63a8\u7406\u4fdd\u6301\u6027\u80fd / Maintains performance with few inference steps"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [photonic computing], [spatial photonic Ising machine, Hadamard product, amplitude-only modulation, rank-free coupling, incoherent light field]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ze Zheng, Yuegang Li, Hang Xu, Jingzheng Huang, Tailong Xiao, Guihua Zeng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Hefei National Laboratory, Shanghai Research Center for Quantum Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21587",children:"https://arxiv.org/pdf/2512.21587"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an amplitude-only modulated, rank-free spatial photonic Ising machine (AR-SPIM) that eliminates the need for repeated/auxiliary operations by reformulating the Ising Hamiltonian as a sum of Hadamard products. 2. Demonstrates direct mapping of a 797-spin Ising model with external fields into an incoherent light field using aligned amplitude modulators, achieving high encoding accuracy (correlation >0.98). 3. Shows the system's capability for ground-state search with <0.3% error, complex phase transition observation, and scalable spin counts for sparse problems by removing zero-valued terms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fe8a02df2acd50923ce3480e4b3fb28b68540ae3331e12dd83853a5b046a5c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a new spatial photonic Ising machine (AR-SPIM) that uses an amplitude-only modulation scheme to encode arbitrary-rank spin-spin couplings and external fields directly into an incoherent light field. The method reformulates the Ising Hamiltonian as a sum of Hadamard products, enabling efficient and accurate computation. The demonstrated system achieves high-speed iterations, low error rates for optimization problems, and offers scalability for practical applications in optimization and quantum simulations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Incorporating rank-free coupling and external field via an amplitude-only modulated spatial photonic Ising machine<br>\u632f\u5e45\u8c03\u5236\u7a7a\u95f4\u5149\u5b50\u4f0a\u8f9b\u673a"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Existing SPIMs sacrifice efficiency or scale to encode high-rank coupling and external fields.<br>\u73b0\u6709SPIM\u7f16\u7801\u9ad8\u79e9\u8026\u5408\u548c\u5916\u573a\u65f6\u727a\u7272\u6548\u7387\u6216\u89c4\u6a21\u3002"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Reformulate Hamiltonian as sum of Hadamard products; map to incoherent light via amplitude modulators.<br>\u5c06\u54c8\u5bc6\u987f\u91cf\u91cd\u5199\u4e3a\u54c8\u8fbe\u739b\u79ef\u4e4b\u548c\uff1b\u901a\u8fc7\u632f\u5e45\u8c03\u5236\u5668\u6620\u5c04\u5230\u975e\u76f8\u5e72\u5149\u573a\u3002"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>797 spins, >0.98 correlation, <0.3% error rate, enables phase transition observation.<br>797\u4e2a\u81ea\u65cb\uff0c>0.98\u76f8\u5173\u6027\uff0c<0.3%\u9519\u8bef\u7387\uff0c\u652f\u6301\u76f8\u53d8\u89c2\u6d4b\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Tilt Matching for Scalable Sampling and Fine-Tuning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Tilt Matching, stochastic interpolants, flow matching, unnormalized densities, fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Peter Potaptchik, Cheuk-Kit Lee, Michael S. Albergo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Harvard University, University of Oxford, Kempner Institute, IAIFI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21829",children:"https://arxiv.org/pdf/2512.21829"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Tilt Matching, a simple, scalable algorithm for sampling and fine-tuning based on stochastic interpolants and an implicit stochastic optimal control solution., 2. Demonstrates that the method has strictly lower variance than standard flow matching and does not require gradients of the reward or backpropagation through trajectories., 3. Empirically shows state-of-the-art results on sampling under Lennard-Jones potentials and competitive performance on fine-tuning Stable Diffusion without reward multipliers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc2b096f1ad1f69d17a5fbdbac71d5ccac470d3cc86a47d74fed9dba5202c88_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper introduces Tilt Matching, a new algorithm for sampling from complex distributions and fine-tuning generative models. It works by deriving a velocity field from stochastic interpolants to target a "tilted" distribution, offering lower variance than flow matching and avoiding gradient computations. The method is shown to be scalable and effective, achieving strong results on molecular sampling and image generation tasks.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Tilt Matching for Scalable Sampling and Fine-Tuning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Sampling from unnormalized densities and fine-tuning generative models] --\x3e Problem_Detail[\u6311\u6218/Challenges: Requires scalable, low-variance methods without reward gradients]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Tilt Matching] --\x3e Method_Detail1[\u57fa\u7840/Basis: Dynamical equation relating flow matching velocity to tilted distribution]\n    Method_Detail1 --\x3e Method_Detail2[\u7279\u6027/Properties: Implicitly solves stochastic optimal control, lower variance, no reward gradients needed]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Empirical Verification] --\x3e Results_Detail1[\u5e94\u7528/Applications: State-of-the-art on Lennard-Jones potentials, competitive on Stable Diffusion fine-tuning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] Modeling high dimensional point clouds with the spherical cluster model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [clustering], [spherical cluster model, high-dimensional median, non-smooth optimization, Clarke gradient, stratified cell complex]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fr\xe9d\xe9ric Cazals, Antoine Commaret, Louis Goldenberg"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universit\xe9 C\xf4te d'Azur, Inria, Ecole Polytechnique"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21960",children:"https://arxiv.org/pdf/2512.21960"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Showed that fitting a spherical cluster model is a strictly convex but non-smooth combinatorial optimization problem. 2. Presented an exact solver using the Clarke gradient on a stratified cell complex derived from an arrangement of hyperspheres. 3. Demonstrated through experiments that the exact solver is significantly faster than BFGS heuristics for many datasets and that the model's center behaves as a parameterized high-dimensional median."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c9618adf1a45723b5b136e8c21eaa91dc645be7664e2fae293ae569f2adde20_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces the Spherical Cluster (SC) model for approximating high-dimensional point clouds with a sphere. It proposes an exact solver for this non-smooth optimization problem, which is shown to be much faster than heuristic methods for many datasets, especially in high dimensions. The model's center is found to act as a robust, parameterized median."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Modeling high dimensional point clouds with the spherical cluster model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4e3a\u9ad8\u7ef4\u70b9\u4e91\u5efa\u6a21/Modeling high-dimensional point clouds]\n    C --\x3e C1[\u7403\u5f62\u805a\u7c7b\u6a21\u578b/Spherical Cluster Model]\n    C --\x3e C2[\u7cbe\u786e\u6c42\u89e3\u5668\u4f7f\u7528Clarke\u68af\u5ea6/Exact solver using Clarke gradient]\n    D --\x3e D1[\u7cbe\u786e\u7b97\u6cd5\u6bd4BFGS\u5feb\u5f97\u591a/Exact algorithm much faster than BFGS]\n    D --\x3e D2[\u4e2d\u5fc3\u8868\u73b0\u4e3a\u53c2\u6570\u5316\u9ad8\u7ef4\u4e2d\u4f4d\u6570/Center acts as parameterized high-dimensional median]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251229] A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [dynamical systems, numerical linear algebra], [linear conservation laws, Frobenius norm, orthogonal projection, matrix correction, data-driven models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," John M. Mango, Ronald Katende"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Makerere University, Kabale University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22084",children:"https://arxiv.org/pdf/2512.22084"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),"  1. Provides a closed-form, Frobenius-norm optimal projection to enforce linear conservation laws on any learned linear dynamical operator. 2. Proves that the correction is uniquely defined, low-rank, and fully determined by the constraint violation, reducing to a rank-one update for a single invariant. 3. Demonstrates that the method enforces exact conservation while minimally perturbing the learned dynamics, verified with a numerical example."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff37179b43efd7a7b64ff062f4e49c8aeaa1dc0b65febe24067b7ddd46dba12c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of learned linear dynamical models violating known linear conservation laws. It proposes a closed-form orthogonal projection that minimally perturbs a learned operator in the Frobenius norm to exactly satisfy the constraints. The method provides a general, optimal post-processing step for embedding invariants into data-driven linear models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Learned linear models violate known linear conservation laws."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Apply orthogonal projection A* = \xc2 - C(C\u1d40C)\u207b\xb9C\u1d40\xc2 to enforce C\u1d40A=0."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Enforces exact conservation with minimal perturbation; correction is unique and low-rank."]'}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2025-12-30",children:"2025-12-30"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [adversarial robustness, graph pruning, message passing, spurious connections, graph defense]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yongyu Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Michigan Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22128",children:"https://arxiv.org/pdf/2512.22128"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel graph pruning framework that uses adversarial robustness evaluation to identify fragile graph components. 2. Proposes a method that selectively prunes edges based on robustness scores to improve model reliability and resilience. 3. Validates the framework by instantiating it on three representative GNN architectures and demonstrating significant defense enhancement in high-perturbation regimes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658f9bc62a6d2a57e4602f69b9d0c69056551601abd2ba0336e97d592bae1dc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks and noise in graph structure. It proposes a pruning framework that uses adversarial robustness scores to identify and remove detrimental edges, resulting in cleaner and more resilient graph representations. Experiments show this method significantly strengthens GNN defenses against high levels of perturbation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(GNNs\u5bf9\u56fe\u7ed3\u6784\u548c\u7279\u5f81\u7684\u6270\u52a8\u654f\u611f/GNNs vulnerable to graph & feature perturbations)\n    C --\x3e C1(\u57fa\u4e8e\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u5206\u7684\u526a\u679d\u6846\u67b6/Pruning framework guided by adversarial robustness scores)\n    D --\x3e D1(\u663e\u8457\u589e\u5f3a\u9ad8\u6270\u52a8\u4e0b\u7684\u9632\u5fa1\u80fd\u529b/Significantly enhances GNN defense in high-perturbation regime)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent reinforcement learning], [ad-hoc teamwork, retrieval-augmented generation, teammate modeling, Overcooked]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Conor Wallace, Umer Siddique, Yongcan Cao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at San Antonio"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22129",children:"https://arxiv.org/pdf/2512.22129"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("RECOLLAB: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("Ad-hoc Teammate Modeling<br>Ad-hoc\u961f\u53cb\u5efa\u6a21")\n    Problem --\x3e P2("Brittle Conventional Models<br>\u4f20\u7edf\u6a21\u578b\u8106\u5f31\u6027")\n    Method --\x3e M1("COLLAB: LLM-based Framework<br>\u57fa\u4e8eLLM\u7684\u6846\u67b6")\n    Method --\x3e M2("RECOLLAB: Adds RAG<br>\u589e\u52a0RAG\u68c0\u7d22")\n    Results --\x3e R1("Improved Adaptation<br>\u63d0\u5347\u9002\u5e94\u6027")\n    Results --\x3e R2("Pareto-Optimal Trade-offs<br>\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [edge computing, tensor parallelism, parameter server, device heterogeneity, fault-tolerance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Edinburgh, Johns Hopkins University, Cisco Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22142",children:"https://arxiv.org/pdf/2512.22142"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel selective hybrid tensor parallelism method to finely partition training operations for edge devices. 2. A parameter server-centric training framework to cope with device memory limits and avoid communication bottlenecks. 3. A cost optimization model to guide device selection and workload distribution, effectively handling device heterogeneity and churn."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of decentralized foundation model training on edge devices, which is hindered by memory limits, communication overhead, and device heterogeneity. It proposes Cleave, a new paradigm that uses selective hybrid tensor parallelism and a parameter server framework to partition training efficiently. The evaluation shows Cleave matches cloud-based training performance, scales to thousands of devices, and handles failures with much faster recovery than prior methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[On Harnessing Idle Compute at the Edge for Foundation Model Training] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3/Existing edge training falls short]\nB --\x3e B2[\u8bbe\u5907\u5185\u5b58\u4e0e\u901a\u4fe1\u74f6\u9888/Device memory & communication bottlenecks]\nB --\x3e B3[\u8bbe\u5907\u5f02\u6784\u6027\u4e0e\u52a8\u6001\u6027/Device heterogeneity & dynamism]\nC --\x3e C1[\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c/Selective hybrid tensor parallelism]\nC --\x3e C2[\u53c2\u6570\u670d\u52a1\u5668\u6846\u67b6/Parameter server framework]\nC --\x3e C3[\u6210\u672c\u4f18\u5316\u6a21\u578b/Cost optimization model]\nD --\x3e D1[\u5339\u914d\u4e91\u7aef\u8bad\u7ec3\u6027\u80fd/Matches cloud-based training]\nD --\x3e D2[\u6269\u5c55\u81f3\u6570\u5343\u8bbe\u5907/Scales to thousands of devices]\nD --\x3e D3[\u5feb\u901f\u6545\u969c\u6062\u590d/Fast failure recovery]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [hpc], [gpu kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," School of Software Engineering, Xi\u2019an Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22147",children:"https://arxiv.org/pdf/2512.22147"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Full builds & runs are expensive in large applications/\u5927\u578b\u5e94\u7528\u4e2d\u5b8c\u6574\u6784\u5efa\u4e0e\u8fd0\u884c\u6210\u672c\u9ad8]\n    C --\x3e C1[Construct Minimal Executable Program (MEP) for kernel/\u4e3a\u5185\u6838\u6784\u5efa\u6700\u5c0f\u53ef\u6267\u884c\u7a0b\u5e8f]\n    C --\x3e C2[Multi-round iterative optimization with LLM feedback/\u57fa\u4e8eLLM\u53cd\u9988\u7684\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316]\n    C --\x3e C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/\u96c6\u6210\u81ea\u52a8\u9519\u8bef\u4fee\u590d\u4e0e\u6027\u80fd\u6a21\u5f0f\u7ee7\u627f]\n    D --\x3e D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/\u83b7\u5f97\u663e\u8457\u52a0\u901f\u6bd4]\n    D --\x3e D2[Cross-platform portability (NVIDIA, DCU)/\u8de8\u5e73\u53f0\u53ef\u79fb\u690d\u6027]\n    D --\x3e D3[Surpasses direct LLM optimization/\u8d85\u8d8a\u76f4\u63a5LLM\u4f18\u5316]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal representation learning], [additive noise model, Wasserstein auto-encoder, identifiability, unsupervised learning, causal discovery]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hans Jarett J. Ong, Brian Godwin S. Lim, Dominic Dayta, Renzo Roel P. Tan, Kazushi Ikeda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nara Institute of Science and Technology, Kyoto University, Ateneo de Manila University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22150",children:"https://arxiv.org/pdf/2512.22150"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LANCA, a novel autoencoder framework that operationalizes the Additive Noise Model (ANM) as an inductive bias for unsupervised causal representation learning., 2. Provided a theoretical analysis showing that the ANM constraint restricts admissible transformations to the affine class, resolving component-wise indeterminacy., 3. Introduced a deterministic WAE architecture with a differentiable ANM layer to explicitly optimize for residual independence, overcoming limitations of stochastic VAEs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/383ef9534443c860dced6678ba459335afb022e411bf183cc7f2e3dbad2bb385_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of unsupervised causal representation learning by proposing LANCA, a method that uses the Additive Noise Model as an inductive bias within a deterministic autoencoder framework. Theoretically, it shows this constraint narrows down the solution space, and empirically, LANCA outperforms existing methods on synthetic and photorealistic benchmarks by being more robust to spurious correlations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Unsupervised Causal Representation Learning via LANCA] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u65e0\u76d1\u7763\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u53ef\u8bc6\u522b\u6027\u6311\u6218/Identifiability Challenge in Unsupervised Causal Representation Learning]\n    C --\x3e C1[\u63d0\u51faLANCA\uff1a\u4f7f\u7528ANM\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e/Propose LANCA: Using ANM as Inductive Bias]\n    C --\x3e C2[\u786e\u5b9a\u6027WAE\u4e0e\u53ef\u5faeANM\u5c42/Deterministic WAE with Differentiable ANM Layer]\n    D --\x3e D1[\u7406\u8bba\uff1a\u9650\u5236\u53d8\u6362\u4e3a\u4eff\u5c04\u7c7b/Theory: Restricts Transformations to Affine Class]\n    D --\x3e D2[\u5b9e\u8bc1\uff1a\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eSOTA/Empirical: Outperforms SOTA on Benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (rlhf)], [reward model, video generation, reward hacking, bradley-terry loss, hierarchical attention]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22170",children:"https://arxiv.org/pdf/2512.22170"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u6570\u636e\u6807\u6ce8\u566a\u58f0/Annotation Noise]\n    B --\x3e B2[\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb/Reward Hacking]\n    B --\x3e B3[\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u4e0d\u8db3/Under-explored RM Architecture]\n    C --\x3e C1[\u5355\u9879\u76ee\u4e8c\u5143\u6807\u6ce8\u4e0e\u8de8\u63d0\u793a\u914d\u5bf9/Single-item Binary Annotations & Cross-prompt Pairing]\n    C --\x3e C2[\u5206\u5c42\u6e10\u8fdb\u67e5\u8be2\u6ce8\u610f\u529b/Hierarchical Progressive Query Attention]\n    C --\x3e C3[\u6539\u8fdb\u7684BT\u635f\u5931\u51fd\u6570/Modified BT Loss for Win-Tie]\n    D --\x3e D1[\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u63d0\u5347/Improved Direct RM Evaluation Metrics]\n    D --\x3e D2[\u89c6\u9891\u751f\u6210\u540e\u8bad\u7ec3\u6548\u679c\u589e\u5f3a/Enhanced Efficacy of Post-training on Video Generation Models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Wireless Traffic Prediction with Large Language Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatio-temporal forecasting], [large language model, wireless traffic prediction, spatial-temporal correlation, prompt engineering, fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22178",children:"https://arxiv.org/pdf/2512.22178"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Wireless Traffic Prediction with Large Language Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: LLMs overlook spatial dependencies in city-scale wireless traffic)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: TIDES framework with clustering, prompt engineering, and DeepSeek spatial alignment module)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA baselines in accuracy and robustness for 6G network management)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [anomaly detection], [manifold learning, normalizing flows, dual-centroid compactness loss, out-of-distribution detection, zero-shot generalization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rajeeb Thapa Chhetri, Zhixiong Chen, Saurab Thapa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mercy University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22179",children:"https://arxiv.org/pdf/2512.22179"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes Latent Sculpting, a novel two-stage framework that decouples manifold structure learning from density estimation for OOD anomaly detection. 2. Introduces the Dual-Centroid Compactness Loss (DCCL) to actively sculpt a compact, low-entropy latent manifold for benign data. 3. Demonstrates superior zero-shot generalization on the CIC-IDS-2017 benchmark, significantly outperforming supervised and unsupervised baselines on complex distribution shifts like "Infiltration".']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfcac5687161ad2a09f74eab3c1b7f91a350a2435fb71a0c791f2e305dff108c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper addresses the problem of "Generalization Collapse" in supervised models when detecting Out-of-Distribution (OOD) anomalies. It proposes Latent Sculpting, a two-stage method that first uses a novel loss to sculpt a compact latent manifold for benign data and then applies a normalizing flow for density estimation. The results show this approach enables robust zero-shot anomaly detection, significantly outperforming existing methods on unseen attack scenarios.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Latent Sculpting for Zero-Shot Generalization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u6cdb\u5316\u5d29\u6e83 / Generalization Collapse]\n    B --\x3e B2[OOD\u5f02\u5e38\u68c0\u6d4b\u5931\u8d25 / OOD Anomaly Detection Failure]\n    C --\x3e C1[\u6f5c\u5728\u7a7a\u95f4\u96d5\u523b / Latent Sculpting]\n    C1 --\x3e C2[\u9636\u6bb51: DCCL\u635f\u5931 / Stage 1: DCCL Loss]\n    C1 --\x3e C3[\u9636\u6bb52: MAF\u5bc6\u5ea6\u4f30\u8ba1 / Stage 2: MAF Density Estimation]\n    D --\x3e D1[\u96f6\u6837\u672cF1\u5206\u65700.87 / Zero-Shot F1-Score 0.87]\n    D --\x3e D2[\u6e17\u900f\u653b\u51fb\u68c0\u6d4b\u738788.89% / Infiltration Detection 88.89%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [dimensionality reduction], [Locally Linear Embedding (LLE), AI-enhanced LLE, medical data analysis, medical billing, transcription]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National University of Science and Technology (NUST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22182",children:"https://arxiv.org/pdf/2512.22182"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Enhancing Medical Data Analysis through AI-Enhanced LLE] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Handling complex high-dimensional medical data for billing and transcription)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Integrating AI with Locally Linear Embedding (LLE))\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved data processing accuracy and operational efficiency)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Dueling Double Deep Q-Network, curriculum learning, tennis simulation, sequential decision-making, sports analytics]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vishnu Mohan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22186",children:"https://arxiv.org/pdf/2512.22186"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Tennis strategy optimization as a sequential decision-making challenge with hierarchical scoring, stochasticity, and opponent adaptation)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Dueling Double Deep Q-Network (DDQN) trained with curriculum learning in a custom tennis simulation environment)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: High win rates (98-100%) and stable convergence, but reveals a defensive policy bias, highlighting reward design limitations)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [prognostics & health management (phm)], [Neural Networks, Convolutional Neural Networks, Reinforcement Learning, Uncertainty Quantification, Physics-Informed Machine Learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jose I. Aizpurua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of the Basque Country (UPV/EHU)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22190",children:"https://arxiv.org/pdf/2512.22190"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the application of Neural Networks (NNs) and their variants, specifically Convolutional Neural Networks (CNNs), for transformer condition monitoring using diverse data modalities. 2. Discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control in transformer health management. 3. Provides perspectives on emerging research directions at the intersection of physics-informed machine learning and transformer Prognostics & Health Management (PHM)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73f2611de029a059f651f47ffd6b707f684cdbc0c0f865e4c8568c2765f5fede_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of traditional, rule-based transformer condition monitoring by proposing the use of machine learning, particularly Neural Networks and their variants. It explores Convolutional Neural Networks for processing diverse sensor data and discusses Reinforcement Learning for control, concluding that physics-informed ML provides a powerful framework for more accurate diagnostics, prognostics, and decision-making in power transformer health management."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Physics-Informed ML for Transformer Condition Monitoring \u2013 Part I"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Traditional monitoring struggles with uncertainty & complexity"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Use Neural Networks, CNNs, and Reinforcement Learning"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Enables accurate diagnostics, prognostics, and control"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [physics-informed machine learning], [Physics-Informed Neural Networks (PINNs), Bayesian PINNs, uncertainty quantification, Prognostics & Health Management (PHM), transformer condition monitoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jose I. Aizpurua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of the Basque Country (UPV/EHU)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22189",children:"https://arxiv.org/pdf/2512.22189"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Physics-Informed Neural Networks (PINNs) for integrating physics into neural network training for transformer applications like thermal modeling and insulation ageing. 2. Presents Bayesian PINNs as a framework to quantify epistemic uncertainty, enabling robust predictions under sparse data conditions. 3. Outlines emerging research directions for physics-aware and trustworthy machine learning in the management of critical power assets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d981ba5912467b9788cda82e843171a8af166b1ec965beb37ff22f102f1e02c8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using Physics-Informed Neural Networks (PINNs) and their Bayesian extension to improve transformer condition monitoring. The method integrates physical laws directly into the learning process and quantifies model uncertainty, aiming to deliver more reliable predictions with limited data. The work highlights the potential of physics-aware machine learning for robust prognostics and health management of critical power infrastructure."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Physics-Informed Machine Learning for Transformer Condition Monitoring \u2013 Part II] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u7269\u7406\u673a\u5236\uff0c\u6570\u636e\u7a00\u758f\u4e0b\u4e0d\u53ef\u9760/Traditional NNs lack physics, unreliable with sparse data]\n    C --\x3e C1[\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc/PINNs]\n    C --\x3e C2[\u8d1d\u53f6\u65afPINNs\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027/Bayesian PINNs for epistemic UQ]\n    D --\x3e D1[\u63d0\u9ad8\u7a00\u758f\u6570\u636e\u4e0b\u7684\u9c81\u68d2\u6027/Improved robustness under sparse data]\n    D --\x3e D2[\u4e3a\u5173\u952e\u8d44\u4ea7\u63d0\u4f9b\u53ef\u4fe1\u7684\u673a\u5668\u5b66\u4e60/Trustworthy ML for critical assets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [regularization theory], [Spectral Bias, L2 Regularization, Frequency Principle, Spectral Suppression Ratio, Inductive Bias]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiahao Lu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22192",children:"https://arxiv.org/pdf/2512.22192"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/lujiahao760/FrequencyRegularization",children:"https://github.com/lujiahao760/FrequencyRegularization"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Introduced a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during CNN training. 2. Proposed a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. 3. Empirically revealed a critical Accuracy-Robustness Trade-off in L2-regularized models, showing their sensitivity to broadband noise but superior robustness to high-frequency information loss.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7443604e19a6c972d6ac6ad332d345bd15a272d6f58a489ef07c494950a3d274_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the spectral inductive bias of deep neural networks, focusing on how regularization techniques like L2 affect feature frequency selection. The authors propose a visual diagnostic framework and a new metric to quantify spectral suppression, demonstrating that L2 regularization strongly suppresses high-frequency energy and leads to a trade-off between accuracy and robustness. The work confirms that regularization enforces a strong spectral bias towards low-frequency structures, providing a signal-processing perspective on generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u7406\u89e3\u6b63\u5219\u5316\u7684\u7269\u7406\u673a\u5236<br/>Understanding the physical mechanism of regularization]\n    B --\x3e B2[\u63a2\u7d22\u7279\u5f81\u9891\u7387\u9009\u62e9<br/>Exploring feature frequency selection]\n    C --\x3e C1[\u89c6\u89c9\u8bca\u65ad\u6846\u67b6<br/>Visual Diagnostic Framework]\n    C --\x3e C2[\u8c31\u6291\u5236\u6bd4 (SSR)<br/>Spectral Suppression Ratio (SSR)]\n    D --\x3e D1[L2\u6b63\u5219\u5316\u6291\u5236\u9ad8\u9891\u80fd\u91cf<br/>L2 regularization suppresses high-frequency energy]\n    D --\x3e D2[\u63ed\u793a\u51c6\u786e\u7387-\u9c81\u68d2\u6027\u6743\u8861<br/>Reveals Accuracy-Robustness Trade-off]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [retrieval-augmented generation, key-value cache, flash storage, prefill optimization, power efficiency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Seoul National University, Samsung Electronics"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22195",children:"https://arxiv.org/pdf/2512.22195"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/kunwooshin/MatKV",children:"https://github.com/kunwooshin/MatKV"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["MatKV: Trading Compute for Flash Storage in LLM Inference"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>RAG\u63a8\u7406\u4e2dprefill\u9636\u6bb5\u8ba1\u7b97\u5f00\u9500\u5927<br>High compute cost of prefill in RAG inference"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u9884\u8ba1\u7b97\u5e76\u7269\u5316KV\u5411\u91cf\u5230\u95ea\u5b58<br>Precompute & materialize KVs to flash storage"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>\u63a8\u7406\u65f6\u95f4\u4e0e\u80fd\u8017\u51cf\u534a<br>Halves inference time & power consumption"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [semantic change detection], [diachronic embeddings, orthogonal Procrustes, lexical drift]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qizhi Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," PingCAP, Data & AI-Innovation Lab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22196",children:"https://arxiv.org/pdf/2512.22196"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1("\u6570\u5b57\u4eba\u6587\u4e2d\u8bed\u4e49\u53d8\u8fc1\u5206\u6790<br>Digital Humanities Semantic Shift Analysis")\n    C --\x3e C1("\u53ef\u590d\u73b0\u7684\u4e13\u5bb6\u7cfb\u7edf\u6d41\u7a0b<br>Reproducible Expert-System Pipeline")\n    C1 --\x3e C2("\u5206\u65f6\u6bb5\u8bcd\u5d4c\u5165\u4e0e\u5bf9\u9f50<br>Temporal Embeddings & Alignment")\n    C2 --\x3e C3("\u51e0\u4f55\u4f4d\u79fb\u4e0e\u90bb\u57df\u53d8\u5316\u5ea6\u91cf<br>Geometric & Neighborhood Metrics")\n    D --\x3e D1("\u53ef\u89c6\u5316\u6cd5\u5f8b\u6982\u5ff5\u6f14\u53d8<br>Visualizing Legal Concept Evolution")\n    D1 --\x3e D2("\u63ed\u793a\u4e0e\u5386\u53f2\u4e8b\u4ef6\u7684\u5173\u8054<br>Revealing Links to Historical Events")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [dynamic routing, residual networks, cosine incompatibility, Gumbel-Softmax, FLOPs regularization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yogeswar Reddy Thota"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Texas at Dallas"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22206",children:"https://arxiv.org/pdf/2512.22206"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Modern residual networks perform redundant computation for all inputs]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [intrinsic motivation, homeostatic control, adaptive optimization, non-stationary learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dhruv Tiwari"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Lovely Professional University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22200",children:"https://arxiv.org/pdf/2512.22200"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework, Emotion-Inspired Learning Signals (EILS), that models emotions as continuous, homeostatic appraisal signals (e.g., Curiosity, Stress, Confidence) for adaptive control. 2. Formalizes these signals as vector-valued internal states derived from interaction history to dynamically modulate the agent's optimization landscape in real-time. 3. Hypothesizes that this closed-loop homeostatic regulation enables superior sample efficiency and adaptation to non-stationary environments compared to standard baselines like PPO."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a45d2a56af1becf3eaddb05dbaeff3cf5453d19d41771b6d8ce1c1a70d3825c2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies the fragility of standard AI agents that rely on static, external rewards in open-ended environments. It proposes the Emotion-Inspired Learning Signals (EILS) framework, which uses bio-inspired internal signals like curiosity and stress to dynamically control learning. The authors hypothesize this approach will lead to more robust, adaptive, and sample-efficient autonomous agents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[EILS: A Homeostatic Framework] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u9759\u6001\u5916\u90e8\u5956\u52b1/Static Extrinsic Reward]\n    Problem --\x3e P2[\u8106\u5f31\u6027\uff0c\u65e0\u6cd5\u9002\u5e94/Fragile, Non-Adaptive]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u60c5\u7eea\u542f\u53d1\u4fe1\u53f7/Emotion-Inspired Signals]\n    Method --\x3e M2[\u52a8\u6001\u7a33\u6001\u8c03\u8282/Dynamic Homeostatic Control]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5047\u8bbe: \u66f4\u9ad8\u6837\u672c\u6548\u7387/Hypothesis: Higher Sample Efficiency]\n    Results --\x3e R2[\u5047\u8bbe: \u66f4\u597d\u975e\u5e73\u7a33\u9002\u5e94/Hypothesis: Better Non-Stationary Adaptation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [multimodal large language models], [Moxin, open-source LLM, vision-language-action, Model Openness Framework, multimodal models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22208",children:"https://arxiv.org/pdf/2512.22208"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/moxin-org/Moxin-LLM",children:"https://github.com/moxin-org/Moxin-LLM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Open-Source Multimodal Moxin Models<br/>\u5f00\u6e90\u591a\u6a21\u6001Moxin\u6a21\u578b] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Proprietary vs. Open-Source LLMs<br/>\u95ed\u6e90\u4e0e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b]\n    B --\x3e B2[Need for transparent, capable open models<br/>\u9700\u8981\u900f\u660e\u3001\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b]\n    C --\x3e C1[Develop Moxin 7B under Model Openness Framework<br/>\u57fa\u4e8e\u6a21\u578b\u5f00\u653e\u6846\u67b6\u5f00\u53d1Moxin 7B]\n    C --\x3e C2[Create variants: VLM, VLA, Chinese<br/>\u521b\u5efa\u53d8\u4f53: VLM, VLA, \u4e2d\u6587\u6a21\u578b]\n    D --\x3e D1[Superior performance in evaluations<br/>\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02]\n    D --\x3e D2[Full release of models, code, data<br/>\u5b8c\u6574\u53d1\u5e03\u6a21\u578b\u3001\u4ee3\u7801\u3001\u6570\u636e]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Transformer Reconstructed with Dynamic Value Attention"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [transformer architecture], [dynamic value attention, single-head attention, feed forward network removal]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiaowei Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," College of Artificial Intelligence, China University of Petroleum (Beijing)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22212",children:"https://arxiv.org/pdf/2512.22212"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Dynamic Value Attention (DVA), a method to dynamically decide a value for each query, addressing the limitation of static values in standard attention heads. 2. Enabled the removal of redundant multi-head attention, reducing the architecture to a single-head attention mechanism. 3. Demonstrated that the subsequent feed-forward network can be entirely removed, as the revised embeddings already capture sufficient contextual information."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79cd017ace993f77d704944ed5eeaa739c2f83168d5d9f241b0355966c04ea5f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a limitation in the standard Transformer where static values are used for all queries within an attention head. It proposes Dynamic Value Attention (DVA), which computes a dynamic value per query, allowing the model to use only a single attention head and remove the feed-forward network entirely. Experiments show DVA reduces training time by 37.6% while improving learning capability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Transformer Reconstructed with Dynamic Value Attention") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u9759\u6001\u503c/Static Value")\n    Problem --\x3e P2("\u591a\u5934\u5197\u4f59/Multi-head Redundancy")\n    Method --\x3e M1("\u52a8\u6001\u503c\u6ce8\u610f\u529b/Dynamic Value Attention (DVA)")\n    Method --\x3e M2("\u5355\u5934\u67b6\u6784/Single-head Architecture")\n    Method --\x3e M3("\u79fb\u9664\u524d\u9988\u7f51\u7edc/Remove Feed-Forward Network")\n    Results --\x3e R1("\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1137.6%/37.6% Training Time Saved")\n    Results --\x3e R2("\u5b66\u4e60\u80fd\u529b\u63d0\u5347/Learning Capability Increased")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On the Existence and Behaviour of Secondary Attention Sinks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [attention sinks, transformer, mlp, attention mechanism, large language models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Imperial College London, UnlikelyAI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22213",children:"https://arxiv.org/pdf/2512.22213"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies and characterizes a new class of "secondary attention sinks" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink\'s direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper identifies a new phenomenon called "secondary attention sinks" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["On the Existence and Behaviour of Secondary Attention Sinks<br/>\u4e8c\u6b21\u6ce8\u610f\u529b\u6c47\u7684\u5b58\u5728\u4e0e\u884c\u4e3a"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>\u5148\u524d\u7814\u7a76\u4ec5\u5173\u6ce8\u6301\u4e45\u7684\u4e3b\u6c47\uff08\u5982BOS\uff09"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Extensive experiments across 11 model families<br/>\u5bf911\u4e2a\u6a21\u578b\u7cfb\u5217\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>\u6b21\u7ea7\u6c47\u901a\u8fc7\u4e2d\u95f4\u5c42MLP\u5f62\u6210<br/>2. L2-norm determines sink score & lifetime<br/>L2\u8303\u6570\u51b3\u5b9a\u6c47\u5206\u6570\u4e0e\u5bff\u547d<br/>3. Sink levels are deterministic in large models<br/>\u5927\u6a21\u578b\u4e2d\u6c47\u5c42\u7ea7\u66f4\u786e\u5b9a"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [heterophily, interpretability, combinatorial inference, hybrid learning, node classification]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Soroush Vahidi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New Jersey Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22221",children:"https://arxiv.org/pdf/2512.22221"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an interpretable, adaptive framework for node classification using explicit combinatorial inference instead of deep message passing, with a tunable scoring function. 2. Introduces a validation-gated hybrid strategy that optionally refines combinatorial predictions with a lightweight neural model only when beneficial. 3. Ensures a leakage-free evaluation protocol by computing all adaptation signals strictly from training data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31111cc9c8d13c529d7f271adf29c47c440b43adfaa3481dde88da7f87b76463_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of node classification on heterophilic graphs where standard GNNs struggle. It proposes an interpretable framework based on combinatorial scoring and a conditional hybrid learning strategy, achieving competitive performance with modern GNNs while offering better interpretability, tunability, and efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("GNNs\u5728\u5f02\u914d\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73/GNNs struggle on heterophilic graphs")\n    Method --\x3e M1("\u7ec4\u5408\u63a8\u7406\u4e0e\u8bc4\u5206\u51fd\u6570/Combinatorial Inference & Scoring")\n    Method --\x3e M2("\u9a8c\u8bc1\u95e8\u63a7\u6df7\u5408\u7b56\u7565/Validation-gated Hybrid Strategy")\n    Results --\x3e R1("\u6027\u80fd\u6709\u7ade\u4e89\u529b/Competitive Performance")\n    Results --\x3e R2("\u53ef\u89e3\u91ca\u6027\u4e0e\u9ad8\u6548\u6027/Interpretability & Efficiency")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [megakernel, kernel fusion, SM-level graph, software pipelining, CUDA]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22219",children:"https://arxiv.org/pdf/2512.22219"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/mirage-project/mirage",children:"https://github.com/mirage-project/mirage"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Mirage Persistent Kernel<br>\u5e7b\u5f71\u6301\u4e45\u5185\u6838] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Kernel-per-operator execution<br>limits GPU optimization<br>\u9010\u7b97\u5b50\u5185\u6838\u6267\u884c\u9650\u5236GPU\u4f18\u5316]\n    C --\x3e C1[SM-level graph &<br>mega-kernel runtime<br>SM\u7ea7\u56fe\u4e0e\u5de8\u578b\u5185\u6838\u8fd0\u884c\u65f6]\n    D --\x3e D1[Reduces inference latency<br>by up to 1.7x<br>\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe1.7\u500d]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] M\xfcntz-Sz\xe1sz Networks: Neural Architectures with Learnable Power-Law Bases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural network architecture], [M\xfcntz-Sz\xe1sz Networks, fractional power bases, physics-informed neural networks, universal approximation, singular function approximation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gnankan Landry Regis N'guessan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22222",children:"https://arxiv.org/pdf/2512.22222"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces M\xfcntz-Sz\xe1sz Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the M\xfcntz-Sz\xe1sz theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces M\xfcntz-Sz\xe1sz Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN's universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[M\xfcntz-Sz\xe1sz Networks: Neural Architectures with Learnable Power-Law Bases] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Standard neural networks poorly approximate singular/fractional power functions common in physics]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes MSN with learnable fractional power bases, replacing fixed activations]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: MSN achieves superior approximation rates, lower error with fewer parameters, and significant improvement on PINN benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video understanding], [streaming video, multimodal large language models, event segmentation, hierarchical representation, elastic-scale]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Ant Group"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22226",children:"https://arxiv.org/pdf/2512.22226"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/zheng980629/VideoScaffold",children:"https://github.com/zheng980629/VideoScaffold"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [retrieval-augmented generation (RAG), network traffic analysis, large language models (LLMs), hierarchical retrieval, explainable AI]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," North Carolina A&T State University, University of Texas at San Antonio"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22223",children:"https://arxiv.org/pdf/2512.22223"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/270771/llm-traffictraffic",children:"https://github.com/270771/llm-traffictraffic"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Traditional traffic analysis systems have high false positives and lack interpretability.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Multi-stage framework using traffic summarization, RAG, and LLM reasoning with a hierarchical retrieval pipeline.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Achieves 95.95%-98.82% accuracy, outperforms baselines, and provides explainable, evidence-grounded responses.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [representation analysis], [sentence embeddings, probing, hierarchical geometry, transformer models, cognitive states]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sophie Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22227",children:"https://arxiv.org/pdf/2512.22227"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Constructed a novel dataset of 480 sentences annotated with continuous energy scores and discrete tier labels for seven ordered cognitive categories. 2. Demonstrated that both continuous scores and discrete tier labels are reliably decodable from fixed transformer sentence embeddings using linear and nonlinear probes, with nonlinear probes providing consistent gains. 3. Provided statistical and qualitative evidence (via permutation tests, UMAP visualizations, and confusion matrices) that the embedding space exhibits a hierarchical geometric organization aligned with human-defined cognitive attributes, beyond surface word statistics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fec0b31a0f9f75593cbc3cdadecae63f4a1a7b7b6d910165a358bc72dde0f1d7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether transformer-based sentence embeddings encode a hierarchical structure aligned with cognitive states. The authors construct an annotated dataset and use linear and nonlinear probes to decode continuous scores and discrete labels from embeddings, finding reliable recoverability and a structured geometric gradient. The results show that transformer embedding spaces exhibit a systematic organization corresponding to interpretable cognitive attributes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Do sentence embeddings encode hierarchical cognitive structure?")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Probe analysis on annotated dataset with linear/nonlinear classifiers")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Reliable decoding, hierarchical geometry aligned with cognitive attributes")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] We are not able to identify AI-generated images"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [image forensics], [AI-generated images, human evaluation, MidJourney, CC12M, synthetic media detection]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Adrien Pav\xe3o"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," (Institution not explicitly stated in provided content. Author name is Adrien Pav\xe3o; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22236",children:"https://arxiv.org/pdf/2512.22236"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[We are not able to identify AI-generated images] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Can humans reliably distinguish AI-generated images from real photos?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Interactive web experiment with a curated dataset of 120 difficult images (CC12M real vs. MidJourney AI-generated)]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Average human accuracy is 54% (near random), response time ~7.3s, highlighting human insufficiency and need for guidelines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Diffusion Language Models, FlexAttention, Group Relative Policy Optimization, LMDeploy, blockwise training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fudan University, Shanghai Innovation Institute, OpenMoss Team"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22234",children:"https://arxiv.org/pdf/2512.22234"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/OpenMOSS/DiRL",children:"https://github.com/OpenMOSS/DiRL"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DiRL: An Efficient Post-Training Framework for Diffusion Language Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[dLLMs\u540e\u8bad\u7ec3\u4f4e\u6548/Post-training for dLLMs is inefficient]\n    B --\x3e B2[\u8bad\u7ec3\u4e0e\u63a8\u7406\u76ee\u6807\u4e0d\u5339\u914d/Training-Inference objective mismatch]\n    C --\x3e C1[DiRL\u6846\u67b6/DiRL Framework]\n    C1 --\x3e C1_1[\u6574\u5408FlexAttention\u4e0eLMDeploy/Integrates FlexAttention & LMDeploy]\n    C1 --\x3e C1_2[\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3/Two-stage post-training (SFT+RL)]\n    C --\x3e C2[DiPO\u7b97\u6cd5/DiPO Algorithm]\n    C2 --\x3e C2_1[\u65e0\u504fGRPO\u5b9e\u73b0/Unbiased GRPO for dLLMs]\n    D --\x3e D1[\u9ad8\u6548\u8bad\u7ec3\u4e0e\u63a8\u7406/Efficient Training & Inference]\n    D --\x3e D2[\u6570\u5b66SOTA\u6027\u80fd/Math SOTA Performance]\n    D --\x3e D3[\u8d85\u8d8aQwen2.5\u7cfb\u5217/Surpasses Qwen2.5 series]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Enhanced geometry prediction in laser directed energy deposition using meta-learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [meta-learning], [meta-learning, model-agnostic meta-learning, reptile, laser-directed energy deposition, bead geometry prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abdul Malik Al Mardhouf Al Saadi, Amrita Basak"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22241",children:"https://arxiv.org/pdf/2512.22241"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Enhanced geometry prediction in L-DED using meta-learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Data scarcity & heterogeneity in L-DED geometry prediction]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Meta-learning (MAML & Reptile) for cross-dataset knowledge transfer]\n    D[\u5173\u952e\u7ed3\u679c/Results: Accurate prediction with few examples, outperforms conventional NN]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical imaging], [algorithmic fairness, subgroup performance analysis, JustEFAB framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Radboud University Medical Center, Radboud University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22242",children:"https://arxiv.org/pdf/2512.22242"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>\u80ba\u764c\u7b5b\u67e5\u98ce\u9669\u4f30\u8ba1\u6a21\u578b\u7684\u516c\u5e73\u6027\u8bc4\u4f30] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br/>AI\u80ba\u764c\u98ce\u9669\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u4e9a\u7ec4\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u662f\u5426\u516c\u5e73\uff1f<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u4f7f\u7528JustEFAB\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u5728NLST\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br/>\u53d1\u73b0Sybil\u548cVenkadesh21\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u3001\u65e0\u6cd5\u7528\u6df7\u6742\u56e0\u7d20\u89e3\u91ca\u7684\u6027\u80fd\u5dee\u5f02<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [transfer learning], [TabPFN, FT-Transformer, permutation-based variable importance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Maynooth University, University College Dublin, Queen's University Belfast, Teagasc, Agri-Food and Biosciences Institute (AFBI)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22243",children:"https://arxiv.org/pdf/2512.22243"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Evaluated and compared the performance of multiple deep learning and transfer learning models (MLP, TabPFN, TabNet, FT-Transformer) for predicting mycotoxin contamination in oats. 2. Applied these models to a multi-response prediction task using a dataset of environmental, agronomic, and geographical predictors from Irish oat samples. 3. Conducted a permutation-based variable importance analysis, identifying weather history in the 90-day pre-harvest period and seed moisture content as the most influential predictors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64407e4c1b4c79c19f30f80502744b2e2eadb71d078c9e48b84cad3e1286130e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study uses neural networks and transfer learning to predict mycotoxin contamination in Irish oat crops. The models, including TabPFN, TabNet, and FT-Transformer, were evaluated on a multi-response task, with TabPFN achieving the best overall performance. The analysis found that weather patterns before harvest and seed moisture are the most critical factors for prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Predicting Mycotoxin Contamination in Irish Oats<br>\u9884\u6d4b\u7231\u5c14\u5170\u71d5\u9ea6\u4e2d\u7684\u9709\u83cc\u6bd2\u7d20\u6c61\u67d3] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Mycotoxin contamination risks food safety and agricultural productivity.<br>\u9709\u83cc\u6bd2\u7d20\u6c61\u67d3\u5a01\u80c1\u98df\u54c1\u5b89\u5168\u548c\u519c\u4e1a\u751f\u4ea7\u529b\u3002]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Use neural networks and transfer learning (TabPFN, TabNet, FT-Transformer) for multi-response prediction.<br>\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u548c\u8fc1\u79fb\u5b66\u4e60\u8fdb\u884c\u591a\u54cd\u5e94\u9884\u6d4b\u3002]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>TabPFN performed best; weather history and seed moisture are key predictors.<br>TabPFN\u8868\u73b0\u6700\u4f73\uff1b\u5929\u6c14\u5386\u53f2\u548c\u79cd\u5b50\u6c34\u5206\u662f\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [knowledge distillation, reinforcement learning, vision-language models, progressive masking, offline RL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," NVIDIA"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22238",children:"https://arxiv.org/pdf/2512.22238"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5927\u578bVLM\u96be\u4ee5\u90e8\u7f72\u5230\u79fb\u52a8/\u8fb9\u7f18\u8bbe\u5907/Large VLMs are impractical for mobile/edge deployment]\n    B --\x3e B2[\u5e08\u751f\u6a21\u578b\u5c3a\u5bf8\u5dee\u8ddd\u5bfc\u81f4\u77e5\u8bc6\u84b8\u998f\u4e0d\u7a33\u5b9a/Large size gap causes unstable distillation]\n    C --\x3e C1[\u63a9\u7801\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u6846\u67b6/Mask-progressive RL distillation framework]\n    C --\x3e C2[\u5148\u63a9\u7801\u6559\u5e08\u975e\u4e3b\u5bfc\u6743\u91cd\uff0c\u518d\u6e10\u8fdb\u6062\u590d/First mask non-dominant teacher weights, then progressively restore]\n    C --\x3e C3[\u79bb\u7ebfRL\u9636\u6bb5\u4f7f\u7528\u51c6\u786e\u6027\u548c\u84b8\u998f\u5956\u52b1/Offline RL stage with accuracy and distillation rewards]\n    D --\x3e D1[\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u7d27\u51d1\u578bVLM/Outperforms existing compact VLMs on diverse benchmarks]\n    D --\x3e D2[\u6e10\u8fdb\u589e\u52a0\u6559\u5e08\u5c3a\u5bf8\u5e26\u6765\u66f4\u5e73\u6ed1\u6536\u655b\u548c\u66f4\u5f3a\u6cdb\u5316/Gradually increasing teacher size yields smoother convergence & stronger generalization]\n    D --\x3e D3[\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u90e8\u7f72VLM\u7684\u53ef\u6269\u5c55\u8def\u5f84/Provides a scalable path toward efficient, deployable VLMs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretability], [mechanistic multiplicity, explanatory stability, stochastic optimization, model explanations, diagnostic framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chama Bensmail"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Hertfordshire, Omics Data Solutions LTD"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22240",children:"https://arxiv.org/pdf/2512.22240"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/bensmailchama-boop/EvoXplain",children:"https://github.com/bensmailchama-boop/EvoXplain"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[EvoXplain Paper] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u9ad8\u7cbe\u5ea6\u6a21\u578b\u662f\u5426\u5171\u4eab\u76f8\u540c\u5185\u90e8\u903b\u8f91?<br/>Do high-accuracy models share the same internal logic?]\n    C --\x3e C1[\u8de8\u91cd\u590d\u8bad\u7ec3\u6d4b\u91cf\u89e3\u91ca\u7a33\u5b9a\u6027<br/>Measure explanation stability across repeated training]\n    C --\x3e C2[\u5c06\u89e3\u91ca\u89c6\u4e3a\u4f18\u5316\u8fc7\u7a0b\u6837\u672c<br/>Treat explanations as samples from optimization]\n    D --\x3e D1[\u53d1\u73b0\u89e3\u91ca\u7684\u591a\u6a21\u6001\u6027<br/>Found explanatory multimodality]\n    D --\x3e D2[\u903b\u8f91\u56de\u5f52\u7b49\u6a21\u578b\u4e5f\u663e\u793a\u591a\u79cd\u673a\u5236<br/>Models like Logistic Regression show multiple mechanisms]\n    D --\x3e D3[\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u6a21\u578b\u7c7b\u5c5e\u6027<br/>Reframe interpretability as model-class property]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [uncertainty estimation, calibration, linear probe, brier score, llm-as-judge]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Meta (FAIR at Meta, Meta Superintelligence Labs)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22245",children:"https://arxiv.org/pdf/2512.22245"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and \u224810x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model's hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Calibrating LLM Judges<br/>\u6821\u51c6LLM\u6cd5\u5b98] --\x3e B[Problem: LLM judges lack efficient, calibrated uncertainty<br/>\u95ee\u9898\uff1aLLM\u6cd5\u5b98\u7f3a\u4e4f\u9ad8\u6548\u3001\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1]\n    A --\x3e C[Method: Linear probes on hidden states with Brier score loss<br/>\u65b9\u6cd5\uff1a\u57fa\u4e8eBrier\u5206\u6570\u635f\u5931\u7684\u9690\u72b6\u6001\u7ebf\u6027\u63a2\u9488]\n    A --\x3e D[Results: Better calibration, 10x speedup, robust generalization<br/>\u7ed3\u679c\uff1a\u66f4\u597d\u7684\u6821\u51c6\uff0c10\u500d\u52a0\u901f\uff0c\u9c81\u68d2\u7684\u6cdb\u5316]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [physics-informed machine learning], [amortized inference, sim-to-real transfer, model rocketry, neural network, parameter estimation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rohit Pandey, Rohan Pandey"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Bellevue High School, University of Washington"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22248",children:"https://arxiv.org/pdf/2512.22248"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates model rocket parameter estimation as an amortized inference problem and demonstrates neural networks can invert physics simulations from sparse observations. 2. Proposes a simulation-based amortized inference approach that enables zero-shot sim-to-real transfer for aerodynamic parameter estimation. 3. Provides quantitative analysis of the sim-to-real gap and shows the learned model reduces apogee prediction error compared to a traditional baseline (OpenRocket)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b12172a46941853e291b33e21d912e272992d414baf27e4495c7137fe4266c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a simulation-based amortized inference method that trains a neural network on synthetic flight data to predict aerodynamic parameters like drag coefficient from a single apogee measurement. The trained model is applied directly to real flights without fine-tuning, achieving promising zero-shot sim-to-real transfer and reducing apogee prediction error compared to traditional tools."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Amortized Inference for Model Rocket Aerodynamics") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Estimating aerodynamic parameters from sparse real flight data is difficult and costly.")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Train neural network on synthetic data to invert physics simulator for amortized inference.")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Promising zero-shot sim-to-real transfer with reduced apogee prediction error.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Affine Divergence: Aligning Activation Updates Beyond Normalisation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [optimization theory], [activation updates, gradient descent, normalisation, PatchNorm, affine divergence]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," George Bird"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Manchester"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22247",children:"https://arxiv.org/pdf/2512.22247"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies a systematic mismatch between mathematically ideal and effective activation updates during gradient descent, providing a new theoretical framework for understanding optimization. 2. Derives normalisation from first principles as a solution to this mismatch and proposes a functionally distinct, non-scale-invariant alternative that outperforms conventional normalisers. 3. Introduces "PatchNorm", a new compositionally inseparable normaliser for convolutional layers, and argues for reframing normalisers as activation-function-like maps with parameterised scaling.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ed42bcb2d9358f1931e9e0ea31246380c7ef78409f6004933a0fb2461eb9d1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper identifies a mismatch between the ideal steepest-descent direction for activations and their effective updates during gradient descent. It proposes a theoretical framework that derives normalisation as a solution and introduces a new alternative, including "PatchNorm" for convolutions, which empirically outperforms standard normalisers. This reframes normalisation\'s role and questions the standard affine+nonlinear model-building approach.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[The Affine Divergence: Aligning Activation Updates Beyond Normalisation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u6fc0\u6d3b\u66f4\u65b0\u4e0d\u5339\u914d/Activation update mismatch]\n    P1 --\x3e P2[\u975e\u7406\u60f3\u7f29\u653e/Non-ideal scaling]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u7406\u8bba\u63a8\u5bfc/Theoretical derivation]\n    M1 --\x3e M2[\u63d0\u51fa\u65b0\u65b9\u6848/Propose new solutions]\n    M2 --\x3e M3[PatchNorm for convolution]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u91cd\u6784\u5f52\u4e00\u5316/Reframe normalisation]\n    R1 --\x3e R2[\u65b0\u65b9\u6848\u6709\u6548/New methods outperform]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Analyzing Skill Element in Online Fantasy Cricket"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [game theory and decision making], [statistical framework, team selection strategies, dynamic tournament model, softmax reweighting, IPL dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sarthak Sarkar, Supratim Das, Purushottam Saha, Diganta Mukherjee, Tridib Mukherjee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Statistical Institute, Kolkata"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22254",children:"https://arxiv.org/pdf/2512.22254"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Development of a statistical framework to assess the role of skill in online fantasy cricket. 2. Construction and analysis of a range of deterministic and stochastic team selection strategies. 3. Introduction of a dynamic tournament model with agent populations evolving via a softmax reweighting mechanism."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e92b7b67260659549974352e85a2e375a90fd015a5561a1d909fb586dcfd635_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops a statistical framework to analyze whether success in online fantasy cricket is driven by skill or chance. It constructs various team selection strategies and a dynamic tournament model, testing them on IPL 2024 data. The results provide quantitative evidence supporting the presence of a skill element in these platforms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Analyzing Skill Element in Online Fantasy Cricket] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Fantasy sports: skill or chance?]\n    C --\x3e C1[Statistical framework]\n    C1 --\x3e C2[Team selection strategies]\n    C2 --\x3e C3[Deterministic & Stochastic]\n    C1 --\x3e C4[Dynamic tournament model]\n    C4 --\x3e C5[Softmax reweighting]\n    C --\x3e C6[Experiments on IPL 2024]\n    D --\x3e D1[Evidence for skill element]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [graph attention network, link prediction, transfer learning, graph transformer, contrastive loss]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Jinan University, Great Bay University, JD Technology, University of Illinois Chicago"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22252",children:"https://arxiv.org/pdf/2512.22252"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/DSI-Lab1/GAATNet",children:"https://github.com/DSI-Lab1/GAATNet"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Graph Attention-based Adaptive Transfer Learning for Link Prediction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Challenges in large-scale sparse graphs and cross-dataset transfer learning for link prediction]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes GAATNet with distant neighbor embeddings and lightweight self-adapter for adaptive transfer]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves SOTA performance on seven datasets, provides scalable GNN-transfer learning solution]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [human motion segmentation], [temporal vision semantics, subspace clustering, large language model, temporal regularizer, feedback framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zheng Xing, Weibing Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shenzhen University, Shenzhen MSU-BIT University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22249",children:"https://arxiv.org/pdf/2512.22249"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem: \u4f20\u7edf\u65e0\u76d1\u7763\u4eba\u4f53\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\u5ffd\u7565\u4e86\u65f6\u5e8f\u8bed\u4e49\u7684\u4f5c\u7528 / Traditional unsupervised HMS overlooks temporal semantics)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method: \u5229\u7528LLM\u63d0\u53d6\u65f6\u5e8f\u89c6\u89c9\u8bed\u4e49\uff0c\u5e76\u878d\u5165\u5b50\u7a7a\u95f4\u805a\u7c7b\u6846\u67b6 / Use LLM to extract TVS and integrate it into subspace clustering)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results: \u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5 / Outperforms SOTA on four benchmark datasets)\n    C --\x3e C1(LLM\u67e5\u8be2 / LLM Query: \u5224\u65ad\u76f8\u90bb\u5e27\u662f\u5426\u63cf\u8ff0\u76f8\u540c\u8fd0\u52a8 / Determine if consecutive frames depict the same motion)\n    C --\x3e C2(\u65f6\u5e8f\u6b63\u5219\u5316 / Temporal Regularizer: \u8bf1\u5bfc\u76f8\u90bb\u5e27\u5171\u4eab\u76f8\u4f3c\u5b50\u7a7a\u95f4\u5d4c\u5165 / Induces similar subspace embeddings for temporal neighbors)\n    C --\x3e C3(\u53cd\u9988\u4f18\u5316 / Feedback Optimization: \u57fa\u4e8e\u5206\u5272\u7ed3\u679c\u8fed\u4ee3\u4f18\u5316\u5d4c\u5165 / Iteratively optimizes embedding based on segmentation output)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [biomedical knowledge graph, graph attention network, gene perturbation, multimodal embeddings, PrimeKG++]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pascal Passigan, Kevin zhu, Angelina Ning"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Massachusetts Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22251",children:"https://arxiv.org/pdf/2512.22251"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Predicting granular gene expression changes (perturbations) from drugs, beyond binary drug-disease associations.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Merge PrimeKG++ & LINCS L1000 into a BKG; use Graph Attention Network (GAT) to predict delta expression.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Outperforms MLP baselines; ablation shows KG edges enhance prediction for mechanistic modeling.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [prompt engineering], [Logic Sketch Prompting, deterministic prompting, interpretability, rule adherence, clinical decision support]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Satvik Tripathi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Pennsylvania"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22258",children:"https://arxiv.org/pdf/2512.22258"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/satviktri/LSP",children:"https://github.com/satviktri/LSP"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Logic Sketch Prompting (LSP)] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs unreliable on tasks needing strict rules & determinism]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Lightweight framework with typed variables, condition evaluators, rule validator]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Highest accuracy/F1 vs. baselines; suitable for clinical/safety-critical systems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [reasoning], [chain-of-thought, synthetic data, distribution shift, fine-tuning, reasoning robustness]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Universit\xe9 de Montr\xe9al, Microsoft Research India, Google DeepMind, Periodic Labs"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22255",children:"https://arxiv.org/pdf/2512.22255"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model's reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model's distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data's distribution is closer to the model's own. The key conclusion is that aligning the training data distribution with the model's is more critical for performance than the correctness of the final answers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Shape of Thought / \u601d\u7ef4\u5f62\u6001] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Correctness vs. Distribution / \u6b63\u786e\u6027\u4e0e\u6570\u636e\u5206\u5e03]\n    B1 --\x3e B2{Does correctness guarantee better reasoning? / \u6b63\u786e\u6027\u4fdd\u8bc1\u66f4\u597d\u7684\u63a8\u7406\u5417?}\n    C --\x3e C1[Train on Incorrect Synthetic CoT / \u4f7f\u7528\u9519\u8bef\u7684\u5408\u6210CoT\u8bad\u7ec3]\n    C --\x3e C2[Paraphrase Human Traces / \u6539\u5199\u4eba\u7c7b\u6807\u6ce8\u7684\u63a8\u7406\u94fe]\n    C --\x3e C3[Introduce Flawed Steps / \u5f15\u5165\u6709\u7f3a\u9677\u7684\u63a8\u7406\u6b65\u9aa4]\n    D --\x3e D1[Synthetic Incorrect > Human Correct / \u9519\u8bef\u7684\u5408\u6210\u6570\u636e\u4f18\u4e8e\u6b63\u786e\u7684\u4eba\u7c7b\u6570\u636e]\n    D --\x3e D2[Distribution Alignment is Key / \u6570\u636e\u5206\u5e03\u5bf9\u9f50\u662f\u5173\u952e]\n    D --\x3e D3[Final Answer \u2260 Faithful Reasoning / \u6700\u7ec8\u7b54\u6848 \u2260 \u5fe0\u5b9e\u63a8\u7406\u8fc7\u7a0b]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [clinical prediction], [synthetic data, class imbalance, permutation feature importance, tabular data, mortality prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Daniil Burakov, Ivan Petrov, Dmitrii Khelimskii, Ivan Bessonov, Mikhail Lazarev"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HSE University, Meshalkin National Medical Research Center, Tyumen Cardiology Research Center"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22259",children:"https://arxiv.org/pdf/2512.22259"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed and evaluated machine learning models for predicting 3-year cardiac mortality after PCI using a dataset of patients with bifurcation lesions. 2. Demonstrated that augmenting the training set with synthetic samples effectively addresses class imbalance, improving minority-class recall and probability quality with minimal impact on AUROC. 3. Identified key clinical predictors (Age, Ejection Fraction, Peripheral Artery Disease, Cerebrovascular Disease) through feature importance analysis and highlighted the brittleness of models on external validation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8124ba060c0a4a10502266c7255b089103b4cb860b15006e80971c9e28cfdc68_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study developed machine learning models to predict cardiac death within three years for patients undergoing percutaneous coronary intervention (PCI). To handle class imbalance, the authors augmented the real patient data with synthetic samples, which improved the models' ability to identify high-risk patients. The analysis identified key risk factors and demonstrated that data augmentation can reduce model brittleness in imbalanced clinical prediction tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9884\u6d4bPCI\u672f\u540e\u5fc3\u810f\u6b7b\u4ea1\u98ce\u9669/Predict cardiac death risk post-PCI]\n    B --\x3e B2[\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898/Class imbalance issue]\n    C --\x3e C1[\u5e94\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b/Apply multiple ML models]\n    C --\x3e C2[\u751f\u6210\u5e76\u6dfb\u52a0\u5408\u6210\u6570\u636e/Generate & add synthetic data]\n    C --\x3e C3[\u4f7f\u7528\u6392\u5217\u7279\u5f81\u91cd\u8981\u6027/Use permutation feature importance]\n    D --\x3e D1[\u5408\u6210\u6570\u636e\u63d0\u5347\u5c11\u6570\u7c7b\u53ec\u56de/Synthetic data improves minority-class recall]\n    D --\x3e D2[\u8bc6\u522b\u5173\u952e\u7279\u5f81: \u5e74\u9f84, \u5c04\u8840\u5206\u6570\u7b49/Identify key features: Age, Ejection Fraction, etc.]\n    D --\x3e D3[\u5916\u90e8\u9a8c\u8bc1\u6027\u80fd\u4e0b\u964d/Performance drop on external validation]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [physics-informed machine learning], [physics-constrained data generation, ablation study, grating coupler, Fabry-Perot oscillations, energy conservation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rahul D Ray"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," BITS Pilani, Hyderabad Campus"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22261",children:"https://arxiv.org/pdf/2512.22261"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Identifies the "physics constraint paradox," demonstrating that explicit energy conservation enforcement can be mathematically redundant in physically consistent models. 2. Shows that removing Fabry-Perot oscillations significantly reduces bandwidth variability and improves downstream ML model accuracy for bandwidth prediction. 3. Reveals a subtle pitfall where standard noise-addition-and-renormalization pipelines can introduce unphysical negative absorption values.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c510a23d89c2ae438eca78e6b993f8ce6b30f94405a1290b3509d74226bafc1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates physics-constrained data generation for machine learning through an ablation study of a grating coupler spectrum generator. It finds that explicitly enforcing certain physical constraints, like energy conservation, can be redundant, while others, like Fabry-Perot oscillations, can hinder machine learning performance for specific prediction tasks. The main conclusion is that increased physical realism in data generation does not always improve ML learnability, and ML performance can be used to diagnose the relevance of physical constraints."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["The Physics Constraint Paradox / \u7269\u7406\u7ea6\u675f\u6096\u8bba"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Physics-constrained data generation may over-constrain models. / \u7269\u7406\u7ea6\u675f\u6570\u636e\u751f\u6210\u53ef\u80fd\u8fc7\u5ea6\u7ea6\u675f\u6a21\u578b"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Systematic ablation study of a physics-informed generator. / \u5bf9\u7269\u7406\u4fe1\u606f\u751f\u6210\u5668\u8fdb\u884c\u7cfb\u7edf\u6d88\u878d\u7814\u7a76"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Explicit energy conservation is redundant; Removing Fabry-Perot oscillations improves ML learnability. / \u663e\u5f0f\u80fd\u91cf\u5b88\u6052\u662f\u5197\u4f59\u7684\uff1b\u79fb\u9664\u6cd5\u5e03\u91cc-\u73c0\u7f57\u632f\u8361\u53ef\u63d0\u5347\u673a\u5668\u5b66\u4e60\u53ef\u5b66\u4e60\u6027"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph representation learning], [temporal motifs, dynamic graphs, llm agent, structure-aware dispatcher, prompting techniques]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tianjin University, Beihang University, Guangxi Normal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22266",children:"https://arxiv.org/pdf/2512.22266"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Wjerry5/LLMTM",children:"https://github.com/Wjerry5/LLMTM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[LLMs\u5904\u7406\u52a8\u6001\u56fe\u65f6\u6001\u6a21\u4f53\u5206\u6790\u80fd\u529b\u672a\u77e5/LLMs' capability for temporal motif analysis on dynamic graphs is unexplored]\n    C --\x3e C1[\u63d0\u51fa\u57fa\u51c6LLMTM\u4e0e\u667a\u80fd\u4f53/Propose benchmark LLMTM and an agent]\n    C --\x3e C2[\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u8c03\u5ea6\u5668/Propose structure-aware dispatcher]\n    D --\x3e D1[\u8c03\u5ea6\u5668\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u5e76\u964d\u4f4e\u6210\u672c/Dispatcher maintains high accuracy while reducing cost]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [object detection], [RGB-LWIR fusion, multispectral imagery, YOLO, adaptive framework, illumination conditions]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," George Mason University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22263",children:"https://arxiv.org/pdf/2512.22263"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Evaluating an Adaptive Multispectral Turret System] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[RGB\u5728\u4f4e\u5149\u4e0b\u8868\u73b0\u5dee/RGB struggles in low-light]\n    Problem --\x3e P2[\u70ed\u6210\u50cf\u7f3a\u4e4f\u989c\u8272\u7eb9\u7406/Thermal lacks color & texture]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u81ea\u9002\u5e94RGB-LWIR\u878d\u5408\u6846\u67b6/Adaptive RGB-LWIR fusion framework]\n    Method --\x3e M2[\u8bad\u7ec333\u4e2aYOLO\u6a21\u578b/Trained 33 YOLO models]\n    Method --\x3e M3[11\u79cd\u878d\u5408\u6bd4\u4f8b/11 fusion ratios]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u5168\u5149\u6a21\u578b: 92.8%\u7f6e\u4fe1\u5ea6/Full-light model: 92.8% confidence]\n    Results --\x3e R2[\u5fae\u5149\u6a21\u578b: 92.0%\u7f6e\u4fe1\u5ea6/Dim-light model: 92.0% confidence]\n    Results --\x3e R3[\u65e0\u5149\u6a21\u578b: 71.0%\u7f6e\u4fe1\u5ea6/No-light model: 71.0% confidence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [photonic neural networks, transfer matrix, Slicing method, back-propagation, simulation framework]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Politecnico di Torino, Ghent University - imec, University College London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22264",children:"https://arxiv.org/pdf/2512.22264"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation for training Photonic Neural Networks (PNNs). 2. Introduces LuxIA, a unified simulation and training framework that integrates the Slicing method to enable scalable PNN training. 3. Demonstrates through experiments that LuxIA surpasses existing tools in speed and scalability for training large-scale PNNs on standard datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/362a4190a58349fa96aae555a8a4643a7fdd50b962ff88817d9c12e4678fbe98_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the scalability challenges in simulating and training large-scale Photonic Neural Networks (PNNs) by introducing the Slicing method for efficient transfer matrix computation. The method is integrated into the LuxIA framework, which significantly reduces memory usage and training time. Experimental results show LuxIA outperforms existing tools, enabling the exploration of larger and more complex photonic architectures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[LuxIA: A Lightweight Unitary matriX-based Framework] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5f53\u524dPNN\u4eff\u771f\u5de5\u5177\u53ef\u6269\u5c55\u6027\u5dee<br>Current PNN simulation tools lack scalability]\n    C --\x3e C1[\u63d0\u51faSlicing\u65b9\u6cd5<br>Propose the Slicing method]\n    C --\x3e C2[\u6784\u5efaLuxIA\u7edf\u4e00\u6846\u67b6<br>Build the unified LuxIA framework]\n    D --\x3e D1[\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4e0e\u65f6\u95f4\u6d88\u8017<br>Significantly reduces memory and time consumption]\n    D --\x3e D2[\u5728\u901f\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u5de5\u5177<br>Outperforms existing tools in speed and scalability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [bayesian optimization], [Dirichlet Process Mixture Model, Gaussian Process Regression, Transformer, TabNet, XGBoost]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amgad Ahmed Ali Ibrahim, Hein Htet, Ryoji Asahi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nagoya University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22279",children:"https://arxiv.org/pdf/2512.22279"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hierarchical stacking optimization framework (SoDip) integrating a Transformer for text, TabNet/XGBoost for multimodal features, and GPR with DPMM for uncertainty. 2. Curated a diverse dataset for radiation-induced grafting using automated tools to handle numerical and textual variables. 3. Demonstrated ~33% performance improvement over standard GPR with calibrated confidence intervals for identifying low-reproducibility regimes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e1ff3feaf3b0e588ef295bb1595e516fbcdb32563ea40c93095217ac66e1fc5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses reproducibility issues in radiation-induced graft polymerization by proposing SoDip, a hierarchical data-driven framework that combines Transformer encoders, multimodal feature models, and Bayesian optimization with uncertainty quantification. The method integrates sparse textual and numerical data, showing a 33% improvement over Gaussian Process Regression and providing reliable confidence estimates. This establishes a foundation for morphology-aware, reproducible design in polymer research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SoDip: Hierarchical Stacking Optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Radiation-induced grafting reproducibility limited by unreported base-film morphology variability)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical framework with Transformer (text), TabNet/XGBoost (features), GPR+DPMM (uncertainty), Bayesian Optimization)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: ~33% improvement over GPR, calibrated confidence intervals, integrates sparse multimodal data)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [deterministic memory, fixed-point arithmetic, vector embeddings, approximate nearest neighbor search, state machine]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Varshith Gudur"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher (Valori Kernel Project)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22280",children:"https://arxiv.org/pdf/2512.22280"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/varshith-Git/Valori-Kernel",children:"https://github.com/varshith-Git/Valori-Kernel"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Valori: A Deterministic Memory Substrate for AI Systems] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: AI\u5185\u5b58\u975e\u786e\u5b9a\u6027/AI Memory Non-Determinism]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u56fa\u5b9a\u70b9\u7b97\u672f\u4e0e\u72b6\u6001\u673a/Fixed-Point Arithmetic & State Machine]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u8de8\u5e73\u53f0\u6bd4\u7279\u4e00\u81f4\u6027/Cross-Platform Bit-Identical Results]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [Generative Adversarial Networks, Non-Intrusive Load Monitoring, Clustering, LSTM, Pattern Generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kyungpook National University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22287",children:"https://arxiv.org/pdf/2512.22287"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model's interpretability and scalability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Cluster Aggregated GAN (CAG)"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: \u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\uff0c\u73b0\u6709GAN\u65b9\u6cd5\u5bf9\u6240\u6709\u8bbe\u5907\u4e00\u89c6\u540c\u4ec1\uff0c\u5ffd\u7565\u95f4\u6b47\u6027\u548c\u6301\u7eed\u6027\u8bbe\u5907\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u4fdd\u771f\u5ea6\u6709\u9650"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u6df7\u5408\u751f\u6210\u6846\u67b6\uff0c\u6839\u636e\u8bbe\u5907\u884c\u4e3a\u7279\u5f81\u8def\u7531\u5230\u4e13\u95e8\u5206\u652f\uff1a\u95f4\u6b47\u6027\u8bbe\u5907\u4f7f\u7528\u805a\u7c7b\u6a21\u5757\u548c\u4e13\u7528\u751f\u6210\u5668\uff1b\u6301\u7eed\u6027\u8bbe\u5907\u4f7f\u7528LSTM\u751f\u6210\u5668"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u5728UVIC\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u5728\u771f\u5b9e\u6027\u3001\u591a\u6837\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u805a\u7c7b\u4f5c\u4e3a\u4e3b\u52a8\u751f\u6210\u7ec4\u4ef6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal inference], [Double Machine Learning, Moderated Mediation, Algorithmic Control, Nonmonotonic Effects, Gig Economy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Anna University, National Institute of Technology Tiruchirappalli, University of Missouri"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22290",children:"https://arxiv.org/pdf/2512.22290"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a "murky middle" of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["\u5f53\u7b97\u6cd5\u7ba1\u7406\u4eba\u7c7b: \u4f30\u7b97\u7b97\u6cd5\u63a7\u5236\u5bf9\u96f6\u5de5\u5de5\u4eba\u7ee9\u6548\u548c\u798f\u7949\u975e\u7ebf\u6027\u6548\u5e94\u7684\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5 / When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898: \u7b97\u6cd5\u7ba1\u7406\u4e0b\uff0c\u4ee5\u4eba\u4e3a\u672c\u7684\u7ba1\u7406\u80fd\u5426\u6301\u7eed\uff1f\u5de5\u4eba\u5bf9\u7b97\u6cd5\u7684\u53cd\u5e94\u662f\u975e\u7ebf\u6027\u7684 / Problem: Can person-centered management survive algorithmic management? Worker responses are nonlinear."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5: \u4f7f\u7528\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4f30\u7b97\u6709\u8c03\u8282\u7684\u4e2d\u4ecb\u6a21\u578b\uff0c\u65e0\u4e25\u683c\u51fd\u6570\u5f62\u5f0f\u9650\u5236 / Method: Double Machine Learning framework to estimate a moderated mediation model without restrictive functional forms."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c: \u53d1\u73b0\u975e\u5355\u8c03\u6a21\u5f0f\u3002\u6a21\u7cca\u7684\u7b97\u6cd5\u76d1\u7763\u524a\u5f31\u7ee9\u6548\u8054\u7cfb\uff0c\u900f\u660e\u53ef\u89e3\u91ca\u7684\u76d1\u7763\u5219\u52a0\u5f3a\u5b83 / Results: Found a nonmonotonic pattern. Murky oversight weakens the performance link, transparent and explainable oversight strengthens it."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [Masked Diffusion Models, Markov Decision Process, Group Relative Policy Optimization, inference schedule optimization, trajectory-level training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University (Leap Lab), Anyverse Dynamics"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22288",children:"https://arxiv.org/pdf/2512.22288"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://co-grpo.github.io",children:"https://co-grpo.github.io"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u5339\u914d/Mismatch between Training & Inference]\n    B1 --\x3e B2[\u8bad\u7ec3: \u5355\u6b65BERT\u5f0f/Training: Single-step BERT-style]\n    B1 --\x3e B3[\u63a8\u7406: \u591a\u6b65\u6709\u8c03\u5ea6/Inference: Multi-step with Schedule]\n    C --\x3e C1[\u7edf\u4e00MDP/Unified MDP]\n    C1 --\x3e C2[\u8054\u5408\u4f18\u5316\u6a21\u578b\u4e0e\u8c03\u5ea6/Jointly Optimize Model & Schedule]\n    C2 --\x3e C3[\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316/Group Relative Policy Optimization]\n    D --\x3e D1[\u63d0\u5347\u751f\u6210\u8d28\u91cf/Improved Generation Quality]\n    D1 --\x3e D2[\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u9a8c\u8bc1/Validated on Four Benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [Physics-informed neural networks, Kolmogorov-Arnold networks, Adaptive weighting, B-splines, Partial differential equations]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Guokan Chen, Yao Xiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fujian University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22283",children:"https://arxiv.org/pdf/2512.22283"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[PINNs struggle with multi-scale/high-frequency PDEs / PINNs\u5728\u5904\u7406\u591a\u5c3a\u5ea6/\u9ad8\u9891PDE\u65f6\u9047\u5230\u56f0\u96be]\n    P1 --\x3e P2[Issues: Gradient flow stiffness & spectral bias / \u95ee\u9898: \u68af\u5ea6\u6d41\u521a\u5ea6\u548c\u8c31\u504f\u5dee]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[Architecture: Kolmogorov-Arnold Network (KAN) with learnable B-splines / \u67b6\u6784: \u57fa\u4e8e\u53ef\u5b66\u4e60B\u6837\u6761\u7684KAN]\n    Method --\x3e M2[Strategy: Adaptive weighting with dynamic decay upper bound / \u7b56\u7565: \u5e26\u52a8\u6001\u8870\u51cf\u4e0a\u754c\u7684\u81ea\u9002\u5e94\u52a0\u6743]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[Faster convergence & higher accuracy / \u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6]\n    R1 --\x3e R2[Improvement: At least one order of magnitude / \u63d0\u5347: \u81f3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7]\n    Results --\x3e R3[Benchmarks: Klein-Gordon, Burgers, Helmholtz equations / \u57fa\u51c6: Klein-Gordon, Burgers, Helmholtz\u65b9\u7a0b]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [language model safety], [sparse autoencoder, feature orthogonalization, stealth slip, pragmatic interpretation, statistical co-occurrence]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tsogt-Ochir Enkhbayar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mongol-AI (inferred from email domain)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22293",children:"https://arxiv.org/pdf/2512.22293"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Empirically demonstrates that warning-framed training data fails to teach language models to avoid warned-against behaviors, showing generation rates similar to direct exposure. 2. Provides a mechanistic interpretation using sparse autoencoders, identifying a failure of feature orthogonalization where "describing" and "performing" an action activate overlapping latent features. 3. Identifies and names the "stealth slip" phenomenon, where conversational preambles can rotate activations into subspaces undetectable by linear probes, and shows that training-time feature ablation, not prompting, is required to address the issue.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fddb803a434c3d49e04dd722184b33f238c4b81254540d6bb0d1961a3d09e1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates why language models trained on warning-framed examples (e.g., "DO NOT USE") still learn to generate the warned-against content. Through behavioral experiments and sparse autoencoder analysis, it finds that models learn statistical co-occurrences rather than pragmatic intent, due to overlapping latent features for description and action. The core conclusion is that current architectures prioritize pattern completion over understanding speaker intent, requiring training-time interventions like feature ablation for correction.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Do warning-framed examples teach models to avoid bad behavior?")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Behavioral analysis & Sparse Autoencoder mechanistic interpretability")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: No. Models learn statistical co-occurrence, not pragmatic intent.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multi-Head Spectral-Adaptive Graph Anomaly Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph anomaly detection], [spectral graph neural network, hypernetwork, Chebyshev filter, teacher-student contrastive learning, Barlow Twins loss]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," People's Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22291",children:"https://arxiv.org/pdf/2512.22291"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a 'spectral fingerprint'. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multi-Head Spectral-Adaptive Graph Anomaly Detection] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u56fa\u5b9a\u6ee4\u6ce2\u5668\u5bfc\u81f4\u8fc7\u5e73\u6ed1\u4e0e\u7f3a\u4e4f\u9002\u5e94\u6027/Fixed filters cause over-smoothing & lack adaptability]\n    C --\x3e C1[\u57fa\u4e8e\u8c31\u6307\u7eb9\u7684\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc/Lightweight hypernetwork based on spectral fingerprint]\n    C --\x3e C2[\u52a8\u6001\u751f\u6210\u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\u53c2\u6570/Dynamically generates Chebyshev filter parameters]\n    C --\x3e C3[\u53cc\u6b63\u5219\u5316\u7b56\u7565\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83/Dual regularization prevents mode collapse]\n    D --\x3e D1[\u6709\u6548\u4fdd\u7559\u9ad8\u9891\u5f02\u5e38\u4fe1\u53f7/Effectively preserves high-frequency anomaly signals]\n    D --\x3e D2[\u5728\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u8d8a/Outperforms SOTA on heterogeneous datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Quantum Machine Learning, Mixture-of-Experts, Parameterized Quantum Circuits, Quantum Router, Interference Hypothesis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Reda Heddad, Lamiae Bouanane"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Al Akhawayn University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22296",children:"https://arxiv.org/pdf/2512.22296"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Design of a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture with a Quantum Router for an ablation study to isolate the source of quantum advantage. 2. Validation of the Interference Hypothesis, demonstrating the Quantum Router's topological advantage and superior parameter efficiency on non-linearly separable data. 3. Empirical analysis of the architecture's robustness against quantum noise, confirming its feasibility for near-term (NISQ) hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407a17903b2360f889e7afbc3b5f776273cc33f5cf3daf0fa26caf69bc1ed179_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture that uses a Quantum Router to address limitations like expert imbalance in classical MoE systems. The core finding is that the Quantum Router, leveraging quantum interference, provides a topological advantage for routing complex data more efficiently than classical routers. The method is shown to be robust to noise and feasible for near-term quantum hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Hybrid Quantum-Classical Mixture of Experts") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("MoE\u6311\u6218:\u4e13\u5bb6\u4e0d\u5e73\u8861,\u8def\u7531\u590d\u6742/MoE Challenges: Expert Imbalance, Routing Complexity")\n    Method --\x3e M1("\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784/Hybrid Quantum-Classical Architecture")\n    Method --\x3e M2("\u91cf\u5b50\u8def\u7531\u7f51\u7edc/Quantum Router")\n    Method --\x3e M3("\u5229\u7528\u91cf\u5b50\u5e72\u6d89/Utilizes Quantum Interference")\n    Results --\x3e R1("\u9a8c\u8bc1\u5e72\u6d89\u5047\u8bf4/Validates Interference Hypothesis")\n    Results --\x3e R2("\u62d3\u6251\u4f18\u52bf,\u9ad8\u6548\u89e3\u8026\u6570\u636e/Topological Advantage, Efficiently Untangles Data")\n    Results --\x3e R3("NISQ\u786c\u4ef6\u53ef\u884c/Feasible for NISQ Hardware")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [human activity recognition], [driver monitoring systems, edge AI, quantization, temporal decision head, confounder-aware labeling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vesal Ahsani, Babak Hossein Khalaj"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sharif University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22298",children:"https://arxiv.org/pdf/2512.22298"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5b9e\u65f6DMS\u9700\u6c42 / Real-time DMS needs low latency, low cost, low power]\n    C --\x3e C1[\u7d27\u51d1\u5355\u6444\u50cf\u5934\u7cfb\u7edf / Compact single-camera pipeline]\n    C1 --\x3e C2[\u7d27\u51d1\u89c6\u89c9\u6a21\u578b / Compact per-frame vision model]\n    C1 --\x3e C3[\u6297\u6df7\u6dc6\u6807\u7b7e\u8bbe\u8ba1 / Confounder-aware label design]\n    C1 --\x3e C4[\u65f6\u5e8f\u51b3\u7b56\u5934 / Temporal decision head]\n    D --\x3e D1[\u6027\u80fd: 16 FPS (RPi5), 25 FPS (Edge TPU) / Performance: 16 FPS (RPi5), 25 FPS (Edge TPU)]\n    D --\x3e D2[\u9a8c\u8bc1: \u771f\u5b9e\u8f66\u8f86\u6d4b\u8bd5 / Validation: Real in-vehicle tests]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [transportation safety analytics], [Random Forest, Kernel Density Estimation (KDE), Moran's I, Highway Safety Manual (HSM), Negative Binomial Regression]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jennifer Sawyer, Julian Allagan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Elizabeth City State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22302",children:"https://arxiv.org/pdf/2512.22302"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Applied and compared advanced statistical and machine learning methods (Random Forest, KDE, Negative Binomial Regression) to rural highway crash data, demonstrating a methodological advancement beyond basic techniques. 2. Validated spatial clustering of accidents using Moran's I test and identified specific crash hotspots via KDE, extending previous hotspot analysis. 3. Showed that a Random Forest classifier for injury severity prediction outperformed the standard Highway Safety Manual (HSM) Safety Performance Function (SPF) model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87320e45eec3961a5ef58f2548cb06120fd2e4baee855440998207874d568dbe_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study analyzes traffic accident data from a rural highway using advanced statistical and machine learning techniques, including Random Forest and spatial analysis. The proposed Random Forest model for predicting injury severity achieved 67% accuracy, outperforming the standard HSM model, and spatial analysis confirmed crash clustering near intersections. The results provide actionable insights for targeted safety interventions on US 158."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Statistical and Machine Learning Analysis of Traffic Accidents on US 158") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Rural highway traffic safety analysis")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: KDE, Random Forest, Moran\'s I, HSM SPF comparison")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: RF outperforms HSM, hotspots identified, clustering confirmed")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [MLOps, champion-challenger framework, out-of-time validation, probability of default, data drift]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sultan Amed, Chan Yu Hang, Sayantan Banerjee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Management Indore, National University of Singapore"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22305",children:"https://arxiv.org/pdf/2512.22305"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PDx, an adaptive, MLOps-driven decision system for end-to-end lifecycle management of credit risk models. 2. Introduces a dynamic champion-challenger framework with regular model updates and out-of-time validation to combat data drift. 3. Empirically demonstrates that decision tree-based ensemble models perform best for default classification but require frequent retraining, and validates PDx's effectiveness across multiple lending datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c779eabc4c2fa3f75801b8b61c3eed3c105bd748ce6aa84ff37317acd929db3a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PDx, an adaptive credit risk forecasting system that uses an MLOps pipeline and a champion-challenger framework to continuously monitor, retrain, and validate models against data drift. The study finds decision tree ensembles are most effective but degrade without updates, and shows PDx mitigates value erosion in digital lending, especially for short-term loans."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PDx - Adaptive Credit Risk Forecasting Model / PDx - \u81ea\u9002\u5e94\u4fe1\u7528\u98ce\u9669\u9884\u6d4b\u6a21\u578b] --\x3e B[Problem: Static PD models degrade, hard to deploy & maintain / \u95ee\u9898: \u9759\u6001PD\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u96be\u4ee5\u90e8\u7f72\u7ef4\u62a4]\n    A --\x3e C[Method: MLOps pipeline with dynamic champion-challenger framework / \u65b9\u6cd5: \u91c7\u7528\u52a8\u6001\u51a0\u519b-\u6311\u6218\u8005\u6846\u67b6\u7684MLOps\u6d41\u7a0b]\n    A --\x3e D[Results: Decision tree ensembles best, PDx mitigates value erosion / \u7ed3\u679c: \u51b3\u7b56\u6811\u96c6\u6210\u6548\u679c\u6700\u4f73\uff0cPDx\u51cf\u5c11\u4ef7\u503c\u4fb5\u8680]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] LLMBoost: Make Large Language Models Stronger with Boosting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [ensemble learning, boosting, cross-model attention, chain training, near-parallel inference]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beihang University, China Telecom eSurfing Cloud"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22309",children:"https://arxiv.org/pdf/2512.22309"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[LLMBoost: Make Large Language Models Stronger with Boosting] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing LLM ensemble methods treat models as black boxes, ignoring internal representations.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: A boosting-inspired framework with cross-model attention, chain training, and near-parallel inference.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Consistently boosts accuracy and reduces inference latency on reasoning tasks.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [hardware security, model protection], [logic locking, intellectual property protection, hardware accelerator, model theft, supply chain security]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northwestern University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22307",children:"https://arxiv.org/pdf/2512.22307"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (<0.1% for 7,168 key bits)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators] --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem: Model IP Protection & Supply Chain Threats)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method: Hardware-Software Co-design with Logic Locking)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results: Resists Attacks, <0.1% Overhead)\n    Problem --\x3e P1(\u6a21\u578b\u76d7\u7a83/Model Theft)\n    Problem --\x3e P2(\u6a21\u578b\u7834\u574f/Model Corruption)\n    Problem --\x3e P3(\u4fe1\u606f\u6cc4\u9732/Information Leakage)\n    Method --\x3e M1(\u8f6f\u4ef6\u4fa7: \u795e\u7ecf\u5143\u5d4c\u5165\u5bc6\u94a5/Software: Key Embedding in Neurons)\n    Method --\x3e M2(\u786c\u4ef6\u4fa7: \u8f7b\u91cf\u7ea7\u9501\u5b9a\u6a21\u5757/Hardware: Lightweight Locking Module)\n    Results --\x3e R1(\u62b5\u5fa1\u4f18\u5316\u653b\u51fb/Withstands Oracle-Guided Attacks)\n    Results --\x3e R2(\u4f4e\u8ba1\u7b97\u5f00\u9500/Low Computational Overhead)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [optimistic feasible search, closed-loop decision-making, demographic parity, bandit feedback, threshold policy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wenzhang Du"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Mahanakorn University of Technology, International College (MUTIC)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22313",children:"https://arxiv.org/pdf/2512.22313"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Optimistic Feasible Search (OFS), a simple grid-based method for constrained closed-loop threshold learning using optimism under uncertainty. 2. Introduced synthetic and semi-synthetic closed-loop benchmarks with stable contraction dynamics and real-world datasets (German Credit, COMPAS) to evaluate feedback effects. 3. Demonstrated that OFS achieves near-oracle performance with higher reward and lower cumulative constraint violation compared to unconstrained and primal-dual bandit baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baad942bfd56e4e5f21dcd74fbfaad4a484372898d0862f9bc3c15ebb2b11958_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses online learning of a threshold policy under fairness and service constraints in closed-loop decision systems with bandit feedback. It proposes Optimistic Feasible Search (OFS), which selects thresholds based on optimistic confidence bounds to maximize reward while minimizing constraint violations. Experiments show OFS outperforms baselines and achieves near-oracle performance across synthetic and real-world benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[Closed-loop decision-making with fairness constraints and feedback effects]\nC --\x3e C1[Optimistic Feasible Search (OFS) using confidence bounds on a grid of thresholds]\nD --\x3e D1[Higher reward, lower constraint violation, near-oracle performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] LangPrecip: Language-Aware Multimodal Precipitation Nowcasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [weather forecasting], [multimodal nowcasting, rectified flow, semantic motion constraint, latent space integration, large-scale dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22317",children:"https://arxiv.org/pdf/2512.22317"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: \u77ed\u671f\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u6761\u4ef6\uff0c\u672a\u6765\u8fd0\u52a8\u7ea6\u675f\u5f31]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u8bed\u8a00\u611f\u77e5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u6c14\u8c61\u6587\u672c\u4f5c\u4e3a\u8bed\u4e49\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728Rectified Flow\u8303\u5f0f\u4e0b\u8fdb\u884c\u6f5c\u7a7a\u95f4\u96c6\u6210]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728\u745e\u5178\u548cMRMS\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\uff0c\u572880\u5206\u949f\u9884\u89c1\u671f\uff0c\u5f3a\u964d\u6c34CSI\u63d0\u5347\u8d8560%\u548c19%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [knowledge graph embeddings], [probabilistic embeddings, uncertainty quantification, out-of-distribution detection, semantic uncertainty, structural uncertainty]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chorok Lee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22318",children:"https://arxiv.org/pdf/2512.22318"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and formalizes the fundamental limitation of relation-agnostic uncertainty in probabilistic KG embeddings, proving an impossibility result for detecting novel relational contexts using only entity-level statistics. 2. Proposes a novel decomposition of uncertainty into complementary semantic (entity variance) and structural (entity-relation co-occurrence) components, proving their non-redundancy and the superiority of their combination. 3. Introduces the CAGP method that combines semantic and structural uncertainty with learned weights, achieving significant improvements (60-80% relative gain) in temporal OOD detection and selective prediction performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5efb28f1949226a86993ed6d92d592762282d32746964ad0c85ab5a6de90ee36_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that probabilistic knowledge graph embeddings use relation-agnostic entity variances, which conflates two distinct types of out-of-distribution data: emerging entities and novel relational contexts. To address this, the authors propose decomposing uncertainty into semantic and structural components and introduce the CAGP method to combine them. This approach achieves a 60-80% relative improvement in temporal OOD detection performance over existing baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Decomposing Uncertainty in Probabilistic KG Embeddings] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Relation-agnostic entity variance fails to detect novel relational contexts]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Decompose uncertainty into semantic (entity variance) and structural (entity-relation co-occurrence) components]\n    D[\u5173\u952e\u7ed3\u679c/Results: CAGP method achieves 60-80% relative improvement in OOD detection]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [self-verifying agent, proactive evidence seeking, LLM-as-a-Judge, 3C Principles, agentic reinforcement learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Peking University, Tencent"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22322",children:"https://arxiv.org/pdf/2512.22322"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://huggingface.co/collections/yolay/smartsnap",children:"https://huggingface.co/collections/yolay/smartsnap"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n    D[\u5173\u952e\u7ed3\u679c/Results: Performance gains up to 26.08%; competitive with larger models]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [TimeXer, Global M2 Liquidity, exogenous variable, long-horizon forecasting]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sravan Karthick T"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," RV College of Engineering (RVCE), Bengaluru, India"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22326",children:"https://arxiv.org/pdf/2512.22326"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Bitcoin\u4ef7\u683c\u957f\u671f\u9884\u6d4b\u7684\u6781\u7aef\u6ce2\u52a8\u6027\u548c\u975e\u5e73\u7a33\u6027/Bitcoin's extreme volatility & non-stationarity for long-horizon forecasting)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u96c6\u6210\u5168\u7403M2\u6d41\u52a8\u6027\u4f5c\u4e3a\u5916\u751f\u53d8\u91cf\uff0c\u4f7f\u7528TimeXer\u67b6\u6784/Integrate Global M2 Liquidity as exogenous variable using TimeXer architecture)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u572870\u5929\u9884\u6d4b\u8303\u56f4\u5185\uff0cMSE\u964d\u4f4e\u8d85\u8fc789%/At 70-day horizon, MSE reduced by over 89%)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Emotion classification using EEG headset signals and Random Forest"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [affective computing], [EEG, Random Forest, emotion classification, brain-computer interface, real-time prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ricardo Vasquez, Diego Riofr\xedo-Luzcando, Joe Carrion-Jumbo, Cesar Guevara"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad Internacional SEK, Universidad Indoam\xe9rica, The Institute of Mathematical Sciences (ICMAT-CSIC)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22333",children:"https://arxiv.org/pdf/2512.22333"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a model for classifying human emotions (happiness, sadness, relaxation) using EEG signals from a consumer-grade headset (EMOTIV EPOC). 2. Applied the Random Forest algorithm to achieve high accuracy, particularly for happiness (97.21%). 3. Implemented a real-time emotion prediction system that captures EEG signals, processes them, and visually displays the predicted emotion."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e19385b22eca0965c66f7006e387468776c757a86a9b784693bc7b77b3c7533_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a system to classify human emotions (happiness, sadness, relaxation) from EEG signals using a Random Forest model. The model was trained on data from 50 participants and achieved high accuracy, especially for happiness. The work was extended to create a real-time prediction algorithm that outputs the result with representative images."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Emotion classification using EEG headset signals and Random Forest] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u4eceEEG\u4fe1\u53f7\u4e2d\u68c0\u6d4b\u548c\u5206\u7c7b\u60c5\u7eea\uff1f/How to detect and classify emotions from EEG signals?)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528EMOTIV EPOC\u91c7\u96c6EEG\u6570\u636e\uff0c\u5e76\u5e94\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u5206\u7c7b/Use EMOTIV EPOC to collect EEG data and apply Random Forest model for classification)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u5feb\u4e50\u5206\u7c7b\u51c6\u786e\u738797.21%\uff0c\u5b9e\u73b0\u5b9e\u65f6\u60c5\u7eea\u9884\u6d4b\u7b97\u6cd5/Happiness classification accuracy 97.21%, implemented a real-time emotion prediction algorithm)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [LoRA, catastrophic forgetting, KL divergence, instruction-tuning, parameter-efficient fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IBM Research, Mila, Universit\xe9 de Montr\xe9al"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22337",children:"https://arxiv.org/pdf/2512.22337"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model's general knowledge while maintaining plasticity for new tasks, applied to Qwen models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model's original capabilities while allowing adaptation to new instructions, with minimal computational overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["\u8bba\u6587\u6807\u9898: The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem: LoRA\u5fae\u8c03\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8/Catastrophic forgetting in LoRA fine-tuning"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u6b63\u5219\u5316\u8fd1\u4f3c\u56de\u653e/Regularized Approximate Replay (KL\u60e9\u7f5a+\u4ea4\u9519\u6570\u636e/KL penalty + interleaved data)"]\n    Results["\u5173\u952e\u7ed3\u679c/Results: \u4fdd\u7559\u901a\u7528\u77e5\u8bc6\uff0c\u7ef4\u6301\u53ef\u5851\u6027/Preserves general knowledge without hindering plasticity"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [pseudo-colouring, few-shot learning, prototypical networks, ResNet-18, explainability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alaa Alahmadi, Mohamed Hasan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Newcastle University, University of Leeds"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22349",children:"https://arxiv.org/pdf/2512.22349"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --\x3e B1\n    A --\x3e B2\n    A --\x3e B3\n    B1[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e C1[\u6570\u636e\u6548\u7387\u4f4e/Lack of data efficiency]\n    B1 --\x3e C2[\u53ef\u89e3\u91ca\u6027\u5dee/Limited explainability]\n    B1 --\x3e C3[\u4e34\u5e8a\u53ef\u9760\u6027\u53d7\u9650/Constrained clinical reliability]\n    B2[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e D1[\u611f\u77e5\u542f\u53d1\u7684\u4f2a\u7740\u8272\u6280\u672f/Perception-informed pseudo-colouring]\n    D1 --\x3e E1[\u7f16\u7801\u4e34\u5e8a\u7279\u5f81/Encode clinical features (e.g., QT-interval)]\n    D1 --\x3e E2[\u7ed3\u6784\u5316\u989c\u8272\u8868\u793a/Structured colour representations]\n    B2 --\x3e D2[\u539f\u578b\u7f51\u7edc\u4e0eResNet-18/Prototypical networks & ResNet-18]\n    B2 --\x3e D3[\u805a\u5408\u591a\u4e2a\u5fc3\u8df3\u5468\u671f/Aggregate multiple cardiac cycles]\n    B3[\u5173\u952e\u7ed3\u679c/Results] --\x3e F1[\u5b9e\u73b0\u5c11\u6837\u672c\u4e0e\u5355\u6837\u672c\u5b66\u4e60/Achieve few-shot & one-shot learning]\n    B3 --\x3e F2[\u63d0\u5347\u53ef\u89e3\u91ca\u6027/Improve explainability (guide attention)]\n    B3 --\x3e F3[\u6865\u63a5\u6570\u636e\u6548\u7387\u4e0e\u56e0\u679c\u63a8\u7406/Bridge data efficiency & causal reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [Cyber-Physical Systems Security], [False Data Injection (FDI), Physics-Informed Neural Network (PINN), Multi-Agent Reinforcement Learning (MARL)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mohammad Zakaria Haider, Amit Kumar Podder, Prabin Mali, Aranya Chakrabortty, Sumit Paudyal, Mohammad Ashiqur Rahman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Florida International University, North Carolina State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22381",children:"https://arxiv.org/pdf/2512.22381"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PHANTOM, a physics-aware adversarial attack framework that integrates a federated learning-enabled PINN as a digital twin for accurate modeling of EV charging systems. 2. Develops a multi-agent RL environment using DQN and SAC to generate stealthy FDI attack strategies that bypass conventional detection. 3. Constructs a T&D co-simulation platform to demonstrate the cascading, cross-boundary grid impacts (e.g., load imbalance, voltage instability) of the learned attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc4a49b77c0e517eadb20d321d77564888677d1f33b27adf452e13f7c0ffcb8c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PHANTOM, a physics-aware adversarial attack framework against federated learning-coordinated EV charging management. It uses a PINN-based digital twin and multi-agent RL to generate stealthy false data injection attacks, which are shown through co-simulation to cause significant grid instability, highlighting the need for physics-aware cybersecurity in vehicle-grid integration."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PHANTOM: Physics-Aware Adversarial Attacks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: EV Charging Grid Security)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: PINN Digital Twin + Multi-Agent RL)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Stealthy Attacks Cause Grid Instability)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Self-Evaluation Unlocks Any-Step Text-to-Image Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [text-to-image generation, flow matching, self-evaluation, any-step inference, from-scratch training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Hong Kong, Adobe Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22374",children:"https://arxiv.org/pdf/2512.22374"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables "any-step" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Traditional models need many steps or a teacher model] --\x3e Problem_Sub1[\u4f20\u7edf\u6a21\u578b\u9700\u8981\u591a\u6b65\u6216\u6559\u5e08\u6a21\u578b/Traditional models need many steps or a teacher]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Self-Evaluating Model (Self-E)] --\x3e Method_Sub1[\u7ed3\u5408\u6d41\u5339\u914d\u4e0e\u81ea\u8bc4\u4f30/Combines Flow Matching & Self-Evaluation]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Unified any-step model] --\x3e Results_Sub1[\u5c11\u6b65\u4e0e\u591a\u6b65\u5747\u8868\u73b0\u4f18\u5f02/Excels at both few-step and many-step]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [hyperparameter transfer, Complete(d)P parameterisation, per-module hyperparameter optimisation, scaling laws, evolutionary strategy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bruno Mlodozeniec, Pierre Ablin, Louis B\xe9thune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Apple, University of Cambridge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22382",children:"https://arxiv.org/pdf/2512.22382"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Completed Hyperparameter Transfer<br/>\u8d85\u53c2\u6570\u8fc1\u79fb\u7814\u7a76] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Hyperparameter tuning is critical for large models<br/>\u5927\u6a21\u578b\u8d85\u53c2\u6570\u8c03\u4f18\u81f3\u5173\u91cd\u8981]\n    B --\x3e B2[Transferring optimal HPs across scales is challenging<br/>\u8de8\u89c4\u6a21\u6700\u4f18\u8d85\u53c2\u6570\u8fc1\u79fb\u56f0\u96be]\n    C --\x3e C1[Propose Complete(d)P parameterisation<br/>\u63d0\u51faComplete(d)P\u53c2\u6570\u5316\u65b9\u6cd5]\n    C --\x3e C2[Enable per-module HP optimisation & transfer<br/>\u5b9e\u73b0\u6a21\u5757\u7ea7\u8d85\u53c2\u6570\u4f18\u5316\u4e0e\u8fc1\u79fb]\n    D --\x3e D1[Direct HP transfer to ~600x larger scale<br/>\u8d85\u53c2\u6570\u53ef\u76f4\u63a5\u8fc1\u79fb\u81f3\u7ea6600\u500d\u89c4\u6a21]\n    D --\x3e D2[Per-module HPs yield training speedup<br/>\u6a21\u5757\u7ea7\u8d85\u53c2\u6570\u5e26\u6765\u8bad\u7ec3\u52a0\u901f]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Graph Neural Networks, Multi-armed Bandits, Layer-wise Sampling, Node Importance, Efficient Training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22388",children:"https://arxiv.org/pdf/2512.22388"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[BLISS: Bandit Layer Importance Sampling Strategy] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bad\u7ec3\u5927\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8ba1\u7b97\u6210\u672c\u9ad8/High computational cost for training GNNs on large graphs]\n    C --\x3e C1[\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u8282\u70b9/Using multi-armed bandits to dynamically select informative nodes]\n    C --\x3e C2[\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u81ea\u9002\u5e94\u8282\u70b9\u91cd\u8981\u6027/Balancing exploration and exploitation, adapting to node importance]\n    D --\x3e D1[\u4fdd\u6301\u6216\u8d85\u8fc7\u5168\u6279\u6b21\u8bad\u7ec3\u7684\u7cbe\u5ea6/Maintains or exceeds full-batch training accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [latent diffusion model, differentiable physics, inverse problems, parameter estimation, flow in porous media]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zihan Lin, QiZhi He"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Minnesota"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22421",children:"https://arxiv.org/pdf/2512.22421"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LD-DIM, a novel framework that integrates a pretrained latent diffusion prior with a differentiable PDE solver for high-dimensional inverse problems. 2. Enables stable gradient-based optimization in a low-dimensional latent space, improving numerical conditioning and preserving sharp discontinuities. 3. Demonstrates superior numerical stability and reconstruction accuracy compared to PINNs and physics-embedded VAE baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb75073de82d3659f4e40522fca5a1fe8743332c5df8e21ca285525b2a200bca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces LD-DIM, a method that combines a latent diffusion model with a differentiable numerical solver to reconstruct heterogeneous parameter fields from sparse observations in PDE-constrained inverse problems. It shows improved stability and accuracy over existing baselines while maintaining sharp material interfaces."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: High-dimensional, ill-posed PDE inverse problems with sparse observations)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: LD-DIM integrates latent diffusion prior with differentiable PDE solver)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Improved stability & accuracy, preserves sharp interfaces, outperforms PINNs/VAE)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image segmentation], [hyperspherical learning, native sparse attention, anisotropic patch embed]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UC Santa Barbara, Allen Institute for Cell Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22423",children:"https://arxiv.org/pdf/2512.22423"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Causality-Inspired Safe Residual Correction for Multivariate Time Series"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [residual correction, causality-inspired encoder, non-degradation guarantee, safety mechanism, multivariate time series]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jianxiang Xie, Yuncheng Hua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of New South Wales, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22428",children:"https://arxiv.org/pdf/2512.22428"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CRC, a plug-and-play residual correction framework explicitly designed to guarantee non-degradation of model performance. 2. Introduces a causality-inspired encoder that decouples self- and cross-variable dynamics to expose direction-aware structure for safer correction. 3. Designs a strict four-fold safety mechanism to govern the correction process and prevent harmful updates, ensuring high non-degradation rates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7659f53cdce63801de4caead558593caeed0c4662c0c76c85dae564f12a6d78_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that existing post-hoc residual correction methods for multivariate time series forecasting are greedy and can degrade performance. To solve this, it proposes CRC, a causality-inspired safe residual correction framework with a four-fold safety mechanism. Experiments show CRC consistently improves accuracy while ensuring exceptionally high non-degradation rates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Causality-Inspired Safe Residual Correction for Multivariate Time Series] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u6b8b\u5dee\u6821\u6b63\u65b9\u6cd5\u8d2a\u5a6a\u4e14\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d/Existing greedy correction can degrade performance]\n    C --\x3e C1[\u56e0\u679c\u542f\u53d1\u7684\u7f16\u7801\u5668\u5206\u79bb\u53d8\u91cf\u52a8\u6001/Causality-inspired encoder decouples dynamics]\n    C --\x3e C2[\u56db\u91cd\u5b89\u5168\u673a\u5236\u9632\u6b62\u6709\u5bb3\u66f4\u65b0/Four-fold safety mechanism prevents harmful updates]\n    D --\x3e D1[CRC\u63d0\u5347\u51c6\u786e\u6027\u5e76\u4fdd\u8bc1\u9ad8\u975e\u9000\u5316\u7387/CRC improves accuracy & ensures high non-degradation rate]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [analog circuit design, multi-agent framework, stratified memory, simulation-grounded feedback, self-evolving]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University, Kansas State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22435",children:"https://arxiv.org/pdf/2512.22435"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[Analog circuit design is knowledge-intensive and relies on human intuition]\n    B --\x3e B2[Existing LLM-based methods lack feedback and generalization]\n    C --\x3e C1[Self-evolving multi-agent framework]\n    C --\x3e C2[Three-stage agent explorations with stratified memory]\n    C --\x3e C3[Iterative refinement via simulation-grounded feedback]\n    D --\x3e D1[10x overall pass rate improvement]\n    D --\x3e D2[48x Pass@1 improvement]\n    D --\x3e D3[4x reduction in parameter search space]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [rag (retrieval-augmented generation)], [hierarchical filtering, two-pass generation, citation verification, query formulation, model cascade]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Cattalyya Nuengsigkapian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Google"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22442",children:"https://arxiv.org/pdf/2512.22442"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HiFi-RAG] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5f00\u653e\u57dfRAG\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u4e0e\u610f\u56fe\u5bf9\u9f50/Open-domain RAG faces irrelevant info & intent misalignment]\n    C --\x3e C1[\u5206\u5c42\u8fc7\u6ee4\u4e0e\u4e24\u9636\u6bb5\u751f\u6210/Hierarchical Filtering & Two-Pass Generation]\n    C1 --\x3e C2[\u4f7f\u7528Gemini Flash\u8fdb\u884c\u8fc7\u6ee4/Use Gemini Flash for filtering]\n    C1 --\x3e C3[\u4f7f\u7528Gemini Pro\u8fdb\u884c\u751f\u6210/Use Gemini Pro for generation]\n    D --\x3e D1[\u5728MMU-RAGent\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms baseline on MMU-RAGent]\n    D --\x3e D2[\u5728\u81ea\u5b9a\u4e49\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u63d0\u5347/Substantial gains on custom test set]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [LoRA, Parameter-Efficient Fine-Tuning, Activation Function Annealing, Non-linear Adaptation, Model Merging]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiacheng Li, Jianchao Tan, Zhidong Yang, Feiye Huo, Yerui Sun, Yuchen Xie, Xunliang Cai"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Meituan, Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22455",children:"https://arxiv.org/pdf/2512.22455"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes AFA-LoRA, a novel training strategy that introduces non-linear expressivity into LoRA while preserving its seamless mergeability., 2. Introduces an annealed activation function that transitions from non-linear to linear during training, enabling strong initial learning and final linear integration., 3. Demonstrates the method's effectiveness across multiple tasks, including supervised fine-tuning, reinforcement learning, and speculative decoding, reducing the performance gap with full-parameter training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a5573f75296283a39c5bdbbb0c94652e0aeb935ae378c12528544ca5e188deb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the limited expressive power of linear Low-Rank Adaptation (LoRA) by proposing AFA-LoRA, a method that uses an annealed activation function to enable non-linear training while ensuring the final adapter remains mergeable. This approach narrows the performance gap between LoRA and full-parameter fine-tuning across various tasks, offering a more powerful and practical parameter-efficient adaptation paradigm."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>LoRA\u7ebf\u6027\u9002\u914d\u7684\u8868\u8fbe\u80fd\u529b\u6709\u9650<br>LoRA's linear adaptation limits expressive power]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u5f15\u5165\u9000\u706b\u6fc0\u6d3b\u51fd\u6570<br>Introduce annealed activation function]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u7f29\u5c0fLoRA\u4e0e\u5168\u53c2\u6570\u8bad\u7ec3\u7684\u5dee\u8ddd<br>Reduces gap between LoRA and full-parameter training]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AMBIT: Augmenting Mobility Baselines with Interpretable Trees"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [urban computing, spatial data science], [origin-destination flow prediction, spatial interaction models, gradient-boosted trees, SHAP analysis, gray-box model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qizhi Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," PingCAP, Data & AI-Innovation Lab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22466",children:"https://arxiv.org/pdf/2512.22466"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AMBIT: Augmenting Mobility Baselines with Interpretable Trees] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Conflicting needs for high accuracy and clear interpretability in OD flow prediction]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Gray-box framework augmenting physical baselines with interpretable tree models]\n    D[\u5173\u952e\u7ed3\u679c/Results: Physics-grounded residuals approach black-box accuracy; POI-anchored residuals are most robust]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] GLUE: Gradient-free Learning to Unify Experts"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [expert mixing, model initialization, gradient-free optimization, SPSA, transfer learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jong-Ik Park, Shreyas Chaudhari, Srinivasa Pranav, Carlee Joe-Wong, Jos\xe9 M. F. Moura"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22467",children:"https://arxiv.org/pdf/2512.22467"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GLUE, a gradient-free method to learn mixture coefficients for blending expert models into a single initialization prior for a target domain. 2. Introduces a two-point (SPSA) update rule that requires only two forward passes per step, avoiding expensive backpropagation through the full network. 3. Demonstrates that GLUE outperforms heuristic blending baselines and matches or outperforms gradient-based learning of mixture coefficients across multiple datasets and architectures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58e1db10a62e7b3683bf994e8914b7bcb88a1d8d308b99746a86c5ebb6d9b5c6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of initializing a model for a new target domain by blending multiple pretrained expert models. It proposes GLUE, a gradient-free method that learns the blending coefficients efficiently using only forward passes. Experiments show GLUE creates a better initialization prior, leading to higher fine-tuned accuracy than heuristic methods and comparable or better performance than gradient-based approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GLUE: Gradient-free Learning to Unify Experts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u4ee5\u521d\u59cb\u5316\u65b0\u76ee\u6807\u57df\u6a21\u578b/How to effectively fuse multiple expert models to initialize a new target domain model]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faGLUE\uff0c\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u81ea\u7531\u7684\u4e24\u70b9SPSA\u66f4\u65b0\u5b66\u4e60\u6df7\u5408\u7cfb\u6570\u7684\u65b9\u6cd5/Proposes GLUE, a gradient-free method using two-point SPSA updates to learn mixture coefficients]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: GLUE\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e0a\u8d85\u8d8a\u4e86\u542f\u53d1\u5f0f\u57fa\u7ebf\uff0c\u5e76\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u6df7\u5408\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18/GLUE outperforms heuristic baselines and matches or outperforms gradient-based mixing across experiments]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Bayesian Geometry of Transformer Attention"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [interpretability], [Bayesian inference, transformer attention, mechanistic interpretability, Bayesian wind tunnels, geometric analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Columbia University, Dream Sports, Google DeepMind"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22471",children:"https://arxiv.org/pdf/2512.22471"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces "Bayesian wind tunnels" as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled "Bayesian wind tunnel" tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["The Bayesian Geometry of Transformer Attention<br/>Transformer\u6ce8\u610f\u529b\u7684\u8d1d\u53f6\u65af\u51e0\u4f55"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>Do transformers perform genuine Bayesian inference or just pattern matching?<br/>Transformer\u662f\u8fdb\u884c\u771f\u6b63\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u8fd8\u662f\u4ec5\u4ec5\u6a21\u5f0f\u5339\u914d?"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>Construct \'Bayesian wind tunnels\' with known posteriors<br/>\u6784\u5efa\u5177\u6709\u5df2\u77e5\u540e\u9a8c\u7684\'\u8d1d\u53f6\u65af\u98ce\u6d1e\'"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br/>Transformers implement Bayesian inference via geometric mechanism; MLPs fail<br/>Transformer\u901a\u8fc7\u51e0\u4f55\u673a\u5236\u5b9e\u73b0\u8d1d\u53f6\u65af\u63a8\u7406\uff1bMLP\u5931\u8d25"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [imbalanced learning], [collaborative optimization, density-aware, region-guided boosting, sample weight update, dynamic sampling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chuantao Li, Zhi Li, Jiahao Xu, Jie Li, Sheng Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Guangdong Ocean University, University of Electronic Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22478",children:"https://arxiv.org/pdf/2512.22478"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ChuantaoLi/DARG",children:"https://github.com/ChuantaoLi/DARG"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a collaborative optimization Boosting model for multiclass imbalanced learning that integrates imbalanced learning and model training. 2. Designs a noise-resistant weight update mechanism and a dynamic sampling strategy using density and confidence factors. 3. Achieves tight integration of modules for weight updates, sample region partitioning, and region-guided sampling to enhance performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d829fa49f320d741a98494ca09a1f6019a6cccef04a93af08effdf4810adc20_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of classification bias in multiclass imbalanced data by proposing a collaborative optimization Boosting model. The method integrates density-aware and region-guided techniques to update sample weights and perform dynamic sampling, achieving improved performance over existing baselines on 20 public datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u5206\u7c7b\u504f\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u534f\u540c\u4f18\u5316\u4e0d\u5e73\u8861\u5b66\u4e60\u4e0e\u6a21\u578b\u8bad\u7ec3]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u534f\u540c\u4f18\u5316Boosting\u6a21\u578b\uff0c\u96c6\u6210\u5bc6\u5ea6\u56e0\u5b50\u548c\u7f6e\u4fe1\u5ea6\u56e0\u5b50\uff0c\u8bbe\u8ba1\u6297\u566a\u58f0\u6743\u91cd\u66f4\u65b0\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u572820\u4e2a\u516c\u5171\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e8\u4e2a\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [biomedical signal processing], [self-supervised learning, surface electromyography, rotary position encoding, spectral pre-training, movement decoding]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22481",children:"https://arxiv.org/pdf/2512.22481"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Decoding fine-grained movement from noisy, non-stationary sEMG signals for prosthetic control]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Domain-specific SSL with spectral pre-training and Cylindrical Rotary Position Embedding (CyRoPE)]\n    D[\u5173\u952e\u7ed3\u679c/Results: New SOTA performance, outperforms supervised & generic SSL baselines, validated on amputation data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [network intrusion detection], [concept drift, latent space alignment, graph neural network (GNN), IoT botnet detection, variational autoencoder]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hassan Wasswa, Timothy Lynar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of New South Wales"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22488",children:"https://arxiv.org/pdf/2512.22488"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework's effectiveness in dynamic IoT environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection / \u9762\u5411\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u5b89\u5168\uff1a\u6982\u5ff5\u6f02\u79fb\u9c81\u68d2\u7684\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u68c0\u6d4b"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results"]\n    Problem --\x3e P1["AI\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6 / AI models rely on stationary datasets"]\n    Problem --\x3e P2["\u771f\u5b9e\u6d41\u91cf\u5b58\u5728\u6982\u5ff5\u6f02\u79fb / Real-world traffic suffers concept drift"]\n    Problem --\x3e P3["\u73b0\u6709\u65b9\u6848\u91cd\u8bad\u7ec3\u5f00\u9500\u5927 / Existing solutions have high retraining cost"]\n    Method --\x3e M1["\u5b66\u4e60\u5386\u53f2\u6d41\u91cf\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a / Learn latent space of historical traffic"]\n    Method --\x3e M2["\u5bf9\u9f50\u6a21\u578b\u6620\u5c04\u65b0\u6d41\u91cf / Alignment model maps new traffic"]\n    Method --\x3e M3["\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b / Graph Neural Network for classification"]\n    Results --\x3e R1["\u4fdd\u6301\u9c81\u68d2\u68c0\u6d4b\u6027\u80fd / Maintains robust detection performance"]\n    Results --\x3e R2["\u9002\u7528\u4e8e\u52a8\u6001\u5927\u89c4\u6a21\u73af\u5883 / Suitable for dynamic, large-scale environments"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Quest for Winning Tickets in Low-Rank Adapters"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [parameter-efficient fine-tuning], [Lottery Ticket Hypothesis, Low-Rank Adaptation, Parameter-Efficient Fine-Tuning, Sparse Subnetworks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Australian Institute for Machine Learning (Adelaide University), Monash University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22495",children:"https://arxiv.org/pdf/2512.22495"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Empirically validates that the Lottery Ticket Hypothesis (LTH) holds within Low-Rank Adaptation (LoRA) methods, revealing the existence of sparse, high-performing subnetworks ("winning tickets") in adapters. 2. Discovers that the effectiveness of these sparse subnetworks depends more on the sparsity distribution across layers than on the specific weights selected. 3. Proposes Partial-LoRA, a novel method to systematically identify and train these sparse low-rank adapters, achieving significant parameter reduction (up to 87%) while maintaining or improving performance across vision and language tasks.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19921047ca06f45b5a51cefcf5b1cd9502b9ad5650b66c746d8b15f622036ef0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether the Lottery Ticket Hypothesis extends to parameter-efficient fine-tuning, specifically Low-Rank Adaptation (LoRA). The authors propose Partial-LoRA, a method to identify and train sparse, task-aligned subnetworks within LoRA adapters. Experiments show Partial-LoRA can reduce trainable parameters by up to 87% while matching or surpassing the performance of dense adapters."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Quest for Winning Tickets in Low-Rank Adapters] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Does LTH hold in PEFT/LoRA?<br>LTH\u662f\u5426\u9002\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Partial-LoRA<br>\u63d0\u51faPartial-LoRA\u65b9\u6cd5]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Sparse adapters work,<br>up to 87% fewer params<br>\u7a00\u758f\u9002\u914d\u5668\u6709\u6548\uff0c\u53c2\u6570\u51cf\u5c11\u9ad8\u8fbe87%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [role-based fault tolerance, RL post-training, UCX communication, warm standby, Effective Training Time Ratio (ETTR)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22492",children:"https://arxiv.org/pdf/2512.22492"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Role-Based Fault Tolerance System for LLM RL Post-Training] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[RL\u540e\u8bad\u7ec3\u6df7\u5408\u8bad\u7ec3\u4e0e\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6613\u53d7\u53cc\u65b9\u6545\u969c\u5f71\u54cd/RL post-training mixes training & inference, vulnerable to faults from both]\n    B --\x3e B2[\u73b0\u6709\u5bb9\u9519\u6846\u67b6\u672a\u9488\u5bf9RL\u7684\u5f02\u6b65\u6267\u884c\u4f18\u5316/Existing FT frameworks not optimized for RL's async execution]\n    C --\x3e C1[\u57fa\u4e8e\u89d2\u8272\u7684\u6545\u969c\u9694\u79bb\u4e0e\u6062\u590d/Role-based fault isolation & recovery]\n    C --\x3e C2[\u68c0\u6d4b-\u91cd\u542f-\u91cd\u8fde\u8303\u5f0f/Detect-Restart-Reconnect paradigm]\n    C2 --\x3e C21[\u89d2\u8272\u611f\u77e5\u76d1\u63a7/Role-aware monitoring]\n    C2 --\x3e C22[\u975e\u4e2d\u65ad\u5f0f\u91cd\u542f/Non-disruptive restart with warm standbys]\n    C2 --\x3e C23[\u52a8\u6001UCX\u70b9\u5bf9\u70b9\u901a\u4fe1\u91cd\u8fde/Dynamic UCX P2P reconnection]\n    D --\x3e D1[ETTR\u8d85\u8fc780%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u768460%/ETTR >80%, better than baseline 60%]\n    D --\x3e D2[\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u52a0\u5feb8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Decomposing Task Vectors for Refined Model Editing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [model editing], [task vector, parameter decomposition, invariant subspace, concept interference, LoRA]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Australian Institute for Machine Learning (University of Adelaide), Monash University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22511",children:"https://arxiv.org/pdf/2512.22511"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A principled decomposition method to separate a task vector into shared and unique components., 2. Application of the decomposition to improve multi-task merging in image classification, enable clean style mixing in diffusion models, and reduce toxicity in language models., 3. A new framework for understanding and controlling task vector arithmetic to address interference during concept composition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e938f563a374559600295d0e58643d1a4f7e55e5b56e3e6aca0b3daec488d0b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of unpredictable outcomes when performing arithmetic operations on task vectors due to overlapping concepts. It proposes a method to decompose each task vector into shared and unique components using invariant subspaces. This enables more precise model editing, demonstrated by improved multi-task merging, clean style mixing, and significant toxicity reduction while preserving general knowledge."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Decomposing Task Vectors for Refined Model Editing"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Task vector arithmetic causes unpredictable interference due to overlapping concepts."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Decompose task vectors into shared and unique components via invariant subspaces."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Improved multi-task merging, clean style mixing in diffusion models, and toxicity reduction in language models."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [hallucination detection], [correctness prediction, metadata signals, prompting strategies, log probability, response consistency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22508",children:"https://arxiv.org/pdf/2512.22508"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Predicting LLM Correctness in Prosthodontics<br/>\u9884\u6d4bLLM\u5728\u53e3\u8154\u4fee\u590d\u5b66\u4e2d\u7684\u6b63\u786e\u6027] --\x3e B(Problem/\u6838\u5fc3\u95ee\u9898: LLM hallucinations in high-stakes healthcare domains<br/>\u9ad8\u98ce\u9669\u533b\u7597\u9886\u57df\u4e2d\u7684LLM\u5e7b\u89c9\u95ee\u9898)\n    A --\x3e C(Method/\u4e3b\u8981\u65b9\u6cd5: Use metadata & hallucination signals to build correctness predictors<br/>\u4f7f\u7528\u5143\u6570\u636e\u548c\u5e7b\u89c9\u4fe1\u53f7\u6784\u5efa\u6b63\u786e\u6027\u9884\u6d4b\u5668)\n    A --\x3e D(Results/\u5173\u952e\u7ed3\u679c: Improved accuracy & precision; metadata not reliable for hallucination prediction; prompting alters behavior<br/>\u63d0\u5347\u51c6\u786e\u7387\u548c\u7cbe\u786e\u5ea6\uff1b\u5143\u6570\u636e\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u5e7b\u89c9\uff1b\u63d0\u793a\u7b56\u7565\u6539\u53d8\u6a21\u578b\u884c\u4e3a)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [adversarial robustness], [Spiking Neural Networks, surrogate gradient, adversarial attack, gradient vanishing, adaptive optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22522",children:"https://arxiv.org/pdf/2512.22522"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u68af\u5ea6\u6d88\u5931/Gradient Vanishing")\n    Problem --\x3e P2("\u5bf9\u6297\u8bc4\u4f30\u4e0d\u53ef\u9760/Unreliable Adversarial Evaluation")\n    Method --\x3e M1("\u7406\u8bba\u5206\u6790\u68af\u5ea6\u6d88\u5931/Theoretical Analysis of Gradient Vanishing")\n    Method --\x3e M2("\u81ea\u9002\u5e94\u9510\u5ea6\u66ff\u4ee3\u68af\u5ea6/Adaptive Sharpness Surrogate Gradient (ASSG)")\n    Method --\x3e M3("\u7a33\u5b9a\u81ea\u9002\u5e94\u6295\u5f71\u68af\u5ea6\u4e0b\u964d/Stable Adaptive PGD (SA-PGD)")\n    Results --\x3e R1("\u653b\u51fb\u6210\u529f\u7387\u5927\u5e45\u63d0\u5347/Substantially Increased Attack Success Rate")\n    Results --\x3e R2("\u63ed\u793a\u9c81\u68d2\u6027\u88ab\u9ad8\u4f30/Revealed Overestimated Robustness")\n    Results --\x3e R3("\u63d0\u4f9b\u66f4\u53ef\u9760\u8bc4\u4f30/Provided More Reliable Evaluation")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time-series forecasting], [encoder-decoder, latent bottleneck representations, learnable queries, generalized forecasting]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jaebin Lee, Hankook Lee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sungkyunkwan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22550",children:"https://arxiv.org/pdf/2512.22550"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/efficient-learning-lab/TimePerceiver",children:"https://github.com/efficient-learning-lab/TimePerceiver"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TIMEPERCEIVER] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u7f16\u7801\u5668\uff0c\u9884\u6d4b\u4e0e\u8bad\u7ec3\u5206\u79bb/Prior work focuses on encoder, treats prediction & training separately]\n    C --\x3e C1[\u5e7f\u4e49\u9884\u6d4b\u4efb\u52a1: \u5916\u63a8\u3001\u63d2\u503c\u3001\u586b\u8865/Generalized forecasting: extrapolation, interpolation, imputation]\n    C --\x3e C2[\u7f16\u7801: \u6f5c\u5728\u74f6\u9888\u8868\u793a/Encoding: Latent bottleneck representations]\n    C --\x3e C3[\u89e3\u7801: \u53ef\u5b66\u4e60\u67e5\u8be2/Decoding: Learnable queries]\n    D --\x3e D1[\u6027\u80fd\u663e\u8457\u8d85\u8d8aSOTA/Outperforms SOTAs significantly]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [game theory], [pure-strategy Nash equilibrium, continuous games, policy competition, gradient-based algorithm, grid-based search]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chuang-Chieh Lin, Chi-Jen Lu, Po-An Chen, Chih-Chieh Hung"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Taiwan Ocean University, Academia Sinica, National Yang Ming Chiao Tung University, National Chung Hsing University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22552",children:"https://arxiv.org/pdf/2512.22552"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulated a two-party policy competition game and validated the isotonicity hypothesis of winning probability through simulations. 2. Proved the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional policy spaces. 3. Proposed and experimentally validated a decentralized gradient-based algorithm and a polynomial-time grid-based search algorithm for finding approximate PSNEs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ef0be95e6f7da168b174f0a6d600f4ee8f0f3d2bcaba49c996d43be6d1be4b7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper models two-party policy competition as a continuous non-cooperative game where parties choose policy vectors and a party's payoff is the expected utility for its supporters. The authors prove the existence of a pure-strategy Nash equilibrium and propose two algorithms\u2014a gradient-based method and a grid-based search\u2014that efficiently find approximate equilibria."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5efa\u6a21\u4e24\u515a\u653f\u7b56\u7ade\u4e89/Model two-party policy competition]\n    C --\x3e C1[\u5f62\u5f0f\u5316\u4e3a\u8fde\u7eed\u535a\u5f08/Formulate as continuous game]\n    C --\x3e C2[\u9a8c\u8bc1\u7b49\u6e17\u6027\u5047\u8bbe/Validate isotonicity hypothesis]\n    C --\x3e C3[\u8bbe\u8ba1\u68af\u5ea6\u4e0e\u7f51\u683c\u7b97\u6cd5/Design gradient & grid algorithms]\n    D --\x3e D1[\u8bc1\u660e\u7eaf\u7b56\u7565\u7eb3\u4ec0\u5747\u8861\u5b58\u5728\u6027/Prove PSNE existence]\n    D --\x3e D2[\u7b97\u6cd5\u5feb\u901f\u6536\u655b/Algorithm converges rapidly]\n    D --\x3e D3[\u627e\u5230\u8fd1\u4f3c\u5747\u8861/Find approximate equilibrium]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [disaggregated infrastructure, hardware-affinity mapping, fine-grained asynchrony]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HKUST, Alibaba Group"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22560",children:"https://arxiv.org/pdf/2512.22560"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/alibaba/ROLL",children:"https://github.com/alibaba/ROLL"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On Admissible Rank-based Input Normalization Operators"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [machine learning theory], [rank-based normalization, differentiable sorting, monotone invariance, batch independence, Lipschitz continuity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taeyun Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Seoul National University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22587",children:"https://arxiv.org/pdf/2512.22587"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identified and formalized three axioms (invariance and stability properties) that a valid rank-based input normalization operator must satisfy. 2. Proved a structural characterization theorem showing any admissible operator must factor into a feature-wise rank representation and a monotone-Lipschitz scalarization map. 3. Constructed a minimal operator meeting the proposed axioms, empirically demonstrating the non-triviality of the constraints and delineating the design space from existing differentiable sorting methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49ce297e3baab406a1befffbcf116551c1492e7a6451fb1af02514d5f1ddb86b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that widely used differentiable sorting/ranking operators are structurally unstable under monotone transformations and batch variations. To address this, it proposes formal axioms for stable rank-based normalization and proves that any admissible operator must have a specific factored structure. The authors construct a minimal operator satisfying these axioms, formally separating valid normalization from existing differentiable sorting approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["On Admissible Rank-based Input Normalization Operators<br>\u53ef\u5bb9\u8bb8\u7684\u57fa\u4e8e\u6392\u5e8f\u7684\u8f93\u5165\u5f52\u4e00\u5316\u7b97\u5b50"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Existing differentiable sorting/ranking operators are unstable under monotone transforms & batch shifts."]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose three axioms for invariance/stability; prove structural factorization theorem."]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Construct minimal admissible operator; delineate design space from prior methods."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [data mining], [K-means clustering, Association Rule Mining, crash pattern analysis, automated vehicles, NHTSA data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jewel Rana Palit, Vijayalakshmi K Kumarasamy, Osama A. Osman"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Tennessee at Chattanooga, Collier County Government"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22589",children:"https://arxiv.org/pdf/2512.22589"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a two-stage data mining framework combining K-means clustering and Association Rule Mining to analyze AV crash data. 2. Applied the framework to a large-scale dataset of over 2,500 AV crash records from NHTSA, covering SAE Levels 2 and 4. 3. Uncovered interpretable multivariate relationships between crash patterns and environmental/operational factors, providing actionable insights for AV safety."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c3b11717362343c168a38a05a8cdd65b40a65cd3df1b048cfb816c2493f25b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study analyzes crash patterns in SAE Level 2 and 4 Automated Vehicles using a large-scale NHTSA dataset. It proposes a two-stage data mining framework: first using K-means clustering to segment crashes into behavioral clusters, then applying Association Rule Mining to find relationships between crash factors within each cluster. The results provide actionable guidance for improving AV safety and deployment strategies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining<br/>\u8bba\u6587\u6807\u9898"]\n    Root --\x3e Problem["AV safety concerns in mixed traffic; Limited analysis across SAE levels<br/>\u6838\u5fc3\u95ee\u9898/Problem"]\n    Root --\x3e Method["Two-stage framework: K-means clustering + Association Rule Mining (ARM)<br/>\u4e3b\u8981\u65b9\u6cd5/Method"]\n    Root --\x3e Results["Uncovered crash dynamics & multivariate relationships; Actionable guidance for stakeholders<br/>\u5173\u952e\u7ed3\u679c/Results"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [Gated Recurrent Units, parallel neural networks, cryptocurrency price prediction, mean absolute percentage error, recurrent neural networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Milad Asadpour, Alireza Rezaee, Farshid Hajati"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Tehran, University of New England"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22599",children:"https://arxiv.org/pdf/2512.22599"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Parallel Gated Recurrent Units (PGRU) model for cryptocurrency price forecasting. 2. Employs parallel and independent recurrent neural networks with distinct price-related feature inputs. 3. Demonstrates higher accuracy and efficiency with lower computational cost and less input data compared to existing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1b0d946f543a3b63440ffb89cc57536eb655abef18c7f847881484fd9e06122_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a new deep learning model called Parallel Gated Recurrent Units (PGRU) for predicting cryptocurrency prices. The model uses parallel recurrent neural networks that process different price features independently, and their outputs are combined by another neural network for the final forecast. Experimental results show the model achieves low prediction errors (e.g., 2.641% MAPE) with higher efficiency and lower computational cost than previous methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Cryptocurrency price forecasting challenge]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Parallel Gated Recurrent Units (PGRU) model]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves low MAPE (e.g., 2.641%), higher accuracy & efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [flow matching, conformer generation, energy guidance, ground-state identification, molecular geometry]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Guikun Xu, Xiaohan Yi, Peilin Zhao, Yatao Bian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, National University of Singapore, Tsinghua University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22597",children:"https://arxiv.org/pdf/2512.22597"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Rich-XGK/EnFlow.git",children:"https://github.com/Rich-XGK/EnFlow.git"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes EnFlow, a unified framework that couples flow matching with an explicitly learned energy model for conformer generation. 2. Introduces an energy-guided sampling scheme along a non-Gaussian flow matching path to steer trajectories toward lower-energy regions, improving fidelity in few-step regimes. 3. Enables accurate ground-state identification by using the learned energy function to rank generated ensembles, reducing prediction errors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8be1fad6f55102be97d0cd80a5b8da136a8f70c287b7f11b3a11156b5bc80a04_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents EnFlow, a method that integrates flow matching with an energy model to guide the generation of molecular conformers. By using energy-gradient guidance during sampling, it improves conformational accuracy with few steps and enables better identification of the ground-state structure. Experiments show it outperforms state-of-the-art methods on standard benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Fragmented learning-based approaches for conformer generation lack reliable energy calibration or ensemble diversity.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: EnFlow couples flow matching with a learned energy model and uses energy-guided sampling along a non-Gaussian path.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improves generation metrics with 1-2 steps and reduces ground-state prediction errors on GEOM datasets.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [LSTM, Multilayer Perceptron (MLP), Gray Wolf Optimizer (GWO), gold price prediction, trading strategy]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hesam Taghipour, Alireza Rezaee, Farshid Hajati"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Tehran, University of New England"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22606",children:"https://arxiv.org/pdf/2512.22606"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a hybrid LSTM-MLP model for multi-timeframe (daily and monthly) gold price forecasting. 2. Utilized the Gray Wolf Optimizer (GWO) to optimize the number of neurons in the neural networks for improved accuracy. 3. Developed and backtested a trading strategy based on the model's predictions, reporting a high simulated return of 171% over three months."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46725f71da8c268509e63f86ad823980c405c360b229a1a903d358d4d2dd61d5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an AI-based model for predicting gold prices. It uses two LSTM networks for daily and monthly forecasts, integrates their outputs with an MLP, and optimizes the network structure using the Gray Wolf Optimizer. The model achieved low prediction errors and a high simulated trading return, demonstrating its potential for financial forecasting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Gold Price Prediction Using LSTM and MLP with GWO] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u91d1\u878d\u5e02\u573a\u9884\u6d4b\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027/Financial market forecasting is complex and challenging]\n    C --\x3e C1[\u4f7f\u7528\u4e24\u4e2aLSTM\u7f51\u7edc\u8fdb\u884c\u6bcf\u65e5\u548c\u6bcf\u6708\u9884\u6d4b/Use two LSTM networks for daily and monthly forecasting]\n    C --\x3e C2[\u901a\u8fc7MLP\u6574\u5408LSTM\u8f93\u51fa/Integrate LSTM outputs via MLP]\n    C --\x3e C3[\u4f7f\u7528\u7070\u72fc\u4f18\u5316\u5668(GWO)\u4f18\u5316\u7f51\u7edc\u795e\u7ecf\u5143/Use Gray Wolf Optimizer (GWO) to optimize network neurons]\n    D --\x3e D1[\u65e5\u6536\u76d8\u4ef7\u9884\u6d4bMAE\u4e3a$0.21/Daily closing price prediction MAE is $0.21]\n    D --\x3e D2[\u6708\u4ef7\u683c\u9884\u6d4bMAE\u4e3a$22.23/Monthly price prediction MAE is $22.23]\n    D --\x3e D3[\u6a21\u62df\u4ea4\u6613\u7b56\u7565\u4e09\u4e2a\u6708\u56de\u62a5\u7387171%/Simulated trading strategy achieved 171% return in three months]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [communication compression, error feedback, biased compression, control variates, distributed gradient descent]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Irvine; University of British Columbia; Vector Institute"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22623",children:"https://arxiv.org/pdf/2512.22623"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Compressed Aggregate Feedback (CAFe), a novel framework for biased compression in distributed learning that uses the previous round's aggregated update as a shared control variate, eliminating the need for client-side state. 2. Proposed Server-Guided Compressed Aggregate Feedback (CAFe-S), an extension that leverages a small server-side dataset to generate a more accurate predictor update, improving convergence when server data is representative. 3. Provided theoretical convergence guarantees for both CAFe and CAFe-S in non-convex settings, proving CAFe's superiority over standard distributed compressed gradient descent and showing CAFe-S's improved rate with more representative server data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/988420401d89faee52e66a7e209751875e33b7f833b8dc7f136f1f1d2650454d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the communication bottleneck and privacy issues in Federated Learning by proposing two novel compression frameworks, CAFe and CAFe-S, which enable biased compression without requiring client-side state. CAFe uses the previous aggregated update as a shared control variate, while CAFe-S leverages a small server dataset for a better predictor. Theoretical and experimental results demonstrate their superiority over existing compression schemes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Communication Compression for Distributed Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u901a\u4fe1\u74f6\u9888\u4e0e\u9690\u79c1/Communication Bottleneck & Privacy]\nB1 --\x3e B2[\u6709\u504f\u538b\u7f29\u9700\u8981\u5ba2\u6237\u7aef\u72b6\u6001/Biased Compression Needs Client State]\nC --\x3e C1[CAFe: \u805a\u5408\u53cd\u9988/CAFe: Aggregate Feedback]\nC --\x3e C2[CAFe-S: \u670d\u52a1\u5668\u5f15\u5bfc\u53cd\u9988/CAFe-S: Server-Guided Feedback]\nC1 --\x3e C3[\u4f7f\u7528\u4e0a\u4e00\u8f6e\u805a\u5408\u66f4\u65b0\u4f5c\u4e3a\u5171\u4eab\u63a7\u5236\u53d8\u91cf/Use Previous Aggregate Update as Shared Control Variate]\nC2 --\x3e C4[\u4f7f\u7528\u670d\u52a1\u5668\u6570\u636e\u751f\u6210\u9884\u6d4b\u5668/Use Server Data to Generate Predictor]\nD --\x3e D1[\u7406\u8bba\u6536\u655b\u4fdd\u8bc1/Theoretical Convergence Guarantees]\nD --\x3e D2[\u4f18\u4e8e\u73b0\u6709\u65b9\u6848/Superior to Existing Schemes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [binary tree compression, Transformer, scalable inference, power allocation, cell-free]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Irched Chafaa, Giacomo Bacci, Luca Sanguinetti"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Pisa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22639",children:"https://arxiv.org/pdf/2512.22639"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel hybrid Tree-Transformer architecture for scalable per-user power allocation in wireless networks. 2. Introduces a method that compresses user features via a binary tree to a global root representation, applies a Transformer encoder only to the root, and decodes powers with a shared decoder, achieving logarithmic depth and linear total complexity. 3. Demonstrates that the model achieves near-optimal performance for the max-min fairness problem in cell-free massive MIMO systems while significantly reducing inference time compared to full-attention baselines, without needing retraining for different network sizes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967237dcd593c2fe2d6f3b16cb72307d4ae0dbaa94a8031ad460a273da33c12a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high computational cost of Transformer models for power allocation in large-scale wireless networks. It proposes a hybrid Tree-Transformer architecture that compresses user features into a root node for processing, achieving linear complexity and scalable inference. The model demonstrates near-optimal performance with significantly reduced inference time for cell-free massive MIMO systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Transformer\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6269\u5c55/Poor scalability of Transformer for power allocation"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u6811-Transformer\u6df7\u5408\u67b6\u6784/Tree-Transformer hybrid architecture<br>\uff08\u4e8c\u53c9\u6811\u538b\u7f29\uff0c\u6839\u8282\u70b9\u5904\u7406/Binary tree compression, root processing\uff09"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u65f6\u95f4/Near-optimal performance, significantly reduced inference time"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Scaling Unverifiable Rewards: A Case Study on Visual Insights"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [Test-Time Scaling, multi-agent pipeline, process-based refinement, LLM-as-Judge, unverifiable rewards]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Minnesota"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22650",children:"https://arxiv.org/pdf/2512.22650"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://minnesotanlp.github.io/insight-scaling-webpage",children:"https://minnesotanlp.github.io/insight-scaling-webpage"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u591a\u9636\u6bb5\u4efb\u52a1\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u5956\u52b1/Multi-stage tasks lack verifiable rewards]\n    B --\x3e B2[\u57fa\u4e8e\u8bc4\u5224\u7684\u4f18\u5316\u6613\u7d2f\u79ef\u8bef\u5dee/Judge-based refinement prone to error accumulation]\n    C --\x3e C1[\u9009\u62e9\u6027\u6d4b\u8bd5\u65f6\u6269\u5c55/Selective Test-Time Scaling]\n    C --\x3e C2[\u8de8\u9636\u6bb5\u5206\u914d\u8ba1\u7b97\u8d44\u6e90/Distribute compute across stages]\n    C --\x3e C3[\u65e9\u671f\u526a\u679d\u4f4e\u8d28\u91cf\u5206\u652f/Prune low-quality branches early]\n    D --\x3e D1[\u63d0\u5347\u5e73\u5747\u5206\u6570/Increased mean scores (61.64 to 65.86)]\n    D --\x3e D2[\u964d\u4f4e\u65b9\u5dee/Reduced variance]\n    D --\x3e D3[\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u9f50\u7684\u8bc4\u5224\u6a21\u578b/Judge model aligned with human experts]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [medical signal processing], [electroencephalography, multi-disorder classification, sensitivity-oriented modeling, clinical calibration, feature importance analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology, Kharagpur"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22656",children:"https://arxiv.org/pdf/2512.22656"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a clinically calibrated, sensitivity-prioritized machine learning framework for classifying eleven diverse neurological disorders from EEG data, addressing severe class imbalance. 2. Establishes realistic performance baselines for multi-disorder EEG classification, demonstrating recall exceeding 80% for most disorders with significant gains for low-prevalence conditions after threshold calibration. 3. Provides physiologically plausible feature importance analysis that aligns with established clinical EEG markers, validating the model's clinical relevance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/636894ed25045336665fcbac593a58130411dff99c5d2a3e5a0cd50067ee44ec_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This study addresses the challenge of automated, multi-disorder screening from clinical EEG data by developing disorder-aware machine learning models with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity. The method uses a multi-domain feature set and is evaluated on a large, heterogeneous dataset, achieving high recall for most neurological disorder categories. The results establish performance baselines and demonstrate that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in clinical practice."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Manual EEG interpretation is slow and variable; existing automation lacks multi-disorder support.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Use multi-domain EEG features and train sensitivity-calibrated models under class imbalance.]\n    D[\u5173\u952e\u7ed3\u679c/Results: High recall (>80%) for most disorders; feature importance aligns with clinical knowledge.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [multi-task learning, inter-task consistency, digital pathology, foundation models, combinatorial partial supervision]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela S\xfcsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University Hospital Bonn, Technical University of Munich"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22666",children:"https://arxiv.org/pdf/2512.22666"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["INTERACT-CMIL: CMIL\u5206\u7ea7<br>INTERACT-CMIL: CMIL Grading"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>CMIL\u5206\u7ea7\u56f0\u96be\uff0c\u4e3b\u89c2\u6027\u5f3a<br>CMIL grading is difficult and subjective"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>\u591a\u4efb\u52a1\u5171\u4eab\u5b66\u4e60\u4e0e\u4efb\u52a1\u95f4\u4e00\u81f4\u6027<br>Multi-task Shared Learning & Inter-Task Consistency"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u9884\u6d4b<br>Significant performance gains, interpretable predictions"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [3D Inception, two-stream networks, CNN-RNN, EchoNet-Dynamic, video analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shravan Saranyan, Pramit Saha"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Branham High School, University of Oxford"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22657",children:"https://arxiv.org/pdf/2512.22657"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u624b\u52a8\u8bc4\u4f30LVEF\u8017\u65f6\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02/Manual LVEF assessment is time-consuming and has inter-observer variability]\n    C --\x3e C1[\u7cfb\u7edf\u8bc4\u4f303D Inception\u3001\u53cc\u6d41\u548cCNN-RNN\u67b6\u6784/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]\n    C --\x3e C2[\u5728EchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30/Train and evaluate on the EchoNet-Dynamic dataset]\n    D --\x3e D1[\u6539\u8fdb\u76843D Inception\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0cRMSE\u4e3a6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]\n    D --\x3e D2[\u66f4\u5c0f\u3001\u66f4\u7b80\u5355\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u66f4\u597d/Smaller, simpler models generalize better]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Quantum Generative Models, Computational Fluid Dynamics, Lattice Boltzmann Method, Vector Quantized Variational Autoencoder, Quantum Circuit Born Machine]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Achraf Hsain, Fouad Mohammed Abbou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Al Akhawayn University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22672",children:"https://arxiv.org/pdf/2512.22672"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A complete open-source pipeline bridging CFD simulation and quantum machine learning. 2. The first empirical study of quantum generative modeling on compressed latent representations of physics simulations. 3. A comparative analysis of quantum (QCBM, QGAN) and classical (LSTM) generative models for a physics-derived latent distribution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6094a8215d7304400e5580e08cc0e81135106aaf9b51ef11b7f4c5d62734237e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper explores the application of quantum generative models to Computational Fluid Dynamics (CFD) data. The authors compress fluid simulation data into a discrete latent space using a VQ-VAE and then compare quantum (QCBM, QGAN) and classical (LSTM) models for generating samples from this distribution. Under their experimental conditions, the quantum models, particularly the QCBM, outperformed the classical baseline in generating samples closer to the true distribution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Quantum Generative Models for CFD: Latent Space Learning in LBM Simulations"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Modeling compressed CFD latent distributions"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: VQ-VAE compression + QCBM/QGAN vs LSTM"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Quantum models (QCBM best) outperformed classical LSTM"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [width pruning, expansion ratio, Maximum Absolute Weight (MAW), GLU-MLP, instruction-following]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Pere Martra"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad Internacional Men\xe9ndez Pelayo (UIMP)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22671",children:"https://arxiv.org/pdf/2512.22671"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: How does structured width pruning affect different LLM capabilities?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]\n    D[\u5173\u952e\u7ed3\u679c/Results: Knowledge \u2193, Instruction-following \u2191, Truthfulness \u2191, Efficiency trade-offs]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [decentralized learning, multi-task representation learning, low-rank structure, communication complexity, projected gradient descent]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Donghwa Kang, Shana Moothedath"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Iowa State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22675",children:"https://arxiv.org/pdf/2512.22675"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new alternating projected gradient descent and minimization algorithm for decentralized multi-task representation learning with provable accuracy guarantees. 2. Provides comprehensive theoretical analysis of time, communication, and sample complexities, showing communication complexity is independent of target accuracy. 3. Identifies regimes (e.g., large number of nodes, low bandwidth) where the decentralized algorithm can outperform centralized federated learning approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a6cd15411863b0b8dc43a27acc5efd5e57130db256aae85b8195706d6a64ca_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies decentralized multi-task representation learning where tasks share a low-rank structure. The authors propose a new alternating projected gradient descent algorithm with provable guarantees, showing its communication cost is independent of the target accuracy. Numerical simulations validate the theory and demonstrate scenarios where decentralized learning outperforms centralized federated methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\nB --\x3e B1[\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u4efb\u52a1\u8868\u793a\u5b66\u4e60/Decentralized Multi-Task Representation Learning in Data-Scarce Environments]\nC --\x3e C1[\u4ea4\u66ff\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u4e0e\u6700\u5c0f\u5316\u7b97\u6cd5/Alternating Projected Gradient Descent and Minimization Algorithm]\nD --\x3e D1[\u901a\u4fe1\u590d\u6742\u5ea6\u4e0e\u76ee\u6807\u7cbe\u5ea6\u65e0\u5173/Communication Complexity Independent of Target Accuracy]\nD --\x3e D2[\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u4f18\u4e8e\u4e2d\u5fc3\u5316\u65b9\u6cd5/Decentralized Algorithm Outperforms Centralized Counterpart in Certain Regimes]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image registration], [Neural ODEs, Structural Descriptors, Diffeomorphic Registration, Multimodal, Local Mutual Information]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Salvador Rodriguez-Sanz, Monica Hernandez"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Zaragoza, Aragon Institute for Engineering Research (I3A)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22689",children:"https://arxiv.org/pdf/2512.22689"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f20\u7edf\u7b97\u6cd5\u5047\u8bbe\u5f3a\u5ea6\u76f8\u5173\uff0c\u9650\u4e8e\u5355\u6a21\u6001/Traditional methods assume intensity correlation, limited to monomodal]\n    B --\x3e B2[\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6cdb\u5316\u6027\u5dee/Learning-based models need large datasets, poor generalization]\n    C --\x3e C1[\u57fa\u4e8e\u5b9e\u4f8b\u7684\u6846\u67b6/Instance-specific framework]\n    C --\x3e C2[\u4f7f\u7528\u795e\u7ecfODE\u4e0e\u7ed3\u6784\u63cf\u8ff0\u7b26/Using Neural ODEs & Structural Descriptors]\n    C --\x3e C3[\u6574\u5408\u5c40\u90e8\u4e92\u4fe1\u606f/Integrating Local Mutual Information]\n    D --\x3e D1[\u8d85\u8d8aSOTA\u7ed3\u679c/Surpassing SOTA results]\n    D --\x3e D2[\u5bf9\u6b63\u5219\u5316\u9c81\u68d2/Robust to varying regularization]\n    D --\x3e D3[\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u5c3a\u5ea6/Efficient & suitable for varying scales]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["[arXiv251230] Learning with the ",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsx)(n.mi,{children:"p"})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"p"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,r.jsx)(n.span,{className:"mord mathnormal",children:"p"})]})})]}),"-adics"]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [representation learning], [p-adic numbers, ultrametric space, hierarchical representation, non-archimedean geometry, semantic networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Andr\xe9 F. T. Martins"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Instituto Superior T\xe9cnico, Universidade de Lisboa; Instituto de Telecomunica\xe7\xf5es"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22692",children:"https://arxiv.org/pdf/2512.22692"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Learning with the p-adics] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u73b0\u6709ML\u57fa\u4e8e\u5b9e\u6570\u57df/Existing ML uses real numbers]\nB --\x3e B2[\u662f\u5426\u53ef\u7528\u5176\u4ed6\u57df?/Alternative fields possible?]\nC --\x3e C1[\u7814\u7a76p-adic\u6570\u57df/Study p-adic number field Q_p]\nC --\x3e C2[\u5229\u7528\u8d85\u5ea6\u91cf\u7ed3\u6784/Exploit ultrametric structure]\nD --\x3e D1[\u6784\u5efa\u5206\u7c7b\u56de\u5f52\u6a21\u578b/Build classification & regression models]\nD --\x3e D2[\u8868\u793a\u5b66\u4e60\u4e0e\u8bed\u4e49\u7f51\u7edc/Representation learning & semantic networks]\nD --\x3e D3[\u5f00\u542f\u65b0\u7814\u7a76\u65b9\u5411/Open new research directions]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [predictive modeling], [power outage prediction, LSTM, socio-economic factors, machine learning, resilience]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Antar Kumar Biswas, Masoud H. Nazari"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Wayne State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22699",children:"https://arxiv.org/pdf/2512.22699"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel learning-based framework specifically for predicting low-probability, high-consequence (LPHC) power outages during extreme events. 2. Integrates a comprehensive set of features from public data, including weather, socio-economic, infrastructure, and seasonal event data, to reveal community vulnerability patterns. 3. Empirically validates the framework on a large-scale Michigan dataset, demonstrating that the LSTM model achieves the lowest prediction error and identifying correlations between economic/infrastructure factors and outage occurrence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3d66053b661eb55e66139efa4ce581315354dbf33b1490862dcf700576f9ef5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning framework to predict power outages caused by extreme events by integrating weather, socio-economic, and infrastructure data. The authors evaluate four models (RF, SVM, AdaBoost, LSTM) on a dataset from Michigan and find that the LSTM performs best, with results showing that better economic conditions and infrastructure are linked to fewer outages."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Predictive Modeling of Power Outages during Extreme Events<br>\u6781\u7aef\u4e8b\u4ef6\u4e0b\u7684\u505c\u7535\u9884\u6d4b\u5efa\u6a21] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Predicting LPHC power outages<br>\u9884\u6d4b\u4f4e\u6982\u7387\u9ad8\u540e\u679c\u505c\u7535)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: ML framework integrating multi-source data<br>\u96c6\u6210\u591a\u6e90\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: LSTM best; Socio-economic factors matter<br>LSTM\u6700\u4f18\uff1b\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u91cd\u8981)\n    B --\x3e E(Data: EAGLE-I, weather, socio-economic, infrastructure<br>\u6570\u636e: \u505c\u7535\u8bb0\u5f55\u3001\u5929\u6c14\u3001\u793e\u4f1a\u7ecf\u6d4e\u3001\u57fa\u7840\u8bbe\u65bd)\n    C --\x3e F(Models: RF, SVM, AdaBoost, LSTM<br>\u6a21\u578b: \u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u81ea\u9002\u5e94\u63d0\u5347\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc)\n    D --\x3e G(Conclusion: Stronger economy & infrastructure \u2192 fewer outages<br>\u7ed3\u8bba: \u7ecf\u6d4e\u4e0e\u57fa\u7840\u8bbe\u65bd\u66f4\u5f3a\u2192\u505c\u7535\u66f4\u5c11)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] What Matters in Deep Learning for Time Series Forecasting?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time series forecasting], [channel-independence, locality, globality, forecasting model card]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Valentina Moretti, Andrea Cini, Ivan Marisca, Cesare Alippi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IDSIA (Universit\xe0 della Svizzera italiana), Politecnico di Milano"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22702",children:"https://arxiv.org/pdf/2512.22702"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Analyzes the design space of deep learning for time series forecasting, emphasizing principles like locality and globality over specific sequence modeling layers. 2. Highlights how overlooked implementation details (e.g., parameter sharing) fundamentally alter model classes and empirical results. 3. Proposes an auxiliary forecasting model card to systematically characterize architectures based on key design choices, advocating for improved benchmarking practices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97a34d1ef2fb71ea41bf423f0bb9c22a61efb8b9f97da7ec25666712a5fa044b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper critically examines deep learning architectures for time series forecasting, arguing that foundational design principles (e.g., locality vs. globality) are more crucial for accuracy than complex sequence modeling layers. It shows that simple, well-designed models can match state-of-the-art performance and reveals how implementation details significantly impact results. The authors propose a forecasting model card to standardize architecture characterization and call for a rethink of benchmarking practices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root(What Matters in Deep Learning for Time Series Forecasting?) --\x3e Problem(\u6838\u5fc3\u95ee\u9898/Problem)\n    Root --\x3e Method(\u4e3b\u8981\u65b9\u6cd5/Method)\n    Root --\x3e Results(\u5173\u952e\u7ed3\u679c/Results)\n    Problem --\x3e P1(\u5927\u91cf\u65b0\u67b6\u6784\u4e0e\u77db\u76fe\u7ed3\u679c\u96be\u4ee5\u8bc4\u4f30\u7ec4\u4ef6\u8d21\u732e/Many new architectures and contradictory results make it hard to assess component contributions)\n    Method --\x3e M1(\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u539f\u5219\u8bbe\u8ba1\u6a21\u578b/Grounding model design on forecasting principles)\n    Method --\x3e M2(\u5206\u6790\u5c40\u90e8\u6027\u4e0e\u5168\u5c40\u6027\u7b49\u6982\u5ff5/Analyzing concepts like locality and globality)\n    Method --\x3e M3(\u63d0\u51fa\u8f85\u52a9\u9884\u6d4b\u6a21\u578b\u5361/Proposing an auxiliary forecasting model card)\n    Results --\x3e R1(\u8bbe\u8ba1\u539f\u5219\u6bd4\u7279\u5b9a\u5e8f\u5217\u5c42\u66f4\u91cd\u8981/Design principles more relevant than specific sequence layers)\n    Results --\x3e R2(\u7b80\u5355\u8bbe\u8ba1\u53ef\u5339\u914dSOTA/Simple, well-designed architectures can match SOTA)\n    Results --\x3e R3(\u547c\u5401\u91cd\u65b0\u601d\u8003\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5/Call for rethinking benchmarking practices)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [hope speech detection], [transformer models, multilingual classification, low-resource languages, XLM-RoBERTa, UrduBERT]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ahmed Abdullah, Sana Fatima, Haroon Mahmood"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," FAST-National University, Al Ain University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22705",children:"https://arxiv.org/pdf/2512.22705"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GHaLIB: \u591a\u8bed\u8a00\u5e0c\u671b\u8bed\u97f3\u68c0\u6d4b\u6846\u67b6 / GHaLIB: Multilingual Hope Speech Detection Framework] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results]\n    B --\x3e B1[\u5e0c\u671b\u8bed\u97f3\u5728NLP\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3 / Hope speech underrepresented in NLP]\n    B --\x3e B2[\u4f4e\u8d44\u6e90\u8bed\u8a00(\u5982\u4e4c\u5c14\u90fd\u8bed)\u7f3a\u4e4f\u8d44\u6e90 / Lack of resources for low-resource languages (e.g., Urdu)]\n    C --\x3e C1[\u4f7f\u7528\u9884\u8bad\u7ec3\u591a\u8bed\u8a00Transformer\u6a21\u578b / Use pretrained multilingual Transformer models]\n    C --\x3e C2[\u7b80\u5355\u9884\u5904\u7406\u4e0e\u5206\u7c7b\u5668\u8bad\u7ec3 / Simple preprocessing & classifier training]\n    D --\x3e D1[\u4e4c\u5c14\u90fd\u8bed\u4e8c\u5143\u5206\u7c7bF1: 95.2% / Urdu binary F1: 95.2%]\n    D --\x3e D2[\u4e4c\u5c14\u90fd\u8bed\u591a\u7c7b\u5206\u7c7bF1: 65.2% / Urdu multi-class F1: 65.2%]\n    D --\x3e D3[\u591a\u8bed\u8a00\u6a21\u578b\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883 / Multilingual models viable for low-resource settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Memento-II: Learning by Stateful Reflective Memory"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [stateful reflective decision process, episodic memory, policy iteration, continual learning, retrieval-augmented generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jun Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University College London (UCL)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22716",children:"https://arxiv.org/pdf/2512.22716"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Memento-II: Learning by Stateful Reflective Memory] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca/Lack of theoretical explanation for memory-based continual learning in LLM agents]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u72b6\u6001\u5316\u53cd\u601d\u51b3\u7b56\u8fc7\u7a0b/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u4e0e\u6536\u655b\u4fdd\u8bc1/Provides theoretical framework and convergence guarantees for optimal policy]\n    C --\x3e E[\u5199\u5165\u5bf9\u5e94\u7b56\u7565\u8bc4\u4f30/Writing corresponds to policy evaluation]\n    C --\x3e F[\u8bfb\u53d6\u5bf9\u5e94\u7b56\u7565\u6539\u8fdb/Reading corresponds to policy improvement]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [text classification], [data augmentation, imbalanced dataset, social media analysis, natural language processing, pregnancy outcome]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Badsha Biswas"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," George Mason University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22732",children:"https://arxiv.org/pdf/2512.22732"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel approach to use public social media data (e.g., Twitter) as an adjunctive resource for studying negative pregnancy outcomes, addressing data scarcity in traditional epidemiological research. 2. Constructs an NLP pipeline to automatically identify and classify pregnancy experiences from unstructured, noisy social media text, distinguishing between positive and negative outcomes. 3. Investigates and evaluates various data augmentation techniques specifically to address the severe class imbalance inherent in social media data for this sensitive health domain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb9c28a95fb880100ca20beffad94909ef9e73c38a75789c38961258454c014a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of classifying negative pregnancy outcomes from imbalanced social media data. It proposes an NLP pipeline to extract and categorize pregnancy experiences from Twitter and investigates data augmentation techniques to balance the dataset. The research demonstrates the viability of social media data as a supplementary resource for epidemiological studies on pregnancy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u5a74\u513f\u6b7b\u4ea1\u7387\u9ad8\uff0c\u8d1f\u9762\u598a\u5a20\u7ed3\u5c40\u6570\u636e\u7a00\u7f3a / High infant mortality, scarce data on negative pregnancy outcomes]\n    B --\x3e B2[\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e0d\u5e73\u8861\u3001\u6709\u566a\u58f0 / Social media data is imbalanced and noisy]\n    C --\x3e C1[\u6784\u5efaNLP\u6d41\u6c34\u7ebf\u5206\u7c7b\u598a\u5a20\u7ed3\u5c40 / Build NLP pipeline to classify pregnancy outcomes]\n    C --\x3e C2[\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e / Use data augmentation for imbalanced data]\n    D --\x3e D1[\u9a8c\u8bc1\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u8d44\u6e90\u7684\u53ef\u884c\u6027 / Validate social media data as an adjunctive resource]\n    D --\x3e D2[\u4e3a\u672a\u6765\u5065\u5eb7\u7814\u7a76\u63d0\u4f9b\u6846\u67b6 / Provide a framework for future health studies]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [context folding, long-horizon RL, non-stationary observation, gradient dilution, selective segment training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22733",children:"https://arxiv.org/pdf/2512.22733"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/SHAO-Jiaqi757/FoldAct",children:"https://github.com/SHAO-Jiaqi757/FoldAct"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations\u2014separated loss, consistency loss, and selective training\u2014to stabilize training and improve efficiency. The method achieves stable training and a 5.19\xd7 speedup."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents] --\x3e B[\u6838\u5fc3\u95ee\u9898 / Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5 / Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c / Results]\n    B --\x3e B1[\u975e\u5e73\u7a33\u89c2\u6d4b\u5206\u5e03 / Non-stationary Observation Distribution]\n    B --\x3e B2[\u68af\u5ea6\u7a00\u91ca / Gradient Dilution]\n    B --\x3e B3[\u8ba1\u7b97\u6210\u672c\u9ad8 / High Computational Cost]\n    C --\x3e C1[\u5206\u79bb\u635f\u5931\u8ba1\u7b97 / Separated Loss Computation]\n    C --\x3e C2[\u5168\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u635f\u5931 / Full Context Consistency Loss]\n    C --\x3e C3[\u9009\u62e9\u6027\u7247\u6bb5\u8bad\u7ec3 / Selective Segment Training]\n    D --\x3e D1[\u7a33\u5b9a\u8bad\u7ec3 / Stable Training]\n    D --\x3e D2[5.19\u500d\u52a0\u901f / 5.19\xd7 Speedup]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-task learning], [negative transfer, data imbalance, task independence, metal alloys, property prediction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sungwoo Kang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Korea University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22740",children:"https://arxiv.org/pdf/2512.22740"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Empirical demonstration of a dichotomy where MTL degrades regression but improves classification for metal alloy property prediction. 2. Quantitative analysis linking MTL failure to severe data imbalance and near-zero inter-task dependencies. 3. Practical guidelines for materials informatics: use independent models for precise regression and MTL for classification tasks requiring high recall."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5a4e5491f4e12b8112ec2efc4d5432ecc40f379be7394990c01d0cb507caab_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tests the assumption of Multi-Task Learning (MTL) in materials informatics by predicting three metal alloy properties. The results show MTL harms regression performance due to negative transfer from data imbalance but improves classification recall, as the properties are found to be independent. The work concludes with recommendations for when to use or avoid MTL in material discovery."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[When Does Multi-Task Learning Fail? <br/>\u591a\u4efb\u52a1\u5b66\u4e60\u4f55\u65f6\u5931\u6548\uff1f] --\x3e B{\u6838\u5fc3\u95ee\u9898/Problem};\n    A --\x3e C{\u4e3b\u8981\u65b9\u6cd5/Method};\n    A --\x3e D{\u5173\u952e\u7ed3\u679c/Results};\n    B --\x3e B1[MTL Assumption in Materials <br/>\u6750\u6599\u4e2d\u7684MTL\u5047\u8bbe];\n    C --\x3e C1[Compare STL, MTL, Structured MTL <br/>\u6bd4\u8f83\u5355\u4efb\u52a1\u3001\u6807\u51c6MTL\u3001\u7ed3\u6784\u5316MTL];\n    D --\x3e D1[Regression Degrades <br/>\u56de\u5f52\u6027\u80fd\u4e0b\u964d];\n    D --\x3e D2[Classification Improves <br/>\u5206\u7c7b\u6027\u80fd\u63d0\u5347];\n    D --\x3e D3[Properties Independent <br/>\u5c5e\u6027\u76f8\u4e92\u72ec\u7acb];"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [text-to-sql], [semantic validation, hierarchical representation, logical plan, abstract syntax tree, nested message passing neural network]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rihong Qiu, Zhibang Yang, Xinke Jiang, Weibin Liao, Xin Gao, Xu Chu, Junfeng Zhao, Yasha Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Peking University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22744",children:"https://arxiv.org/pdf/2512.22744"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans) and local details (via Abstract Syntax Trees) for semantic validation. 2. Employed a Nested Message Passing Neural Network (NMPNN) to capture relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. 3. Introduced an AST-driven sub-SQL augmentation strategy to generate high-quality negative samples for robust optimization of fine-grained semantic inconsistencies."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2268a4534cffc0a143786b6b8d80dbddb03e7bf7ffd9234c8d468b2a06e151f0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of semantic validation in Text-to-SQL systems by proposing HEROSQL, a hierarchical representation method that combines global intent and local SQL details using Logical Plans and Abstract Syntax Trees, enhanced by a Nested Message Passing Neural Network. It also introduces an AST-driven augmentation strategy for generating negative samples. Experiments show that HEROSQL outperforms state-of-the-art methods in detecting semantic inconsistencies, improving AUPRC by 9.40% and AUROC by 12.35% on average."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HEROSQL: Bridging Global Intent with Local Details] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Semantic validation in Text-to-SQL, capturing global intent and local details]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical representation (LP + AST), NMPNN, AST-driven augmentation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA, +9.40% AUPRC, +12.35% AUROC]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [demand learning, instrumental variables, deep neural networks, regret bound, confounding]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Rui Ai, David Simchi-Levi, Feng Zhu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Massachusetts Institute of Technology (MIT)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22749",children:"https://arxiv.org/pdf/2512.22749"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed an algorithm for dynamic service fee pricing with optimal regret, showing a phase transition based on supply-side noise. 2. Demonstrated that non-i.i.d. actions can serve as instrumental variables to address confounding in demand learning. 3. Proposed a novel homeomorphic construction to establish estimation bounds for learning demand with deep neural networks without requiring star-shapedness assumptions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a53b1242a3048a4f1795be36eec0fe0875f0b532bc8001a000dcba26693c66da_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the problem of dynamic service fee pricing on third-party platforms, where only equilibrium price and quantity are observable, creating a confounding demand learning problem. The authors develop an algorithm with optimal regret, using non-i.i.d. actions as instrumental variables and a novel homeomorphic construction for deep neural network-based demand estimation. The results show that supply-side noise fundamentally impacts learnability and the method is validated with simulations and real-world data from Zomato and Lyft."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: \u5728\u6df7\u6742\u56e0\u7d20\u4e0b\u5b66\u4e60\u9700\u6c42/Demand Learning under Confounding"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u975e\u72ec\u7acb\u540c\u5206\u5e03\u884c\u52a8\u4f5c\u4e3a\u5de5\u5177\u53d8\u91cf/Using non-i.i.d. actions as Instrumental Variables"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: \u6700\u4f18\u9057\u61be\u4e0e\u76f8\u53d8/Optimal Regret & Phase Transition"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [public health informatics], [multi-level modeling, XGBoost, environmental vulnerability index, NHANES, machine learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Eswarasanthosh Kumar Mamillapalli, Nishtha Sharma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," n/a (Inferred from email: nishtha ",(0,r.jsx)(n.a,{href:"mailto:sharma@nh.gov",children:"sharma@nh.gov"})," suggests potential affiliation with a state health department, but no clear academic/research institution is specified. The other author's email is a personal Gmail address.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22758",children:"https://arxiv.org/pdf/2512.22758"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel micro-macro machine learning framework that integrates individual-level health data with macro-level environmental data to predict childhood obesity risk. 2. Constructed a composite environmental vulnerability index (EnvScore) from USDA and EPA data to quantify state-level structural risk factors. 3. Demonstrated a scalable, data-driven modeling pipeline that reveals geographic alignment between high environmental burden and predicted individual obesity risk, enabling identification of environment-driven health disparities."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7578fbec69afd6c4e44bd1d6715e6f98a34a451a47c81c03388bed0ae331bd4c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a machine learning framework that combines individual health data from NHANES with environmental data (food access, air quality) from USDA/EPA to predict childhood obesity. The best-performing model was XGBoost, and a state-level environmental risk score was created. The study found a strong geographic correlation between areas of high environmental burden and high predicted obesity risk, showing the value of multi-scale data integration for public health."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk<br>\u9884\u6d4b\u513f\u7ae5\u80a5\u80d6\u98ce\u9669\u7684\u5fae-\u5b8f\u89c2\u673a\u5668\u5b66\u4e60\u6846\u67b6] --\x3e B(Problem: Childhood obesity is influenced by multi-level factors, but studies often analyze them independently.<br>\u6838\u5fc3\u95ee\u9898: \u513f\u7ae5\u80a5\u80d6\u53d7\u591a\u5c42\u9762\u56e0\u7d20\u5f71\u54cd\uff0c\u4f46\u7814\u7a76\u5e38\u5b64\u7acb\u5206\u6790)\n    A --\x3e C(Method: Integrated NHANES microdata with USDA/EPA macrodata. Used ML models (e.g., XGBoost) and created an EnvScore index.<br>\u4e3b\u8981\u65b9\u6cd5: \u6574\u5408NHANES\u5fae\u89c2\u6570\u636e\u4e0eUSDA/EPA\u5b8f\u89c2\u6570\u636e\u3002\u4f7f\u7528ML\u6a21\u578b\u5e76\u521b\u5efaEnvScore\u6307\u6570)\n    A --\x3e D(Results: XGBoost performed best. High environmental burden states aligned with high predicted obesity risk.<br>\u5173\u952e\u7ed3\u679c: XGBoost\u8868\u73b0\u6700\u4f73\u3002\u9ad8\u73af\u5883\u8d1f\u62c5\u5dde\u4e0e\u9ad8\u9884\u6d4b\u80a5\u80d6\u98ce\u9669\u5206\u5e03\u4e00\u81f4)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Active Constraint Learning in High Dimensions from Demonstrations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [robot learning], [active learning, constraint inference, Gaussian processes, learning from demonstration]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zheng Qiu, Chih-Yuan Chiu, Glen Chou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22757",children:"https://arxiv.org/pdf/2512.22757"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Active Constraint Learning in High Dimensions from Demonstrations") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Data-inefficient constraint inference from demonstrations")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Iterative active learning with Gaussian Processes")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Outperforms baseline with sparse, informative demonstrations")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Understanding the Mechanisms of Fast Hyperparameter Transfer"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [hyperparameter optimization], [hyperparameter transfer, scale-aware hyperparameters, Maximal Update Parameterization (\u03bcP), compute-optimal grid search]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nikhil Ghosh, Denny Wu, Alberto Bietti"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Flatiron Institute, New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22768",children:"https://arxiv.org/pdf/2512.22768"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Develops a formal conceptual framework defining "fast" hyperparameter transfer and proves its equivalence to "useful" transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Understanding the Mechanisms of Fast Hyperparameter Transfer<br>\u7406\u89e3\u5feb\u901f\u8d85\u53c2\u6570\u8fc1\u79fb\u7684\u673a\u5236"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Standard HP tuning is too expensive for large models.<br>\u6807\u51c6HP\u8c03\u4f18\u5bf9\u4e8e\u5927\u6a21\u578b\u8fc7\u4e8e\u6602\u8d35"] --\x3e P1["\u5b50\u95ee\u9898/Sub-problem<br>How to define and understand \'fast\' HP transfer?<br>\u5982\u4f55\u5b9a\u4e49\u548c\u7406\u89e3\'\u5feb\u901f\'HP\u8fc1\u79fb\uff1f"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Develop a formal framework for HP transfer.<br>\u5efa\u7acbHP\u8fc1\u79fb\u7684\u5f62\u5f0f\u5316\u6846\u67b6"] --\x3e M1["\u65b9\u6cd5\u6b65\u9aa4/Step<br>Define \'fast\' vs \'useful\' transfer.<br>\u5b9a\u4e49\'\u5feb\u901f\'\u4e0e\'\u6709\u7528\'\u8fc1\u79fb"]\n    Method --\x3e M2["\u65b9\u6cd5\u6b65\u9aa4/Step<br>Analyze problem structure & \xb5P.<br>\u5206\u6790\u95ee\u9898\u7ed3\u6784\u4e0e\xb5P"]\n    Method --\x3e M3["\u65b9\u6cd5\u6b65\u9aa4/Step<br>Propose trajectory decomposition hypothesis.<br>\u63d0\u51fa\u8f68\u8ff9\u5206\u89e3\u5047\u8bbe"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Fast transfer is equivalent to useful transfer.<br>\u5feb\u901f\u8fc1\u79fb\u7b49\u4ef7\u4e8e\u6709\u7528\u8fc1\u79fb"] --\x3e R1["\u7ed3\u679c/Result<br>Transfer success depends on problem structure.<br>\u8fc1\u79fb\u6210\u529f\u53d6\u51b3\u4e8e\u95ee\u9898\u7ed3\u6784"]\n    Results --\x3e R2["\u7ed3\u679c/Result<br>Empirical evidence supports the hypothesis.<br>\u5b9e\u8bc1\u8bc1\u636e\u652f\u6301\u8be5\u5047\u8bbe"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal inference], [causal transportability, domain adaptation, few-shot learning, circuit composition, distribution shift]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kasra Jalaldoust, Elias Bareinboim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Columbia University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22777",children:"https://arxiv.org/pdf/2512.22777"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Generalization under distribution shift]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Circuit-TR algorithm based on causal transportability]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Theoretical characterization of few-shot learnability]\nB --\x3e B1[\u9886\u57df\u6cdb\u5316\u4e0e\u9002\u5e94/Domain Generalization & Adaptation]\nC --\x3e C1[\u6a21\u5757\u5b66\u4e60\u4e0e\u7535\u8def\u7ec4\u5408/Module Learning & Circuit Composition]\nC --\x3e C2[\u56e0\u679c\u56fe\u4e0e\u673a\u5236\u5171\u4eab/Causal Graph & Mechanism Sharing]\nD --\x3e D1[\u53ef\u8fc1\u79fb\u6027\u6807\u51c6/Transportability Criteria]\nD --\x3e D2[\u7535\u8def\u89c4\u6a21\u590d\u6742\u5ea6/Circuit Size Complexity]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [temporal graph neural networks, explainable ai, graph explanation, recurrent neural networks, breadth-first search]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xuyan Li, Jie Wang, Zheng Yan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xidian University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22772",children:"https://arxiv.org/pdf/2512.22772"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[TGNN\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027/Lack of TGNN transparency & explainability]\n    B --\x3e B2[\u73b0\u6709\u65b9\u6cd5\u901a\u7528\u6027\u5dee\u3001\u6548\u7387\u4f4e\u3001\u4e0d\u53cb\u597d/Existing methods lack generality, efficiency, user-friendliness]\n    C --\x3e C1[\u63d0\u53d6\u8282\u70b9\u5e8f\u5217\u4f5c\u4e3a\u7edf\u4e00\u7279\u5f81/Extract node sequences as unified features]\n    C --\x3e C2[\u4f7f\u7528BFS\u548c\u65f6\u95f4\u4fe1\u606f\u6784\u5efa\u5e8f\u5217/Use BFS & temporal info to construct sequences]\n    C --\x3e C3[\u57fa\u4e8eRNN\u7684\u751f\u6210\u6a21\u578b/RNN-based generative model]\n    D --\x3e D1[\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c/Experiments on 6 datasets]\n    D --\x3e D2[\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5/Outperforms existing baselines]\n    D --\x3e D3[\u901a\u7528\u3001\u9ad8\u6548\u3001\u7528\u6237\u53cb\u597d/Generality, efficiency, user-friendliness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [quantum-inspired machine learning], [spectral decomposition, Hamiltonian learning, semantic wavefunctions, operator calculus, emergent manifolds]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Truong Son Nguyen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Arizona State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22774",children:"https://arxiv.org/pdf/2512.22774"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Schr\xf6dinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Schr\xf6dinger AI: A Unified Spectral-Dynamical Framework<br>\u859b\u5b9a\u8c14AI: \u7edf\u4e00\u8c31-\u52a8\u529b\u5b66\u6846\u67b6"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Limitations of conventional ML<br>\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u5c40\u9650"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Quantum-inspired framework<br>\u91cf\u5b50\u542f\u53d1\u7684\u6846\u67b6"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Empirical demonstrations<br>\u5b9e\u8bc1\u6f14\u793a"]\n    Problem --\x3e P1["Struggle with uncertainty & adaptation<br>\u96be\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4e0e\u9002\u5e94\u6027"]\n    Problem --\x3e P2["Brittle symbolic reasoning<br>\u8106\u5f31\u7684\u7b26\u53f7\u63a8\u7406"]\n    Method --\x3e M1["Time-independent wave-energy solver<br>\u65f6\u95f4\u65e0\u5173\u6ce2\u80fd\u6c42\u89e3\u5668"]\n    Method --\x3e M2["Time-dependent dynamical solver<br>\u65f6\u95f4\u76f8\u5173\u52a8\u529b\u5b66\u6c42\u89e3\u5668"]\n    Method --\x3e M3["Low-rank operator calculus<br>\u4f4e\u79e9\u7b97\u5b50\u6f14\u7b97"]\n    Results --\x3e R1["Emergent semantic manifolds<br>\u6d8c\u73b0\u7684\u8bed\u4e49\u6d41\u5f62"]\n    Results --\x3e R2["Dynamic reasoning adaptation<br>\u52a8\u6001\u63a8\u7406\u9002\u5e94"]\n    Results --\x3e R3["Exact operator generalization<br>\u7cbe\u786e\u7b97\u5b50\u6cdb\u5316"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Discovering Transmission Dynamics of COVID-19 in China"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [information extraction], [natural language processing, transmission chain, epidemiological analysis, data mining, statistical analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhou Yang, Edward Dougherty, Chen Zhang, Zhenhe Pan, Fang Jin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," George Washington University, Roger Williams University, Texas Tech University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22787",children:"https://arxiv.org/pdf/2512.22787"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Constructed transmission/tracking chains for COVID-19 in China by applying NLP and manual curation to data mined from diverse public sources. 2. Conducted a comprehensive spatiotemporal analysis by integrating case tracking data with population mobility data from Wuhan. 3. Quantified key transmission dynamics, revealing regional differences, hospitalization timelines, and the evolution of infection sources over the course of the pandemic."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eca2aa3a96ff37b70cb26e6ad2cfa0f84cc9a5cf8c76b6e1404ce5fa2474828e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the transmission dynamics of COVID-19 in China by mining and processing public case reports using NLP to construct transmission chains. The analysis integrates these chains with mobility data to quantify spatiotemporal spread. Key findings include significant regional differences in infection rates, rapid hospitalization of symptomatic cases, and a shift in infection sources from travel to social activities over time."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Discovering Transmission Dynamics of COVID-19 in China] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u5206\u6790\u4e2d\u56fdCOVID-19\u4f20\u64ad\u673a\u5236/Analyze China COVID-19 transmission mechanisms]\n    C --\x3e C1[\u4ece\u591a\u6e90\u6536\u96c6\u75c5\u4f8b\u6570\u636e/Collect case data from multiple sources]\n    C --\x3e C2[\u5e94\u7528NLP\u6784\u5efa\u4f20\u64ad\u94fe/Apply NLP to construct transmission chains]\n    C --\x3e C3[\u7ed3\u5408\u79fb\u52a8\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\u5206\u6790/Combine mobility data for statistical analysis]\n    D --\x3e D1[\u5927\u57ce\u5e02\u611f\u67d3\u66f4\u591a\uff0c\u5b58\u5728\u5730\u533a\u5dee\u5f02/Larger cities have more infections, regional differences exist]\n    D --\x3e D2[79%\u6709\u75c7\u72b6\u80055\u5929\u5185\u4f4f\u9662/79% symptomatic hospitalized within 5 days]\n    D --\x3e D3[\u611f\u67d3\u6e90\u4ece\u65c5\u884c\u8f6c\u5411\u793e\u4ea4\u6d3b\u52a8/Infection source shifted from travel to social activities]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] CNSight: Evaluation of Clinical Note Segmentation Tools"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [text segmentation], [clinical note segmentation, transformer models, large language models, MIMIC-IV, rule-based baselines]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Southern California"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22795",children:"https://arxiv.org/pdf/2512.22795"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[CNSight: \u4e34\u5e8a\u7b14\u8bb0\u5206\u5272\u5de5\u5177\u8bc4\u4f30 / CNSight: Evaluation of Clinical Note Segmentation Tools]\n    Root --\x3e Problem[\u4e34\u5e8a\u7b14\u8bb0\u975e\u7ed3\u6784\u5316 / Clinical Notes Unstructured]\n    Root --\x3e Method[\u8bc4\u4f30\u89c4\u5219/\u53d8\u6362\u5668/\u5927\u8bed\u8a00\u6a21\u578b / Evaluate Rule-based/Transformer/LLMs]\n    Root --\x3e Results[\u5927\u6a21\u578b\u6027\u80fd\u6700\u4f73 / Large Models Best Performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [open-set recognition], [spherical normalization, Mahalanobis distance, electronic nose, open-set recognition, feature drift]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuai Chen, Chen Wang, Ziran Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," School of Mechanical Engineering, Shandong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22792",children:"https://arxiv.org/pdf/2512.22792"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Feature drift & unknown gas interference in E-nose"] --\x3e P1["\u4fe1\u53f7\u6f02\u79fb/Feature Distribution Shift"]\n    Problem --\x3e P2["\u672a\u77e5\u6c14\u4f53\u5e72\u6270/Unknown Gas Interference"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>SNM-Net Framework"] --\x3e M1["\u51e0\u4f55\u89e3\u8026/Geometric Decoupling<br>Cascaded Batch & L2 Norm"]\n    Method --\x3e M2["\u9a6c\u6c0f\u8ddd\u79bb\u8bc4\u5206/Mahalanobis Distance Scoring"]\n    Method --\x3e M3["\u67b6\u6784\u65e0\u5173/Architecture-Agnostic<br>CNN, RNN, Transformer"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>State-of-the-art performance"] --\x3e R1["\u9ad8AUROC/High AUROC: 0.9977"]\n    Results --\x3e R2["\u9ad8\u672a\u77e5\u6c14\u4f53\u68c0\u6d4b\u7387/High Unknown Detection: 99.57%"]\n    Results --\x3e R3["\u5f3a\u9c81\u68d2\u6027/High Robustness<br>Low std. dev."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion models], [reinforcement learning, knowledge distillation, policy optimization, denoising paths, model agnostic]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sharif University of Technology, Alan Turing Institute, London School of Economics"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22802",children:"https://arxiv.org/pdf/2512.22802"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ReDiF: Reinforced Distillation for Few Step Diffusion] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Diffusion\u6a21\u578b\u91c7\u6837\u6162/Slow sampling in diffusion models)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u84b8\u998f\u6846\u67b6/Reinforcement learning based distillation framework)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u66f4\u5c11\u6b65\u9aa4\uff0c\u6027\u80fd\u66f4\u4f18/Fewer steps, superior performance)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] MoR: Mixture Of Representations For Mixed-Precision Training"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [mixed-precision training, FP8, dynamic quantization, tensor representation, low-precision training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nvidia, Meta"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22804",children:"https://arxiv.org/pdf/2512.22804"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[MoR: Mixture Of Representations For Mixed-Precision Training] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Successful mixed-precision training requires the right combination of methods.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Dynamic, property-aware quantization framework selecting between representations (e.g., FP8/BF16).]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>Achieves 98.38% FP8 quantization, preserves model quality, enables lower precision formats.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [climate informatics], [long-range distillation, synthetic training data, subseasonal-to-seasonal forecasting, probabilistic forecasting, autoregressive models]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Scott A. Martin, Noah Brenowitz, Dale Durran, Michael Pritchard"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," NVIDIA Research, University of Washington"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22814",children:"https://arxiv.org/pdf/2512.22814"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes long-range distillation, a method to train a long-timestep probabilistic model using massive synthetic data from an autoregressive teacher model. 2. Demonstrates the generation and use of over 10,000 years of simulated climate data from the DLESyM model for training. 3. Shows that distilled models achieve S2S forecast skill comparable to ECMWF ensembles after fine-tuning, with skill scaling with synthetic data volume."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b3dd444e162236a7d1cc0aaa69d999407d7ebf78184fada39f979dccbd0692f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of long-range weather forecasting by introducing long-range distillation, a method that trains a single-step probabilistic model using a massive synthetic dataset generated by an autoregressive AI model. The distilled model, trained on over 10,000 years of simulated climate, achieves subseasonal-to-seasonal forecast skill comparable to state-of-the-art ensemble methods, demonstrating that AI-generated synthetic data can effectively scale long-range forecast skill."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Long-Range Distillation Paper] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Long-range AI weather forecasting is limited by error accumulation and scarce training data for slow climate modes.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Distill a long-timestep probabilistic student model using 10,000+ years of synthetic data from an autoregressive teacher model (DLESyM).)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Student model skill scales with synthetic data, approaches teacher skill, and matches ECMWF ensemble skill after ERA5 fine-tuning.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [curriculum learning, goal-conditioned reinforcement learning, temporal variance, student-teacher paradigm, Q-function]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gaurav Chaudhary, Laxmidhar Behera"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IIT Kanpur"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22824",children:"https://arxiv.org/pdf/2512.22824"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum for accelerating Goal-Conditioned RL. 2. Establishes a theoretical connection between the temporal variance of Q-values and policy evolution. 3. Demonstrates the algorithm-agnostic nature of the approach, showing consistent improvements across 11 robotic manipulation and maze navigation tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c410b802a9fdb44624e8bf6d607803fec918def24d7a3c81bf3fb12d7fb2e63_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the sample inefficiency of uniform goal selection in multi-goal reinforcement learning. It proposes a TEACH framework where a teacher module dynamically selects goals with the highest temporal variance in Q-values to create an adaptive curriculum. The method is shown to improve learning efficiency over state-of-the-art curriculum learning methods across diverse robotic tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Uniform goal selection is sample inefficient in multi-goal RL/\u591a\u76ee\u6807RL\u4e2d\u5747\u5300\u76ee\u6807\u9009\u62e9\u6837\u672c\u6548\u7387\u4f4e]\n    C --\x3e C1[Student-Teacher paradigm with Temporal Variance-Driven Curriculum/\u57fa\u4e8e\u65f6\u5e8f\u65b9\u5dee\u7684\u5e08\u751f\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f]\n    C --\x3e C2[Teacher prioritizes goals with highest Q-value temporal variance/\u6559\u5e08\u6a21\u5757\u4f18\u5148\u9009\u62e9Q\u503c\u65f6\u5e8f\u65b9\u5dee\u6700\u9ad8\u7684\u76ee\u6807]\n    D --\x3e D1[Consistent improvements over SOTA methods/\u76f8\u6bd4SOTA\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb]\n    D --\x3e D2[Evaluated on 11 robotic manipulation and navigation tasks/\u572811\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4e0e\u5bfc\u822a\u4efb\u52a1\u4e0a\u9a8c\u8bc1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [video generation], [Human-Object Interaction, Diffusion Transformer, Relative Coordinate Maps, Progressive Curriculum Learning, Geometry Consistency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22854",children:"https://arxiv.org/pdf/2512.22854"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://neutrinoliu.github.io/byteloom/",children:"https://neutrinoliu.github.io/byteloom/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u591a\u89c6\u56fe\u4fe1\u606f\u6ce8\u5165\u673a\u5236/Existing methods lack multi-view injection]\n    B --\x3e B2[\u4e25\u91cd\u4f9d\u8d56\u624b\u90e8\u7f51\u683c\u6807\u6ce8/Heavy reliance on hand mesh annotations]\n    C --\x3e C1[\u63d0\u51faRCM-cache\u673a\u5236/Propose RCM-cache mechanism]\n    C --\x3e C2[\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u5b66\u4e60/Design progressive curriculum learning]\n    D --\x3e D1[\u4fdd\u6301\u7269\u4f53\u51e0\u4f55\u4e00\u81f4\u6027/Preserves object geometry consistency]\n    D --\x3e D2[\u751f\u6210\u5e73\u6ed1\u8fd0\u52a8\u89c6\u9891/Generates smooth motion videos]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [blockchain security, IoT security, adversarial machine learning], [Fully Homomorphic Encryption, Attribute-Based Access Control, Multi-Agent Reinforcement Learning, Byzantine Fault Tolerance, Trust-Based Consensus]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Soham Padia, Dhananjay Vaidya, Ramchandra Mangrulkar"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University, Dwarkadas J. Sanghvi College of Engineering"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22860",children:"https://arxiv.org/pdf/2512.22860"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel trust-based delegated consensus framework for blockchain IoT that integrates Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation. 2. Systematically compares the performance of three reinforcement learning approaches (RL, DRL, MARL) against five distinct and sophisticated adversarial attack families. 3. Empirically demonstrates that Multi-Agent RL (MARL) provides superior defense against collusive attacks and identifies the catastrophic vulnerability of all learning agents to Time-Delayed Poisoning (sleeper) attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373f990d6c218afb4f9e660bb84fd86dc8e9d7006d2b7bdef3d956a991960bff_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses securing blockchain-enabled IoT networks by proposing a trust-based consensus framework that combines privacy-preserving techniques (FHE and ABAC) with learning-based defenses. It compares RL, DRL, and MARL against five attack types, finding MARL most effective against collusive attacks but revealing that all methods are highly vulnerable to time-delayed poisoning attacks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Adaptive Trust Consensus for Blockchain IoT<br/>\u533a\u5757\u94fe\u7269\u8054\u7f51\u81ea\u9002\u5e94\u4fe1\u4efb\u5171\u8bc6"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["Securing Blockchain IoT Against Attacks<br/>\u4fdd\u62a4\u533a\u5757\u94fe\u7269\u8054\u7f51\u514d\u53d7\u653b\u51fb"] --\x3e A1["Naive Malicious Attack (NMA)<br/>\u7b80\u5355\u6076\u610f\u653b\u51fb"]\n    Problem --\x3e A2["Collusive Rumor Attack (CRA)<br/>\u5408\u8c0b\u8c23\u8a00\u653b\u51fb"]\n    Problem --\x3e A3["Adaptive Adversarial Attack (AAA)<br/>\u81ea\u9002\u5e94\u5bf9\u6297\u653b\u51fb"]\n    Problem --\x3e A4["Byzantine Fault Injection (BFI)<br/>\u62dc\u5360\u5ead\u6545\u969c\u6ce8\u5165"]\n    Problem --\x3e A5["Time-Delayed Poisoning (TDP)<br/>\u65f6\u95f4\u5ef6\u8fdf\u6295\u6bd2"]\n\n    Method["Trust Framework with FHE & ABAC + Learning Defenses<br/>\u57fa\u4e8eFHE\u548cABAC\u7684\u4fe1\u4efb\u6846\u67b6\u4e0e\u5b66\u4e60\u9632\u5fa1"] --\x3e M1["Reinforcement Learning (RL)<br/>\u5f3a\u5316\u5b66\u4e60"]\n    Method --\x3e M2["Deep RL (DRL)<br/>\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60"]\n    Method --\x3e M3["Multi-Agent RL (MARL)<br/>\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60"]\n\n    Results["Key Experimental Findings<br/>\u5173\u952e\u5b9e\u9a8c\u7ed3\u679c"] --\x3e R1["MARL best vs. Collusive Attacks<br/>MARL\u5bf9\u5408\u8c0b\u653b\u51fb\u6700\u4f73"]\n    Results --\x3e R2["DRL & MARL perfect vs. Adaptive Attacks<br/>DRL\u548cMARL\u5b8c\u7f8e\u9632\u5fa1\u81ea\u9002\u5e94\u653b\u51fb"]\n    Results --\x3e R3["All agents fail vs. Time-Delayed Poisoning<br/>\u6240\u6709\u667a\u80fd\u4f53\u5728\u5ef6\u8fdf\u6295\u6bd2\u653b\u51fb\u4e0b\u5931\u6548"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent reinforcement learning], [Reinforcement Networks, directed acyclic graph (DAG), credit assignment, LevelEnv, hierarchical RL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22876",children:"https://arxiv.org/pdf/2512.22876"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Reinforcement Networks] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: End-to-end training of multi-component AI systems]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: MARL agents organized in a DAG (Reinforcement Networks)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Improved performance, unified framework for structured MARL]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["[arXiv251230] Fundamental Novel Consistency Theory: ",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsx)(n.mi,{children:"H"})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"H"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.6833em"}}),(0,r.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.08125em"},children:"H"})]})})]}),"-Consistency Bounds"]})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [statistical learning theory], [H-consistency bounds, surrogate loss, minimizability gaps, adversarial robustness, comp-sum losses]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yutao Zhong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22880",children:"https://arxiv.org/pdf/2512.22880"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),"  1. Introduces a novel theoretical framework for deriving H-consistency bounds, which provide stronger and more informative guarantees than Bayes-consistency or H-calibration by accounting for the hypothesis set. 2. Establishes the first H-consistency bounds for a wide range of losses in binary and multi-class classification, including convex surrogates, max/sum/constrained losses, and comp-sum losses (e.g., cross-entropy), and extends the analysis to adversarial scenarios. 3. Analyzes the growth rates of H-consistency bounds, proving a universal square-root growth rate for smooth surrogates, and introduces the analysis of minimizability gaps to guide the selection of surrogate loss functions for learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16541a1efc1f9f95010f468aecfaeb95d92c6aea20df9566f6ef60456e371fde_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a new theoretical framework for analyzing the estimation error of target losses using surrogate losses in machine learning. It introduces H-consistency bounds, which offer stronger guarantees by incorporating the hypothesis set, and derives these bounds for various loss functions in both standard and adversarial settings. The main conclusion is that these bounds provide a more precise tool for understanding surrogate loss performance and can guide the design of robust learning algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Fundamental Novel Consistency Theory: H-Consistency Bounds] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1("\u8bad\u7ec3\u635f\u5931\u4e0e\u76ee\u6807\u635f\u5931\u4e0d\u4e00\u81f4<br/>Surrogate vs. Target Loss Mismatch")\n    C --\x3e C1("\u63d0\u51faH-\u4e00\u81f4\u6027\u8fb9\u754c\u7406\u8bba\u6846\u67b6<br/>Propose H-Consistency Bounds Framework")\n    C --\x3e C2("\u5206\u6790\u4e8c\u5143/\u591a\u7c7b\u5206\u7c7b\u7684\u635f\u5931\u51fd\u6570<br/>Analyze Losses for Binary/Multi-class")\n    C --\x3e C3("\u6269\u5c55\u5230\u5bf9\u6297\u6027\u573a\u666f<br/>Extend to Adversarial Setting")\n    D --\x3e D1("\u5f97\u5230\u7d27\u7684\u5206\u5e03\u4f9d\u8d56/\u72ec\u7acb\u8fb9\u754c<br/>Tight Distribution-Dependent/Independent Bounds")\n    D --\x3e D2("\u9996\u6b21\u4e3a\u591a\u79cd\u635f\u5931\u63a8\u5bfcH-\u4e00\u81f4\u6027\u8fb9\u754c<br/>First H-Consistency Bounds for Various Losses")\n    D --\x3e D3("\u8bc1\u660e\u5e73\u6ed1\u4ee3\u7406\u7684\u5e73\u65b9\u6839\u589e\u957f\u901f\u7387<br/>Square-Root Growth Rate for Smooth Surrogates")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [machine learning theory], [multi-expert deferral, abstention, H-consistency, surrogate losses, two-stage learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anqi Mao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22886",children:"https://arxiv.org/pdf/2512.22886"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced new surrogate loss families and proved strong consistency guarantees for multi-class learning with abstention, resolving open questions. 2. Designed new surrogate losses with H-consistency bounds for general multi-expert deferral in classification, leading to effective algorithms. 3. Proposed a novel framework and surrogate losses for regression with deferral, accommodating multiple experts and various cost structures."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6952a3ce4104bfb4e2da48692fa87841baf950369a1d08640407f2002ece4c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This thesis addresses the problems of learning with abstention and multi-expert deferral to improve the reliability and efficiency of models like LLMs. It proposes new surrogate loss formulations for classification and regression, proves strong theoretical consistency guarantees, and demonstrates the empirical effectiveness of the resulting algorithms on datasets like CIFAR-10 and CIFAR-100."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898 / Problem<br>LLMs face hallucinations & high cost<br>Need reliable & efficient prediction"] --\x3e P1["\u5b50\u95ee\u98981 / Sub-Problem 1<br>Learning with Abstention"]\n    Problem --\x3e P2["\u5b50\u95ee\u98982 / Sub-Problem 2<br>Multi-Expert Deferral"]\n    Problem --\x3e P3["\u5b50\u95ee\u98983 / Sub-Problem 3<br>Regression with Deferral"]\n    Method["\u4e3b\u8981\u65b9\u6cd5 / Method<br>Design new surrogate losses<br>Prove H-consistency guarantees"] --\x3e M1["\u65b9\u6cd51 / Method 1<br>For abstention (score-based & predictor-rejector)"]\n    Method --\x3e M2["\u65b9\u6cd52 / Method 2<br>For multi-expert deferral"]\n    Method --\x3e M3["\u65b9\u6cd53 / Method 3<br>For regression deferral framework"]\n    Results["\u5173\u952e\u7ed3\u679c / Results<br>Strong consistency guarantees<br>Superior empirical performance"] --\x3e R1["\u7ed3\u679c1 / Result 1<br>Resolved open questions in abstention"]\n    Results --\x3e R2["\u7ed3\u679c2 / Result 2<br>New effective algorithms for deferral"]\n    Results --\x3e R3["\u7ed3\u679c3 / Result 3<br>Versatile framework for regression"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Debugging Tabular Log as Dynamic Graphs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [dynamic graph, graph neural network, tabular log, log debugging, heterogeneous nodes]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Amazon"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22903",children:"https://arxiv.org/pdf/2512.22903"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GraphLogDebugger, a novel framework that models tabular log data as dynamic graphs with heterogeneous nodes for objects and events, 2. Demonstrates that a simple dynamic GNN can outperform large language models (LLMs) in debugging tasks using this graph representation, 3. Validates the approach on real-world datasets from computer systems and academic papers, showing improved flexibility and scalability over LLM-based methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bfc0dd96717274f366abd002f1a9147f7afbdaba795d251628919a0d25289925_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GraphLogDebugger, a framework that converts tabular log data into dynamic graphs to detect inconsistencies in real-world systems. By representing logs as evolving graphs with object and event nodes, a lightweight dynamic Graph Neural Network effectively debugs logs, outperforming larger LLM-based models in experiments on system and academic log datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[DEBUGGING TABULAR LOG AS DYNAMIC GRAPHS] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs and heavy models lack flexibility and scalability for tabular log debugging]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: GraphLogDebugger models logs as dynamic graphs with heterogeneous nodes and edges]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Simple dynamic GNN outperforms LLMs in debugging on real-world datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [Casing Collar Locator (CCL), ARM Cortex-M7, Depthwise Separable Convolutions, MACs, Inference Latency]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22901",children:"https://arxiv.org/pdf/2512.22901"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight "Collar Recognition Nets" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 \xb5s, proving feasibility for downhole power/space constraints.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight "Collar Recognition Nets" optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A Neural Network-Based Real-time Casing Collar Recognition System<br>\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u5957\u7ba1\u63a5\u7b8d\u8bc6\u522b\u7cfb\u7edf"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u4fe1\u53f7\u8870\u51cf\u5bfc\u81f4\u4e95\u4e0b\u5b9a\u4f4d\u4e0d\u51c6\u786e<br>Signal degradation compromises downhole positioning"]\n    Method["\u4e3aARM Cortex-M7\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7CRN\u7f51\u7edc<br>Lightweight CRNs optimized for ARM Cortex-M7"]\n    Results["8208 MACs, F1=0.972, 343.2\xb5s\u5ef6\u8fdf<br>8208 MACs, F1=0.972, 343.2\xb5s latency"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Federated Multi-Task Clustering"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [federated clustering, spectral clustering, multi-task learning, tensor methods, ADMM]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," South China University of Technology, Dalian University of Technology, Xidian University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22897",children:"https://arxiv.org/pdf/2512.22897"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Federated Multi-Task Clustering] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Centralized models inapplicable to decentralized environments/\u96c6\u4e2d\u5f0f\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u73af\u5883]\n    B --\x3e B2[Poor generalization due to unreliable pseudo-labels/\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u5dee]\n    B --\x3e B3[Failure to capture latent client correlations/\u672a\u80fd\u6355\u83b7\u5ba2\u6237\u7aef\u95f4\u7684\u6f5c\u5728\u5173\u8054]\n    C --\x3e C1[Client-side personalized clustering module/\u5ba2\u6237\u7aef\u4e2a\u6027\u5316\u805a\u7c7b\u6a21\u5757]\n    C --\x3e C2[Server-side tensorial correlation module/\u670d\u52a1\u5668\u7aef\u5f20\u91cf\u5173\u8054\u6a21\u5757]\n    C --\x3e C3[ADMM-based distributed algorithm/\u57fa\u4e8eADMM\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5]\n    D --\x3e D1[Outperforms baselines and SOTA/\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u548c\u524d\u6cbf\u65b9\u6cd5]\n    D --\x3e D2[Validated on real-world datasets/\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [educational data mining], [cognitive diagnosis, meta-learning, continual learning, long-tailed distribution, parameter protection mechanism]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jin Wu, Chanjin Zheng"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Institute of Artificial Intelligence for Education, East China Normal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22904",children:"https://arxiv.org/pdf/2512.22904"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes MetaCD, a novel framework that integrates meta-learning and continual learning for cognitive diagnosis. 2. Uses meta-learning to learn an optimal initialization to alleviate the long-tailed data problem, enabling good performance with few samples. 3. Incorporates a continual learning parameter protection mechanism to adapt to dynamic data changes and new tasks while preventing catastrophic forgetting."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d62de4bde455a8e131e7247a8b8ce8fa94feb82289f4b3dc1660955e7645dfa_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes MetaCD, a meta-learning framework based on continual learning, to address the challenges of long-tailed data distribution and dynamic changes in cognitive diagnosis for intelligent education. It uses meta-learning for optimal initialization and a parameter protection mechanism for continual adaptation, achieving superior accuracy and generalization on five real-world datasets compared to baselines."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u957f\u5c3e\u6570\u636e\u5206\u5e03/Long-tailed Data Distribution]\n    Problem --\x3e P2[\u6570\u636e\u7684\u52a8\u6001\u53d8\u5316/Dynamic Data Changes]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5143\u5b66\u4e60\u6700\u4f18\u521d\u59cb\u5316/Meta-learning for Optimal Initialization]\n    Method --\x3e M2[\u6301\u7eed\u5b66\u4e60\u53c2\u6570\u4fdd\u62a4\u673a\u5236/Continual Learning Parameter Protection]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u63d0\u5347\u5355\u4efb\u52a1\u53ef\u5851\u6027/Improves Single-task Plasticity]\n    Results --\x3e R2[\u786e\u4fdd\u5e8f\u5217\u4efb\u52a1\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u6027/Ensures Sequential Task Stability & Generalization]\n    Results --\x3e R3[\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf/Outperforms Baselines on Five Real-world Datasets]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Q-learning, ensemble learning, satisficing, distillation, bounded rationality]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," \xdcnver \xc7ift\xe7i"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tekirda\u011f Nam\u0131k Kemal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22910",children:"https://arxiv.org/pdf/2512.22910"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Sat-EnQ] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Deep Q-Learning Instability]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Two-Phase Satisficing Ensemble]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Variance Reduction & Robustness]\n    B --\x3e B1[\u65e9\u671f\u8bad\u7ec3\u4e0d\u7a33\u5b9a/Early Training Instability]\n    B --\x3e B2[\u9ad8\u65b9\u5dee\u4e0e\u707e\u96be\u6027\u5931\u8d25/High Variance & Catastrophic Failure]\n    C --\x3e C1[\u9636\u6bb51: \u6ee1\u8db3\u5316\u96c6\u6210\u8bad\u7ec3/Phase 1: Satisficing Ensemble Training]\n    C --\x3e C2[\u9636\u6bb52: \u84b8\u998f\u4e0e\u5fae\u8c03/Phase 2: Distillation & Fine-tuning]\n    D --\x3e D1[3.8\u500d\u65b9\u5dee\u964d\u4f4e/3.8x Variance Reduction]\n    D --\x3e D2[0%\u707e\u96be\u6027\u5931\u8d25/0% Catastrophic Failure]\n    D --\x3e D3[2.5\u500d\u8ba1\u7b97\u6548\u7387\u63d0\u5347/2.5x Compute Efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Geometric Structural Knowledge Graph Foundation Model"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [knowledge graph reasoning], [structural foundation model, geometric attention, inductive link prediction, multi-head transformation, relational fusion]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Stuttgart, University of Southampton, Shahrood University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22931",children:"https://arxiv.org/pdf/2512.22931"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Geometric Structural Knowledge Graph Foundation Model] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u5173\u7cfb\u8f6c\u6362\uff0c\u8868\u8fbe\u80fd\u529b\u53d7\u9650/Existing methods rely on single relational transformation, limiting expressiveness]\n    C --\x3e C1[\u5f15\u5165\u591a\u5934\u51e0\u4f55\u6ce8\u610f\u529b/Multi-head geometric attention]\n    C --\x3e C2[\u5e76\u884c\u591a\u79cd\u51e0\u4f55\u53d8\u6362/Parallel geometric transformations]\n    C --\x3e C3[\u5173\u7cfb\u6761\u4ef6\u6ce8\u610f\u529b\u878d\u5408/Relational conditioned attention fusion]\n    D --\x3e D1[\u572856\u4e2aKG\u4e0a\u8d85\u8d8aULTRA/Outperforms ULTRA on 56 KGs]\n    D --\x3e D2[\u96f6\u6837\u672c\u5f52\u7eb3\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u63d0\u5347/Improves zero-shot inductive link prediction]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multiple Token Divergence: Measuring and Steering In-Context Computation Density"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [language model interpretability], [in-context computation, KL divergence, decoding method, computational effort, prediction head]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vincent Herrmann, Eric Alcaide, Michael Wand, J\xfcrgen Schmidhuber"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Swiss AI Lab IDSIA/USI/SUPSI, King Abdullah University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22944",children:"https://arxiv.org/pdf/2512.22944"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Multiple Token Divergence (MTD), a simple and non-invasive metric to measure a language model's in-context computational effort by comparing the output distributions of the full model and a shallow auxiliary head. 2. Introduces Divergence Steering, a novel decoding method that uses MTD to control the computational character of generated text. 3. Empirically demonstrates that MTD effectively distinguishes task complexity, correlates with problem difficulty, and that lower MTD is associated with more accurate reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef98fdc1c198bde00876e87ff3cca172c3925998a10f9847042108bebc8c5e99_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of measuring the in-context computational effort of language models. It proposes Multiple Token Divergence (MTD), a lightweight metric based on KL divergence between output distributions, and a corresponding decoding method called Divergence Steering. The authors show that MTD effectively correlates with task difficulty and can be used to analyze and steer model computation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Multiple Token Divergence<br/>\u591a\u4ee4\u724c\u6563\u5ea6] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem<br/>\u5982\u4f55\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8ba1\u7b97\u52aa\u529b\uff1f<br/>How to measure in-context computational effort?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u63d0\u51fa\u591a\u4ee4\u724c\u6563\u5ea6(MTD)<br/>Propose Multiple Token Divergence (MTD)]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results<br/>MTD\u6709\u6548\u533a\u5206\u4efb\u52a1\u590d\u6742\u5ea6<br/>MTD effectively distinguishes task complexity]\n    B --\x3e B1[\u73b0\u6709\u6307\u6807\u5982\u4e0b\u4e00\u4ee4\u724c\u635f\u5931\u65e0\u6548<br/>Metrics like next-token loss fail]\n    C --\x3e C1[\u8ba1\u7b97\u5b8c\u6574\u6a21\u578b\u4e0e\u6d45\u5c42\u8f85\u52a9\u5934\u7684\u8f93\u51fa\u5206\u5e03KL\u6563\u5ea6<br/>KL divergence between full model and shallow head outputs]\n    C --\x3e C2[\u63d0\u51fa\u6563\u5ea6\u5f15\u5bfc\u89e3\u7801\u65b9\u6cd5<br/>Propose Divergence Steering decoding]\n    D --\x3e D1[MTD\u4e0e\u95ee\u9898\u96be\u5ea6\u6b63\u76f8\u5173<br/>MTD correlates positively with problem difficulty]\n    D --\x3e D2[\u4f4eMTD\u4e0e\u66f4\u51c6\u786e\u7684\u63a8\u7406\u76f8\u5173<br/>Lower MTD associated with more accurate reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] APO: Alpha-Divergence Preference Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (rlhf)], [alpha-divergence, preference optimization, mode collapse, anchored coordinates, gradient variance]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wang Zixian"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," China Mobile Communications Group Shandong Co., Ltd. Tai\u2019an Branch"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22953",children:"https://arxiv.org/pdf/2512.22953"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces APO, an anchored framework using Csisz\xe1r alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[APO: Alpha-Divergence Preference Optimization] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u4e24\u79cd\u5206\u6b67\u6743\u8861 / Two Divergence Trade-off]\n    P1 --\x3e P2[\u524d\u5411KL\u8986\u76d6\u4f46\u4fdd\u5b88 / Forward KL: Mode-Covering but Conservative]\n    P1 --\x3e P3[\u53cd\u5411KL\u5bfb\u6c42\u4f46\u6613\u5d29\u6e83 / Reverse KL: Mode-Seeking but Collapses]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u951a\u5b9a\u6846\u67b6 / Anchored Framework]\n    M1 --\x3e M2[\u4f7f\u7528\u03b1-\u6563\u5ea6\u63d2\u503c / Use \u03b1-Divergence to Interpolate]\n    M2 --\x3e M3[\u8c03\u5ea6\u03b1\u503c / Schedule \u03b1 Value]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u7ade\u4e89\u6027\u6027\u80fd / Competitive Performance]\n    Results --\x3e R2[\u4fdd\u6301\u7a33\u5b9a\u6027 / Maintains Training Stability]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [synthetic data generation], [synthetic dataset, longitudinal data, feedback-driven simulation, behavioral modeling, benchmarking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wafaa El Husseini"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated. Affiliation inferred from email domain (gmail) is insufficient. Likely independent or institutional affiliation not provided in the excerpt."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22956",children:"https://arxiv.org/pdf/2512.22956"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces FLOW, a novel synthetic longitudinal dataset modeling daily interactions between workload, lifestyle, and wellbeing. 2. Provides a configurable data generation tool for reproducible experimentation under adjustable assumptions. 3. Creates a publicly available, controlled experimental environment for methodological development and benchmarking where real-world data is inaccessible."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abeb0ef64c6a9a1178335cb2abf4717155e2b0e41b97fbf7173bb6448bdb8de9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces FLOW, a synthetic longitudinal dataset generated via a rule-based, feedback-driven simulation to model daily interactions between work and wellbeing variables. It addresses the lack of accessible real-world data due to privacy and logistical constraints. The dataset and its configurable generation tool are released as a public resource to support reproducible research, methodological benchmarking, and education."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u7eb5\u5411\u5de5\u4f5c\u4e0e\u798f\u7949\u6570\u636e/Lack of accessible longitudinal work-wellbeing data]\n    B --\x3e B2[\u9690\u79c1\u3001\u4f26\u7406\u548c\u540e\u52e4\u7ea6\u675f/Privacy, ethical, and logistical constraints]\n    C --\x3e C1[\u57fa\u4e8e\u89c4\u5219\u7684\u53cd\u9988\u9a71\u52a8\u6a21\u62df/Rule-based, feedback-driven simulation]\n    C --\x3e C2[\u751f\u6210\u5408\u6210\u6570\u636e\u96c6/Generate synthetic dataset]\n    D --\x3e D1[\u5305\u542b1000\u4e2a\u4e2a\u4f53\u7684\u4e24\u5e74\u6bcf\u65e5\u6570\u636e/2-year daily data for 1000 individuals]\n    D --\x3e D2[\u53ef\u914d\u7f6e\u7684\u6570\u636e\u751f\u6210\u5de5\u5177/Configurable data generation tool]\n    D --\x3e D3[\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90/Publicly available benchmarking resource]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [biomedical signal processing], [multi-scale feature extraction, hierarchical BiLSTM, class-weighted loss, temporal modeling, sleep staging]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shahid Beheshti University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22976",children:"https://arxiv.org/pdf/2512.22976"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a context-aware and interpretable framework combining compact multi-scale feature extraction with hierarchical temporal modeling (BiLSTM) for single-channel EEG sleep staging. 2. Addresses class imbalance, especially for the N1 stage, using class-weighted loss functions and data augmentation techniques. 3. Introduces a sub-epoch chunking and probability averaging strategy to enhance contextual representation and robustness in predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c34e7add689f932eed2a6d2c02f530d1c09fc8e52104033716567656d05392d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel deep learning framework for automatic sleep staging using single-channel EEG. The method integrates multi-scale feature extraction with hierarchical sequence learning (BiLSTM) and employs strategies like chunk-based probability averaging to handle class imbalance and improve context modeling. The approach achieves state-of-the-art performance on the SleepEDF datasets, with a significant improvement in detecting the challenging N1 sleep stage, while maintaining model interpretability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: A Context-Aware Temporal Modeling for EEG Sleep Staging] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u7761\u7720\u5206\u671f/Sleep Staging]\n    B --\x3e B2[\u7c7b\u522b\u4e0d\u5e73\u8861/Class Imbalance]\n    B --\x3e B3[\u6a21\u578b\u53ef\u89e3\u91ca\u6027/Interpretability]\n    C --\x3e C1[\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6/Multi-scale Feature Extraction]\n    C --\x3e C2[\u5206\u5c42\u65f6\u5e8f\u5efa\u6a21/Hierarchical Temporal Modeling (BiLSTM)]\n    C --\x3e C3[\u6570\u636e\u589e\u5f3a\u4e0e\u52a0\u6743\u635f\u5931/Data Augmentation & Weighted Loss]\n    D --\x3e D1[\u603b\u4f53\u51c6\u786e\u7387 89.72%/Overall Accuracy 89.72%]\n    D --\x3e D2[\u5b8f\u5e73\u5747F1\u5206\u6570 85.46%/Macro F1-score 85.46%]\n    D --\x3e D3[N1\u9636\u6bb5F1\u5206\u6570 61.7%/N1 Stage F1-score 61.7%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Fusion or Confusion? Multimodal Complexity Is Not All You Need"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal training], [multimodal learning, late-fusion, hyperparameter tuning, empirical study, reliability checklist]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tillmann Rheude, Roland Eils, Benjamin Wild"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Berlin Institute of Health at Charit\xe9 - Universit\xe4tsmedizin Berlin, Intelligent Medicine Institute at Fudan University, Freie Universit\xe4t Berlin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22991",children:"https://arxiv.org/pdf/2512.22991"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A large-scale benchmark of 19 multimodal architectures under a unified experimental protocol across nine diverse datasets. 2. The proposal of SimBaMM, a simple late-fusion Transformer baseline, which performs comparably to more complex methods under standardized conditions. 3. The provision of a pragmatic reliability checklist to promote robust and trustworthy future evaluations in multimodal learning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d808ed59e8dc58198573816741433c5c0873c34c1d5fe67d63a8ffb9d40342a6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper challenges the assumption that architectural complexity is necessary for performance in multimodal learning. Through a large-scale empirical study, it shows that a simple late-fusion Transformer baseline (SimBaMM) performs comparably to 19 more complex methods when all are rigorously tuned and evaluated under standardized conditions. The authors argue for a shift in research focus from architectural novelty to methodological rigor."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Fusion or Confusion? Multimodal Complexity Is Not All You Need] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u67b6\u6784\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\uff1f<br>Is architectural complexity necessary in multimodal learning?]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u7b80\u5355\u57fa\u7ebfSimBaMM<br>Large-scale empirical study, propose simple baseline SimBaMM]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u590d\u6742\u65b9\u6cd5\u5e76\u4e0d\u7a33\u5b9a\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf<br>Complex methods do not reliably outperform simple baseline]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Merge before Forget: A Single LoRA Continual Learning via Continual Merging"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [continual learning, LoRA, catastrophic forgetting, orthogonal basis, parameter-efficient]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fuli Qiao, Mehrdad Mahdavi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Pennsylvania State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23017",children:"https://arxiv.org/pdf/2512.23017"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel continual learning method that sequentially merges LoRA updates into a single unified LoRA, maintaining constant memory complexity with respect to the number of tasks. 2. Introduces orthogonal basis extraction from previous LoRA to initialize new task learning, minimizing task interference. 3. Employs a time-aware scaling mechanism to balance new and old knowledge during merging, improving performance over asymmetric LoRA merging."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eb3af6647a5e47f9615997c14929688be663c5428838d0271ab503cc1b8c78_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the issues of memory growth and task interference in LoRA-based continual learning for LLMs. It proposes a method that orthogonally initializes and sequentially merges LoRAs into a single LoRA, using a time-aware scaling mechanism. The approach demonstrates effectiveness and efficiency in mitigating catastrophic forgetting while maintaining constant memory usage."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Merge before Forget: A Single LoRA Continual Learning via Continual Merging"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Catastrophic forgetting, memory growth, task interference in LoRA continual learning"] --\x3e P1["\u53c2\u6570\u589e\u957f/Parameter growth"]\n    Problem --\x3e P2["\u4efb\u52a1\u5e72\u6270/Task interference"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Orthogonal initialization & sequential merging of LoRAs"] --\x3e M1["\u6b63\u4ea4\u521d\u59cb\u5316/Orthogonal basis initialization"]\n    Method --\x3e M2["\u6301\u7eed\u5408\u5e76/Continual merging"]\n    Method --\x3e M3["\u65f6\u95f4\u611f\u77e5\u7f29\u653e/Time-aware scaling"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Constant memory, minimized interference, improved performance"] --\x3e R1["\u6052\u5b9a\u5185\u5b58/Constant memory complexity"]\n    Results --\x3e R2["\u6700\u5c0f\u5316\u5e72\u6270/Minimized task interference"]\n    Results --\x3e R3["\u6027\u80fd\u63d0\u5347/Improved performance"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [interpretability], [chain-of-thought, faithfulness, causal mediation analysis, biasing features, explainability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kerem Zaman, Shashank Srivastava"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UNC Chapel Hill"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23032",children:"https://arxiv.org/pdf/2512.23032"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Biasing Features \u6307\u6807\u5c06\u4e0d\u5b8c\u6574\u6027\u8bef\u5224\u4e3a\u4e0d\u5fe0\u5b9e\u6027<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]\n    C --\x3e C1[\u63d0\u51fa faithful@k \u6307\u6807\u5e76\u589e\u52a0\u63a8\u7406\u4ee4\u724c\u9884\u7b97<br/>Propose faithful@k metric & increase inference token budget]\n    C --\x3e C2[\u4f7f\u7528\u56e0\u679c\u4e2d\u4ecb\u5206\u6790<br/>Use Causal Mediation Analysis]\n    D --\x3e D1[\u8bb8\u591a\u88ab\u6807\u8bb0\u4e3a\u4e0d\u5fe0\u5b9e\u7684 CoT \u88ab\u5176\u4ed6\u6307\u6807\u5224\u5b9a\u4e3a\u5fe0\u5b9e<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]\n    D --\x3e D2[\u66f4\u5927\u7684\u4ee4\u724c\u9884\u7b97\u663e\u8457\u63d0\u9ad8\u63d0\u793a\u8bcd\u663e\u5316\u7387<br/>Larger token budgets greatly increase hint verbalization]\n    D --\x3e D3[\u672a\u663e\u5316\u7684\u63d0\u793a\u8bcd\u4ecd\u53ef\u901a\u8fc7 CoT \u56e0\u679c\u4e2d\u4ecb\u9884\u6d4b<br/>Non-verbalized hints can causally mediate predictions through CoT]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Mechanistic Analysis of Circuit Preservation in Federated Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Mechanistic Interpretability, circuit collapse, weight sparsity, Intersection-over-Union, Non-IID data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Lahore University of Management Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23043",children:"https://arxiv.org/pdf/2512.23043"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ha405/FedMI",children:"https://github.com/ha405/FedMI"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a novel mechanistic interpretability (MI) framework to analyze the internal failure mode of FedAvg under Non-IID data, introducing the concept of "circuit collapse". 2. Demonstrates the use of inherently interpretable, weight-sparse neural networks to identify and track functional circuits across clients and communication rounds in FL. 3. Provides the first mechanistic evidence, quantified via Intersection-over-Union (IoU), that Non-IID data causes structural circuit divergence and degradation, reframing statistical drift as a failure of mechanistic preservation.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34be0814c16f6dee307b048c588d94abf81104abdb1f1491d1c23901df46fc7d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper investigates why Federated Learning (FedAvg) performance degrades under Non-IID data by applying Mechanistic Interpretability. The method uses weight-sparse networks to identify and track functional "circuits" across clients, measuring their preservation with IoU. The main conclusion is that Non-IID data causes "circuit collapse" due to conflicting updates, providing a mechanistic explanation for the accuracy drop.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Mechanistic Analysis of Circuit Preservation in Federated Learning] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: FL\u5728Non-IID\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u673a\u5236\u539f\u56e0/Mechanistic cause of FL performance drop under Non-IID data]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7f51\u7edc\u8bc6\u522b\u548c\u8ffd\u8e2a\u7535\u8def/Using interpretable sparse networks to identify & track circuits]\n    D[\u5173\u952e\u7ed3\u679c/Results: Non-IID\u6570\u636e\u5bfc\u81f4\u7535\u8def\u5d29\u6e83\uff0c\u9996\u6b21\u63d0\u4f9b\u673a\u5236\u6027\u8bc1\u636e/Non-IID data causes circuit collapse, first mechanistic evidence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [physics-informed neural networks, multimodal foundation model, partial differential equations, multi-operator learning, zero-shot fine-tuning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Min Zhu, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer, Lu Lu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Yale University, Johns Hopkins University, University of Notre Dame, University of California Los Angeles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23056",children:"https://arxiv.org/pdf/2512.23056"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing PDEs during both pretraining and adaptation, moving beyond purely data-driven approaches. 2. Introduces a method that takes symbolic PDE representations as input and automatically assembles PDE residual losses via vectorized derivative computation, enabling unified physics-informed training across diverse equation families. 3. Demonstrates effective zero-shot physics-informed fine-tuning to unseen PDE families, achieving low error using only PDE residuals and initial/boundary conditions without labeled solution data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db03e9853bb30e32977fcd1b0e24e9037354712e7841661bdbb52b955e3b2849_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PI-MFM, a physics-informed multimodal foundation model framework for solving partial differential equations. It integrates governing equations directly into the training and adaptation process, enabling data-efficient and transferable learning of PDE solution operators. The method outperforms data-driven models, especially with sparse data, and shows strong zero-shot adaptation capabilities to new PDE families."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PI-MFM: Physics-informed multimodal foundation model for solving PDEs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u6709\u591a\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u4e14\u5ffd\u7565\u7269\u7406\u89c4\u5f8b/Existing multi-operator learning is data-hungry and neglects physics]\n    C --\x3e C1[\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u6846\u67b6/Propose physics-informed multimodal foundation model (PI-MFM) framework]\n    C --\x3e C2[\u8f93\u5165\u7b26\u53f7PDE\u8868\u793a\uff0c\u81ea\u52a8\u7ec4\u88c5\u6b8b\u5dee\u635f\u5931/Input symbolic PDEs, auto-assemble residual losses]\n    C --\x3e C3[\u5728\u9884\u8bad\u7ec3\u548c\u9002\u914d\u4e2d\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u63a7\u5236\u65b9\u7a0b/Directly enforce governing equations in pretraining & adaptation]\n    D --\x3e D1[\u572813\u4e2aPDE\u57fa\u51c6\u4e0a\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b/Outperforms data-driven models on 13 PDE benchmarks]\n    D --\x3e D2[\u5728\u7a00\u758f\u6570\u636e\u3001\u566a\u58f0\u4e0b\u66f4\u9c81\u68d2/More robust with sparse data & noise]\n    D --\x3e D3[\u5b9e\u73b0\u96f6\u6837\u672c\u7269\u7406\u4fe1\u606f\u5fae\u8c03\u81f3\u672a\u89c1PDE\u65cf/Achieves zero-shot physics-informed fine-tuning to unseen PDE families]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Reward Model Selection Crisis in Personalized Alignment"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [alignment & personalization], [reward-guided decoding, policy accuracy, Pref-LaMP benchmark]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Edinburgh, Agency for Science, Technology and Research (A*STAR)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23067",children:"https://arxiv.org/pdf/2512.23067"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Reward Model Selection Crisis in Personalized Alignment] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Standard RM accuracy fails to predict deployment performance for personalized alignment]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Introduce policy accuracy metric and Pref-LaMP benchmark for direct evaluation]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Weak correlation between RM & policy accuracy; ICL outperforms reward-guided decoding]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [memory & caching], [Selective State Space Models, analytical differentiation, memory complexity, Tiled Operator-Space Evolution, Phase Gradient Flow]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shuhuan Wang, Yuzhen Xie, Jiayi Li, Yinliang Diao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," South China Agricultural University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23068",children:"https://arxiv.org/pdf/2512.23068"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Phase Gradient Flow (PGF), a framework for computing exact analytical derivatives for SSMs without materializing the intermediate computational graph. 2. Proposes Tiled Operator-Space Evolution (TOSE) to reframe SSM dynamics, achieving O(1) memory complexity relative to sequence length. 3. Demonstrates significant practical improvements, including a 94% reduction in peak VRAM and a 23x throughput increase, enabling chromosome-scale sensitivity analysis on a single GPU."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/821c684e39ce34e6f8c4157ec1cc31189105864e7ee30f0d13f0b32203a73fc5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the O(L) memory bottleneck in gradient-based sensitivity analysis for Selective State Space Models (SSMs). It proposes Phase Gradient Flow (PGF), which uses Tiled Operator-Space Evolution (TOSE) to compute exact analytical derivatives with O(1) memory complexity. This enables the handling of extreme-length sequences (e.g., 128,000 steps) on consumer hardware, significantly reducing memory usage and increasing throughput compared to standard backpropagation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[SSM\u68af\u5ea6\u5206\u6790\u5b58\u5728O(L)\u5185\u5b58\u5899/SSM Gradient Analysis has O(L) Memory Wall]\nC --\x3e C1[\u76f8\u4f4d\u68af\u5ea6\u6d41 (PGF) \u6846\u67b6/Phase Gradient Flow (PGF) Framework]\nC --\x3e C2[\u5e73\u94fa\u7b97\u5b50\u7a7a\u95f4\u6f14\u5316 (TOSE)/Tiled Operator-Space Evolution (TOSE)]\nD --\x3e D1[O(1) \u5185\u5b58\u590d\u6742\u5ea6/O(1) Memory Complexity]\nD --\x3e D2[94% VRAM\u51cf\u5c11, 23x \u541e\u5410\u63d0\u5347/94% VRAM Reduction, 23x Throughput Gain]\nD --\x3e D3[\u652f\u6301\u8d85\u957f\u5e8f\u5217 (128k\u6b65) \u5206\u6790/Enables Extreme-Length (128k-step) Analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [vision-language models], [Mask Fine-Tuning (MFT), Parameter Efficient Fine-Tuning (PEFT), Low-Rank Adaptation (LoRA), structural reparameterization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Northeastern University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23073",children:"https://arxiv.org/pdf/2512.23073"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Ming-K9/MFT-VLM",children:"https://github.com/Ming-K9/MFT-VLM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model's existing knowledge, not just from updating weights."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models<br>\u91cd\u65b0\u601d\u8003\u5fae\u8c03\uff1a\u89e3\u9501\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u80fd\u529b"]\n    Root --\x3e Problem["Problem: Traditional fine-tuning overlooks underutilized structures in pre-trained VLMs.<br>\u6838\u5fc3\u95ee\u9898\uff1a\u4f20\u7edf\u5fae\u8c03\u5ffd\u7565\u4e86\u9884\u8bad\u7ec3VLM\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u7ed3\u6784\u3002"]\n    Root --\x3e Method["Method: Apply Mask Fine-Tuning (MFT) to VLMs for structural reparameterization.<br>\u4e3b\u8981\u65b9\u6cd5\uff1a\u5c06\u63a9\u7801\u5fae\u8c03\uff08MFT\uff09\u5e94\u7528\u4e8eVLM\u8fdb\u884c\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u3002"]\n    Root --\x3e Results["Results: MFT surpasses LoRA and full fine-tuning without altering the backbone.<br>\u5173\u952e\u7ed3\u679c\uff1aMFT\u8d85\u8d8a\u4e86LoRA\u548c\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u4e14\u4e0d\u6539\u53d8\u4e3b\u5e72\u7f51\u7edc\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Mixture-of-Experts, Federated Learning, Load Balancing, Expert Assignment, Non-IID Data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The affiliations are not explicitly listed in the provided content. Based on the author names and common patterns, it is likely from a Chinese university or research institute (e.g., Tsinghua University, Peking University, Chinese Academy of Sciences). A specific institution cannot be reliably inferred."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23070",children:"https://arxiv.org/pdf/2512.23070"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FLEX-MoE, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. 2. Introduces client-expert fitness scores to quantify expert suitability for local datasets using training feedback. 3. Employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4afb2d6d1d993854d25c34b1c5c48d2b0479953fa05feac83a247dd11404202_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenges of deploying Mixture-of-Experts models in Federated Learning, specifically resource constraints on edge devices and expert load imbalance caused by non-IID data. It proposes FLEX-MoE, a framework that uses client-expert fitness scores and an optimization algorithm to assign experts to clients for specialization while balancing system-wide expert utilization. Experiments on three datasets show that FLEX-MoE achieves superior performance and maintains balanced expert utilization in resource-constrained scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem] --\x3e P1[\u8d44\u6e90\u53d7\u9650\u8bbe\u5907/Resource-constrained Edge Devices]\n    Problem --\x3e P2[\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861/Expert Load Imbalance]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method] --\x3e M1[\u5ba2\u6237\u7aef-\u4e13\u5bb6\u9002\u5e94\u5ea6\u5206\u6570/Client-Expert Fitness Scores]\n    Method --\x3e M2[\u57fa\u4e8e\u4f18\u5316\u7684\u4e13\u5bb6\u5206\u914d/Optimization-based Expert Assignment]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u6027\u80fd\u4f18\u8d8a/Superior Performance]\n    Results --\x3e R2[\u4fdd\u6301\u4e13\u5bb6\u8d1f\u8f7d\u5747\u8861/Maintains Balanced Expert Utilization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Trust Region Masking for Long-Horizon LLM Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [trust region, policy gradient, off-policy mismatch, KL divergence, sequence-level masking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," (Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23075",children:"https://arxiv.org/pdf/2512.23075"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Trust Region Masking for Long-Horizon LLM RL] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Off-policy mismatch in LLM-RL<br/>\u5bfc\u81f4\u7ecf\u5178\u4fe1\u4efb\u57df\u8fb9\u754c\u5931\u6548]\n    C --\x3e C1[\u63d0\u51fa\u4fe1\u4efb\u57df\u63a9\u7801(TRM)<br/>Propose Trust Region Masking (TRM)]\n    C1 --\x3e C2[\u5e8f\u5217\u7ea7\u63a9\u7801<br/>Sequence-level Masking]\n    D --\x3e D1[\u63a8\u5bfc\u66f4\u7d27\u7684\u7406\u8bba\u8fb9\u754c<br/>Derive Tighter Bounds (O(T), O(T^{3/2}))]\n    D --\x3e D2[\u63d0\u4f9b\u975e\u5e73\u51e1\u7684\u5355\u8c03\u6539\u8fdb\u4fdd\u8bc1<br/>Provide Non-vacuous Guarantees]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multimodal Functional Maximum Correlation for Emotion Recognition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multimodal learning], [self-supervised learning, dual total correlation, functional maximum correlation analysis, affective computing, physiological signals]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Southeast University, Westlake University, Vrije Universiteit Amsterdam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23076",children:"https://arxiv.org/pdf/2512.23076"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/DY9910/MFMC",children:"https://github.com/DY9910/MFMC"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method's effectiveness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[MFMC for Emotion Recognition] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u60c5\u611f\u72b6\u6001\u8868\u73b0\u4e3a\u8de8\u7cfb\u7edf\u7684\u534f\u8c03\u4f46\u5f02\u8d28\u7684\u751f\u7406\u53cd\u5e94\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u591a\u6a21\u6001\u9ad8\u9636\u4ea4\u4e92\u3002]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faMFMC\u6846\u67b6\uff0c\u901a\u8fc7Dual Total Correlation\u76ee\u6807\u548cFunctional Maximum Correlation Analysis\u6700\u5927\u5316\u9ad8\u9636\u591a\u6a21\u6001\u4f9d\u8d56\u6027\u3002]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6216\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347CEAP-360VR\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5bf9\u4e3b\u4f53\u95f4\u53d8\u5f02\u6027\u9c81\u68d2\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [reinforcement learning, training-inference mismatch, vocabulary pruning, gradient estimation, numerical stability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," (Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23087",children:"https://arxiv.org/pdf/2512.23087"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability "tail" tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability "tail" tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d / Training-Inference Mismatch]\n    B1 --\x3e B2[\u5c3e\u90e8token\u5bfc\u81f4\u68af\u5ea6\u4e0d\u7a33\u5b9a / Tail tokens destabilize gradient estimation]\n    C --\x3e C1[\u52a8\u6001\u526a\u679d\u8bcd\u6c47\u8868 / Dynamic Vocabulary Pruning]\n    C1 --\x3e C2[\u6392\u9664\u6781\u7aef\u5c3e\u90e8token / Exclude extreme tail tokens]\n    D --\x3e D1[\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3 / Achieves stable training]\n    D --\x3e D2[\u7406\u8bba\u754c\u5b9a\u4f18\u5316\u504f\u5dee / Theoretically bounds optimization bias]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Imitation Learning, Reinforcement Learning, KL divergence, Dense Gradient, Sparse Gradient]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yingru Li, Ziniu Li, Jiacai Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly stated in provided content."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23097",children:"https://arxiv.org/pdf/2512.23097"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Hybrid Online RL and IL for LLMs] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Train-inference distribution mismatch in LLM fine-tuning]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Unified framework combining Imitation Learning and Reinforcement Learning]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [reinforcement learning, vision-language model, supervised fine-tuning, generalization paradox, cross-dataset transferability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23090",children:"https://arxiv.org/pdf/2512.23090"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model's ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Benchmark Success, Clinical Failure<br>\u57fa\u51c6\u6210\u529f\uff0c\u4e34\u5e8a\u5931\u8d25] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[RL\u4f18\u5316\u63d0\u5347\u57fa\u51c6\u6027\u80fd\u4f46\u635f\u5bb3\u6cdb\u5316<br>RL improves benchmarks but harms generalization]\n    C --\x3e C1[\u4f7f\u7528SFT+GRPO\u8bad\u7ec3ChexReason VLM<br>Train ChexReason VLM with SFT+GRPO]\n    D --\x3e D1[GRPO\u63d0\u5347CheXpert\u6027\u80fd23%<br>GRPO improves CheXpert by 23%]\n    D --\x3e D2[GRPO\u5bfc\u81f4NIH\u6027\u80fd\u4e0b\u964d19%<br>GRPO degrades NIH by 19%]\n    D --\x3e D3[SFT\u68c0\u67e5\u70b9\u63d0\u5347\u8de8\u6570\u636e\u96c6\u6cdb\u5316<br>SFT checkpoint improves cross-dataset generalization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [self-supervised learning, representation learning, distributed learning, decentralized clustering, contextual data]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Messina, University of Klagenfurt, University of Innsbruck"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23096",children:"https://arxiv.org/pdf/2512.23096"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Introduces Osmotic Learning (OSM-L), a novel self-supervised paradigm for learning from distributed data without raw data exchange. 2. Proposes an "osmosis" process that aligns local representations to converge to a dynamic equilibrium, capturing contextual patterns. 3. Demonstrates that OSM-L functions as a decentralized clustering mechanism, identifying correlated data groups during training.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that extracts higher-level latent knowledge from decentralized data sources without sharing raw data. It achieves this through an iterative "osmosis" process that aligns local representations to converge to a contextual equilibrium, also enabling decentralized clustering. Experimental results show OSM-L achieves high accuracy in local information alignment while preserving contextual integrity.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Extracting meaningful knowledge from distributed, heterogeneous data without raw data exchange]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Osmotic Learning (OSM-L) - self-supervised paradigm using iterative alignment and "osmosis" for representation convergence]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Achieves >0.99 alignment accuracy and preserves contextual integrity; enables decentralized clustering]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [statistical learning theory], [uniform convergence, calibration, low-dimensional structure, vision-language models, sample complexity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Paul M. Thompson"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stevens Institute of Neuroimaging and Informatics, University of Southern California"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23109",children:"https://arxiv.org/pdf/2512.23109"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u73b0\u4ee3\u751f\u6210\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66/\u533b\u7597\u51b3\u7b56\u4e2d\u9700\u8981\u51c6\u786e\u4e14\u6821\u51c6\u826f\u597d\u7684\u6982\u7387\u9884\u6d4b / Modern generative & VLMs need accurate, calibrated predictions for scientific/medical decisions]\n    B --\x3e B2[\u5e73\u5747\u6027\u80fd\u826f\u597d\u65f6\uff0c\u7f55\u89c1\u60c5\u51b5\u6216\u7279\u5b9a\u5b50\u7fa4\u4ecd\u53ef\u80fd\u51fa\u73b0\u5927\u8bef\u5dee / Large errors can persist for rare conditions/subgroups despite low average loss]\n    B --\x3e B3[\u9700\u8981\u4f55\u79cd\u7ed3\u6784\u5047\u8bbe\u624d\u80fd\u5b9e\u73b0\u5177\u6709\u5b9e\u7528\u6837\u672c\u91cf\u7684\u5747\u5300\u6cdb\u5316\uff1f / What structural assumptions enable uniform generalization with practical sample sizes?]\n    C --\x3e C1[\u5206\u6790\u7531\u63d0\u793a\u6216\u8bed\u4e49\u5d4c\u5165\u5728\u53d7\u9650\u8868\u793a\u7a7a\u95f4\u4e2d\u8bf1\u5bfc\u51fa\u7684\u5206\u7c7b\u5668\u65cf / Analyze induced families of classifiers from varying prompts/embeddings in a restricted space]\n    C --\x3e C2[\u5047\u8bbe\u6a21\u578b\u8f93\u51fa\u5bf9\u4f4e\u7ef4\u8bed\u4e49\u8868\u793a\u5e73\u6ed1\u4f9d\u8d56 / Assume model outputs depend smoothly on a low-dimensional semantic representation]\n    C --\x3e C3[\u5e94\u7528\u7ecf\u5178\u5747\u5300\u6536\u655b\u5de5\u5177 / Apply classical uniform convergence tools]\n    D --\x3e D1[\u5728Lipschitz\u7a33\u5b9a\u6027\u4e0b\uff0c\u4e3aVLM\u8bf1\u5bfc\u5206\u7c7b\u5668\u7684\u51c6\u786e\u6027\u548c\u6821\u51c6\u529f\u80fd\u63d0\u4f9b\u6709\u9650\u6837\u672c\u5747\u5300\u6536\u655b\u754c / Provide finite-sample uniform convergence bounds for accuracy & calibration of VLM-induced classifiers under Lipschitz stability]\n    D --\x3e D2[\u6837\u672c\u590d\u6742\u5ea6\u53d6\u51b3\u4e8e\u5185\u5728/\u6709\u6548\u7ef4\u5ea6\uff0c\u800c\u975e\u73af\u5883\u7ef4\u5ea6 / Sample complexity depends on intrinsic/effective dimension, not ambient dimension]\n    D --\x3e D3[\u8c31\u76f8\u5173\u8fb9\u754c\u9610\u660e\u7279\u5f81\u503c\u8870\u51cf\u5982\u4f55\u63a7\u5236\u6570\u636e\u9700\u6c42 / Spectrum-dependent bounds show how eigenvalue decay governs data requirements]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [regression], [squeeze and excitation, channel attention, residual connections, multi-layer perceptron, penetration acceleration]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yankang Li, Changsheng Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanjing University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23131",children:"https://arxiv.org/pdf/2512.23131"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed SE-MLP, a novel MLP architecture integrating a channel attention mechanism for feature prediction. 2. Incorporated residual connections into the MLP framework to enhance model stability and performance. 3. Demonstrated the model's superior accuracy, generalization, and engineering applicability for rapidly predicting penetration acceleration features."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dab35d2c00d22564e8f3e36c067728bb8b4d1bb60f2ed675bfe34288c07503a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SE-MLP, a multi-layer perceptron model enhanced with squeeze-and-excitation channel attention and residual connections, to rapidly predict prior acceleration features for penetration signals. The model establishes a nonlinear mapping from physical parameters to acceleration features, outperforming baseline models like MLP, XGBoost, and Transformer in accuracy and stability. The results validate its feasibility and provide a practical basis for engineering applications in penetration fuse design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u4fb5\u5f7b\u52a0\u901f\u5ea6\u5148\u9a8c\u7279\u5f81\u83b7\u53d6\u8017\u65f6\u8017\u529b/Prior acceleration features are expensive and time-consuming to obtain]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faSE-MLP\u6a21\u578b\uff0c\u96c6\u6210\u901a\u9053\u6ce8\u610f\u529b\u4e0e\u6b8b\u5dee\u8fde\u63a5/Proposed SE-MLP integrating channel attention and residual connections]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u6cdb\u5316\u6027\u597d\uff0c\u5de5\u7a0b\u8bef\u5dee\u53ef\u63a5\u53d7/High prediction accuracy, good generalization, acceptable engineering error]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (rlhf)], [preference optimization, direct preference optimization, self-reflection, invariance, bradley-terry model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yu Li, Tian Lan, Zhengling Qi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," George Washington University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23126",children:"https://arxiv.org/pdf/2512.23126"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO's superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[DPO Limitations<br/>DPO\u7684\u5c40\u9650\u6027]\n    B1 --\x3e B2[Lacks Invariance<br/>\u7f3a\u4e4f\u4e0d\u53d8\u6027]\n    B1 --\x3e B3[Suboptimal Use of Data<br/>\u6570\u636e\u5229\u7528\u6b21\u4f18]\n    C --\x3e C1[Propose InSPO<br/>\u63d0\u51faInSPO]\n    C1 --\x3e C2[Globally Optimal Policy<br/>\u5168\u5c40\u6700\u4f18\u7b56\u7565]\n    C2 --\x3e C3[Conditions on Context & Alternative<br/>\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4e0e\u5907\u9009\u7b54\u6848]\n    D --\x3e D1[Theoretical Superiority<br/>\u7406\u8bba\u4f18\u8d8a\u6027]\n    D --\x3e D2[Practical Improvement<br/>\u5b9e\u9645\u63d0\u5347]\n    D2 --\x3e D3[Better Win Rates<br/>\u66f4\u9ad8\u7684\u80dc\u7387]\n    D2 --\x3e D4[No Inference Overhead<br/>\u65e0\u63a8\u7406\u5f00\u9500]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [machine learning security], [TTPs, threat graph, multi-agent RAG, model stealing, jailbreaking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Armstrong Foundjem, Lionel Nganyewou Tidjon, Leuson Da Silva, Foutse Khomh"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Polytechnique Montr\xe9al (based on author affiliations and sMIEEE notation)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23132",children:"https://arxiv.org/pdf/2512.23132"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a large-scale empirical analysis of ML security, extracting 93 distinct threats from multiple sources including real-world incidents and code repositories. 2. Developed a multi-agent RAG system to automatically build an ontology-driven threat graph linking TTPs, vulnerabilities, and lifecycle stages from over 300 articles. 3. Identified unreported threats and dominant attack patterns (e.g., commercial LLM API model stealing, preference-guided jailbreaks) and highlighted vulnerability clusters in ML libraries with poor patch propagation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3da226bc9e7639392b95f6f00808f162580d1a90050c1261b3a0703259e42cf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper characterizes modern security risks in AI systems by analyzing threats from multiple sources and using a multi-agent RAG system to construct a threat graph. The analysis uncovers unreported attack vectors and dominant TTPs, concluding that adaptive, ML-specific security frameworks are urgently needed to mitigate supply-chain and inference-time risks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u4f20\u7edf\u5b89\u5168\u6846\u67b6\u7f3a\u4e4f\u9488\u5bf9ML\u7684\u5a01\u80c1\u5efa\u6a21/Traditional cybersecurity lacks ML-specific threat modeling]\nC --\x3e C1[\u591a\u667a\u80fd\u4f53RAG\u7cfb\u7edf\u5206\u6790\u5a01\u80c1/Multi-agent RAG system analyzes threats]\nC1 --\x3e C2[\u6784\u5efa\u672c\u4f53\u9a71\u52a8\u7684\u5a01\u80c1\u56fe\u8c31/Builds ontology-driven threat graph]\nD --\x3e D1[\u8bc6\u522b\u672a\u62a5\u544a\u7684\u5a01\u80c1/Identifies unreported threats]\nD --\x3e D2[\u53d1\u73b0\u4e3b\u8981\u7684\u653b\u51fbTTPs/Identifies dominant attack TTPs]\nD --\x3e D3[\u5f3a\u8c03\u81ea\u9002\u5e94ML\u5b89\u5168\u6846\u67b6\u7684\u5fc5\u8981\u6027/Highlights need for adaptive ML security frameworks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Principled Algorithms for Optimizing Generalized Metrics in Binary Classification"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [binary classification], [metric optimization, H-consistency, surrogate loss, cost-sensitive learning, METRO]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Anqi Mao, Mehryar Mohri, Yutao Zhong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Courant Institute of Mathematical Sciences (NYU), Google Research"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23133",children:"https://arxiv.org/pdf/2512.23133"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Reformulated the optimization of generalized classification metrics (e.g., F\u03b2, Jaccard) as a generalized cost-sensitive learning problem. 2. Designed novel surrogate loss functions with provable H-consistency guarantees for this problem. 3. Developed the METRO algorithm with strong finite-sample generalization bounds, offering a principled alternative to existing threshold-based methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57d73e082d311622a2b2e8ab3bd4da18cd359831ac4e0139fd4555ea2ba93861_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of directly optimizing non-decomposable binary classification metrics like the F\u03b2-measure. The authors propose a principled framework that reformulates metric optimization as a cost-sensitive learning problem, leading to new surrogate losses and the METRO algorithm. Experiments show the proposed method is effective and outperforms prior baseline approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Principled Algorithms for Optimizing Generalized Metrics in Binary Classification] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4f18\u5316\u4e0d\u5e73\u8861/\u4ee3\u4ef7\u654f\u611f\u6307\u6807\u5982F\u03b2, Jaccard/Optimizing imbalanced/cost-sensitive metrics e.g., F\u03b2, Jaccard]\n    B --\x3e B2[\u73b0\u6709\u57fa\u4e8e\u9608\u503c\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1/Existing threshold-based methods lack theoretical guarantees]\n    C --\x3e C1[\u5c06\u6307\u6807\u4f18\u5316\u91cd\u6784\u4e3a\u4ee3\u4ef7\u654f\u611f\u5b66\u4e60\u95ee\u9898/Reformulate metric optimization as cost-sensitive learning]\n    C --\x3e C2[\u8bbe\u8ba1\u5177\u6709H-\u4e00\u81f4\u6027\u7684\u66ff\u4ee3\u635f\u5931\u51fd\u6570/Design surrogate losses with H-consistency]\n    C --\x3e C3[\u63d0\u51faMETRO\u7b97\u6cd5/Propose METRO algorithm]\n    D --\x3e D1[\u63d0\u4f9b\u6709\u9650\u6837\u672c\u6cdb\u5316\u4fdd\u8bc1/Provide finite-sample generalization bounds]\n    D --\x3e D2[\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u57fa\u7ebf/Experiments show superiority over baselines]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [graph neural networks, transformer, dynamic functional connectivity, longitudinal fMRI, multimodal fusion]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Runzhi Zhou, Xi Luo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Texas Health Science Center at Houston"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23137",children:"https://arxiv.org/pdf/2512.23137"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel time-aware Graph Neural Network model with Transformer Fusion (GNN-TF) for integrating non-Euclidean brain connectivity and Euclidean tabular data. 2. Introduces an end-to-end framework that leverages the temporal order of longitudinal data for forecasting future clinical outcomes. 3. Demonstrates superior predictive performance for forecasting future tobacco use compared to established machine learning and deep learning models on a longitudinal fMRI dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6106dd6744068e14ed98be012be1d450afa8c685980d7b00ae696592ebd7665_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces GNN-TF, a time-aware model that integrates dynamic brain connectivity graphs and tabular data using a transformer for fusion, to forecast future tobacco use. It is evaluated on longitudinal fMRI data from the NCANDA study and is shown to outperform other state-of-the-art methods in predictive accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Integrating non-Euclidean brain imaging and Euclidean tabular data for forecasting future outcomes in longitudinal studies is challenging.)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Proposes a time-aware Graph Neural Network with Transformer Fusion (GNN-TF) to integrate dynamic brain connectivity and tabular data.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: GNN-TF outperforms state-of-the-art models in predicting future tobacco use on the NCANDA dataset.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Weak Signal Learning Dataset and Its Baseline Method"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [weak signal learning], [weak signal learning, dual-view representation, class imbalance, low SNR, multi-source complementarity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xianqi Liu, Xiangru Li, Lefeng He, Ziyu Fang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," South China Normal University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23160",children:"https://arxiv.org/pdf/2512.23160"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Constructed the first specialized dataset for weak signal feature learning, featuring low SNR dominance and extreme class imbalance., 2. Proposed a dual-view representation (vector + time-frequency map) and the PDVFN model tailored for low SNR, distribution skew, and dual imbalance., 3. Established a foundational benchmark for future weak signal learning (WSL) research, demonstrating improved accuracy and robustness in challenging scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/426f082649cd556014218d670accdb2545dfd625d214b420b39fcce46739f01d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of weak signal learning (WSL) by constructing the first dedicated dataset with low SNR and class imbalance, and proposes a dual-view PDVFN model to extract complementary features. The method shows higher accuracy and robustness in handling weak signals, noise, and imbalance. This work provides a dataset, baseline model, and foundation for future WSL research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A Weak Signal Learning Dataset and Its Baseline Method<br>\u5f31\u4fe1\u53f7\u5b66\u4e60\u6570\u636e\u96c6\u53ca\u5176\u57fa\u7ebf\u65b9\u6cd5] --\x3e B(Problem: Lack of dedicated datasets for weak signal learning<br>\u6838\u5fc3\u95ee\u9898: \u7f3a\u4e4f\u4e13\u95e8\u7684\u5f31\u4fe1\u53f7\u5b66\u4e60\u6570\u636e\u96c6)\n    A --\x3e C(Method: Propose dual-view representation & PDVFN model<br>\u4e3b\u8981\u65b9\u6cd5: \u63d0\u51fa\u53cc\u89c6\u56fe\u8868\u793a\u548cPDVFN\u6a21\u578b)\n    A --\x3e D(Results: Higher accuracy/robustness, provides dataset & baseline<br>\u5173\u952e\u7ed3\u679c: \u66f4\u9ad8\u51c6\u786e\u7387/\u9c81\u68d2\u6027\uff0c\u63d0\u4f9b\u6570\u636e\u96c6\u548c\u57fa\u7ebf)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Diffusion-based Decentralized Federated Multi-Task Representation Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [decentralized learning, multi-task representation learning, projected gradient descent, diffusion-based consensus, sample complexity]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Donghwa Kang, Shana Moothedath"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Iowa State University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23161",children:"https://arxiv.org/pdf/2512.23161"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel decentralized projected gradient descent-based algorithm for multi-task representation learning using a diffusion-based communication strategy. 2. Provides theoretical guarantees, including a lower bound on sample complexity and an upper bound on iteration complexity for the proposed algorithm. 3. Demonstrates through analysis and simulations that the algorithm is fast, communication-efficient, and outperforms benchmark methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb19ab164bf383bc246da4c65f212d69031d00619ab4c384a67af532a30b00e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of decentralized multi-task representation learning, where multiple linear regression models share a common low-dimensional representation across a network of nodes. The authors propose a diffusion-based decentralized algorithm using alternating projected gradient descent and minimization to recover the shared feature matrix. Theoretical analysis proves the algorithm's efficiency in terms of sample and iteration complexity, and numerical simulations validate its superior performance compared to benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Diffusion-based Decentralized Federated Multi-Task Representation Learning") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Decentralized Multi-Task Representation Learning is underexplored")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Diffusion-based Decentralized Projected Gradient Descent Algorithm")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Provable guarantees, Fast and communication-efficient")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Evaluating Parameter Efficient Methods for RLVR"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Parameter-Efficient Fine-Tuning, Reinforcement Learning with Verifiable Rewards, LoRA, Spectral Collapse, Mathematical Reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qingyu Yin, Yulun Wu, Zhennan Shen, Sunbowen Li, Zhilin Wang, Yanshu Li, Chak Tou Leong, Jiale Kang, Jinjin Gu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang University, HKUST, WUST, USTC, Brown University, Hong Kong Polytechnic University, INSAIT"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23165",children:"https://arxiv.org/pdf/2512.23165"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Conducted the first comprehensive evaluation of over 12 PEFT methods for RLVR, challenging the default use of standard LoRA. 2. Identified that structural PEFT variants (DoRA, AdaLoRA, MiSS) consistently outperform LoRA in this setting. 3. Discovered and explained the failure of SVD-informed initialization methods (e.g., PiSSA) due to a "spectral collapse" phenomenon and misalignment with RL optimization.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87ad6f372a6a1a3b13e37f8468a6816e52a585402f7e4505a01391ffaed0621c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper systematically evaluates Parameter-Efficient Fine-Tuning (PEFT) methods for Reinforcement Learning with Verifiable Rewards (RLVR) on mathematical reasoning tasks. It finds that structural variants like DoRA outperform standard LoRA, while SVD-based methods fail due to spectral collapse, and extreme parameter reduction bottlenecks performance. The work provides a guide for selecting PEFT methods in RLVR."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Evaluating Parameter Efficient Methods for RLVR] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[RLVR\u4e2d\u6700\u4f73PEFT\u67b6\u6784\u672a\u77e5 / Optimal PEFT architecture for RLVR is unknown]\n    C --\x3e C1[\u7cfb\u7edf\u8bc4\u4f3012+\u79cdPEFT\u65b9\u6cd5 / Systematically evaluate 12+ PEFT methods]\n    C --\x3e C2[\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u6d4b\u8bd5 / Test on mathematical reasoning benchmarks]\n    D --\x3e D1[\u7ed3\u6784\u53d8\u4f53\u4f18\u4e8e\u6807\u51c6LoRA / Structural variants outperform standard LoRA]\n    D --\x3e D2[SVD\u521d\u59cb\u5316\u5bfc\u81f4\u8c31\u5d29\u6e83 / SVD initialization causes spectral collapse]\n    D --\x3e D3[\u6781\u7aef\u53c2\u6570\u51cf\u5c11\u635f\u5bb3\u63a8\u7406\u80fd\u529b / Extreme parameter reduction harms reasoning]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [LLM planning, Monte Carlo Tree Search (MCTS), multi-agent architecture, symbolic reasoning, self-correction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IBM T.J. Watson Research Center, Vanderbilt University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23167",children:"https://arxiv.org/pdf/2512.23167"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/IBM/SPIRAL",children:"https://github.com/IBM/SPIRAL"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs falter at complex planning, linear reasoning lacks self-correction]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Integrates three LLM agents (Planner, Simulator, Critic) into MCTS loop]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA agents, achieves 83.6% accuracy on DailyLifeAPIs, superior token efficiency]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [vertical federated learning, machine unlearning, primal-dual optimization, sample unlearning, label unlearning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yu Jiang, Xindi Tong, Ziyao Liu, Xiaoxi Zhang, Kwok-Yan Lam, Chee Wei Tan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nanyang Technological University, Sun Yat-sen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23171",children:"https://arxiv.org/pdf/2512.23171"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FedORA, a primal-dual optimization framework for sample and label unlearning in Vertical Federated Learning (VFL). 2. Introduces a new unlearning loss function that promotes classification uncertainty instead of misclassification. 3. Employs an adaptive step size and an asymmetric batch design to enhance stability and reduce computational costs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05dcd254c3941fc5cbf6bc3c063f2a726b8607659a3f7b4a526ad900e9e2b5de_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of data removal (unlearning) in Vertical Federated Learning (VFL), where different parties hold different features of the same data samples. The authors propose FedORA, a method that formulates unlearning as a constrained optimization problem solved via a primal-dual algorithm. Experiments show FedORA achieves unlearning effectiveness and model utility comparable to retraining from scratch, but with lower computational and communication costs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning"]\n    Root --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Unlearning in VFL is challenging due to distributed features and cross-party coordination."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose FedORA: a primal-dual optimization framework with a new uncertainty-promoting loss."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Achieves effective unlearning & utility preservation with lower overhead vs. retraining."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [molecular language modeling], [HELM notation, DeBERTa, cyclic peptide, membrane permeability, peptide-protein interaction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Seungeon Lee, Takuto Koyama, Itsuki Maeda, Shigeyuki Matsumoto, Yasushi Okuno"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kyoto University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23175",children:"https://arxiv.org/pdf/2512.23175"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes HELM-BERT, the first encoder-based peptide language model trained on HELM notation, designed to capture hierarchical dependencies. 2. Pre-trains the model on a curated corpus of 39,079 chemically diverse linear and cyclic peptides. 3. Demonstrates superior performance over SMILES-based models in downstream tasks like cyclic peptide membrane permeability and peptide-protein interaction prediction."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83ec939e21fe471473f433077555782bca673e92ad1bfd3578b0fe729e20446_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces HELM-BERT, a transformer model based on DeBERTa and trained on HELM notation to better represent therapeutic peptides. It shows that this approach significantly outperforms existing SMILES-based models in predicting key peptide properties, demonstrating the data-efficiency advantages of topology-aware representations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u73b0\u6709\u5206\u5b50\u8868\u793a(\u5982SMILES)\u65e0\u6cd5\u6709\u6548\u6355\u6349\u80bd\u7684\u5316\u5b66\u4e0e\u62d3\u6251\u590d\u6742\u6027/Existing molecular representations fail to capture peptide complexity]\n    C --\x3e C1[\u57fa\u4e8eDeBERTa, \u4f7f\u7528HELM\u7b26\u53f7\u8bad\u7ec3\u9996\u4e2a\u7f16\u7801\u5668\u80bd\u8bed\u8a00\u6a21\u578b/Based on DeBERTa, first encoder peptide LM trained on HELM notation]\n    C --\x3e C2[\u572839,079\u4e2a\u591a\u6837\u5316\u80bd\u7684\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3/Pre-trained on a corpus of 39,079 diverse peptides]\n    D --\x3e D1[\u5728\u819c\u6e17\u900f\u6027\u548c\u80bd-\u86cb\u767d\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8eSMILES\u6a21\u578b/Significantly outperforms SMILES models on permeability & interaction prediction]\n    D --\x3e D2[HELM\u8868\u793a\u63d0\u4f9b\u6570\u636e\u6548\u7387\u4f18\u52bf/HELM representations offer data-efficiency advantages]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image classification], [vocal cord ultrasound, image segmentation, VIPRnet, vocal cord paralysis, classification model]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Milwaukee School of Engineering, Emory University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23177",children:"https://arxiv.org/pdf/2512.23177"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Project VIPR: Machine Learning-Assisted Vocal Cord Ultrasound Examination] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: VCUS accuracy is operator-dependent]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use ML models for vocal cord segmentation and VCP classification]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Segmentation accuracy 96%, VIPRnet classification accuracy 99%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PGOT: A Physics-Geometry Operator Transformer for Complex PDEs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [neural operator learning], [Physics-Geometry Operator Transformer, Spectrum-Preserving Geometric Attention, geometric aliasing, linear complexity, spatially adaptive routing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhuo Zhang, Xi Yang, Yuan Zhao, Canqun Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology, National SuperComputer Center in Tianjin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23192",children:"https://arxiv.org/pdf/2512.23192"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes PGOT, a novel transformer architecture designed to reconstruct physical feature learning through explicit geometry awareness for solving PDEs on complex geometries. 2. Introduces Spectrum-Preserving Geometric Attention (SpecGeo-Attention), which uses a physics slicing-geometry injection mechanism to incorporate multi-scale geometric encodings, preserving critical boundary information while maintaining linear computational complexity. 3. Implements a dynamic routing mechanism that adaptively selects low-order linear paths for smooth regions and high-order non-linear paths for discontinuities, enabling high-precision, spatially adaptive modeling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1247f066008cc6ba78670544cbf446b4e7c3e18fb9b55a53416b7a831c93e80f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of modeling PDEs on large-scale unstructured meshes with complex geometries using transformers, where efficient architectures often lose critical boundary information due to geometric aliasing. It proposes the Physics-Geometry Operator Transformer (PGOT), which introduces a geometry-aware attention mechanism and adaptive computational routing to preserve multi-scale features and model shocks precisely. PGOT achieves state-of-the-art performance on standard benchmarks and excels in large-scale industrial design tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PGOT: A Physics-Geometry Operator Transformer for Complex PDEs] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[Transformers\u5efa\u6a21\u590d\u6742\u51e0\u4f55PDEs\u7684\u6311\u6218/Challenge: Modeling PDEs on complex geometries]\n    B --\x3e B2[\u51e0\u4f55\u6df7\u53e0\u5bfc\u81f4\u8fb9\u754c\u4fe1\u606f\u4e22\u5931/Geometric Aliasing loses boundary info]\n    C --\x3e C1[\u63d0\u51faPGOT\u6846\u67b6/Propose PGOT framework]\n    C --\x3e C2[\u8c31\u4fdd\u6301\u51e0\u4f55\u6ce8\u610f\u529b/SpecGeo-Attention]\n    C --\x3e C3[\u52a8\u6001\u8def\u5f84\u8def\u7531/Dynamic path routing]\n    D --\x3e D1[\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5SOTA/SOTA on four benchmarks]\n    D --\x3e D2[\u5de5\u4e1a\u8bbe\u8ba1\u4efb\u52a1\u4f18\u5f02/Excels in industrial design tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [online learning], [Online Newton Step, Mahalanobis projection, regret minimization, exp-concave optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yi-Han Wang, Peng Zhao, Zhi-Hua Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National Key Laboratory for Novel Software Technology, Nanjing University; School of Artificial Intelligence, Nanjing University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23190",children:"https://arxiv.org/pdf/2512.23190"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes LightONS, a simple variant of ONS that reduces computational cost by delaying expensive Mahalanobis projections via a hysteresis mechanism. 2. Achieves optimal O(d log T) regret with a total runtime of O(d\xb2T + d^\u03c9 \u221a(T log T)), improving over ONS's O(d^\u03c9 T) runtime. 3. Provides an SXO algorithm with runtime ~O(d\xb3/\u03b5), solving the COLT'13 open problem posed by Koren."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2632ee5796bfd9a4795aa8e1212d8f65e7f8732a96264f4ae4629a2216e3e5ba_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the computational bottleneck of the Online Newton Step (ONS) algorithm for online exp-concave optimization, where the Mahalanobis projection step is costly. The authors propose LightONS, a simple variant that introduces a hysteresis mechanism to delay expensive projections, preserving optimal regret while significantly reducing runtime. This leads to an efficient stochastic optimization method that resolves a long-standing open problem."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A["A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization<br/>\u5728\u7ebf\u6307\u6570\u51f9\u4f18\u5316\u7684\u7b80\u5355\u3001\u6700\u4f18\u4e14\u9ad8\u6548\u7b97\u6cd5"] --\x3e B["\u6838\u5fc3\u95ee\u9898/Problem"]\n    A --\x3e C["\u4e3b\u8981\u65b9\u6cd5/Method"]\n    A --\x3e D["\u5173\u952e\u7ed3\u679c/Results"]\n    B --\x3e B1["\u5728\u7ebf\u6307\u6570\u51f9\u4f18\u5316(OXO)\u4e2d\uff0cONS\u7b97\u6cd5\u7684Mahalanobis\u6295\u5f71\u8ba1\u7b97\u6210\u672c\u9ad8<br/>High computational cost of Mahalanobis projection in ONS for OXO"]\n    B --\x3e B2["\u968f\u673a\u6307\u6570\u51f9\u4f18\u5316(SXO)\u5b58\u5728COLT\'13\u5f00\u653e\u95ee\u9898<br/>COLT\'13 open problem for SXO"]\n    C --\x3e C1["\u63d0\u51faLightONS\u7b97\u6cd5<br/>Propose LightONS algorithm"]\n    C --\x3e C2["\u5229\u7528\u8fdf\u6ede\u673a\u5236\u5ef6\u8fdf\u6602\u8d35\u6295\u5f71<br/>Delay expensive projections via hysteresis mechanism"]\n    D --\x3e D1["\u4fdd\u6301\u6700\u4f18O(d log T)\u9057\u61be<br/>Preserves optimal O(d log T) regret"]\n    D --\x3e D2["\u603b\u8fd0\u884c\u65f6\u95f4\u964d\u81f3O(d\xb2T + d^\u03c9 \u221a(T log T))<br/>Total runtime reduced to O(d\xb2T + d^\u03c9 \u221a(T log T))"]\n    D --\x3e D3["\u89e3\u51b3SXO\u5f00\u653e\u95ee\u9898\uff0c\u8fd0\u884c\u65f6\u95f4~O(d\xb3/\u03b5)<br/>Solves SXO open problem with runtime ~O(d\xb3/\u03b5)"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [Ordered Layer Freezing, Tensor Operation Approximation, Non-IID Data, Edge Computing, Model Compression]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ziru Niu, Hai Dong, A.K. Qin, Tao Gu, Pengcheng Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," RMIT University, Swinburne University of Technology, Macquarie University, Hohai University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23200",children:"https://arxiv.org/pdf/2512.23200"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed Federated Learning with Ordered Layer Freezing (FedOLF), a method that freezes model layers in a predefined order before training to reduce computation and memory requirements. 2. Introduced Tensor Operation Approximation (TOA) as a lightweight alternative to conventional quantization to further reduce communication and energy costs while preserving model accuracy. 3. Demonstrated superior performance of FedOLF over non-IID data across multiple datasets and model architectures, achieving higher accuracy, energy efficiency, and lower memory footprint compared to existing works."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cf67406a118e6d8eff8ded5d05e3b12c6eeded41300a0b8517fec19cbc8d930_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces FedOLF, a federated learning method that freezes model layers in a predefined order to reduce resource demands on edge devices, combined with a Tensor Operation Approximation technique for efficient communication. The proposed approach is shown to achieve higher accuracy and better energy/memory efficiency than existing methods when training on non-IID data across several benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Energy and Memory-Efficient Federated Learning with Ordered Layer Freezing] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[FL\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u4e0a\u6548\u7387\u4f4e/FL inefficiency on resource-constrained IoT devices]\n    C --\x3e C1[\u6709\u5e8f\u5c42\u51bb\u7ed3/FedOLF: Ordered Layer Freezing]\n    C --\x3e C2[\u5f20\u91cf\u64cd\u4f5c\u8fd1\u4f3c/TOA: Tensor Operation Approximation]\n    D --\x3e D1[\u66f4\u9ad8\u51c6\u786e\u7387/Higher accuracy on non-IID data]\n    D --\x3e D2[\u66f4\u9ad8\u80fd\u6548\u4e0e\u66f4\u4f4e\u5185\u5b58\u5360\u7528/Higher energy efficiency & lower memory footprint]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Anka: A Domain-Specific Language for Reliable LLM Code Generation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Domain-Specific Language, Constrained Syntax, Code Generation, Data Transformation Pipeline, In-Context Learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Saif Khalfan Saif Al Mazrouei"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Wisconsin-Madison"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23214",children:"https://arxiv.org/pdf/2512.23214"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Anka: A Domain-Specific Language for Reliable LLM Code Generation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs make systematic errors in complex multi-step code generation]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Design Anka, a constrained DSL for data transformation pipelines]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: High parse success & task accuracy; Anka outperforms Python on multi-step tasks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [graph federated learning, fairness, overlapping subgraphs, privacy-preserving, weighted aggregation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Xi'an Jiaotong University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23235",children:"https://arxiv.org/pdf/2512.23235"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Uncover and theoretically analyze the unfairness issue in graph federated learning caused by imbalanced overlapping subgraphs across clients. 2. Propose FairGFL, a novel algorithm that uses a privacy-preserving estimation of overlapping ratios and an interpretable weighted aggregation approach to enhance cross-client fairness. 3. Improve the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a fairness problem in graph federated learning when client subgraphs overlap in an imbalanced way. To solve this, it proposes FairGFL, a method that uses privacy-preserving overlap estimation and a fairness-aware regularizer to balance utility and fairness. Experiments show FairGFL outperforms baselines in both utility and fairness on benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA(FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs) --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Imbalanced overlapping subgraphs cause unfairness in GFL)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: FairGFL with privacy-preserving overlap estimation, weighted aggregation, and fairness regularizer)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Outperforms baselines in model utility and fairness)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [gpu kernels], [agentic kernel coding, heterogeneous accelerators, retrieval-augmented prompt synthesis, graph-based search, Triton/CuTe DSL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Meta Platforms"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23236",children:"https://arxiv.org/pdf/2512.23236"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[KernelEvolve: Scaling Agentic Kernel Coding] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[DLRM\u8bad\u7ec3/\u63a8\u7406\u6548\u7387<br/>DLRM Training/Inference Efficiency]\n    B --\x3e B2[\u6a21\u578b\u3001\u5185\u6838\u3001\u786c\u4ef6\u5f02\u6784\u6027<br/>Model, Kernel, Hardware Heterogeneity]\n    C --\x3e C1[\u667a\u80fd\u5185\u6838\u7f16\u7801\u6846\u67b6<br/>Agentic Kernel Coding Framework]\n    C --\x3e C2[\u591a\u62bd\u8c61\u5c42: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]\n    C --\x3e C3[\u56fe\u641c\u7d22\u4e0e\u68c0\u7d22\u589e\u5f3a\u63d0\u793a<br/>Graph Search & Retrieval-Augmented Prompt]\n    D --\x3e D1[100%\u6b63\u786e\u7387, 17\u500d\u52a0\u901f<br/>100% Correctness, 17x Speedup]\n    D --\x3e D2[\u5f00\u53d1\u65f6\u95f4: \u6570\u5468->\u6570\u5c0f\u65f6<br/>Dev Time: Weeks->Hours]\n    D --\x3e D3[\u964d\u4f4e\u65b0\u786c\u4ef6\u7f16\u7a0b\u58c1\u5792<br/>Reduces New Hardware Programmability Barrier]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [flow matching, inverse problem, uniqueness, generative AI, continuity equation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Alexander Korotin, Gudmund Pammer"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Applied AI Institute, Graz University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23265",children:"https://arxiv.org/pdf/2512.23265"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formally defines the inverse problem of flow matching (FM) for distributions with finite exponential moment, 2. Establishes the uniqueness of the solution to the inverse FM problem in the one-dimensional (D=1) setting, 3. Establishes the uniqueness of the solution to the inverse FM problem in the Gaussian case."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3894b8881e76b35830e6504158fd61b8ec4e18f8adfb99a0b03911f3928bd297_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the inverse problem of flow matching, aiming to recover the original transport plan given the initial distribution and the learned velocity field. It proves that the solution to this inverse problem is unique in two specific cases: one-dimensional distributions and Gaussian distributions. The general multidimensional case remains an open problem for future research."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["On the Inverse Flow Matching Problem<br>\u9006\u6d41\u5339\u914d\u95ee\u9898"] --\x3e Problem["\u7814\u7a76\u9006\u6d41\u5339\u914d\u95ee\u9898<br>Inverse Flow Matching Problem"]\n    Root --\x3e Method["\u5206\u6790\u4e00\u7ef4\u4e0e\u9ad8\u65af\u60c5\u5f62<br>Analyze 1D & Gaussian Cases"]\n    Root --\x3e Results["\u8bc1\u660e\u89e3\u7684\u552f\u4e00\u6027<br>Prove Solution Uniqueness"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PFed-Signal: An ADR Prediction Model based on Federated Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [federated learning, transformer, euclidean distance, adverse drug reaction, data cleaning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tao Li, Peilin Li, Kui Lu, Yilei Wang, Junliang Shang, Guangshun Li, Huiyu Zhou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Qufu Normal University, University of Leicester"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23262",children:"https://arxiv.org/pdf/2512.23262"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed PFed-Split, a method to split the original FAERS dataset based on Adverse Drug Reactions (ADRs). 2. Introduced a federated learning-based biased data identification method that uses Euclidean distance to filter out noisy records and generate a clean dataset. 3. Developed an ADR prediction model based on the Transformer architecture, trained on the cleaned dataset, which achieves superior performance in accuracy and signal detection metrics (ROR, PRR) compared to traditional statistical methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c6bff5c67d3939353727926690ff57b0800331885e90983cf3850de46090975_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes PFed-Signal, a federated learning-based model for predicting Adverse Drug Reactions (ADRs). The method first cleans biased data from the FAERS database using Euclidean distance within a federated framework and then trains a Transformer model on the cleaned data for prediction. The results show that this approach outperforms traditional statistical methods in key metrics like accuracy, F1 score, and AUC."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PFed-Signal: An ADR Prediction Model based on Federated Learning] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Biased data in FAERS leads to inaccurate ADR prediction)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Use federated learning & Euclidean distance to clean data, then train Transformer model)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Higher accuracy, F1, recall, AUC than baselines; improved ROR/PRR)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Sparse Autoencoders (SAEs), Low-Rank Adaptation (LoRA), Safety Alignment, Interpretability, Parameter-efficient Fine-tuning (PEFT)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beijing University of Posts and Telecommunications"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23260",children:"https://arxiv.org/pdf/2512.23260"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("LoRA\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027/LoRA lacks interpretability")\n    Problem --\x3e P2("\u5b50\u7a7a\u95f4\u5b66\u4e60\u662f\u9ed1\u76d2\u7684/Subspace learning is black-box")\n    Method --\x3e M1("\u5229\u7528\u9884\u8bad\u7ec3SAE/Use pre-trained SAEs")\n    Method --\x3e M2("\u6784\u5efa\u663e\u5f0f\u4f4e\u79e9\u5b50\u7a7a\u95f4/Construct explicit low-rank subspace")\n    Results --\x3e R1("\u9ad8\u5b89\u5168\u738799.6%/High safety rate 99.6%")\n    Results --\x3e R2("\u53c2\u6570\u9ad8\u65480.19%/Parameter-efficient 0.19%")\n    Results --\x3e R3("\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027/Provides interpretability")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [Energy Systems Optimization], [Modeling to Generate Alternatives, Interpretable Machine Learning, Decision Trees, Energy System Optimization Model, E-molecules]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mahdi Kchaou, Francesco Contino, Diederik Coppitters"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Mechanics, Materials and Civil Engineering (iMMC), Universit\xe9 catholique de Louvain (UCLouvain)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23284",children:"https://arxiv.org/pdf/2512.23284"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Applied Modeling to Generate Alternatives (MGA) to produce a diverse set of near-cost-optimal e-molecule import pathway designs, moving beyond a single optimal solution. 2. Used interpretable machine learning (specifically decision trees) to extract high-level insights and design archetypes from the complex, multi-dimensional solution space generated by MGA. 3. Demonstrated the flexibility of hydrogen import pathways, showing that specific technologies (solar, wind, storage) are not strictly required to stay within a 10% cost margin, and revealed how constraints shift the preferred design archetypes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b329c74249d56df55cd55db503dedb7b277979173c0419ff415764d3a34be11_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of single cost-optimal designs for green e-molecule import pathways by using Modeling to Generate Alternatives to create diverse near-optimal solutions and then applying interpretable machine learning to analyze them. The method is applied to hydrogen import pathways considering different carriers. The main finding is a broad near-optimal space with significant flexibility, where specific renewable sources are not strictly necessary, and constraints shift preferences toward different carrier and technology combinations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u63ed\u793a\u7535\u5b50\u5206\u5b50\u8fdb\u53e3\u8def\u5f84\u7684\u8bbe\u8ba1\u539f\u578b\u4e0e\u7075\u6d3b\u6027<br>Revealing design archetypes and flexibility in e-molecule import pathways] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>\u5355\u4e00\u6210\u672c\u6700\u4f18\u89e3\u8106\u5f31<br>Single cost-optimal solution is fragile]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>MGA\u4e0e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60<br>Modeling to Generate Alternatives & Interpretable ML]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>\u5e7f\u9614\u7684\u8fd1\u4f18\u7a7a\u95f4\u4e0e\u7075\u6d3b\u6027<br>Broad near-optimal space with flexibility]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scientific machine learning], [Physics-Informed Neural Networks, Neural Tangent Kernel, spectral analysis, hard constraints, boundary functions]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuchen Xie, Honghang Chi, Haopeng Quan, Yahui Wang, Wei Wang, Yu Ma"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23295",children:"https://arxiv.org/pdf/2512.23295"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Established a rigorous Neural Tangent Kernel (NTK) framework for Hard-Constraint PINNs (HC-PINNs), deriving the explicit kernel composition law. 2. Revealed that the boundary function acts as a multiplicative spatial modulator and spectral filter, fundamentally altering the learning landscape and potentially causing spectral collapse. 3. Identified the effective rank of the residual kernel as a superior, deterministic predictor of training convergence compared to classical condition numbers."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb45ca3a4beb6281022392033c68aa10cb18c53e62610b8818daca2495a7eee_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the training dynamics of Physics-Informed Neural Networks with hard constraints (HC-PINNs). It establishes an NTK framework to show that the boundary function acts as a spectral filter, and identifies the kernel's effective rank as a key predictor of convergence. The work provides a theoretical foundation for designing boundary functions to avoid optimization stagnation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Spectral Analysis of Hard-Constraint PINNs<br/>\u786c\u7ea6\u675fPINN\u7684\u8c31\u5206\u6790"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br/>HC-PINN\u8bad\u7ec3\u52a8\u6001\u673a\u5236\u4e0d\u660e<br/>HC-PINN training dynamics unexplored"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u5efa\u7acbNTK\u6846\u67b6\u4e0e\u8c31\u5206\u6790<br/>Establish NTK framework & spectral analysis"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br/>\u8fb9\u754c\u51fd\u6570\u662f\u8c31\u6ee4\u6ce2\u5668<br/>\u6709\u6548\u79e9\u9884\u6d4b\u6536\u655b\u6027<br/>Boundary function is spectral filter<br/>Effective rank predicts convergence"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [domain-specific foundation model, agentic physical ai, variance collapse, physics-based validation, policy distillation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23292",children:"https://arxiv.org/pdf/2512.23292"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (>500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model's behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Agentic Physical AI for Nuclear Reactor Control") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>General-purpose models fail at physical control<br>\u901a\u7528\u6a21\u578b\u5728\u7269\u7406\u63a7\u5236\u4e2d\u5931\u8d25"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Agentic Physical AI with physics-based validation<br>\u57fa\u4e8e\u7269\u7406\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u7269\u7406AI"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Variance collapse & emergent policy distillation<br>\u65b9\u5dee\u5d29\u6e83\u4e0e\u7b56\u7565\u84b8\u998f\u6d8c\u73b0"]\n\n    Problem --\x3e P1["Input unfaithfulness / \u8f93\u5165\u4e0d\u5fe0\u5b9e"]\n    Problem --\x3e P2["Semantic vs. physical correctness / \u8bed\u4e49\u4e0e\u7269\u7406\u6b63\u786e\u6027\u51b2\u7a81"]\n\n    Method --\x3e M1["Compact LM (360M params) / \u7d27\u51d1\u8bed\u8a00\u6a21\u578b"]\n    Method --\x3e M2["Physics-driven optimization / \u7269\u7406\u9a71\u52a8\u4f18\u5316"]\n    Method --\x3e M3["Synthetic data scaling (10^3 to 10^5) / \u5408\u6210\u6570\u636e\u7f29\u653e"]\n\n    Results --\x3e R1["Phase transition & >500x variance collapse / \u76f8\u53d8\u4e0e\u65b9\u5dee\u5d29\u6e83"]\n    Results --\x3e R2["Autonomous policy distillation / \u81ea\u4e3b\u7b56\u7565\u84b8\u998f"]\n    Results --\x3e R3["Transferable representations / \u53ef\u8fc1\u79fb\u8868\u5f81"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Lyapunov Optimization, Deep Reinforcement Learning, Edge-Cloud Partitioning, Transformer Decomposition, Queue Stability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Innsbruck, Sharif University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23310",children:"https://arxiv.org/pdf/2512.23310"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Deep learning for pedestrians: backpropagation in Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [backpropagation], [backpropagation, transformers, gradient derivation, LoRA, PyTorch implementation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Laurent Bou\xe9"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Oracle, Microsoft"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23329",children:"https://arxiv.org/pdf/2512.23329"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a vectorized, index-free derivation of backpropagation for transformer architectures, extending previous work on CNNs. 2. Derives gradient expressions for key transformer components like embedding, multi-headed self-attention, layer normalization, and LoRA layers. 3. Includes a complete PyTorch implementation of a minimal GPT-like network alongside analytical gradient expressions for pedagogical clarity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d5ded3610ac31d4f19d95237254231c2e03d3870df7749cabfc471eeb5008ac_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper manually derives the backpropagation algorithm for transformer-based next-token-prediction models using a vectorized, index-free methodology. It provides gradient expressions for core layers (embedding, self-attention, layer norm) and LoRA, aiming to build deeper intuition for how operations influence the final output. A complete PyTorch implementation is also provided to illustrate the theoretical derivations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Deep learning for pedestrians: backpropagation in Transformers] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Understanding the mechanics of backpropagation in transformers]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Apply lightweight, index-free methodology to derive gradients for embedding, self-attention, layer norm, and LoRA]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Provides analytical gradient expressions and a complete PyTorch implementation for a GPT-like network]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Visual Language Hypothesis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [representation learning], [visual language hypothesis, fiber bundle, semantic quotient, expand-and-snap, topology change]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiu Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Bytedance"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23335",children:"https://arxiv.org/pdf/2512.23335"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the "Visual Language Hypothesis," framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an "expand-and-snap" process to achieve the necessary topology change for semantic abstraction.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes the "Visual Language Hypothesis," which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific "expand-and-snap" process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Visual Language Hypothesis] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: What structural properties enable semantic abstraction in vision?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Propose hypothesis of discrete semantic language, derive geometric (fiber bundle) structure]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Semantic invariance needs discriminative target; Model needs "expand-and-snap" for topology change]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [scaling laws], [scaling laws, model ensembling, multi-model collaboration, cross-entropy loss, parameter budget]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23340",children:"https://arxiv.org/pdf/2512.23340"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes the "Law of Multi-model Collaboration," a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the "Law of Multi-model Collaboration," a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["The Law of Multi-Model Collaboration<br>\u591a\u6a21\u578b\u534f\u4f5c\u5b9a\u5f8b"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Lack of scaling theory for multi-model collaboration<br>\u7f3a\u4e4f\u591a\u6a21\u578b\u534f\u4f5c\u7684\u6269\u5c55\u7406\u8bba"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Propose Law of Multi-model Collaboration<br>\u63d0\u51fa\u591a\u6a21\u578b\u534f\u4f5c\u5b9a\u5f8b"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results<br>Ensembles scale better than single models<br>\u96c6\u6210\u6a21\u578b\u6bd4\u5355\u4e00\u6a21\u578b\u6269\u5c55\u6027\u66f4\u597d"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [medical signal processing], [ECG classification, morphology-rhythm disentanglement, Mamba, zero-shot generalization, Power Mean pooling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hai Duong Nguyen, Xuan-The Tran"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23347",children:"https://arxiv.org/pdf/2512.23347"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ECG-RAMBA: Zero-Shot ECG Generalization] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Poor cross-dataset generalization in ECG classification]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Morphology-Rhythm Disentanglement & Long-Range Mamba Modeling]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Strong zero-shot AUC on CPSC-2021 & PTB-XL]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ISOPO: Proximal policy gradients without pi-old"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [natural policy gradient, proximal policy optimization, reinforcement learning fine-tuning, Fisher metric, neural tangent kernel]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nilin Abrahamsen"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," None (No affiliation or email domain provided in the given content)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23353",children:"https://arxiv.org/pdf/2512.23353"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces ISOPO, a method to approximate the natural policy gradient in a single gradient step, contrasting with existing methods like GRPO/PPO that require multiple steps. 2. Proposes a simple form of ISOPO that normalizes log-probability gradients in the Fisher metric before contracting with advantages. 3. Presents a variant that transforms microbatch advantages based on the neural tangent kernel layer-wise, enabling efficient implementation with negligible overhead compared to REINFORCE."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa80f3dc48bfe2cf2fc158ecc6c0bb7aefffb89047088c0b23d1d764889fea16_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Isometric Policy Optimization (ISOPO), a new method for approximating the natural policy gradient in reinforcement learning fine-tuning. Unlike existing proximal policy methods like GRPO/PPO which require multiple gradient steps with a reference policy, ISOPO performs the approximation in a single step by normalizing gradients or transforming advantages, achieving this with minimal computational overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[ISOPO: Proximal policy gradients without pi-old] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Existing methods (GRPO/PPO) require multiple steps to approximate natural policy gradient]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: ISOPO approximates natural gradient in single step via Fisher metric normalization or NTK-based advantage transformation]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Efficient single-step approximation with negligible overhead vs. REINFORCE]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [post-training quantization, W8A8, W4A8, Ascend NPU, Chain-of-Thought (CoT)]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tianjin University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23367",children:"https://arxiv.org/pdf/2512.23367"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[CoT\u63a8\u7406\u5e26\u6765\u9ad8\u5185\u5b58\u4e0e\u5ef6\u8fdf / CoT reasoning causes high memory & latency]\n    B --\x3e B2[Ascend NPU\u90e8\u7f72\u6311\u6218 / Deployment challenge on Ascend NPU]\n    C --\x3e C1[\u4f4e\u6bd4\u7279\u91cf\u5316 / Low-bit Quantization]\n    C --\x3e C2[\u7edf\u4e00\u63a8\u7406\u6846\u67b6 / Unified Inference Framework]\n    C --\x3e C3[\u652f\u6301W8A8\u4e0eW4A8 / Supports W8A8 & W4A8]\n    D --\x3e D1[INT8\u4fdd\u6301>90%\u7cbe\u5ea6 / INT8 preserves >90% accuracy]\n    D --\x3e D2[1.5\u500d\u9884\u586b\u5145\u52a0\u901f / 1.5x prefill speedup]\n    D --\x3e D3[W4A8\u663e\u8457\u51cf\u5c11\u5185\u5b58 / W4A8 greatly reduces memory]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Diffusion priors enhanced velocity model building from time-lag images using a neural operator"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [neural operator, velocity model building, reverse time migration, diffusion model, automatic differentiation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiao Ma, Mohammad Hasyim Taufik, Tariq Alkhalifah"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," King Abdullah University of Science and Technology (KAUST)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23375",children:"https://arxiv.org/pdf/2512.23375"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework that combines generative models (diffusion priors) with neural operators for velocity model building. 2. Uses a neural operator as a fast surrogate for the forward modeling and migration process to generate time-lag images from velocity models. 3. Employs the trained neural operator and automatic differentiation to update the migration velocity, enhanced by a generative model as a regularizer to produce high-resolution, cleaner predictions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0c5cd7e3ef87aa48d65e94484e956e08ebef522c14cc7eee60600b85109e02a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new deep learning framework for efficient, high-resolution velocity model building in seismic imaging. The method combines a neural operator, which acts as a fast surrogate for seismic modeling and migration, with a diffusion generative model that serves as a prior to regularize the solution. Experiments on synthetic and field data show the approach effectively builds cleaner, higher-resolution velocity models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("\u6269\u6563\u5148\u9a8c\u589e\u5f3a\u7684\u57fa\u4e8e\u795e\u7ecf\u7b97\u5b50\u7684\u65f6\u6ede\u56fe\u50cf\u901f\u5ea6\u5efa\u6a21<br>Diffusion priors enhanced velocity model building from time-lag images using a neural operator")\n    Root --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: \u4f20\u7edf\u901f\u5ea6\u5efa\u6a21\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8017\u65f6<br>Conventional VMB is computationally expensive and time-consuming")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: \u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u4e0e\u751f\u6210\u6a21\u578b\uff08\u6269\u6563\u5148\u9a8c\uff09<br>Combines neural operator with generative model (diffusion prior)")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: \u5408\u6210\u4e0e\u5b9e\u5730\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027<br>Synthetic and field data demonstrate effectiveness")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sec], [log anomaly detection], [collaborative transformers, multi-head impressed attention, modality adaptation layer]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Urmia University of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23380",children:"https://arxiv.org/pdf/2512.23380"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/your-repo/CoLog",children:"https://github.com/your-repo/CoLog"}),' (Note: The provided text states "We also provide the implementation of CoLog atthis https URL." but the specific URL is cut off in the input. Based on the placeholder, the typical format is used. If the exact URL is required, it would be the one following "atthis" in the original text.)']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions"]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer"]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [communication & networking], [beyond-diagonal RIS, passive beamforming, hybrid quantum-classical ML, 6G networks, reconfigurable intelligent surfaces]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abd Ullah Khan, Uman Khalid, Muhammad Tanveer, Trung Q. Duong, Hyundong Shin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Kyung Hee University, University of Management and Technology, Memorial University of Newfoundland, Queen's University Belfast"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23400",children:"https://arxiv.org/pdf/2512.23400"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Provides a systematic introduction to Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS), detailing its principles, architecture, advantages, and classification. 2. Presents a case study comparing four beamforming algorithms for BD-RIS, analyzing their performance in terms of sum rate and computation cost. 3. Proposes and analyzes hybrid quantum-classical machine learning models to enhance beam prediction for 6G BD-RIS, validated using the real-world DeepSense 6G dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4756534e0e202824dc7facc5a963e3a3205fc85e2596f60033bcdb6ae4cab497_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Beyond-Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) as a key technology for 6G networks to overcome high-frequency propagation challenges. It systematically reviews BD-RIS principles, analyzes beamforming algorithms, and explores quantum-enhanced machine learning for beam prediction. The work concludes with insights into the practical implications and future potential of BD-RIS for advanced wireless communication."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Beyond-Diagonal RIS for 6G Networks<br/>\u8d85\u8d8a\u5bf9\u89d2\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u7528\u4e8e6G\u7f51\u7edc] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>6G\u9ad8\u9891\u6bb5\u4f20\u64ad\u635f\u8017\u4e0e\u963b\u585e<br/>6G high-frequency propagation loss & blockage]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>\u7cfb\u7edf\u7efc\u8ff0\u3001\u6ce2\u675f\u8d4b\u5f62\u6848\u4f8b\u7814\u7a76\u3001\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408ML<br/>Systematic review, beamforming case study, hybrid quantum-classical ML]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>\u83b7\u5f97BD-RIS\u5b9e\u7528\u89c1\u89e3\u4e0e\u6027\u80fd\u5206\u6790<br/>Derived practical insights & performance analysis for BD-RIS]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Task-driven Heterophilic Graph Structure Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [graph structure learning, heterophilic graphs, spectral filtering, topology inference, graph rewiring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Indian Institute of Science, University of Rochester"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23406",children:"https://arxiv.org/pdf/2512.23406"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FgGSL, an end-to-end framework that jointly learns complementary homophilic and heterophilic graph structures using a learnable masking function and processes them with low- and high-pass graph filter banks. 2. Introduces a label-based structural loss to explicitly promote the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning, and provides theoretical stability bounds for this loss and robustness guarantees for the filters. 3. Demonstrates through experiments on six heterophilic benchmarks that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, validating the benefit of combining frequency information with supervised topology inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3fec9358f601e6c323ecab2e7bcfe1880fd9b9d4351503536e33c58d8cedb6e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of learning discriminative node representations on heterophilic graphs, where connected nodes often have dissimilar labels. The authors propose FgGSL, a framework that jointly learns homophilic and heterophilic graph structures using spectral filters and a task-driven structural loss. Experiments show FgGSL outperforms existing methods, highlighting the advantage of combining frequency guidance with supervised graph inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Task-driven Heterophilic Graph Structure Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: GNNs struggle on heterophilic graphs]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: FgGSL - Jointly learns complementary graphs with spectral filters & label-based loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms SOTA on benchmarks, benefits of frequency-guided inference]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On the Sample Complexity of Learning for Blind Inverse Problems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [inverse problems], [blind inverse problems, Linear Minimum Mean Square Estimators (LMMSEs), Tikhonov regularization, random forward operators, error bounds]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nathan Buskulic, Luca Calatroni, Lorenzo Rosasco, Silvia Villa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universit\xe0 degli studi di Genova, Italian Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23405",children:"https://arxiv.org/pdf/2512.23405"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Deriving closed-form expressions for optimal Linear Minimum Mean Square Estimators (LMMSEs) for blind inverse problems and establishing their equivalence with distribution-dependent Tikhonov regularization. 2. Proving convergence results for these estimators under source condition assumptions. 3. Deriving rigorous finite-sample error bounds that quantify the impact of operator randomness, noise level, and sample count on estimator performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daa1ffc1715fdd9a990c7a6af28c2a415193fdef2bd99ec47e6ece431b0f3406_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a theoretical analysis of learning for blind inverse problems, where the forward operator is unknown. It focuses on Linear Minimum Mean Square Estimators (LMMSEs), deriving their optimal forms and connecting them to Tikhonov regularization. The main conclusion is the establishment of rigorous error bounds and convergence rates that characterize how estimator performance depends on noise, problem conditioning, and the randomness of the forward operator."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[On the Sample Complexity of Learning for Blind Inverse Problems] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u76f2\u9006\u95ee\u9898\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1/Blind inverse problems lack theoretical guarantees]\n    C --\x3e C1[\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u5668\u6846\u67b6/LMMSE framework]\n    C --\x3e C2[\u8fde\u63a5\u5409\u6d2a\u8bfa\u592b\u6b63\u5219\u5316/Link to Tikhonov regularization]\n    D --\x3e D1[\u95ed\u5f0f\u6700\u4f18\u4f30\u8ba1\u5668/Closed-form optimal estimators]\n    D --\x3e D2[\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c/Finite-sample error bounds]\n    D --\x3e D3[\u6536\u655b\u7387\u5206\u6790/Convergence rate analysis]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Theoretical Foundations of Scaling Law in Familial Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [familial models, scaling law, early exiting, IsoFLOP design, compute-optimal training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Institute of Artificial Intelligence (TeleAI), China Telecom"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23407",children:"https://arxiv.org/pdf/2512.23407"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Theoretically and empirically extends the neural scaling law to the "familial models" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (\u03b3\u22480.041), validating the "train once, deploy many" paradigm without compromising compute-optimality.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[Theoretical Foundations of Scaling Law in Familial Models] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edf\u7f29\u653e\u5b9a\u5f8b\u5ffd\u7565\u591a\u6a21\u578b\u8303\u5f0f/Traditional scaling laws overlook the multi-model paradigm]\n    C --\x3e C1[\u5f15\u5165\u7c92\u5ea6\u4f5c\u4e3a\u65b0\u53d8\u91cf/Introduce Granularity (G) as a new variable]\n    C --\x3e C2[\u7edf\u4e00\u51fd\u6570\u5f62\u5f0f L(N, D, G)/Unified functional form L(N, D, G)]\n    C --\x3e C3[\u91c7\u7528IsoFLOP\u5b9e\u9a8c\u8bbe\u8ba1/Employ rigorous IsoFLOP experimental design]\n    D --\x3e D1[\u7c92\u5ea6\u60e9\u7f5a\u9075\u5faa\u5e42\u5f8b/Granularity penalty follows a power law]\n    D --\x3e D2[\u6307\u6570\u6781\u5c0f (\u03b3\u22480.041)/Exponent is extremely small]\n    D --\x3e D3[\u9a8c\u8bc1"\u4e00\u6b21\u8bad\u7ec3\uff0c\u591a\u6b21\u90e8\u7f72"/Validates "train once, deploy many"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [model compression (quantization/pruning)], [intrinsic dimension, low-rank approximation, subspace-native distillation, weight matrices, empirical spectral density]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yusuf Kalyoncuoglu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," RWTH Aachen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23410",children:"https://arxiv.org/pdf/2512.23410"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces "Subspace-Native Distillation" as a novel paradigm to provide a stable geometric coordinate system for student models, enabling "Train Big, Deploy Small".']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Directly Constructing Low-Dimensional Solution Subspaces<br>\u76f4\u63a5\u6784\u5efa\u4f4e\u7ef4\u89e3\u5b50\u7a7a\u95f4"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Large models are redundant for representation but needed for optimization.<br>\u5927\u6a21\u578b\u5bf9\u8868\u793a\u662f\u5197\u4f59\u7684\uff0c\u4f46\u5bf9\u4f18\u5316\u662f\u5fc5\u8981\u7684\u3002"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Construct low-dimensional subspaces, decouple solution geometry.<br>\u6784\u5efa\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u89e3\u8026\u89e3\u51e0\u4f55\u3002"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Head compression by 16x, Subspace-Native Distillation.<br>\u5206\u7c7b\u5934\u538b\u7f2916\u500d\uff0c\u63d0\u51fa\u5b50\u7a7a\u95f4\u539f\u751f\u84b8\u998f\u3002"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Towards Integrating Uncertainty for Domain-Agnostic Segmentation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [semantic segmentation], [uncertainty quantification, domain-agnostic, Segment Anything Model (SAM), Laplace approximation, benchmark]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jesse Brouwers, Xiaoyan Xing, Alexander Timans"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," UvA-Bosch Delta Lab, University of Amsterdam"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23427",children:"https://arxiv.org/pdf/2512.23427"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/JesseBrouw/UncertSAM",children:"https://github.com/JesseBrouw/UncertSAM"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Towards Integrating Uncertainty for Domain-Agnostic Segmentation] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: SAM\u5728\u57df\u504f\u79fb\u6216\u77e5\u8bc6\u6709\u9650\u573a\u666f\u4e0b\u8106\u5f31/SAM vulnerable in shifted or limited-knowledge domains]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u6784\u5efaUncertSAM\u57fa\u51c6\uff0c\u8bc4\u4f30\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u5c1d\u8bd5\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u4f18\u5316/Build UncertSAM benchmark, evaluate post-hoc UQ methods, attempt uncertainty-guided refinement]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0e\u8bef\u5dee\u76f8\u5173\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e0d\u786e\u5b9a\u6027\u6574\u5408\u6f5c\u529b/Laplace approximation yields correlated uncertainty, preliminary potential of integrating UQ]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [gpu kernels], [kernel generation, multi-agent system, domain-specific languages (DSLs), performance tuning, Triton]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Huawei Technologies Co., Ltd., Hunan University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23424",children:"https://arxiv.org/pdf/2512.23424"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system's effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[AKG Kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[AI\u6a21\u578b\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u5185\u6838\u7684\u9700\u6c42 / AI Models Demand High-Performance Kernels]\n    B --\x3e B2[\u786c\u4ef6\u591a\u6837\u6027\u4e0e\u624b\u52a8\u4f18\u5316\u7684\u74f6\u9888 / Hardware Diversity & Manual Optimization Bottleneck]\n    C --\x3e C1[\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u5185\u6838\u751f\u6210\u4e0e\u8c03\u4f18 / Multi-Agent System Automates Kernel Generation & Tuning]\n    C --\x3e C2[\u652f\u6301\u591a\u79cdDSL\u4ee5\u9762\u5411\u4e0d\u540c\u786c\u4ef6\u540e\u7aef / Supports Multiple DSLs for Different Hardware Backends]\n    D --\x3e D1[\u5728KernelBench\u4e0a\u8bc4\u4f30 / Evaluated on KernelBench]\n    D --\x3e D2[\u5e73\u5747\u52a0\u901f1.46\u500d / Average 1.46x Speedup Achieved]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Stochastic Siamese MAE Pretraining for Longitudinal Medical Images"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [medical image analysis], [masked autoencoder, siamese network, stochastic process, longitudinal data, variational inference]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunovi\u0107"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23441",children:"https://arxiv.org/pdf/2512.23441"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer's Disease."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Stochastic Siamese MAE Pretraining for Longitudinal Medical Images] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: MAE lacks temporal awareness for longitudinal medical data.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: STAMP - Stochastic Siamese MAE using conditional variational inference.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms existing methods on AMD and AD progression prediction.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-agent systems], [Model Predictive Control, Coverage-Based Testing, Edge-Case Exploration, Multi-Agent Simulation, Behaviour Coverage]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Manuel Franco-Vivo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Bristol"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23445",children:"https://arxiv.org/pdf/2512.23445"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A systematic approach to measure and assess behaviour coverage within a multi-agent simulation for autonomous vehicle testing. 2. The proposal of a Model Predictive Control (MPC) pedestrian agent designed to generate interesting tests and realistic behaviour. 3. Insights and analysis for improving and optimizing simulation frameworks through behaviour coverage metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385ed91e6ca24b7cc0e8d369cc587a3dd677cf4643f89f27d85aaadf4cd4ea70_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the need for comprehensive testing of autonomous vehicles by analyzing behaviour coverage in multi-agent simulations. It proposes a systematic method to measure coverage and introduces an MPC-based pedestrian agent to generate more realistic and challenging test scenarios. The research concludes that assessing behaviour coverage is crucial for validating the robustness of autonomous systems and improving simulation frameworks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Assessing Behaviour Coverage in a Multi-Agent System Simulation for Autonomous Vehicle Testing] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[\u5982\u4f55\u5168\u9762\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u8986\u76d6\u5ea6\uff1f/How to comprehensively evaluate behaviour coverage of AV systems in simulation?]\nC --\x3e C1[\u5b9a\u4e49\u573a\u666f\u4e0e\u4ea4\u4e92\uff0c\u63d0\u51faMPC\u884c\u4eba\u667a\u80fd\u4f53/Define scenarios & interactions, propose MPC pedestrian agent]\nD --\x3e D1[\u884c\u4e3a\u8986\u76d6\u5ea6\u5bf9\u9a8c\u8bc1\u7cfb\u7edf\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981/Behaviour coverage is crucial for validating system effectiveness]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Mixture-of-Experts, Router-Expert Coupling, Auxiliary Loss, Expert Specialization, Efficient Training]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ang Lv, Jin Ma, Yiyuan Ma, Siyuan Qiao"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," ByteDance, Renmin University of China"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23447",children:"https://arxiv.org/pdf/2512.23447"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel lightweight auxiliary loss (ERC loss) to explicitly couple router decisions with expert capabilities in MoE models. 2. Introduces a computationally efficient method that scales with the square of the number of experts (n^2), independent of batch size, unlike prior token-dependent methods. 3. Enables flexible control and quantitative tracking of expert specialization levels during training, providing new insights into MoE model dynamics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6534d4f2f88c89a450614cf67b57f76f33fc90a18f24833876fd8e55d3e326b9_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the misalignment between router decisions and expert capabilities in Mixture-of-Experts (MoE) models. It proposes an Expert-Router Coupling (ERC) loss, a lightweight auxiliary loss that enforces constraints via perturbed router embeddings to ensure each expert specializes in its routed tokens and each router embedding faithfully represents its expert. The method is shown to be effective and computationally efficient, enabling better control and analysis of expert specialization during large-scale pre-training."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8026\u5408\u4e13\u5bb6\u4e0e\u8def\u7531\u5668<br/>Coupling Experts and Routers in Mixture-of-Experts] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u8def\u7531\u5668\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u5339\u914d<br/>Router decisions misaligned with expert capabilities]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51fa\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931 (ERC Loss)<br/>Propose Expert-Router Coupling (ERC) Loss]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9ad8\u6548\u8ba1\u7b97\uff0c\u53ef\u91cf\u5316\u8ffd\u8e2a\u4e13\u5bb6\u4e13\u4e1a\u5316<br/>Improved performance, efficient computation, quantifiable tracking of specialization]\n    C --\x3e E[\u65b9\u6cd5\u539f\u7406/Mechanism: \u4f7f\u7528\u6270\u52a8\u8def\u7531\u5668\u5d4c\u5165\u4f5c\u4e3a\u4ee3\u7406\u4ee4\u724c<br/>Use perturbed router embeddings as proxy tokens]\n    E --\x3e F[\u7ea6\u675f/Constraints: \u4e13\u5bb6\u5bf9\u81ea\u8eab\u4ee3\u7406\u4ee4\u724c\u6fc0\u6d3b\u6700\u9ad8\uff1b\u4ee3\u7406\u4ee4\u724c\u5f15\u53d1\u5bf9\u5e94\u4e13\u5bb6\u6700\u5f3a\u6fc0\u6d3b<br/>Expert highest activation for own proxy; Proxy elicits strongest activation from corresponding expert]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture of Experts, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Memory-Bandwidth Bottleneck, Dynamic Sparse Dictionary Learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vladimer Khasia"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Independent Researcher"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23448",children:"https://arxiv.org/pdf/2512.23448"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/VladimerKhasia/DSC",children:"https://github.com/VladimerKhasia/DSC"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Dynamic Subspace Composition (DSC), a framework that models weight updates as a residual trajectory within a Star-Shaped Domain using Magnitude-Gated Simplex Interpolation for continuity. 2. Decouples storage and adaptation rank, constructing compositional approximations from a shared basis bank to reduce parameter complexity from O(Mrd) to O(Md) and memory traffic to O(Kd). 3. Introduces Frame-Theoretic regularization and spectral constraints to provide rigorous worst-case bounds on the dynamic update, addressing representation collapse and gradient instability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/713ad8af3e30c492117d0d3f26babbf5fe909df2e68e0ab479fc866276a3d80c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the memory-bandwidth bottleneck and optimization instability in Mixture of Experts (MoE) models. It proposes Dynamic Subspace Composition (DSC), a method that approximates context-dependent weights via a sparse expansion of a shared basis, reducing parameter complexity and memory traffic while ensuring stable updates. The main conclusion is that DSC offers a more efficient and theoretically grounded alternative to standard approaches like Mixture-of-LoRAs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Dynamic Subspace Composition<br/>\u52a8\u6001\u5b50\u7a7a\u95f4\u7ec4\u5408] --\x3e B[Problem<br/>\u6838\u5fc3\u95ee\u9898]\n    A --\x3e C[Method<br/>\u4e3b\u8981\u65b9\u6cd5]\n    A --\x3e D[Results<br/>\u5173\u952e\u7ed3\u679c]\n    B --\x3e B1[MoE suffers from<br/>representation collapse &<br/>memory bottleneck<br/>MoE\u5b58\u5728\u8868\u793a\u5d29\u6e83\u4e0e\u5185\u5b58\u74f6\u9888]\n    C --\x3e C1[Dynamic Sparse Dictionary Learning<br/>\u52a8\u6001\u7a00\u758f\u5b57\u5178\u5b66\u4e60]\n    C1 --\x3e C2[Star-Shaped Domain &<br/>Magnitude-Gated Simplex Interpolation<br/>\u661f\u5f62\u57df\u4e0e\u5e45\u5ea6\u95e8\u63a7\u5355\u7eaf\u5f62\u63d2\u503c]\n    D --\x3e D1[Reduced complexity<br/>O(Md) & O(Kd)<br/>\u964d\u4f4e\u590d\u6742\u5ea6]\n    D --\x3e D2[Frame-Theoretic bounds<br/>\u6846\u67b6\u7406\u8bba\u8fb9\u754c]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [instruction following, hindsight replay, sample-efficient RL]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23457",children:"https://arxiv.org/pdf/2512.23457"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/zhangkc97/HiR",children:"https://github.com/zhangkc97/HiR"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u7a00\u758f/\u4e0d\u53ef\u533a\u5206\u7684\u5956\u52b1\u963b\u788d\u5b66\u4e60<br>Sparse/Indistinguishable Rewards Impede Learning]\n    Method[\u540e\u89c1\u6307\u4ee4\u91cd\u653e (HiR)<br>Hindsight instruction Replay (HiR)]\n    Results[\u8de8\u4efb\u52a1\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548<br>Effective Across Tasks & Computationally Efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning from human feedback (RLHF)], [reward model, inductive bias, information bottleneck, mutual information, reward hacking]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23461",children:"https://arxiv.org/pdf/2512.23461"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Qwen-Applications/DIR",children:"https://github.com/Qwen-Applications/DIR"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method's ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR's effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Eliminating Inductive Bias in Reward Models<br>\u6d88\u9664\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u5f52\u7eb3\u504f\u5dee] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Low-quality RM data with inductive biases<br>\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u5956\u52b1\u653b\u51fb] --\x3e B1[\u4e3e\u4f8b/Example<br>Response length bias<br>\u54cd\u5e94\u957f\u5ea6\u504f\u5dee]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>DIR: Information-theoretic debiasing<br>\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u4f18\u5316\u4e92\u4fe1\u606f] --\x3e C1[\u76ee\u6807/Objective<br>Max MI with preference, Min MI with bias<br>\u6700\u5927\u5316\u504f\u597d\u4e92\u4fe1\u606f\uff0c\u6700\u5c0f\u5316\u504f\u5dee\u4e92\u4fe1\u606f]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Mitigates multiple biases & enhances RLHF<br>\u51cf\u8f7b\u591a\u79cd\u504f\u5dee\u5e76\u63d0\u5347RLHF\u6027\u80fd] --\x3e D1[\u9a8c\u8bc1\u7684\u504f\u5dee/Verified Biases<br>Length, Sycophancy, Format<br>\u957f\u5ea6\u3001\u8fce\u5408\u6027\u3001\u683c\u5f0f]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training (sft/rlhf)], [Parameter-efficient fine-tuning, LoRA, full-rank adaptation, rotational degrees of freedom, hierarchical joint decomposition]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Beihang University, Huazhong University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23485",children:"https://arxiv.org/pdf/2512.23485"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Bane-Elvin/AAAI2026-FRoD",children:"https://github.com/Bane-Elvin/AAAI2026-FRoD"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[FRoD: Full-Rank Efficient Fine-Tuning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Low-rank PEFT methods suffer from slow convergence and limited adaptation capacity]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Hierarchical joint decomposition with rotational degrees of freedom for full-rank updates]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Matches full fine-tuning accuracy using only 1.72% parameters and achieves faster convergence]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [model selection, capability-cost frontier, constrained optimization, deployment-aware leaderboards, compliance trade-offs]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Boston University, Carnegie Mellon University, Universitat Polit\xe8cnica de Catalunya, Oxford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23487",children:"https://arxiv.org/pdf/2512.23487"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results")\n    Problem --\x3e P1("\u80fd\u529b\u6392\u884c\u699c\u4e0e\u90e8\u7f72\u51b3\u7b56\u8131\u8282/Capability-Deployment Gap")\n    Problem --\x3e P2("\u9700\u5e73\u8861\u7528\u6237\u6548\u7528\u3001\u6210\u672c\u3001\u5408\u89c4\u6027/Balance Utility, Cost, Compliance")\n    Method --\x3e M1("\u7406\u8bba: \u57fa\u4e8e\u524d\u6cbf\u7684\u7ea6\u675f\u4f18\u5316/Theoretical Constrained Optimization")\n    Method --\x3e M2("\u5b9e\u73b0: \u63d0\u53d6\u3001\u4f30\u8ba1\u3001\u5b66\u4e60\u3001\u63a8\u8350/Pipeline: Extract, Estimate, Learn, Recommend")\n    Results --\x3e R1("\u6700\u4f18\u914d\u7f6e\u5448\u73b0\u4e09\u533a\u7ed3\u6784/Optimal Configurations Show Three-Regime Structure")\n    Results --\x3e R2("\u90e8\u7f72\u611f\u77e5\u6392\u540d\u4e0d\u540c\u4e8e\u80fd\u529b\u6392\u540d/Deployment-Aware Rankings Differ from Capability-Only")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [communication & networking], [URLLC, Link Adaptation, Device Scheduling, Deep Reinforcement Learning, Bayesian Optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Wuhan University, RWTH Aachen University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23493",children:"https://arxiv.org/pdf/2512.23493"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[URLLC IIoT\u7f51\u7edc\u7684\u591a\u8bbe\u5907\u52a8\u6001\u8c03\u5ea6\u4e0e\u94fe\u8def\u81ea\u9002\u5e94/URLLC IIoT Multi-device Dynamic Scheduling & Link Adaptation]\n    B --\x3e B2[\u4e0d\u5b8c\u7f8e\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f/Imperfect Channel State Information]\n    B --\x3e B3[\u4e25\u683c\u7684\u8bef\u5757\u7387\u7ea6\u675f/Strict Block Error Rate Constraints]\n    C --\x3e C1[\u8d1d\u53f6\u65af\u4f18\u5316\u9a71\u52a8\u7684TD3\u65b9\u6cd5/BO-driven TD3 Method]\n    C --\x3e C2[\u81ea\u9002\u5e94\u786e\u5b9a\u8bbe\u5907\u670d\u52a1\u987a\u5e8f\u4e0eMCS/Adaptively Determine Device Order & MCS]\n    C --\x3e C3[BO\u8bad\u7ec3\u673a\u5236\u6539\u8fdb\u6536\u655b/BO-based Training for Convergence]\n    D --\x3e D1[\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6/Faster Convergence]\n    D --\x3e D2[\u66f4\u9ad8\u7684\u603b\u901f\u7387\u6027\u80fd/Higher Sum-rate Performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Trustworthy Machine Learning under Distribution Shifts"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [trustworthy machine learning], [distribution shift, robustness, explainability, adaptability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhuo Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Sydney"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23524",children:"https://arxiv.org/pdf/2512.23524"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a systematic framework for studying Trustworthy Machine Learning by categorizing three common types of distribution shifts (Perturbation, Domain, Modality). 2. Rigorously investigates trustworthiness through three key aspects: Robustness, Explainability, and Adaptability. 3. Aims to provide effective solutions and fundamental insights to enhance critical ML problems like efficiency and safety under distribution shifts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8153efa2ea5f1ee68438cc93b22e23dd850b2f55b7ebc99a5374a4a32aea0bb4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This thesis addresses the core problem of distribution shift, which limits the reliability and trustworthiness of AI systems. The research proposes a framework that studies three types of distribution shifts and evaluates solutions through the lenses of robustness, explainability, and adaptability. The goal is to develop more reliable, versatile, and responsible machine learning models that can generalize effectively under real-world distribution shifts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Trustworthy Machine Learning under Distribution Shifts<br>\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u4fe1\u673a\u5668\u5b66\u4e60"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Distribution shift limits AI reliability & trust<br>\u5206\u5e03\u504f\u79fb\u9650\u5236AI\u53ef\u9760\u6027\u4e0e\u4fe1\u4efb"] --\x3e P1["\u6270\u52a8\u504f\u79fb/Perturbation Shift"]\n    Problem --\x3e P2["\u57df\u504f\u79fb/Domain Shift"]\n    Problem --\x3e P3["\u6a21\u6001\u504f\u79fb/Modality Shift"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>Study trustworthiness via three aspects<br>\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7814\u7a76\u53ef\u4fe1\u5ea6"] --\x3e M1["\u9c81\u68d2\u6027/Robustness"]\n    Method --\x3e M2["\u53ef\u89e3\u91ca\u6027/Explainability"]\n    Method --\x3e M3["\u9002\u5e94\u6027/Adaptability"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Propose solutions & insights<br>\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4e0e\u6d1e\u89c1"] --\x3e R1["\u589e\u5f3a\u5173\u952e\u95ee\u9898/Enhance critical problems"]\n    R1 --\x3e R1_Sub["\u6548\u7387, \u9002\u5e94\u6027, \u5b89\u5168<br>Efficiency, Adaptability, Safety"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [affective computing], [domain adaptation, graph regularization, EEG, emotion recognition, cross-session]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Maryam Mirzaei, Farzaneh Shayegh, Hamed Narimani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Based on author names and content, specific institution not provided. Could be inferred from typical academic affiliations in this field, but not explicitly stated in the given text."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23526",children:"https://arxiv.org/pdf/2512.23526"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed EGDA, a novel framework integrating domain adaptation with graph-based regularization for cross-session EEG emotion recognition. 2. Introduced a method to jointly align both marginal and conditional distributions while preserving the intrinsic data structure. 3. Demonstrated the discriminative power of the Gamma frequency band and identified critical brain regions (central-parietal and prefrontal) for emotion recognition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97864260ef21b4d340e353b5b1666e9df5860104730e726e9067386c78cadc72_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of cross-session EEG emotion recognition by proposing EGDA, a framework that reduces distribution discrepancies through joint marginal and conditional alignment while using graph regularization to preserve data structure. Experiments on the SEED-IV dataset show EGDA outperforms baselines, achieving robust accuracies. The analysis further identifies the Gamma band and specific brain regions as key for reliable emotion recognition."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Cross-session EEG distribution shifts hinder emotion recognition model generalization.]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: EGDA framework aligns marginal & conditional distributions with graph regularization.]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Achieves robust accuracy on SEED-IV; Gamma band and central-parietal/prefrontal regions are most discriminative.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] VL-RouterBench: A Benchmark for Vision-Language Model Routing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision-language model routing, benchmark, cost-accuracy trade-off, model selection, evaluation protocol]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23562",children:"https://arxiv.org/pdf/2512.23562"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/K1nght/VL-RouterBench",children:"https://github.com/K1nght/VL-RouterBench"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u590d\u73b0\u7684<br>VLM\u8def\u7531\u8bc4\u4f30\u57fa\u51c6<br>Lack of systematic, reproducible<br>benchmark for VLM routing]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>\u57fa\u4e8e\u539f\u59cb\u63a8\u7406\u65e5\u5fd7\u6784\u5efa<br>\u8d28\u91cf\u4e0e\u6210\u672c\u77e9\u9635<br>Construct quality & cost matrices<br>from raw inference logs]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>\u89c2\u5bdf\u5230\u663e\u8457\u7684\u8def\u7531\u589e\u76ca<br>\u4f46\u4e0e\u7406\u60f3\u6027\u80fd\u4ecd\u6709\u5dee\u8ddd<br>Observe significant routability gain<br>but clear gap to ideal Oracle]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Distribution-Free Process Monitoring with Conformal Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [anomaly detection], [Conformal Prediction, Statistical Process Control, Control Charts, Anomaly Detection, Quality Management]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Christopher Burger"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Mississippi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23602",children:"https://arxiv.org/pdf/2512.23602"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A hybrid framework integrating Conformal Prediction's distribution-free guarantees into Statistical Process Control (SPC). 2. Conformal-Enhanced Control Charts that visualize process uncertainty and enable proactive signals like 'uncertainty spikes'. 3. Conformal-Enhanced Process Monitoring that reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b25b4ce8de4803a0d1cc60a16d7221589651a9a271d92f85fb5b6426127e8b4_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the limitations of traditional Statistical Process Control (SPC), which relies on often-violated statistical assumptions, by proposing a hybrid framework that integrates distribution-free Conformal Prediction. The method introduces two novel applications: enhanced control charts for visualizing uncertainty and a p-value chart for formal anomaly detection. The framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability of classic methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Distribution-Free Process Monitoring with Conformal Prediction] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u4f20\u7edfSPC\u4f9d\u8d56\u7edf\u8ba1\u5047\u8bbe\uff0c\u4e0d\u53ef\u9760/Traditional SPC relies on statistical assumptions, unreliable]\n    C --\x3e C1[\u5c06\u4fdd\u5f62\u9884\u6d4b\u4e0eSPC\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6/Hybrid framework integrating Conformal Prediction with SPC]\n    C1 --\x3e C2[\u4fdd\u5f62\u589e\u5f3a\u63a7\u5236\u56fe/Conformal-Enhanced Control Charts]\n    C1 --\x3e C3[\u4fdd\u5f62\u589e\u5f3a\u8fc7\u7a0b\u76d1\u63a7/Conformal-Enhanced Process Monitoring]\n    D --\x3e D1[\u66f4\u9c81\u68d2\u3001\u7edf\u8ba1\u4e25\u8c28\u7684\u8d28\u91cf\u63a7\u5236\u65b9\u6cd5/More robust, statistically rigorous quality control]\n    D --\x3e D2[\u4fdd\u6301\u7ecf\u5178\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6613\u7528\u6027/Maintains interpretability and ease of use of classic methods]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [transfer learning], [Le Cam Distortion, Deficiency Distance, Directional Simulability, Unsupervised Domain Adaptation, Negative Transfer]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Deniz Akdemir"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," None (Institution not specified in provided content)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23617",children:"https://arxiv.org/pdf/2512.23617"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam's theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework's effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful "negative transfer" by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam\'s theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u6807\u51c6UDA\u7684\u7f3a\u9677/Flaw of Standard UDA]\n    Problem --\x3e P2[\u8d1f\u8fc1\u79fb\u4e0e\u4fe1\u606f\u7834\u574f/Negative Transfer & Information Destruction]\n    Method --\x3e M1[Le Cam\u7406\u8bba/Le Cam's Theory]\n    Method --\x3e M2[\u65b9\u5411\u53ef\u6a21\u62df\u6027/Directional Simulability]\n    Method --\x3e M3[Le Cam Distortion\u5ea6\u91cf/Le Cam Distortion Metric]\n    Results --\x3e R1[\u57fa\u56e0\u7ec4\u5b66\u5b8c\u7f8e\u4f30\u8ba1/Perfect Genomics Estimation]\n    Results --\x3e R2[\u96f6\u6e90\u57df\u635f\u5931/Zero Source Utility Loss]\n    Results --\x3e R3[\u5b89\u5168RL\u7b56\u7565\u8f6c\u79fb/Safe RL Policy Transfer]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Regret-Based Federated Causal Discovery with Unknown Interventions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [causal discovery, unknown interventions, differential privacy, \u03a6-CPDAG, regret-based]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Federico Baldo, Charles K. Assaad"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Sorbonne Universit\xe9, INSERM, Institut Pierre Louis d'Epid\xe9miologie et de Sant\xe9 Publique"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23626",children:"https://arxiv.org/pdf/2512.23626"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the \u03a6-Markov Equivalence Class (\u03a6-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the \u03a6-CPDAG. Theoretical and empirical results demonstrate the algorithm's effectiveness and privacy guarantees."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Regret-Based Federated Causal Discovery with Unknown Interventions] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u8054\u90a6\u56e0\u679c\u53d1\u73b0\u4e2d\u5ba2\u6237\u7aef\u5b58\u5728\u672a\u77e5\u5f02\u8d28\u5e72\u9884/Federated causal discovery with unknown, heterogeneous client interventions]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u63d0\u51faI-PERI\u7b97\u6cd5\uff0c\u5229\u7528\u5e72\u9884\u5dee\u5f02\u5b9a\u5411\u8fb9/Propose I-PERI algorithm, orienting edges using intervention differences]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u5b9a\u4e49\u03a6-CPDAG\uff0c\u63d0\u4f9b\u7406\u8bba\u4e0e\u9690\u79c1\u4fdd\u8bc1/Define \u03a6-CPDAG, provide theoretical and privacy guarantees]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Memorization in 3D Shape Generation: An Empirical Study"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [3D shape generation], [memorization, diffusion models, latent vector-set, evaluation framework, data leakage]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Princeton University, Harvard University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23628",children:"https://arxiv.org/pdf/2512.23628"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," github.com/zlab-princeton/3d_mem"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Memorization in 3D Shape Generation: An Empirical Study] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1(3D\u751f\u6210\u6a21\u578b\u662f\u5426\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e?/Do 3D generative models memorize training data?)\n    C --\x3e C1(\u8bbe\u8ba1\u91cf\u5316\u6846\u67b6/Design evaluation framework)\n    C --\x3e C2(\u4f7f\u7528Vecset\u6269\u6563\u6a21\u578b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c/Use Vecset diffusion model for controlled experiments)\n    D --\x3e D1(\u6570\u636e\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u6761\u4ef6\u589e\u52a0\u8bb0\u5fc6/Data diversity & fine-grained conditioning increase memorization)\n    D --\x3e D2(\u9002\u5ea6\u5f15\u5bfc\u89c4\u6a21\u5cf0\u503c\u8bb0\u5fc6/Moderate guidance scale peaks memorization)\n    D --\x3e D3(\u66f4\u957fVecsets\u548c\u65cb\u8f6c\u589e\u5f3a\u53ef\u7f13\u89e3\u8bb0\u5fc6/Longer Vecsets & rotation augmentation mitigate memorization)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [agent system], [multi-agent systems, hierarchical agents, bandit optimization, software engineering agents, SWE-bench]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23631",children:"https://arxiv.org/pdf/2512.23631"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/iamxjy/BOAD-SWE-Agent",children:"https://github.com/iamxjy/BOAD-SWE-Agent"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[BOAD: \u53d1\u73b0\u5206\u5c42\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406 / BOAD: Discovering Hierarchical Software Engineering Agents] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898 / Problem: \u5355\u4e00LLM\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u3001\u5206\u5e03\u5916\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee / Single-agent LLMs generalize poorly on long-horizon, out-of-distribution SWE tasks]\n    C[\u4e3b\u8981\u65b9\u6cd5 / Method: \u5c06\u5206\u5c42\u53d1\u73b0\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u4f18\u5316\u5b50\u4ee3\u7406\u534f\u4f5c / Formulate hierarchy discovery as a multi-armed bandit problem to optimize sub-agent collaboration]\n    D[\u5173\u952e\u7ed3\u679c / Results: \u5728SWE-bench\u4e0a\u8d85\u8d8a\u5355\u4ee3\u7406\u548c\u624b\u52a8\u8bbe\u8ba1\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c36B\u6a21\u578b\u6392\u540d\u7b2c\u4e8c / Outperforms single-agent and manual multi-agent systems on SWE-bench, 36B model ranks second]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [educational ai], [generative AI, fine-tuning, randomized controlled trial, Socratic questioning, pedagogical instruction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Braz\xe3o"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Google, Eedi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23633",children:"https://arxiv.org/pdf/2512.23633"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[AI Tutoring RCT in UK Classrooms] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u4e2a\u6027\u5316\u8f85\u5bfc\u6210\u672c\u9ad8/High cost of 1-to-1 tutoring]\n    Problem --\x3e P2[AI\u8f85\u5bfc\u7684\u6709\u6548\u6027\u4e0e\u5b89\u5168\u6027\u672a\u77e5/Unproven efficacy & safety of AI tutoring]\n    Method --\x3e M1[\u6574\u5408LearnLM\u6a21\u578b/Integrate LearnLM (pedagogically fine-tuned AI)]\n    Method --\x3e M2[\u5728Eedi\u5e73\u53f0\u8fdb\u884cRCT/Conduct RCT on Eedi platform]\n    Method --\x3e M3[\u4e13\u5bb6\u5bfc\u5e08\u76d1\u7763\u8f93\u51fa/Human tutors supervise AI drafts]\n    Results --\x3e R1[76.4%\u6d88\u606f\u88ab\u76f4\u63a5\u6279\u51c6/76.4% messages approved with minimal edits]\n    Results --\x3e R2[\u5b66\u751f\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d/Student performance equal or better]\n    Results --\x3e R3[\u89e3\u51b3\u65b0\u95ee\u9898\u80fd\u529b\u63d0\u53475.5%/5.5% improvement on novel problems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [diffusion models], [score function, approximation theory, deep neural networks, curse of dimensionality, Ornstein-Uhlenbeck process]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Konstantin Yakovlev, Nikita Puchkin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," HSE University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23643",children:"https://arxiv.org/pdf/2512.23643"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Presents a theory for simultaneous approximation of the score function and its derivatives, extending beyond the first-order setting. 2. Derives approximation error bounds that are free from the curse of dimensionality. 3. Relaxes the common assumption of bounded data support, enabling handling of distributions with low-dimensional structure and unbounded support."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3404ba95328f082dc11706309fc0e30ca110b3f842a24921698b46a1065bfda6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops a theoretical framework for using deep neural networks to approximate the score function and its derivatives simultaneously. The method relaxes the typical bounded support assumption and provides error bounds that avoid the curse of dimensionality. The main conclusion is that this theory enables more efficient handling of complex data distributions, which is crucial for improving the convergence of diffusion and ODE-based generative models."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9700\u8981\u540c\u65f6\u903c\u8fd1\u5206\u6570\u51fd\u6570\u53ca\u5176\u5bfc\u6570/Need to approximate score function and its derivatives]\n    B --\x3e B2[\u5904\u7406\u65e0\u754c\u652f\u6491\u548c\u4f4e\u7ef4\u7ed3\u6784\u6570\u636e/Handle data with unbounded support & low-dim structure]\n    C --\x3e C1[\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u6846\u67b6/Deep Neural Network theoretical framework]\n    D --\x3e D1[\u903c\u8fd1\u8bef\u5dee\u65e0\u7ef4\u5ea6\u8bc5\u5492/Approximation bounds free from curse of dimensionality]\n    D --\x3e D2[\u653e\u677e\u6709\u754c\u652f\u6491\u5047\u8bbe/Relaxes bounded support assumption]\n    D --\x3e D3[\u4fdd\u8bc1\u4efb\u610f\u9636\u5bfc\u6570\u903c\u8fd1/Guarantees for derivatives of any order]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Random Controlled Differential Equations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [time-series learning], [controlled differential equations, random features, signature kernels, reservoir computing, rough paths]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Francesco Piatti, Thomas Cass, William F. Turner"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Imperial College London"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23670",children:"https://arxiv.org/pdf/2512.23670"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/FrancescoPiatti/RandomSigJax",children:"https://github.com/FrancescoPiatti/RandomSigJax"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A training-efficient framework combining random features with Controlled Differential Equations (CDEs) to create continuous-time reservoirs for time-series learning. 2. Two novel variants: Random Fourier CDEs (RF-CDEs) for kernel-free RBF approximation and Random Rough DEs (R-RDEs) for stable, efficient modeling of rough-path inputs. 3. Theoretical proof that these models induce the RBF-lifted and rough signature kernels in the infinite-width limit, unifying random-feature reservoirs, continuous-time architectures, and path-signature theory."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68926f52841c63ea8f7ccc6a25444c327f8f05d4f014a21ea65330e8b1d0af5a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces a fast and scalable framework for time-series learning by using large, randomly parameterized Controlled Differential Equations (CDEs) as continuous-time reservoirs, with only a linear readout layer trained. Two specific model variants, RF-CDEs and R-RDEs, are proposed and shown to approximate powerful signature kernels. The methods achieve competitive or state-of-the-art performance on benchmarks, offering a practical alternative to explicit signature computations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Random Controlled Differential Equations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: Efficient learning for time-series data)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Random-feature CDE reservoirs with linear readout)\n    C --\x3e D[RF-CDEs: \u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81/Random Fourier Features]\n    C --\x3e E[R-RDEs: \u968f\u673a\u7c97\u7cd9\u5fae\u5206\u65b9\u7a0b/Random Rough DEs]\n    A --\x3e F(\u5173\u952e\u7ed3\u679c/Results: Competitive SOTA performance, induces signature kernels)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] End-to-End Test-Time Training for Long Context"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [test-time training, meta-learning, sliding-window attention, continual learning, long-context modeling]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel R\xf8d, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Stanford University, UC Berkeley, UC San Diego, Astera Institute, NVIDIA"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23675",children:"https://arxiv.org/pdf/2512.23675"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Formulates long-context modeling as a continual learning problem, enabling a standard sliding-window Transformer to learn at test time via next-token prediction, 2. Uses meta-learning during training to optimize the model's initialization for efficient test-time learning, 3. Achieves scaling performance comparable to full-attention Transformers while maintaining constant inference latency like RNNs, resulting in significant speedups for long contexts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a044c036c50d93caf06213291a7e5d53aecb58cc761c7738c635ddb4234a64d3_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an end-to-end test-time training method for long-context language modeling. It uses a standard sliding-window attention Transformer that learns continuously at test time via next-token prediction, with its initialization optimized via meta-learning during training. The method matches the scaling performance of full-attention Transformers while offering constant inference latency, making it 2.7x faster for 128K contexts."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[End-to-End Test-Time Training for Long Context] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Long-context language modeling]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Test-Time Training with meta-learning & sliding-window attention]\n    D[\u5173\u952e\u7ed3\u679c/Results: Matches full-attention scaling, constant latency, 2.7x faster]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Eliciting Behaviors in Multi-Turn Conversations"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [nlp], [llm evaluation], [behavior elicitation, multi-turn conversation, online methods, dynamic benchmarks, test case generation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Google DeepMind, Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23701",children:"https://arxiv.org/pdf/2512.23701"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes an analytical framework categorizing behavior elicitation methods into three families based on their interaction with the target model (prior knowledge, offline, online). 2. Introduces a generalized multi-turn formulation for online behavior elicitation methods, unifying single-turn and multi-turn settings. 3. Demonstrates the superior efficiency of online methods in discovering failure cases in multi-turn conversations compared to static benchmarks, advocating for a shift to dynamic evaluation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afe3305263c3b509bdd6e846cce6501d101c0ecedb788ef025ac0c9405a28103_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper studies the problem of efficiently eliciting specific behaviors from large language models in multi-turn conversational settings. It introduces a framework for categorizing existing elicitation methods and proposes a generalized online method for multi-turn interactions. The key finding is that online methods can discover many more failure cases with few queries than static benchmarks, highlighting the need for dynamic evaluation approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Eliciting Behaviors in Multi-Turn Conversations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How to efficiently elicit specific behaviors from LLMs in multi-turn conversations?)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Categorizes methods into three families; Proposes a generalized multi-turn online formulation.)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Online methods achieve high success rates with few queries, outperforming static benchmarks.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [Integrated Sensing and Communication (ISAC), Channel State Information (CSI), Attention Model, Irregular Sampling, Wi-Fi Sensing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Gaofeng Dong, Kang Yang, Mani Srivastava"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles (UCLA)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22143",children:"https://arxiv.org/pdf/2512.22143"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes UniFi, the first Wi-Fi ISAC framework that eliminates intrusive packet injection by exploiting irregularly sampled CSI from diverse communication packets across multiple bands. 2. Introduces a CSI sanitization pipeline to harmonize heterogeneous packets and a time-aware attention model that learns directly from non-uniform CSI sequences. 3. Presents CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d5e6f4970d32933635a3174be0b8ba23c4b996e18b04bd75310812bd1071737_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents UniFi, a Wi-Fi sensing framework that solves the problem of communication degradation caused by high-rate probing packets. It achieves this by directly using irregularly sampled Channel State Information from existing communication traffic across multiple frequency bands, combined with a novel sanitization pipeline and a time-aware attention model. Evaluations show that UniFi achieves state-of-the-art sensing accuracy while fully preserving communication throughput."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[UniFi: Wi-Fi Sensing] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: \u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u6ce8\u5165\u63a2\u6d4b\u5305\uff0c\u964d\u4f4e\u901a\u4fe1\u6027\u80fd/Existing systems rely on probing packets, degrading communication]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u5229\u7528\u591a\u9891\u6bb5\u901a\u4fe1\u5305\u7684\u975e\u5747\u5300CSI\uff0c\u4f7f\u7528\u51c0\u5316\u7ba1\u9053\u548c\u6ce8\u610f\u529b\u6a21\u578b/Exploit irregular CSI from multi-band comm packets with sanitization & attention]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u6d88\u9664\u5305\u6ce8\u5165\uff0c\u4fdd\u6301\u901a\u4fe1\u541e\u5410\u91cf\uff0c\u5b9e\u73b0SOTA\u7cbe\u5ea6/Eliminates packet injection, preserves throughput, achieves SOTA accuracy]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Training AI Co-Scientists Using Rubric Rewards"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [research plan generation, self-grading, rubric rewards]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Meta Superintelligence Labs, ELLIS Institute T\xfcbingen, Max Planck Institute for Intelligent Systems, University of Oxford, University of Cambridge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23707",children:"https://arxiv.org/pdf/2512.23707"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A scalable method to automatically extract research goals and goal-specific grading rubrics from existing papers to build a training corpus. 2. A reinforcement learning framework with self-grading, where a frozen initial model acts as the grader using rubrics, enabling unsupervised improvement. 3. Demonstration of significant performance gains (12-22% relative improvement) and cross-domain generalization (e.g., to medical research) validated by human experts and frontier model juries."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/360be253dbca068ab35e1e7dcf794f5e43ae1d6fd7478850692d4a8ffc057d14_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of training language models to generate high-quality, constraint-following research plans. The proposed method uses reinforcement learning with self-grading, where rubrics automatically extracted from research papers provide reward signals. The approach shows significant improvements in plan quality and generalizes across domains like machine learning and medicine, validated by human expert preference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["Training AI Co-Scientists Using Rubric Rewards"] --\x3e Problem["\u6838\u5fc3\u95ee\u9898/Problem: LMs struggle to generate research plans that follow all constraints."]\n    Root --\x3e Method["\u4e3b\u8981\u65b9\u6cd5/Method: RL with self-grading using automatically extracted rubrics."]\n    Root --\x3e Results["\u5173\u952e\u7ed3\u679c/Results: Human experts prefer finetuned model\'s plans; method generalizes across domains."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface], [EEG-to-Voice, mel-spectrogram, domain adaptation, automatic speech recognition, language model correction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hanbeot Park, Yunjeong Cho, Hunhee Kim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Pukyong National University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22146",children:"https://arxiv.org/pdf/2512.22146"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed a direct, open-loop EEG-to-Voice reconstruction pipeline that generates mel-spectrograms from EEG without requiring dynamic time warping or explicit temporal alignment. 2. Applied transfer learning-based domain adaptation by pretraining a subject-specific generator on spoken speech and adapting it to imagined speech. 3. Integrated a minimal language model-based correction module to reduce ASR errors while preserving semantic structure, improving linguistic accuracy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30fda90c01417e82377bfcdd82830a196b29afd74925e2009f1a1a1d02d4d2bd_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a direct EEG-to-Voice paradigm to reconstruct speech from non-invasive EEG signals for both spoken and imagined speech. The method uses a subject-specific generator to produce mel-spectrograms, followed by a vocoder and ASR, and employs domain adaptation from spoken to imagined speech. The results demonstrate the feasibility of open-loop speech reconstruction without explicit temporal alignment, with stable acoustic and linguistic performance."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[EEG-to-Voice Decoding of Spoken and Imagined speech] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: EEG-based speech reconstruction is challenging due to noise, low resolution, and lack of aligned targets for imagined speech.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: Direct, open-loop EEG-to-mel-spectrogram generation with subject-specific generators, domain adaptation from spoken to imagined speech, and optional LM-based ASR correction.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Feasibility demonstrated for both speech types; stable acoustic/linguistic performance; LM correction reduces CER/WER without semantic distortion.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [on-device ai], [indoor vertical hydroponics, IoT sensors, LSTM, DNN, Linear Regression]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Emna Bouzid, Noura Baccar, Kamran Iqbal, Yassine Chaouch, Fares Ben Youssef, Amine Regayeg, Sarra Toumi, Houda Nsir, Amina Mseddi, Leila Costelle"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Arkansas, Little Rock; Mediterranean Institute of Technology, South Mediterranean University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22151",children:"https://arxiv.org/pdf/2512.22151"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Developed a prediction system for basil yield in IoT-enabled indoor vertical hydroponic farms using ML models. 2. Conducted a comparative performance analysis of Linear Regression, LSTM, and DNN models, evaluating accuracy, execution time, and RAM usage. 3. Identified DNN as offering an optimal balance between computational efficiency (speed/RAM) and high prediction accuracy (98%), making it suitable for real-world, resource-conscious deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db5f20edab19e2fc847a47e72931db82045d6fc345183bcd4be1eb31da0f25e0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses water scarcity in agriculture by proposing a machine learning-based system to predict basil yield in IoT-enabled indoor vertical hydroponic farms. It compares Linear Regression, LSTM, and DNN models using sensor data, finding that DNN provides a good trade-off between high accuracy (98%) and computational efficiency, making it suitable for practical deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Paper Title: Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Agriculture water scarcity, need for efficient solutions]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: ML models (LR, LSTM, DNN) trained on IoT sensor data from hydroponic farm]\n    D[\u5173\u952e\u7ed3\u679c/Results: DNN balances accuracy (98%) and computational efficiency for real-life deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [climate informatics], [similarity-preserving representation, latent space clustering, physics-coherent monitoring]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tsinghua University, National Climate Centre, China Meteorological Administration"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22144",children:"https://arxiv.org/pdf/2512.22144"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Introduced an "AI-for-theory" paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an "AI-for-theory" paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI\'s role as a discovery tool for complex systems.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Defining MJO lifecycle is challenging due to propagation; classical methods conflate artifacts with physics.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>AI-for-theory paradigm; Deep learning model (PhysAnchor-MJO-AE) learns similarity-preserving latent representation for objective clustering.]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>First complete six-phase MJO anatomy; New physics-coherent monitoring framework reduces errors by an order of magnitude.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [spatiotemporal forecasting], [4DVarNet, U-Net, sequence-to-sequence, sea level anomaly, neural forecast]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Daria Botvynko, Pierre Hasl\xe9e, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Mont\xe9gut, Anass El Aouni, Julien Le Sommer, Ronan Fablet"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IMT Atlantique, Ifremer, CNRS, Mercator Ocean International"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22152",children:"https://arxiv.org/pdf/2512.22152"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Neural Ocean Forecasting from Sparse Observations] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u7a00\u758f\u536b\u661f\u6570\u636e\u4e0b\u7684\u77ed\u671f\u6d77\u6d0b\u9884\u62a5/Short-term ocean forecasting from sparse satellite data)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u57fa\u4e8eU-Net\u548c4DVarNet\u7684\u7aef\u5230\u7aef\u5e8f\u5217\u9884\u6d4b/End-to-end sequence forecasting using U-Net & 4DVarNet)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u795e\u7ecf\u6a21\u578b\u8d85\u8d8a\u4e1a\u52a1\u5316\u57fa\u7ebf\uff0c\u5728\u591a\u53d8\u533a\u57df\u6539\u8fdb\u663e\u8457/Neural model outperforms operational baseline, notable improvements in high-variability regions)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [constrained sampling], [Langevin Monte Carlo, navigation functions, constrained sampling, non-convex support, adaptive temperature]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Nicolas Zilberstein, Santiago Segarra, Luiz Chamon"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rice University, \xc9cole Polytechnique (Institut Polytechnique de Paris)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22153",children:"https://arxiv.org/pdf/2512.22153"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces shielded Langevin Monte Carlo (LMC) for sampling from distributions with non-convex supports defined by convex sets with convex holes. 2. Incorporates a navigation function-inspired approach using a spatially adaptive temperature and repulsive drift to keep samples within feasible regions. 3. Demonstrates effectiveness through experiments on 2D Gaussian mixture and MIMO symbol detection, showing advantages over unconstrained methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b94714cf81960de10c530b580016be757aee25e63d9344efa57a771bb15c9bc_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes shielded Langevin Monte Carlo, a constrained sampling method that uses navigation potentials to sample from unnormalized target distributions over punctured (non-convex) supports. It modifies the Langevin diffusion with adaptive temperature and repulsive drift to avoid holes. Experiments on Gaussian mixtures and MIMO detection show it outperforms unconstrained sampling."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Sampling from non-convex supports with convex holes")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Shielded LMC with adaptive temperature and repulsive drift")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: Outperforms unconstrained sampling in 2D Gaussian mixture and MIMO detection")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [brain-computer interface (BCI)], [temporal convolution, residual attention, recurrent networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Md Shahriar Sajid, Abhijit Kumar Ghosh, Fariha Nusrat"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rajshahi University of Engineering & Technology, BRAC University, University of Asia Pacific"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22172",children:"https://arxiv.org/pdf/2512.22172"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposed PaperNet, a compact hybrid architecture combining temporal convolutions, channel-wise residual attention, and a lightweight bidirectional recurrent block for EEG classification. 2. Demonstrated high performance (macro-F1 0.96) on the BEED dataset with only ~0.6M parameters under a subject-independent protocol. 3. Provided interpretability through channel-wise attention weights to reveal electrode relevance and validated efficiency for deployment on resource-constrained systems."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42ceecec9d77194ecb3fce40cd41e394fe9fe473136aa06f3cec099aa5631ac0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PaperNet, a lightweight deep learning model that integrates temporal convolutions, channel residual attention, and a bidirectional recurrent block for efficient EEG epilepsy detection. It achieves a macro-F1 score of 0.96 on the BEED dataset with only about 0.6 million parameters, showing balanced performance across classes. The results indicate that combining temporal filtering, channel reweighting, and recurrent context modeling can deliver strong classification without high computational cost."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[PaperNet: EEG\u766b\u75eb\u68c0\u6d4b / PaperNet: EEG Epilepsy Detection] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: EEG\u4fe1\u53f7\u566a\u58f0\u591a\u3001\u53d8\u5f02\u6027\u5927 / Problem: EEG signals are noisy and variable]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: \u65f6\u95f4\u5377\u79ef+\u901a\u9053\u6b8b\u5dee\u6ce8\u610f\u529b+\u8f7b\u91cf\u5faa\u73af\u5757 / Method: Temporal convolutions + Channel residual attention + Lightweight recurrent block]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: \u5b8fF1=0.96, \u53c2\u65700.6M, \u9ad8\u6548\u90e8\u7f72 / Results: Macro-F1=0.96, 0.6M params, efficient deployment]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [ensemble learning], [Fibonacci weighting, Rao-Blackwell optimization, variance reduction, recursive ensemble, orthogonalization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ernest Fokou\xe9"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rochester Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22284",children:"https://arxiv.org/pdf/2512.22284"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Fibonacci Ensembles, a novel ensemble learning framework using normalized Fibonacci weights optimized via orthogonalization and Rao-Blackwellization for systematic variance reduction. 2. Proposes a second-order recursive ensemble dynamic inspired by the Fibonacci sequence to enhance representational depth beyond classical boosting. 3. Develops a General Weighting Theory that unifies various ensemble methods (bagging, boosting, stacking, etc.) under a single mathematical framework as distributional operators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3aac35c4c829c693f4e9607fedf9124e05465f66f615b3d6ff040b6b1d9348a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Fibonacci Ensembles, a new ensemble learning method inspired by the Fibonacci sequence, which uses mathematically optimized Fibonacci weights and a recursive dynamic to reduce variance and improve model depth. Experimental results on one-dimensional regression show it can match or outperform uniform averaging and integrates effectively with orthogonal Rao-Blackwellization. The work suggests Fibonacci ensembles offer a natural and interpretable design within ensemble theory."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[On Fibonacci Ensembles] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: How to design a principled ensemble learning method inspired by natural harmony?]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Use Fibonacci weights with orthogonalization/Rao-Blackwell optimization and recursive dynamics]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Fibonacci weighting matches/improves uniform averaging and integrates with Rao-Blackwellization]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [matrix factorization], [nonnegative matrix factorization, identifiability, latent class analysis, probabilistic latent semantic analysis, end-member analysis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Qianqian Qi, Peter G. M. van der Heijden"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Hangzhou Dianzi University, Utrecht University, University of Southampton"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22282",children:"https://arxiv.org/pdf/2512.22282"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Highlights the similarities among five popular matrix factorization models (LBA, LCA, EMA, PLSA, NMF) that are often presented separately across different fields. 2. Proves a unified identifiability condition, showing that the solution uniqueness for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. 3. Provides a brief review of algorithms for these models and illustrates their application with a social science time budget dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369a6018ccc708c5d8368b9a1290021cb1b1e9d92785fa247417c9aeb167a164_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper reviews and unifies five nonnegative matrix factorization models (NMF, PLSA, LBA, EMA, LCA) from different disciplines, focusing on the identifiability issue. It proves that the uniqueness of solutions for LBA, EMA, LCA, and PLSA is equivalent to the uniqueness of the NMF solution. The work clarifies model similarities, reviews algorithms, and demonstrates application with a real-world dataset."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898: A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: \u8de8\u9886\u57df\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u6a21\u578b\u7684\u76f8\u4f3c\u6027\u4e0e\u53ef\u8bc6\u522b\u6027/Similarity and identifiability of cross-domain nonnegative matrix factorization models]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: \u7406\u8bba\u5206\u6790\u4e0e\u7edf\u4e00\u8bc1\u660e/Theoretical analysis and unified proof]\n    D[\u5173\u952e\u7ed3\u679c/Results: \u8bc1\u660eLBA, EMA, LCA, PLSA\u89e3\u7684\u552f\u4e00\u6027\u4e0eNMF\u89e3\u7684\u552f\u4e00\u6027\u7b49\u4ef7/Proved solution uniqueness of LBA, EMA, LCA, PLSA is equivalent to uniqueness of NMF solution]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [ensemble learning], [weighting theory, spectral complexity, approximation geometry, bias-variance decomposition, constrained quadratic program]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ernest Fokou\xe9"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rochester Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22286",children:"https://arxiv.org/pdf/2512.22286"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Develops a general weighting theory for ensembles that moves beyond variance reduction, formalizing ensembles as linear operators with geometric and spectral constraints. 2. Derives a refined bias-variance-approximation decomposition showing how structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity. 3. Provides a unified theoretical framework that subsumes classical averaging, stacking, and recent Fibonacci-based ensembles, showing optimal weights arise from constrained quadratic programs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a91a59418696aa33fbf07981fc6c57647153631a58f387a01a431ae28237ebb8_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new theoretical framework for ensemble learning that explains its effectiveness beyond the traditional variance-reduction argument, particularly for stable base learners. The method formalizes ensembles as linear operators and shows how structured, non-uniform weighting can optimize performance by managing spectral complexity and approximation geometry. The main conclusion is that the principal role of aggregation for low-variance learners is the redistribution of spectral complexity, establishing a foundation for structure-driven ensemble design."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    A[A General Weighting Theory for Ensemble Learning<br>\u96c6\u6210\u5b66\u4e60\u7684\u4e00\u822c\u52a0\u6743\u7406\u8bba] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1["\u4f20\u7edf\u65b9\u5dee\u7f29\u51cf\u7406\u8bba\u65e0\u6cd5\u89e3\u91ca\u7a33\u5b9a\u57fa\u5b66\u4e60\u5668\u7684\u96c6\u6210\u6548\u679c<br>Traditional variance reduction fails to explain ensembles of stable learners"]\n    C --\x3e C1["\u5c06\u96c6\u6210\u5f62\u5f0f\u5316\u4e3a\u5177\u6709\u51e0\u4f55\u4e0e\u8c31\u7ea6\u675f\u7684\u7ebf\u6027\u7b97\u5b50<br>Formalize ensembles as linear operators with geometric/spectral constraints"]\n    C --\x3e C2["\u63a8\u5bfc\u504f\u5dee-\u65b9\u5dee-\u8fd1\u4f3c\u5206\u89e3<br>Derive bias-variance-approximation decomposition"]\n    D --\x3e D1["\u7ed3\u6784\u5316\u52a0\u6743\u65b9\u6848\u5728\u7406\u8bba\u4e0a\u4f18\u4e8e\u5747\u5300\u5e73\u5747<br>Structured weighting provably dominates uniform averaging"]\n    D --\x3e D2["\u6700\u4f18\u6743\u91cd\u662f\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u7684\u89e3<br>Optimal weights are solutions to constrained QPs"]\n    D --\x3e D3["\u7edf\u4e00\u7406\u8bba\u6db5\u76d6\u7ecf\u5178\u5e73\u5747\u3001\u5806\u53e0\u7b49<br>Unified theory subsumes averaging, stacking, etc."]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [transformer interpretability], [cross-entropy, gradient dynamics, attention mechanism, expectation-maximization, Bayesian inference]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Dream Sports, Columbia University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22473",children:"https://arxiv.org/pdf/2512.22473"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[Transformer\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u5982\u4f55\u5f62\u6210?/How is transformer internal geometry formed?]\nC --\x3e C1[\u63a8\u5bfc\u6ce8\u610f\u529b\u68af\u5ea6\u52a8\u6001/Derive attention gradient dynamics]\nC --\x3e C2[\u5efa\u7acbEM\u7b97\u6cd5\u7c7b\u6bd4/Establish EM algorithm analogy]\nD --\x3e D1[\u53d1\u73b0\u4f18\u52bf\u8def\u7531\u4e0e\u8d23\u4efb\u66f4\u65b0/Discover advantage-based routing & responsibility-weighted update]\nD --\x3e D2[\u68af\u5ea6\u6d41\u5851\u9020\u8d1d\u53f6\u65af\u6d41\u5f62/Gradient flow sculpts Bayesian manifolds]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-target regression], [squeeze-and-excitation blocks, wide-and-deep neural networks, Latin Hypercube Sampling, SHAP analysis, multi-input multi-target learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Behzad Parvaresh, Rahmat K. Adesunkanmi, Adel Alaeddini"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Southern Methodist University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22397",children:"https://arxiv.org/pdf/2512.22397"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduced a data-efficient multi-input, multi-target learning approach integrating Latin Hypercube Sampling (LHS) with a squeeze-and-excitation wide and deep neural network (SE-WDNN) for predicting mechanical and manufacturing properties of additively manufactured fiber-reinforced composites. 2. Demonstrated superior performance of SE-WDNN over baseline models (e.g., feedforward neural networks, XGBoost) with the lowest overall test error (MAPE=12.33%) and statistically significant improvements for several target variables. 3. Provided interpretability through SHAP analysis, identifying reinforcement strategy as the major influence on mechanical performance, enabling guided parameter selection balancing mechanical behavior and manufacturing metrics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46886a511640f1fda32bb5caad6d94ff9e4208332562a065cf26f5a070434085_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of predicting multiple properties in additively manufactured fiber-reinforced composites, where performance is sensitive to process and material parameters. The authors propose a sample-efficient method combining Latin Hypercube Sampling with a novel squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict mechanical and manufacturing properties. The model outperforms several baseline machine learning models, achieving the lowest test error, and SHAP analysis reveals that reinforcement strategy is the most influential factor, demonstrating the approach's effectiveness for interpretable, multi-target prediction in this domain."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites") --\x3e Problem("\u6838\u5fc3\u95ee\u9898/Problem: Exhaustive testing of CFRC-AM properties is impractical due to complex parameter interactions.")\n    Root --\x3e Method("\u4e3b\u8981\u65b9\u6cd5/Method: Integrate LHS-guided experimentation with a novel SE-WDNN model for multi-target prediction.")\n    Root --\x3e Results("\u5173\u952e\u7ed3\u679c/Results: SE-WDNN achieves lowest test error (MAPE=12.33%); SHAP shows reinforcement strategy is key.")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [uncertainty quantification], [sparse variational Gaussian processes, Kolmogorov-Arnold networks, flow reconstruction]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Y. Sungtaek Ju"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Los Angeles"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22426",children:"https://arxiv.org/pdf/2512.22426"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel machine learning framework (SVGP-KAN) for uncertainty-aware flow field reconstruction, combining sparse variational Gaussian processes with Kolmogorov-Arnold network topology. 2. Enables principled epistemic uncertainty quantification, extending classical methods like Linear Stochastic Estimation (LSE) and Spectral Analysis Modal Methods (SAMM). 3. Provides a systematic evaluation demonstrating that the method achieves accuracy comparable to established techniques while offering well-calibrated uncertainty estimates that reliably indicate prediction quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78057a4908248b84f2c93949508c1a8ed187c23a15c17c9cb4d97da3da80d1a6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a machine learning framework called SVGP-KAN for reconstructing time-resolved flow fields from sparse measurements. The method combines sparse variational Gaussian processes with Kolmogorov-Arnold networks to provide accurate reconstructions along with principled uncertainty estimates. The results show the framework achieves comparable accuracy to classical methods while offering reliable uncertainty quantification, which is valuable for experimental design in periodic flows."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u4ece\u7a00\u758f\u6d4b\u91cf\u4e2d\u91cd\u5efa\u77ac\u6001\u6d41\u573a/Reconstruct time-resolved flow fields from sparse measurements]\n    C --\x3e C1[SVGP-KAN\u6846\u67b6/SVGP-KAN framework]\n    C1 --\x3e C2[\u7a00\u758f\u53d8\u5206\u9ad8\u65af\u8fc7\u7a0b/Sparse Variational Gaussian Processes]\n    C1 --\x3e C3[Kolmogorov-Arnold\u7f51\u7edc\u62d3\u6251/Kolmogorov-Arnold Network Topology]\n    D --\x3e D1[\u7cbe\u5ea6\u4e0e\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53/Accuracy comparable to established methods]\n    D --\x3e D2[\u63d0\u4f9b\u826f\u597d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1/Provides well-calibrated uncertainty estimates]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Likelihood-Preserving Embeddings for Statistical Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [statistical inference], [likelihood-preserving embeddings, likelihood-ratio distortion, Hinge Theorem, approximate sufficient statistics, surrogate MLE]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Deniz Akdemir"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Not explicitly provided; inferred from email domain as independent researcher or unspecified institution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22638",children:"https://arxiv.org/pdf/2512.22638"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces the Likelihood-Ratio Distortion metric and the Hinge Theorem, establishing it as the necessary and sufficient condition for preserving likelihood-based inference. 2. Proves an impossibility result for universal likelihood preservation, motivating model-class-specific guarantees. 3. Provides a constructive framework using neural networks as approximate sufficient statistics with explicit bounds linking training loss to inferential guarantees."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aff5ca31934e9408fcf0755ba3d0be2604e2c3a6692b27396516614ac67f2b0a_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem that modern machine learning embeddings often destroy the geometric structure needed for classical likelihood-based statistical inference. It proposes a theory of likelihood-preserving embeddings, centered on controlling the Likelihood-Ratio Distortion, and proves that this control is necessary and sufficient to preserve tests, Bayes factors, and MLEs. The main conclusion is that with this framework, neural network embeddings can be made compatible with classical inference workflows under specific, provable conditions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Likelihood-Preserving Embeddings for Statistical Inference] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: ML embeddings destroy geometric structure for likelihood-based inference]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Theory of likelihood-preserving embeddings using Likelihood-Ratio Distortion metric and Hinge Theorem]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Controlling distortion preserves tests & MLEs; framework for neural networks as approximate sufficient statistics]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Machine learning models for predicting catastrophe bond coupons using climate data"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [financial machine learning], [catastrophe bonds, climate indicators, extremely randomized trees, gradient boosting, risk pricing]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Julia Ko\u0144czal, Micha\u0142 Balcerek, Krzysztof Burnecki"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Wroc\u0142aw University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22660",children:"https://arxiv.org/pdf/2512.22660"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel integration of large-scale climate indicators (e.g., ONI, NAO, SSTs) into the prediction of catastrophe bond coupon rates. 2. Systematically compares the performance of linear regression against advanced tree-based ensemble methods (RF, GBM, ERT, XGBoost) for this financial prediction task. 3. Demonstrates that including climate variables improves predictive accuracy across all models, with Extremely Randomized Trees achieving the best performance, quantifying the influence of climate variability on CAT bond pricing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad97e42c90e08944209aa776783ca45b8fd5114532eaa6a8db48fee77ac0f8e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper investigates the use of machine learning models to predict catastrophe bond coupons by incorporating climate data. The authors combine traditional financial features with climate indicators and compare models like linear regression, random forest, and gradient boosting. The results show that climate variables improve prediction accuracy, with extremely randomized trees performing best, indicating that climate variability significantly impacts bond pricing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Machine learning models for predicting catastrophe bond coupons using climate data] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: \u5982\u4f55\u9884\u6d4b\u5de8\u707e\u503a\u5238\u606f\u7968\uff1f/How to predict CAT bond coupons?)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: \u7ed3\u5408\u6c14\u5019\u6307\u6807\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b/Combine climate indicators with ML models)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: \u6c14\u5019\u53d8\u91cf\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u6781\u7aef\u968f\u673a\u6811\u8868\u73b0\u6700\u4f73/Climate variables improve accuracy, Extremely Randomized Trees perform best)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [medical audio classification], [Audio Spectrogram Transformer, Sharpness-Aware Minimization, ICBHI 2017, class imbalance, loss landscape]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Atakan I\u015f\u0131k, Selin Vulga I\u015f\u0131k, Ahmet Feridun I\u015f\u0131k, Mah\u015fuk Taylan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Ba\u015fkent University, Gaziantep University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22564",children:"https://arxiv.org/pdf/2512.22564"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>\u547c\u5438\u58f0\u97f3\u5206\u7c7b\u7684\u51e0\u4f55\u611f\u77e5\u4f18\u5316] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n\n    B --\x3e B1[\u6570\u636e\u9650\u5236\u4e0e\u8fc7\u62df\u5408<br/>Data Constraints & Overfitting]\n    B1 --\x3e B2[\u6570\u636e\u96c6\u5c0f\u3001\u566a\u58f0\u5927\u3001\u7c7b\u522b\u4e0d\u5e73\u8861<br/>Small, Noisy, Imbalanced Dataset]\n\n    C --\x3e C1[\u4f7f\u7528SAM\u4f18\u5316AST<br/>Enhance AST with SAM]\n    C1 --\x3e C2[\u4f18\u5316\u635f\u5931\u66f2\u9762\u51e0\u4f55<br/>Optimize Loss Surface Geometry]\n    C --\x3e C3[\u52a0\u6743\u91c7\u6837\u7b56\u7565<br/>Weighted Sampling Strategy]\n\n    D --\x3e D1[SOTA\u5206\u6570: 68.10%<br/>SOTA Score: 68.10%]\n    D --\x3e D2[\u9ad8\u654f\u611f\u5ea6: 68.31%<br/>High Sensitivity: 68.31%]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [computational neuroscience], [DFINE, state-space models, intracranial EEG, neural forecasting, brain-computer interfaces]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Kiarash Vaziri, Lucine L. Oganesian, HyeongChan Jo, Roberto M.C. Vera, Charles Y. Liu, Brian Lee, Maryam M. Shanechi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Southern California"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22785",children:"https://arxiv.org/pdf/2512.22785"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Extended the DFINE framework, originally for intracortical recordings, to model multisite human intracranial EEG (iEEG) signals. 2. Demonstrated that DFINE significantly outperforms linear state-space models (LSSMs) and matches or exceeds the accuracy of GRU models in forecasting future neural activity. 3. Showed that DFINE handles missing observations more robustly than baseline models, highlighting its flexible inference capability for practical BCI applications."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b25c55145e75d86ed37939fcb3e028529910a649ed86c8bd2fdb2e8fb91c496_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper extends the DFINE framework to model nonlinear dynamics in human intracranial EEG (iEEG) data. DFINE combines neural networks with a linear state-space model backbone to enable accurate neural forecasting and robust handling of missing data. The results show DFINE outperforms linear models, matches or beats GRU performance, and is particularly effective in high gamma bands, making it a promising tool for next-generation brain-computer interfaces."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root("Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference") --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem("\u6838\u5fc3\u95ee\u9898/Problem") --\x3e P1("\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u795e\u7ecf\u6d3b\u52a8\u4e2d\u7684\u975e\u7ebf\u6027\u7ed3\u6784/Linear models fail to capture nonlinear neural structure")\n    Problem --\x3e P2("RNN\u6a21\u578b\u4e0d\u76f4\u63a5\u5904\u7406\u7f3a\u5931\u89c2\u6d4b\u503c/RNN models do not directly handle missing observations")\n    Method("\u4e3b\u8981\u65b9\u6cd5/Method") --\x3e M1("\u6269\u5c55DFINE\u6846\u67b6\u81f3iEEG\u5efa\u6a21/Extend DFINE framework to iEEG modeling")\n    Method --\x3e M2("\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b/Integrate neural networks with linear state-space models")\n    Results("\u5173\u952e\u7ed3\u679c/Results") --\x3e R1("DFINE\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b/DFINE significantly outperforms LSSM")\n    Results --\x3e R2("DFINE\u5339\u914d\u6216\u8d85\u8fc7GRU\u7684\u9884\u6d4b\u7cbe\u5ea6/DFINE matches or exceeds GRU accuracy")\n    Results --\x3e R3("DFINE\u66f4\u9c81\u68d2\u5730\u5904\u7406\u7f3a\u5931\u6570\u636e/DFINE handles missing observations more robustly")'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Causal-Policy Forest for End-to-End Policy Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal inference], [policy learning, causal forest, conditional average treatment effect (CATE), end-to-end learning, random forests]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Masahiro Kato"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The University of Tokyo"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22846",children:"https://arxiv.org/pdf/2512.22846"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Establishes an equivalence between maximizing policy value and minimizing MSE for CATE under a specific regression model, providing a theoretical foundation. 2. Proposes the causal-policy forest, a novel end-to-end algorithm that modifies the widely-used causal forest for direct policy learning. 3. Integrates policy training steps more tightly than prior methods, avoiding separate nuisance parameter estimation and improving computational efficiency."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2b4476f0b9f739e0d89d20df3fd68607b0631baf53a2956bd05175daf10d348_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an end-to-end algorithm for policy learning in causal inference, called the causal-policy forest. It modifies the causal forest method by leveraging a theoretical equivalence between policy value maximization and CATE estimation. The method unifies policy training steps, is computationally efficient, and bridges the gap between policy learning and CATE estimation in practice."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Causal-Policy Forest for End-to-End Policy Learning] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Policy learning from observational data to recommend optimal treatments] --\x3e Problem_Sub[\u76ee\u6807/Goal: Maximize policy value]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Causal-Policy Forest] --\x3e Method_Sub1[\u57fa\u7840/Foundation: Equivalence of policy value max and CATE MSE min]\n    Method --\x3e Method_Sub2[\u7b97\u6cd5/Algorithm: Modify causal forest for end-to-end policy learning]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Three advantages] --\x3e Results_Sub1[\u4f18\u52bf1/Advantage 1: Bridges policy learning and CATE estimation]\n    Results --\x3e Results_Sub2[\u4f18\u52bf2/Advantage 2: More end-to-end training]\n    Results --\x3e Results_Sub3[\u4f18\u52bf3/Advantage 3: Computationally efficient]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A first-order method for nonconvex-strongly-concave constrained minimax optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [optimization theory], [minimax optimization, augmented Lagrangian method, first-order method, operation complexity, nonconvex-strongly-concave]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhaosong Lu, Sanyou Mei"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Minnesota, The Hong Kong University of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22909",children:"https://arxiv.org/pdf/2512.22909"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a first-order augmented Lagrangian method for solving nonconvex-strongly-concave constrained minimax problems. 2. Develops a new first-order method to solve the resulting unconstrained minimax subproblems by leveraging the strong concavity structure. 3. Establishes an improved operation complexity of O(\u03b5^{-3.5} log \u03b5^{-1}) for finding an \u03b5-KKT solution, which is a factor of \u03b5^{-0.5} better than the previous best-known result."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8900c67d50fc273f41601bb4bbe900a26bc3aacdb1b31495591bc5c452297c76_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of nonconvex-strongly-concave constrained minimax optimization. The authors propose a novel first-order augmented Lagrangian method, where the subproblems are solved by a specially designed first-order algorithm. The main result is that their method achieves an improved operation complexity of O(\u03b5^{-3.5} log \u03b5^{-1}) for finding an approximate KKT solution."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[\u8bba\u6587\u6807\u9898 / Paper Title: A first-order method for nonconvex-strongly-concave constrained minimax optimization] --\x3e B(\u6838\u5fc3\u95ee\u9898 / Problem: \u975e\u51f8-\u5f3a\u51f9\u7ea6\u675f\u6781\u5c0f\u6781\u5927\u4f18\u5316 / Nonconvex-strongly-concave constrained minimax optimization)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5 / Method: \u4e00\u9636\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5 / First-order augmented Lagrangian method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c / Results: \u64cd\u4f5c\u590d\u6742\u5ea6 O(\u03b5^{-3.5} log \u03b5^{-1}) / Operation complexity O(\u03b5^{-3.5} log \u03b5^{-1}))"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Deep Learning for the Multiple Optimal Stopping Problem"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning / optimal stopping], [multiple optimal stopping, dynamic programming principle, neural network approximation, high-dimensional problems, American basket options]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Mathieu Lauri\xe8re, Mehdi Talbi"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Center for Data Science; NYU-ECNU Institute of Mathematical Sciences at NYU Shanghai; NYU Shanghai; Laboratoire de Probabilit\xe9s, Statistiques et Mod\xe9lisation, Universit\xe9 Paris-Cit\xe9"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22961",children:"https://arxiv.org/pdf/2512.22961"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel deep learning framework combining the Dynamic Programming Principle with neural networks to solve high-dimensional multiple optimal stopping problems. 2. Provides theoretical error analysis for both the discrete-time problem (neural network training error) and continuous problems (discretization error). 3. Demonstrates the method's efficiency and scalability through numerical experiments on high-dimensional American basket options and nonlinear utility maximization."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ef52bc9da77a7dcb529e7b4b3c239b993aeb4f5fc7464ce8d4f6e1c468ee677_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a deep learning framework to solve the challenging multiple optimal stopping problem in high dimensions. The method combines the Dynamic Programming Principle with neural network approximation of the value function. Numerical experiments show it is an efficient and scalable solution for problems like pricing American basket options."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Deep Learning for the Multiple Optimal Stopping Problem] --\x3e Problem[\u6838\u5fc3\u95ee\u9898/Problem]\n    Root --\x3e Method[\u4e3b\u8981\u65b9\u6cd5/Method]\n    Root --\x3e Results[\u5173\u952e\u7ed3\u679c/Results]\n    Problem --\x3e P1[\u9ad8\u7ef4\u591a\u505c\u6b62\u95ee\u9898/High-dim Multiple Stopping]\n    Problem --\x3e P2[\u590d\u6742\u9012\u5f52\u4f9d\u8d56/Complex Recursive Dependencies]\n    Method --\x3e M1[\u52a8\u6001\u89c4\u5212\u539f\u7406/DPP]\n    Method --\x3e M2[\u795e\u7ecf\u7f51\u7edc\u503c\u51fd\u6570\u8fd1\u4f3c/Neural Network Value Approximation]\n    Results --\x3e R1[\u7406\u8bba\u8bef\u5dee\u5206\u6790/Theoretical Error Analysis]\n    Results --\x3e R2[\u9ad8\u6548\u53ef\u6269\u5c55\u65b9\u6cd5/Efficient & Scalable Method]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Risk-Averse Learning with Varying Risk Levels"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [online convex optimization], [Conditional Value-at-Risk (CVaR), dynamic regret, zeroth-order optimization, risk-level variation, first-order optimization]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Siyi Wang, Zifan Wang, Karl H. Johansson"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KTH Royal Institute of Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22986",children:"https://arxiv.org/pdf/2512.22986"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces a novel risk-level variation metric to capture the dynamics of changing risk preferences in online optimization. 2. Develops risk-averse learning algorithms for both first-order and zeroth-order information settings under a limited sampling budget. 3. Provides dynamic regret bounds for the proposed algorithms, analyzing their performance in terms of function variation, risk-level variation, and sample count."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19b18e34aeb85bbf659ba6da5ac559cf4534627ec075752ac91c1f532838981e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper tackles risk-averse online optimization in non-stationary environments where both the cost functions and the desired risk level can change over time. The authors propose new algorithms for first-order and zeroth-order settings that use Conditional Value-at-Risk (CVaR) and analyze their dynamic regret, showing adaptability to changing conditions. Numerical experiments validate the effectiveness of the proposed methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Risk-Averse Learning with Varying Risk Levels<br>\u98ce\u9669\u89c4\u907f\u5b66\u4e60\u4e0e\u53d8\u5316\u7684\u98ce\u9669\u6c34\u5e73] --\x3e B(Problem: Safety-critical decision-making in dynamic, risk-sensitive environments<br>\u6838\u5fc3\u95ee\u9898: \u52a8\u6001\u98ce\u9669\u654f\u611f\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5173\u952e\u51b3\u7b56)\n    A --\x3e C(Method: CVaR-based online algorithms for first-order & zeroth-order settings<br>\u4e3b\u8981\u65b9\u6cd5: \u57fa\u4e8eCVaR\u7684\u4e00\u9636\u548c\u96f6\u9636\u5728\u7ebf\u7b97\u6cd5)\n    A --\x3e D(Results: Dynamic regret bounds and numerical validation<br>\u5173\u952e\u7ed3\u679c: \u52a8\u6001\u9057\u61be\u754c\u4e0e\u6570\u503c\u9a8c\u8bc1)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [simulation-based inference], [Bayesian adaptive design, amortized inference, diffusion models, sequential experimental design, policy learning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Niels Bracher, Lars K\xfchmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian B\xfcrkner, Stefan T. Radev"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22999",children:"https://arxiv.org/pdf/2512.22999"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Actively optimizing design variables for parameter estimation] --\x3e Problem_Sub[\u5b50\u95ee\u9898/Sub-problem: Sequential design and inference are typically treated separately]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Jointly amortize design and inference via end-to-end training] --\x3e Method_Sub1[\u7f51\u7edc/Networks: Policy, History, and Inference (Diffusion-based) networks]\n    Method --\x3e Method_Sub2[\u635f\u5931\u51fd\u6570/Loss: Aggregates incremental posterior error reduction]\n    Results[\u5173\u952e\u7ed3\u679c/Results: Superior/competitive performance on standard benchmarks]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Deep Learning for Art Market Valuation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [multi-modal learning], [multi-modal deep learning, visual embeddings, Grad-CAM, hedonic regression, repeated-sales dataset]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23078",children:"https://arxiv.org/pdf/2512.23078"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Deep Learning for Art Market Valuation<br/>\u827a\u672f\u5e02\u573a\u4f30\u503c\u7684\u6df1\u5ea6\u5b66\u4e60] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br/>How to improve art market valuation?<br/>\u5982\u4f55\u6539\u8fdb\u827a\u672f\u5e02\u573a\u4f30\u503c\uff1f]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>\u878d\u5408\u8868\u683c\u4e0e\u56fe\u50cf\u6570\u636e\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60]\n    D[\u5173\u952e\u7ed3\u679c/Results<br/>Visual features help most for fresh-to-market works<br/>\u89c6\u89c9\u7279\u5f81\u5bf9\u9996\u6b21\u4e0a\u5e02\u4f5c\u54c1\u6700\u6709\u5e2e\u52a9]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [federated learning], [L0 regularization, probabilistic gates, communication efficiency, model sparsity, federated stochastic gradient descent]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," \xc5bo Akademi University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23071",children:"https://arxiv.org/pdf/2512.23071"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a novel federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and their continuous relaxation to achieve target sparsity. 2. Derives the L0 constrained stochastic minimization objective from an entropy maximization problem of the stochastic gates. 3. Demonstrates that the method can achieve high target sparsity (down to \u03c1=0.005) under data and client heterogeneity with minimal loss in statistical performance, outperforming magnitude pruning-based methods."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of poor generalizability and communication inefficiency in Federated Learning due to overly dense models. It proposes a method to enforce L0 sparsity constraints via probabilistic gates, deriving the objective from entropy maximization and implementing it with federated stochastic gradient descent. The method is shown to be communication-efficient and achieves high target sparsity with better statistical performance than pruning-based baselines on synthetic and real datasets."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: \u6570\u636e\u4e0e\u6a21\u578b\u56fa\u6709\u7684\u7a00\u758f\u6027\u672a\u88ab\u89e3\u51b3\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5bc6\u3001\u6cdb\u5316\u6027\u5dee\uff0c\u4e14\u5b58\u5728\u6570\u636e\u548c\u5ba2\u6237\u7aef\u53c2\u4e0e\u5f02\u8d28\u6027\u3002]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: \u901a\u8fc7\u6982\u7387\u95e8\u53ca\u5176\u8fde\u7eed\u677e\u5f1b\u5bf9\u975e\u96f6\u53c2\u6570\u5bc6\u5ea6\u65bd\u52a0L0\u7ea6\u675f\uff0c\u76ee\u6807\u6e90\u81ea\u968f\u673a\u95e8\u7684\u71b5\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u57fa\u4e8e\u8054\u90a6\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002]\n    Results[\u5173\u952e\u7ed3\u679c/Results: \u5728\u6570\u636e\u548c\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u4e0b\uff0c\u80fd\u8fbe\u5230\u76ee\u6807\u5bc6\u5ea6(\u03c1)\uff0c\u7edf\u8ba1\u6027\u80fd\u635f\u5931\u6700\u5c0f\uff0c\u4e14\u6bd4\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u66f4\u4f18\u3001\u901a\u4fe1\u9ad8\u6548\u3002]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [generative models], [variational autoencoder, QSAR, molecular generation, Fr\xe9chet ChemNet Distance, retrosynthesis]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tim C. Pearce, Ahmed Ibrahim"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Leicester, University of Cambridge"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23080",children:"https://arxiv.org/pdf/2512.23080"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A novel generative AI framework combining a VAE with a QSAR model to design novel odorant molecules from limited training data., 2. Demonstration of effective latent space structuring for odor likelihood, enabling exploration of novel chemical scaffolds beyond simple derivatization., 3. Comprehensive validation showing generated molecules are syntactically valid, unique, thermodynamically stable, and synthetically viable."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2f78de871dd5cdb6d0663be15fa9ec9b8b77d5cae24344545c68979b5c02eaf_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a generative AI framework that combines a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to design novel odorant molecules. The method structures the VAE's latent space based on odor probability, enabling the generation of valid, unique, and synthetically viable candidate molecules from a limited training set. The results show the model successfully explores novel chemical space, producing stable candidates with practical synthesis routes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:'graph TB\n    Root["QSAR-Guided Generative Framework for Odorant Discovery<br>\u57fa\u4e8eQSAR\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u6c14\u5473\u5206\u5b50\u53d1\u73b0\u6846\u67b6"] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n\n    Problem["\u6838\u5fc3\u95ee\u9898/Problem<br>Challenging to discover novel odorants from vast chemical space with limited data<br>\u4ece\u5de8\u5927\u5316\u5b66\u7a7a\u95f4\u4e2d\u5229\u7528\u6709\u9650\u6570\u636e\u53d1\u73b0\u65b0\u6c14\u5473\u5206\u5b50\u5177\u6709\u6311\u6218\u6027"]\n    Method["\u4e3b\u8981\u65b9\u6cd5/Method<br>VAE + QSAR model for de novo molecular design<br>\u4f7f\u7528VAE\u4e0eQSAR\u6a21\u578b\u8fdb\u884c\u4ece\u5934\u5206\u5b50\u8bbe\u8ba1"]\n    Results["\u5173\u952e\u7ed3\u679c/Results<br>Generates valid, unique, novel, stable, and synthetically viable odorant candidates<br>\u751f\u6210\u6709\u6548\u3001\u72ec\u7279\u3001\u65b0\u9896\u3001\u7a33\u5b9a\u4e14\u53ef\u5408\u6210\u7684\u6c14\u5473\u5206\u5b50\u5019\u9009\u7269"]'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [statistical machine learning], [attenuation bias, latent variable, neural networks, measurement error, joint likelihood]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuan-Sen Ting"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Ohio State University, Max-Planck-Institut f\xfcr Astronomie"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23138",children:"https://arxiv.org/pdf/2512.23138"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/tingyuansen/LatentNN",children:"https://github.com/tingyuansen/LatentNN"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Demonstrates that neural networks suffer from attenuation bias, a systematic underestimation of extreme values due to input measurement errors. 2. Proposes LatentNN, a method that generalizes the latent variable solution from linear regression to neural networks by jointly optimizing network parameters and latent input values. 3. Validates the method's effectiveness in reducing bias across various scenarios, including low signal-to-noise astronomical data, and defines its effective operational regime."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e1e58f31518d154aacb0668496d22b5dba87a170b019457f35d0fd0bfb1e03b_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of attenuation bias, where neural networks systematically underestimate extreme values due to noisy input measurements. It introduces LatentNN, a method that treats true inputs as latent variables and jointly optimizes them with the network parameters by maximizing the joint data likelihood. The results show that LatentNN effectively reduces this bias, especially in the low signal-to-noise regimes common in astronomy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u8870\u51cf\u504f\u5dee / Neural networks suffer from attenuation bias]\n    B1 --\x3e B2[\u8f93\u5165\u6d4b\u91cf\u8bef\u5dee\u5bfc\u81f4\u6781\u7aef\u503c\u88ab\u4f4e\u4f30 / Input measurement errors cause underestimation of extreme values]\n    C --\x3e C1[\u63d0\u51faLatentNN\u65b9\u6cd5 / Propose LatentNN method]\n    C1 --\x3e C2[\u5c06\u771f\u5b9e\u8f93\u5165\u4f5c\u4e3a\u6f5c\u53d8\u91cf / Treat true inputs as latent variables]\n    C2 --\x3e C3[\u8054\u5408\u4f18\u5316\u7f51\u7edc\u53c2\u6570\u548c\u6f5c\u53d8\u91cf / Jointly optimize network parameters and latent values]\n    D --\x3e D1[\u6709\u6548\u51cf\u5c11\u8870\u51cf\u504f\u5dee / Effectively reduces attenuation bias]\n    D1 --\x3e D2[\u5728\u4f4e\u4fe1\u566a\u6bd4\u5929\u6587\u6570\u636e\u4e2d\u8868\u73b0\u826f\u597d / Performs well in low-SNR astronomical data]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Kullback-Leibler divergence, decision paralysis, intent selection, affordance selection, hierarchical decision process]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Wendyam Eric Lionel Ilboudo, Saori C Tanaka"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Nara Institute of Science and Technology"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23144",children:"https://arxiv.org/pdf/2512.23144"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem<br>Decision Paralysis] --\x3e P1[\u6311\u6218/Challenge<br>Choice models assume ready-to-compare options]\n    Problem --\x3e P2[\u73b0\u8c61/Phenomenon<br>Hesitation, freezing, failure to act]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method<br>Computational Account] --\x3e M1[\u67b6\u6784/Architecture<br>Hierarchical decision process]\n    Method --\x3e M2[\u5f62\u5f0f\u5316/Formalization<br>Intent vs. Affordance selection]\n    Method --\x3e M3[\u76ee\u6807/Objective<br>Mixture of reverse-KL & forward-KL]\n    Results[\u5173\u952e\u7ed3\u679c/Results<br>Simulation Outcomes] --\x3e R1[\u884c\u4e3a/Behavior<br>Slow, heavy-tailed response times]\n    Results --\x3e R2[\u5931\u8d25\u6a21\u5f0f/Failure Modes<br>Intent & Affordance saturation]\n    Results --\x3e R3[\u89e3\u91ca/Interpretation<br>Autism as an extreme regime]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [stochastic optimization], [gradient clipping, heavy-tailed noise, nonsmooth convex optimization, convergence analysis, Freedman's inequality]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zijian Liu"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," New York University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23178",children:"https://arxiv.org/pdf/2512.23178"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Provided a refined analysis for Clipped SGD under heavy-tailed noise, achieving faster high-probability convergence rates that depend on a novel "generalized effective dimension" term. 2. Extended the refined analysis to convergence in expectation, obtaining new rates that break previously known lower bounds and proving their optimality by matching newly established lower bounds. 3. Established new lower bounds for both high-probability and in-expectation convergence, completing the theoretical landscape and confirming the optimality of the new in-expectation rates.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14888c84379a942fea9e71d6d6a8df252ea20840f1e922a872a2fa3ee4897b2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper refines the theoretical analysis of Clipped Stochastic Gradient Descent (Clipped SGD) for nonsmooth convex optimization under heavy-tailed gradient noise. By improving the use of Freedman's inequality and providing finer bounds for clipping error, the authors derive faster high-probability convergence rates and new optimal in-expectation rates that surpass previous lower bounds. The work also establishes matching lower bounds, demonstrating the optimality of the proposed analysis for convergence in expectation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\nA --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\nA --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\nB --\x3e B1[Heavy-tailed gradient noise with bounded p-th moment (p\u2208(1,2]) / \u5177\u6709\u6709\u754cp\u9636\u77e9\u7684\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0]\nC --\x3e C1[Refined analysis of Clipped SGD / \u5bf9Clipped SGD\u7684\u7cbe\u7ec6\u5206\u6790]\nC1 --\x3e C2[Better use of Freedman's inequality / \u66f4\u597d\u5730\u5229\u7528Freedman\u4e0d\u7b49\u5f0f]\nC1 --\x3e C3[Finer bounds for clipping error / \u5bf9\u88c1\u526a\u8bef\u5dee\u7684\u66f4\u7cbe\u7ec6\u8fb9\u754c]\nD --\x3e D1[Faster high-probability rates with generalized effective dimension / \u5177\u6709\u5e7f\u4e49\u6709\u6548\u7ef4\u5ea6\u7684\u66f4\u5feb\u9ad8\u6982\u7387\u6536\u655b\u7387]\nD --\x3e D2[New optimal in-expectation rates breaking known lower bounds / \u6253\u7834\u5df2\u77e5\u4e0b\u754c\u7684\u65b0\u6700\u4f18\u671f\u671b\u6536\u655b\u7387]\nD --\x3e D3[New matching lower bounds established / \u5efa\u7acb\u4e86\u65b0\u7684\u5339\u914d\u4e0b\u754c]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Persistent Homology via Finite Topological Spaces"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [other], [topological data analysis], [persistent homology, finite topological spaces, posets, crosscut complexes, stability]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sel\xe7uk Kayacan"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Bah\xe7e\u015fehir University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23348",children:"https://arxiv.org/pdf/2512.23348"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a functorial framework for persistent homology using filtrations of finite topological spaces and posets, bypassing the need for inclusion relations between simplicial complexes. 2. Demonstrates that standard simplifications at the poset level preserve persistent invariants. 3. Proves the stability of the resulting persistence diagrams under metric perturbations in a density-based instantiation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66f9ee5417a1e46f255a691f585eeb3b163bd4b47edaafbe115e358a54f4c884_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a new functorial framework for persistent homology that starts from a finite metric space and constructs a filtration of finite topological spaces, then functorially maps these to posets and simplicial complexes via crosscut constructions. This approach decouples the metric from the homological analysis and does not require inclusion maps between complexes. The authors show that poset-level simplifications preserve persistent invariants and prove the stability of the resulting persistence diagrams."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Persistent Homology via Finite Topological Spaces] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem: Standard persistent homology relies on inclusion-based filtrations of simplicial complexes.]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method: A functorial framework using filtrations of finite topological spaces and posets, mapped to complexes via crosscut constructions.]\n    D[\u5173\u952e\u7ed3\u679c/Results: Poset-level simplifications preserve invariants; persistence diagrams are stable under metric perturbations.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Probabilistic Modelling is Sufficient for Causal Inference"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [causal inference], [probabilistic modelling, causal inference, do-operator, structural causal models, Bayesian networks]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Bruno Mlodozeniec, David Krueger, Richard E. Turner"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Cambridge, Max Planck Institute for Intelligent Systems, MILA"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23408",children:"https://arxiv.org/pdf/2512.23408"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"}),' 1. Demonstrates that standard probabilistic modelling is sufficient for answering causal inference questions without requiring special causal frameworks. 2. Provides concrete examples showing how causal problems (interventional and counterfactual) can be solved by "writing down the probability of everything". 3. Reinterprets established causal tools (e.g., do-operator, do-calculus) as "syntactic sugar" emerging from standard probabilistic modelling, clarifying their utility.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42538108be020614ac69c6d340cfeef4400c649311210c2f7081e50dc3d98ef5_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper argues that causal inference questions can be fully addressed using standard probabilistic modelling and inference, without needing specialized causal tools or notation. The core method is to "write down the probability of everything" to model and solve both interventional and counterfactual problems. The authors conclude that causal-specific frameworks are not fundamentally necessary but can be seen as convenient abstractions built upon probabilistic foundations.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Probabilistic Modelling is Sufficient for Causal Inference] --\x3e B\n    A --\x3e C\n    A --\x3e D\n    B[\u6838\u5fc3\u95ee\u9898/Problem<br>Confusion over tools for causal inference in ML] --\x3e B1[\u58f0\u79f0\u9700\u8981\u4e13\u95e8\u56e0\u679c\u6846\u67b6/Claims need for bespoke causal frameworks]\n    C[\u4e3b\u8981\u65b9\u6cd5/Method<br>Write down the probability of everything] --\x3e C1[\u4f7f\u7528\u6982\u7387\u5efa\u6a21\u89e3\u51b3\u56e0\u679c\u95ee\u9898/Solve causal questions with probabilistic modelling]\n    D[\u5173\u952e\u7ed3\u679c/Results<br>Causal tools are syntactic sugar] --\x3e D1[\u56e0\u679c\u5de5\u5177\u6e90\u4e8e\u6982\u7387\u5efa\u6a21/Causal tools emerge from probabilistic modelling]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [alpha screening, large language models, reinforcement learning, factor investing, economic reasoning]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, StepFun, FinStep"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23515",children:"https://arxiv.org/pdf/2512.23515"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"code:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/FinStep-AI/Alpha-R1",children:"https://github.com/FinStep-AI/Alpha-R1"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Signal decay and regime shifts in non-stationary markets; existing methods overlook semantic rationale for factor relevance.]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Alpha-R1, an 8B-parameter LLM trained via RL, reasons over factor logic and real-time news for context-aware alpha screening.]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results: Outperforms benchmark strategies; exhibits improved robustness to alpha decay across multiple asset pools.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [graph neural networks], [graph U-Net, adaptive pooling, feature fusion, strain field prediction, solid rocket motor]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiada Huang, Hao Ma, Zhibin Shen, Yizhou Qiao, Haiyang Li"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology, Zhengzhou University of Aeronautics"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23443",children:"https://arxiv.org/pdf/2512.23443"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes GrainGNet, an adaptive graph network with an adaptive pooling dynamic node selection mechanism to preserve key mechanical features in critical structural regions. 2. Utilizes feature fusion to transmit deep features and enhance the model's representational capacity for 3D strain field prediction. 3. Demonstrates significant performance improvements, including a 62.8% reduction in mean squared error and a sevenfold training efficiency gain over a baseline graph U-Net, with particular accuracy in high-strain regions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a35459d2c7214afb29d74bb1f279ddd56dc621982ef3910ac226653dcfc465f1_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes GrainGNet, an adaptive fusion graph network, to predict the 3D strain field in solid rocket motor grains, addressing the computational expense of traditional simulations. The model uses adaptive pooling and feature fusion to accurately capture high-strain regions. It achieves a 62.8% reduction in mean squared error and improved training efficiency compared to baseline methods, offering a high-fidelity approach for structural safety evaluation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\nA[Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem: Local high strain causes structural failure; traditional simulations are expensive; surrogate models lack geometric accuracy.]\nA --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method: Proposes GrainGNet with adaptive pooling and feature fusion to preserve key features and enhance representation.]\nA --\x3e D[\u5173\u952e\u7ed3\u679c/Results: 62.8% MSE reduction vs. baseline; 7x training efficiency; 33% error reduction in high-strain regions.]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] A general framework for deep learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [statistical learning theory], [deep neural networks, Bernstein-type inequality, excess risk bound, minimax optimality, mixing processes]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," William Kengne, Modou Wade"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universit\xe9 Jean Monnet, CY Cergy Paris Universit\xe9"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23425",children:"https://arxiv.org/pdf/2512.23425"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Proposes a general theoretical framework for deep learning that unifies analysis for data satisfying a generalized Bernstein-type inequality, encompassing independent and various dependent (mixing) observations. 2. Introduces two novel estimators: a Non-Penalized Deep Neural Network (NPDNN) and a Sparse-Penalized Deep Neural Network (SPDNN) estimator. 3. Establishes minimax optimal (up to logarithmic factors) convergence rates for the expected excess risk of both estimators on H\xf6lder smooth and composition function classes."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4efb1121ddfae593e4dedab080501366fad55274c1b149910f149a1920a0b6e2_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops a general theoretical framework for analyzing deep neural network estimators in settings including nonparametric regression and classification. It proposes two estimators (NPDNN and SPDNN) and derives upper bounds for their expected excess risk for data satisfying a generalized Bernstein-type inequality, covering independent and various dependent data processes. The main conclusion is that both proposed estimators achieve minimax optimal convergence rates in many classical settings."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[A general framework for deep learning] --\x3e B[\u6838\u5fc3\u95ee\u9898/Problem]\n    A --\x3e C[\u4e3b\u8981\u65b9\u6cd5/Method]\n    A --\x3e D[\u5173\u952e\u7ed3\u679c/Results]\n    B --\x3e B1[\u7edf\u4e00\u7406\u8bba\u5206\u6790\u6846\u67b6/Unified theoretical analysis framework]\n    B1 --\x3e B2[\u6570\u636e\u6ee1\u8db3\u5e7f\u4e49Bernstein\u4e0d\u7b49\u5f0f/Data satisfies generalized Bernstein inequality]\n    B2 --\x3e B3[\u5305\u542b\u72ec\u7acb\u4e0e\u6df7\u5408\u89c2\u6d4b/Includes independent and mixing observations]\n    C --\x3e C1[\u63d0\u51fa\u4e24\u79cd\u4f30\u8ba1\u5668/Propose two estimators]\n    C1 --\x3e C2[\u975e\u60e9\u7f5a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc/NPDNN]\n    C1 --\x3e C3[\u7a00\u758f\u60e9\u7f5a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc/SPDNN]\n    D --\x3e D1[\u63a8\u5bfc\u671f\u671b\u8d85\u989d\u98ce\u9669\u4e0a\u754c/Establish upper bounds for expected excess risk]\n    D1 --\x3e D2[H\xf6lder\u51fd\u6570\u7c7b/H\xf6lder function classes]\n    D --\x3e D3[\u8bc1\u660e\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6027/Prove minimax optimality]\n    D3 --\x3e D4[\u591a\u79cd\u7ecf\u5178\u8bbe\u7f6e/Many classical settings]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [stochastic system identification], [overdamped Langevin dynamics, sparse observations, geometric constraints, stochastic control, path augmentation]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Dimitra Maoutsa"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Berlin"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23566",children:"https://arxiv.org/pdf/2512.23566"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. A new framework that reconciles geometric and temporal perspectives by reformulating inference as a stochastic control problem. 2. A method using geometry-driven path augmentation, guided by the system's invariant density, to reconstruct trajectories without assuming specific parametric models. 3. Demonstrating accurate recovery of stochastic dynamics from extremely undersampled data, outperforming existing methods in synthetic benchmarks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7a01c554437b016c9e156f0cb5f7047dd8973704b2623e3d218e1b9c76975c0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the problem of learning stochastic dynamics from sparse temporal observations. It proposes a new framework that uses geometry-driven path augmentation within a stochastic control formulation to infer the underlying laws without parametric assumptions. The method successfully recovers overdamped Langevin dynamics from highly undersampled data, outperforming existing approaches."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem: How to learn stochastic dynamics from sparse observations?);\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method: Reformulate inference as a stochastic control problem with geometry-driven path augmentation);\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results: Accurately recovers dynamics from undersampled data, outperforms existing methods);"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] The Nonstationarity-Complexity Tradeoff in Return Prediction"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [financial machine learning], [non-stationarity, model selection, adaptive window selection, return prediction, tournament procedure]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Columbia University, Stanford University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23596",children:"https://arxiv.org/pdf/2512.23596"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Identifies and formalizes the nonstationarity-complexity tradeoff in return prediction, where complex models reduce misspecification but require longer, more non-stationary training windows. 2. Proposes a novel model selection method that jointly optimizes model class and training window size using an adaptive tournament procedure evaluated on non-stationary validation data. 3. Provides theoretical analysis showing the method balances misspecification error, estimation variance, and non-stationarity, and demonstrates its empirical superiority with significant performance gains in out-of-sample prediction and trading strategy returns, especially during recessions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07919419a5b2bc9758efcadd74b91c11fcdca1529edf541c429c2694c06ddde0_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper addresses the challenge of predicting stock returns in non-stationary environments by identifying a tradeoff between model complexity and non-stationarity. It proposes a new model selection method that jointly chooses the model and its training window via an adaptive tournament. The method outperforms standard benchmarks, particularly during economic recessions, and generates higher cumulative returns for a trading strategy."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[The Nonstationarity-Complexity Tradeoff in Return Prediction] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9884\u6d4b\u4e2d\u7684\u975e\u5e73\u7a33\u6027-\u590d\u6742\u6027\u6743\u8861/Nonstationarity-Complexity Tradeoff in Prediction]\n    C --\x3e C1[\u8054\u5408\u4f18\u5316\u6a21\u578b\u4e0e\u7a97\u53e3\u7684\u9526\u6807\u8d5b\u65b9\u6cd5/Tournament Method for Joint Model & Window Selection]\n    D --\x3e D1[OOS R\xb2\u63d0\u5347 & \u8870\u9000\u671f\u8868\u73b0\u4f18\u5f02/Improved OOS R\xb2 & Superior Recession Performance]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Calibrated Multi-Level Quantile Forecasting"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [online learning, forecasting], [quantile forecasting, calibration, online learning, adversarial robustness, no-regret guarantee]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23671",children:"https://arxiv.org/pdf/2512.23671"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Multi-Level Quantile Tracker (MultiQT), a lightweight online method that wraps any existing forecaster to guarantee multi-level quantile calibration against adversarial distribution shifts. 2. Provides theoretical guarantees including calibration and a no-regret property ensuring asymptotic performance is not worse than the base forecaster. 3. Ensures the corrected forecasts are properly ordered across different quantile levels."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/133e23e4bb898bd730a1ed52ef1e6f20bce16c1d2910d09105ad3e344368fe6f_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper addresses the problem of producing reliable multi-level quantile forecasts that are calibrated. It proposes MultiQT, an online wrapper method that guarantees calibration and proper ordering of forecasts even under adversarial conditions, without asymptotically worsening the base forecaster's performance. Experiments show it significantly improves calibration in epidemic and energy forecasting tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    Root[Calibrated Multi-Level Quantile Forecasting] --\x3e Problem\n    Root --\x3e Method\n    Root --\x3e Results\n    Problem[\u6838\u5fc3\u95ee\u9898/Problem: Need for reliable, calibrated multi-level quantile forecasts] --\x3e P1[\u6821\u51c6\u4fdd\u8bc1/Calibration Guarantee]\n    Problem --\x3e P2[\u987a\u5e8f\u6027/Ordering Constraint]\n    Method[\u4e3b\u8981\u65b9\u6cd5/Method: Multi-Level Quantile Tracker (MultiQT)] --\x3e M1[\u5728\u7ebf\u5305\u88c5\u5668/Online Wrapper]\n    Method --\x3e M2[\u5bf9\u6297\u6027\u9c81\u68d2/Adversarially Robust]\n    Method --\x3e M3[\u65e0\u9057\u61be\u4fdd\u8bc1/No-Regret Guarantee]\n    Results[\u5173\u952e\u7ed3\u679c/Results] --\x3e R1[\u663e\u8457\u6539\u8fdb\u6821\u51c6/Significantly Improves Calibration]\n    Results --\x3e R2[\u5b9e\u8bc1\u9a8c\u8bc1/Empirical Validation in Real Problems]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] Bellman Calibration for V-Learning in Offline Reinforcement Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [ai], [reinforcement learning], [Bellman calibration, off-policy evaluation, value iteration, doubly robust estimator, Markov decision process]"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lars van der Laan, Nathan Kallus"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Washington, Netflix, Cornell University"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.23694",children:"https://arxiv.org/pdf/2512.23694"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"contributions:"})," 1. Introduces Iterated Bellman Calibration, a model-agnostic, post-hoc procedure for calibrating value predictions in infinite-horizon MDPs. 2. Adapts classical calibration methods (histogram, isotonic) to the dynamic, counterfactual setting using a doubly robust pseudo-outcome for off-policy data. 3. Provides finite-sample guarantees for calibration and prediction without requiring Bellman completeness or realizability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9300a417035535b3fec586f3ad8f9e10dae8a084d794d1d413f42e5ba2b05d37_w640_q70.webp"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Iterated Bellman Calibration, a post-hoc method to improve the accuracy of long-term value predictions in offline reinforcement learning. The method repeatedly regresses fitted Bellman targets onto a model's predictions, adapting classical calibration techniques to handle off-policy data. The analysis shows the approach provides finite-sample guarantees for calibrated predictions under weak assumptions, without needing Bellman completeness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mindmap:"})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TB\n    A[Bellman Calibration for V-Learning<br/>Bellman\u6821\u51c6\u7528\u4e8eV\u5b66\u4e60] --\x3e B(\u6838\u5fc3\u95ee\u9898/Problem)\n    A --\x3e C(\u4e3b\u8981\u65b9\u6cd5/Method)\n    A --\x3e D(\u5173\u952e\u7ed3\u679c/Results)\n    B --\x3e B1[\u9884\u6d4b\u957f\u671f\u56de\u62a5\u7684\u6821\u51c6<br/>Calibrating long-term value predictions]\n    B --\x3e B2[\u79bb\u7ebf\u3001\u53cd\u4e8b\u5b9e\u6570\u636e<br/>Offline, counterfactual data]\n    C --\x3e C1[\u8fed\u4ee3\u8d1d\u5c14\u66fc\u6821\u51c6<br/>Iterated Bellman Calibration]\n    C --\x3e C2[\u4f7f\u7528\u53cc\u7a33\u5065\u4f2a\u7ed3\u679c<br/>Using doubly robust pseudo-outcome]\n    C --\x3e C3[\u76f4\u65b9\u56fe/\u4fdd\u5e8f\u56de\u5f52\u9002\u914d<br/>Adapting histogram/isotonic regression]\n    D --\x3e D1[\u6709\u9650\u6837\u672c\u4fdd\u8bc1<br/>Finite-sample guarantees]\n    D --\x3e D2[\u65e0\u9700\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027<br/>No Bellman completeness required]"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv251230] INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [cv], [computational pathology], [graph neural network, survival prediction, spatial transcriptomics, colorectal cancer, histology]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Piotr Keller, Mark Eastwood, Zedong Hu, Aim\xe9e Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, Andr\xe9 D'Hoore, C\xe9dric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," KU Leuven, University of Oxford, University of Cambridge, University of Manchester, University of Bristol, University of Edinburgh, University of Glasgow, University of Sheffield, University of Southampton, University of Warwick, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strathclyde, University of Aberdeen, University of Dundee, University of St Andrews, University of Glasgow, University of Edinburgh, University of Manchester, University of Bristol, University of Cambridge, University of Oxford, University of London, University of Warwick, University of Sheffield, University of Southampton, University of York, University of Birmingham, University of Nottingham, University of Leeds, University of Liverpool, University of Newcastle, University of Exeter, University of Leicester, University of Sussex, University of Surrey, University of Strath"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22262",children:"https://arxiv.org/pdf/2512.22262"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"thumbnail:"})," ",(0,r.jsx)(n.a,{href:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp",children:"https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e16c74c6d57e6b8f3cec00c4cc6267b2b66595c314bba0f371e27e2f86f25458_w640_q70.webp"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);