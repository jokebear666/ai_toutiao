<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251110-20251116" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251110-20251116 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251110-20251116"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251110-20251116 | AI头条"><meta data-rh="true" name="description" content="2025-11-10"><meta data-rh="true" property="og:description" content="2025-11-10"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251110-20251116"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251110-20251116" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251110-20251116" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251110-20251116","item":"https://jokebear666.github.io/ai_toutiao/daily/20251110-20251116"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.bc6dbd00.css">
<script src="/ai_toutiao/assets/js/runtime~main.74e92485.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.16c2ba0f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251110-20251116</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251110-20251116</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-10">2025-11-10<a href="#2025-11-10" class="hash-link" aria-label="Direct link to 2025-11-10" title="Direct link to 2025-11-10" translate="no">​</a></h2>
<p><strong>cs.DC total: 5</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251110] GPU Under Pressure: Estimating Application&#x27;s Stress via Telemetry and Performance Counters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [telemetry parameters, hardware performance counters, throughput measurement, instruction count, stall events]</li>
<li class=""><strong>authors:</strong> Giuseppe Esposito, Juan-David Guerrero-Balaguera, Josie Esteban Rodriguez Condia, Matteo Sonza Reorda, Marco Barbiero, Rossella Fortuna</li>
<li class=""><strong>institution:</strong> Politecnico di Torino, Intesa Sanpaolo S.p.A.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05067</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes combining online telemetry parameters and hardware performance counters to estimate GPU stress caused by applications. The method focuses on measuring throughput, issued instructions, and stall events to assess workload efficiency. Results show this approach can effectively predict reliability issues and aging effects in GPUs under sustained parallel workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251110] The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [storage systems], [Fully Homomorphic Encryption, ASIC, GPU, storage I/O, performance analysis]</li>
<li class=""><strong>authors:</strong> Lei Chen, Erci Xu, Yiming Sun, Shengyu Fan, Xianglong Deng, Guiming Shi, Guang Fan, Liang Kong, Yilan Zhu, Shoumeng Yan, Mingzhe Zhang</li>
<li class=""><strong>institution:</strong> Ant Group, Shanghai Jiaotong University, University of Chinese Academy of Sciences, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04946" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04946</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the impact of storage I/O on Fully Homomorphic Encryption (FHE) application performance. The research finds that storage I/O significantly degrades performance, reducing ASIC performance by up to 357× and GPU performance by up to 22×, highlighting a critical bottleneck in FHE deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251110] Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [hyperdimensional computing, custom GPU instructions, RISC-V GPU, HDC-CNN hybrid models, microbenchmark testing]</li>
<li class=""><strong>authors:</strong> Wakuto Matsumi, Riaz-Ul-Haque Mian</li>
<li class=""><strong>institution:</strong> Shimane University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05053</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes custom GPU instructions for RISC-V GPUs to accelerate hybrid HDC-CNN models. The researchers implemented four specialized instructions optimized for hyperdimensional computing operations, achieving up to 56.2x performance improvement in microbenchmark tests. The work demonstrates RISC-V GPUs&#x27; potential for energy-efficient, high-performance computing through domain-specific instruction customization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251110] CUNQA: a Distributed Quantum Computing emulator for HPC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum computing simulation], [distributed quantum computing, quantum phase estimation, HPC emulation, quantum processing units]</li>
<li class=""><strong>authors:</strong> Jorge Vázquez-Pérez, Daniel Expósito-Patiño, Marta Losada, Álvaro Carballido, Andrés Gómez, Tomás F. Pena</li>
<li class=""><strong>institution:</strong> Galicia Supercomputing Center (CESGA), Universidad de Santiago de Compostela</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05209</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents CUNQA, an open-source distributed quantum computing emulator designed for HPC environments that implements three communication models: no-communication, classical-communication, and quantum-communication. The tool uses the Quantum Phase Estimation algorithm to demonstrate and analyze these distributed quantum computing schemes. CUNQA represents the first emulator capable of modeling all three distributed quantum computing approaches in HPC environments, enabling research and testing before physical quantum hardware becomes available.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251110] Marionette: Data Structure Description and Management for Heterogeneous Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [heterogeneous computing], [C++17 library, data layout abstraction, compile-time abstractions, memory management strategies, CUDA]</li>
<li class=""><strong>authors:</strong> Nuno dos Santos Fernandes, Pedro Tomás, Nuno Roma, Frank Winklmeier, Patricia Conde-Muíño</li>
<li class=""><strong>institution:</strong> Instituto Superior Técnico, INESC-ID, LIP, CERN, University of Oregon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.04853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.04853</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Marionette is a C++17 library that decouples data layout from interface descriptions to enable flexible and portable data structures for heterogeneous computing platforms. It provides efficient data transfers across devices with minimal runtime overhead through compile-time abstractions. The paper demonstrates that Marionette successfully addresses the challenges of adapting object-oriented C++ codebases for hardware acceleration while maintaining compatibility with existing code.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 14</strong></p>
<ul>
<li class="">[arXiv251110] QUESTER: Query Specification for Generative Retrieval <a href="https://arxiv.org/pdf/2511.05301" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning <a href="https://arxiv.org/pdf/2511.05489" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] DeepEyesV2: Toward Agentic Multimodal Model <a href="https://arxiv.org/pdf/2511.05271" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction <a href="https://arxiv.org/pdf/2511.05396" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning <a href="https://arxiv.org/pdf/2511.04856" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models <a href="https://arxiv.org/pdf/2511.04902" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.04883" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting <a href="https://arxiv.org/pdf/2511.04865" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Reasoning Up the Instruction Ladder for Controllable Language Models <a href="https://arxiv.org/pdf/2511.04694" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework <a href="https://arxiv.org/pdf/2511.05385" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Multi-agent Coordination via Flow Matching <a href="https://arxiv.org/pdf/2511.05005" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning <a href="https://arxiv.org/pdf/2511.04949" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale <a href="https://arxiv.org/pdf/2511.04904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones <a href="https://arxiv.org/pdf/2511.05265" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251110] When Data Falls Short: Grokking Below the Critical Threshold <a href="https://arxiv.org/pdf/2511.04760" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning &amp; Scheduling <a href="https://arxiv.org/pdf/2511.04758" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation <a href="https://arxiv.org/pdf/2511.05297" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] LiveStar: Live Streaming Assistant for Real-World Online Video Understanding <a href="https://arxiv.org/pdf/2511.05299" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Efficient Deployment of CNN Models on Multiple In-Memory Computing Units <a href="https://arxiv.org/pdf/2511.04682" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] APP: Accelerated Path Patching with Task-Specific Pruning <a href="https://arxiv.org/pdf/2511.05442" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media <a href="https://arxiv.org/pdf/2511.05357" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning <a href="https://arxiv.org/pdf/2511.04831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] Deep Progressive Training: scaling up depth capacity of zero/one-layer models <a href="https://arxiv.org/pdf/2511.04981" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars <a href="https://arxiv.org/pdf/2511.04798" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251110] A Gate-Based Quantum Genetic Algorithm for Real-Valued Global Optimization <a href="https://arxiv.org/pdf/2511.05254" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-11">2025-11-11<a href="#2025-11-11" class="hash-link" aria-label="Direct link to 2025-11-11" title="Direct link to 2025-11-11" translate="no">​</a></h2>
<p><strong>cs.DC total: 27</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251111] MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stepan Vanecek, Manuel Walter Mussbacher, Dominik Groessler, Urvij Saroliya, Martin Schulz</li>
<li class=""><strong>institution:</strong></li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05958</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] DWM-RO: Decentralized World Models with Reasoning Offloading for SWIPT-enabled Satellite-Terrestrial HetNets</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guangyuan Liu, Yinqiu Liu, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Sumei Sun, Abbas Jamalipour, Ping Zhang</li>
<li class=""><strong>institution:</strong></li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05972" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05972</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Efficient Dynamic MaxFlow Computation on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [Push-Relabel, GPU parallelization, dynamic graphs, batch updates]</li>
<li class=""><strong>authors:</strong> Shruthi Kannappan, Ashwina Kumar, Rupesh Nasre</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Madras</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05895" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05895</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two Push-Relabel based algorithms for dynamic MaxFlow computation on GPUs that efficiently handle both increments and decrements in edge capacities in batches. The algorithms are designed to process evolving real-world graphs without full recomputation. The results show that for small updates, dynamic recomputation is significantly faster than static GPU-based MaxFlow approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [function trace data, kernel-level anomaly detection, domain-adapted LLM, causal reasoning, nanosecond resolution]</li>
<li class=""><strong>authors:</strong> Yuyang Liu, Jingjing Cai, Jiayi Ren, Peng Zhou, Danyang Zhang, Yin Du, Shijian Li</li>
<li class=""><strong>institution:</strong> Zhejiang University, Alibaba Cloud</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05978</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KAT introduces a framework that uses function trace data to detect kernel-level anomalies at nanosecond resolution and integrates these detections with a domain-adapted LLM for causal reasoning. The system achieved high precision (0.884) and recall (0.936) in production evaluations. This significantly improves troubleshooting efficiency and success rates for large model distributed inference systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] LLMs as Packagers of HPC Software</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [retrieval-augmented generation, iterative refinement, repository analysis, diagnostic feedback]</li>
<li class=""><strong>authors:</strong> Caetano Melone, Daniel Nichols, Konstantinos Parasyris, Todd Gamblin, Harshitha Menon</li>
<li class=""><strong>institution:</strong> Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05626</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SpackIt, an end-to-end framework that uses LLMs with repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback to generate Spack recipes for HPC software. Results show this approach increases installation success rates from 20% in zero-shot settings to over 80%, demonstrating the value of retrieval and structured feedback for reliable package synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Distributed Recoverable Sketches (Extended Version)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [network monitoring], [Count-Min Sketch, distributed recovery, incremental updates, sketch partitioning, frequency estimation]</li>
<li class=""><strong>authors:</strong> Diana Cohen, Roy Friedman, Rana Shahout</li>
<li class=""><strong>institution:</strong> Technion, Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05762</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed framework for recovering sketch data after node crashes in network environments, focusing on frequency estimation sketches like Count-Min Sketch. The system explores trade-offs between space consumption, runtime overheads, and recovery traffic while comparing periodic full updates versus incremental updates. The framework is designed to be modular and generic, allowing other data structures to be integrated via an abstract API.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, proximal policy optimization, online convex optimization, hierarchical scheduling, collaborative edge computing]</li>
<li class=""><strong>authors:</strong> Guihang Hong, Tao Ouyang, Kongyange Zhao, Zhi Zhou, Xu Chen</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05915</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CoEdge-RAG, a hierarchical scheduling framework that optimizes retrieval-augmented LLMs in collaborative edge environments. The method uses PPO for online query identification, dynamic inter-node workload balancing, and intra-node resource allocation via online convex optimization. Comprehensive evaluations show the framework achieves significant performance gains of 4.23% to 91.39% over baseline methods across various QA tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Inductive Loop Analysis for Practical HPC Application Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [compiler optimization], [symbolic analysis, loop optimization, software prefetching, pointer incrementation, automatic parallelization]</li>
<li class=""><strong>authors:</strong> Philipp Schaad, Tal Ben-Nun, Patrick Iff, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> ETH Zurich, Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06052" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06052</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SILO, a symbolic inductive loop optimization technique that models data accesses and dependencies as functions of loop nest strides. This approach enables automatic parallelization of sequentially-dependent loops and data movement optimizations. The method achieves up to 12× speedup over state-of-the-art approaches on scientific computing kernels from atmospheric models and numerical solvers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [error-bounded lossy compression, gradient compression, temporal correlation, convolutional kernels, exponential moving average, sign prediction]</li>
<li class=""><strong>authors:</strong> Zhijing Ye, Sheng Di, Jiamin Wang, Zhiqing Zhong, Zhaorui Zhang, Xiaodong Yu</li>
<li class=""><strong>institution:</strong> Stevens Institute of Technology, Argonne National Laboratory, Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05770</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a gradient-aware error-bounded lossy compression method for federated learning that exploits temporal correlations across training rounds and structural patterns in convolutional kernels. The method achieves up to 1.53× higher compression ratios than existing approaches while maintaining model accuracy. When integrated into a real FL framework, it reduces communication time by 76.1%-96.2% under bandwidth-constrained scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [multi-BFT consensus, object-centric execution, lock-based coordination, deadlock resolution]</li>
<li class=""><strong>authors:</strong> Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Mohammad Sadoghi, Yinqian Zhang, Cong Wang, Ivan Beschastnikh, Chen Feng</li>
<li class=""><strong>institution:</strong> University of British Columbia, University of California Davis, City University of Hong Kong, Southern University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.05843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.05843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HYDRA introduces an object-centric execution model that eliminates global ordering in Multi-BFT consensus by partitioning transactions and using lightweight lock-based coordination with deadlock resolution. Experimental results show it outperforms state-of-the-art Multi-BFT protocols, especially in the presence of stragglers. This demonstrates that removing global ordering enables both strong consistency and high performance in scalable consensus systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [shared kv attention, sparse attention, disaggregated infrastructure, kv cache optimization, memory bandwidth optimization]</li>
<li class=""><strong>authors:</strong> Myunghyun Rhee, Sookyung Choi, Euiseok Kim, Joonseop Sim, Youngpyo Joo, Hoshik Kim</li>
<li class=""><strong>institution:</strong> SK hynix Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06010</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MoSKA, a novel architecture that addresses KV cache bottlenecks in long-sequence LLM inference by differentiating between unique and shared context data. The core innovation is Shared KV Attention, which transforms memory-bound operations into compute-bound GEMM operations through request batching. This approach achieves up to 538.7x throughput improvement in high-sharing workloads, providing a scalable path for efficient LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Distributed Deep Learning for Medical Image Denoising with Data Obfuscation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [DistributedDataParallel, Automatic Mixed Precision, U-Net, U-Net++, multi-GPU training, data obfuscation]</li>
<li class=""><strong>authors:</strong> Sulaimon Oyeniyi Adebayo, Ayaz H. Khan</li>
<li class=""><strong>institution:</strong> King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06006" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06006</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements distributed deep learning for medical image denoising using U-Net and U-Net++ architectures with Gaussian noise obfuscation. The optimized training pipeline combining DistributedDataParallel and Automatic Mixed Precision reduced training time by over 60% compared to single-GPU training. Results show U-Net++ achieved superior denoising performance with enhanced structural fidelity while maintaining practical viability for clinical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Elastic Data Transfer Optimization with Hybrid Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [deep reinforcement learning, heuristic-based parallelism, infinite pipelining, network simulator]</li>
<li class=""><strong>authors:</strong> Rasman Mubtasim Swargo, Md Arifuzzaman</li>
<li class=""><strong>institution:</strong> Missouri University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06159</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents LDM, an adaptive data transfer method that combines heuristic-based parallelism, infinite pipelining, and deep reinforcement learning to optimize multiple transfer parameters simultaneously. It introduces a lightweight network simulator that reduces training time from days to minutes. Experimental results show the method achieves up to 9.5× higher throughput compared to state-of-the-art solutions across diverse datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] LiteCast: A Lightweight Forecaster for Carbon Optimizations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [time series forecasting, carbon intensity prediction, lightweight forecasting, energy mix modeling]</li>
<li class=""><strong>authors:</strong> Mathew Joseph, Tanush Savadi, Abel Souza</li>
<li class=""><strong>institution:</strong> Unknown</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06187" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06187</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiteCast is a lightweight time series forecasting method that uses minimal historical energy and weather data to predict grid carbon intensity. The paper demonstrates that preserving forecast rankings rather than achieving high precision drives most carbon savings. Evaluation across 50 regions shows LiteCast achieves 97% of maximum attainable savings while being computationally efficient and adaptive to grid changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [profile-guided optimization, multi-agent framework, hardware profiling, kernel generation, iterative refinement]</li>
<li class=""><strong>authors:</strong> Kelun Lei, Hailong Yang, Huaitao Zhang, Xin You, Kaige Zhang, Zhongzhi Luan, Yi Liu, Depei Qian</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06345" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06345</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PRAGMA introduces a profile-guided multi-agent framework that integrates fine-grained hardware profiling into the kernel optimization loop, enabling LLMs to identify performance bottlenecks and iteratively refine code. The system consistently outperforms baseline approaches without profiling and achieves significant speedups (2.81× on CPU, 2.30× on GPU) compared to Torch implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Reliablocks: Developing Reliability Scores for Optimistic Rollups</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain scalability], [optimistic rollups, reliability scores, smart contracts, AVS components, WASMI]</li>
<li class=""><strong>authors:</strong> Souradeep Das, Ethan Lam, Varun Vaidya, Sanjay Amirthraj</li>
<li class=""><strong>institution:</strong> EigenLayer</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06130" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06130</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Reliablocks, an on-chain reliability index that assesses non-finalized blocks in Optimistic Rollups to help users determine transaction finality. It was developed during the EigenLayer Infinite Hackathon and includes working AVS components, smart contracts, and a dashboard interface. The system aims to provide transparency about block reliability during the 7-day finalization period, benefiting L2 users who cannot run validators themselves.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Towards Optimal Constellation Design for Digital Over-the-Air Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless communications], [digital modulation, constellation design, mean-squared error minimization, additive mapping, generalized Lambert function]</li>
<li class=""><strong>authors:</strong> Saeed Razavikia, Deniz Gündüz, Carlo Fischione</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Imperial College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06372</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a digital modulation framework for over-the-air computation that optimizes constellation design to minimize mean-squared error under power constraints. The authors formulate the problem as a system of nonlinear equations and derive closed-form solutions using the generalized Lambert function in high SNR regimes. The framework provides analytical insights and can be extended to multi-dimensional grids, non-Gaussian noise, and hybrid digital-analog modulation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] FPGA or GPU? Analyzing comparative research for application-specific guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hardware accelerators, FPGA, GPU, performance metrics, energy efficiency, programmability]</li>
<li class=""><strong>authors:</strong> Arnab A Purkayastha, Jay Tharwani, Shobhit Aggarwal</li>
<li class=""><strong>institution:</strong> Western New England University, The Citadel</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06565</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper synthesizes insights from comparative research studies to analyze FPGA and GPU performance across different application domains. By categorizing studies and analyzing key metrics, it provides application-specific guidance for selecting appropriate hardware accelerators. The main conclusion is that FPGAs excel in real-time, power-sensitive tasks while GPUs are better suited for data-intensive parallel processing with mature programming frameworks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Optimizing Long-context LLM Serving via Fine-grained Sequence Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sequence parallelism, chunkwise dynamic sequence parallelism, ring attention, tensor parallelism, disaggregated cluster]</li>
<li class=""><strong>authors:</strong> Cong Li, Yuzhe Yang, Xuegui Zheng, Qifan Yang, Yijin Guan, Size Zheng, Li-Wen Chang, Shufan Liu, Xin Liu, Guangyu Sun</li>
<li class=""><strong>institution:</strong> Peking University, Bytedance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06247</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Chunkwise Dynamic Sequence Parallelism (CDSP), a fine-grained parallelism strategy that assigns SP sizes across intra-request token segments, and builds Tetris, an LLM serving system based on CDSP. Tetris achieves significantly better performance than state-of-the-art systems, reducing time-to-first-token by up to 4.35× and increasing maximum request capacity by up to 45% while optimizing resource utilization in long-context LLM serving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] DMA Collectives for Efficient ML Communication Offloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [DMA collectives, all-gather, all-to-all, RCCL, synchronization optimization, AMD MI300X]</li>
<li class=""><strong>authors:</strong> Suchita Pati, Mahzabeen Islam, Shaizeen Aga, Mohamed Assem Ibrahim</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes offloading machine learning communication collectives to DMA engines on AMD MI300X GPUs, comparing performance against RCCL libraries. The research shows DMA collectives perform well for large data transfers but lag for small sizes due to synchronization overheads. Through optimized implementations leveraging DMA architecture innovations, the authors significantly close this performance gap, making DMA collectives more suitable for mainstream adoption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [input-aware resource allocation, function right-sizing, multi-objective Integer Linear Programming, proactive fault-tolerant redundancy, smart request orchestration]</li>
<li class=""><strong>authors:</strong> Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06599</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Saarthi is an end-to-end serverless framework that uses input-aware resource prediction and multi-objective optimization to dynamically manage function workloads. It achieves up to 1.45x better throughput and 1.84x reduced costs while maintaining 98.3% service level targets compared to baseline OpenFaaS. The system demonstrates significant improvements in serverless computing efficiency through intelligent resource allocation and request orchestration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [approximate computing, quality-aware scheduling, model switching, throughput optimization]</li>
<li class=""><strong>authors:</strong> Shubham Agarwal, Subrata Mitra, Saud Iqbal</li>
<li class=""><strong>institution:</strong> UC Berkeley, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06724" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06724</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Argus is a text-to-image inference serving system that intelligently selects appropriate approximation levels for each prompt to balance quality and throughput. The system dynamically switches between different approximation strategies to meet both quality requirements and throughput targets. Compared to baselines, Argus achieves 10x fewer latency violations, 10% higher quality, and 40% higher throughput on real-world workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Wireless Sensor Networks Nodes Clustering and Optimization Based on Fuzzy C-Means and Water Strider Algorithms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Water Strider Algorithm, Fuzzy C-Means, clustering optimization, energy efficiency, network lifetime, hybrid metaheuristics]</li>
<li class=""><strong>authors:</strong> Raya Majid Alsharfa, Mahmood Mohassel Feghhi, Majid Hameed Majeed</li>
<li class=""><strong>institution:</strong> Middle Technical University, University of Tabriz, Al-Mustaqbal University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06735</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid clustering protocol that combines Water Strider Algorithm for global optimization of cluster-head positions with Fuzzy C-Means for refined node membership assignment. The method significantly improves energy efficiency and network lifetime in wireless sensor networks, outperforming existing hybrid approaches across all performance metrics with statistical validation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Resilient by Design - Active Inference for Distributed Continuum Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [active inference, free-energy principle, Markov blanket, causal fault graph, Bayesian network structure learning]</li>
<li class=""><strong>authors:</strong> Praveen Kumar Donta, Alfreds Lapkovskis, Enzo Mingozzi, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> Stockholm University, University of Pisa, TU Wien, ICREA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07202" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07202</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) that uses causal fault graphs and the free-energy principle to autonomously detect and heal faults in distributed computing continuum systems. The framework continuously monitors system components and performs adaptive reconfiguration to maintain service stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed resilience approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [robotic manipulation], [contact fields, procedural grasp synthesis, geometric computation decoupling]</li>
<li class=""><strong>authors:</strong> Zhao-Heng Yin, Pieter Abbeel</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07418" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07418</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Lightning Grasp introduces a procedural grasp synthesis algorithm that uses Contact Fields to decouple geometric computation from the search process. This approach achieves orders-of-magnitude speed improvements over state-of-the-art methods while generating diverse grasps for irregular objects. The method eliminates the need for manually tuned energy functions and enables real-time grasp synthesis for dexterous robotic hands.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computational fluid dynamics], [GPU acceleration, Preconditioned Conjugate Gradient method, Approximate Symmetric Successive Over-Relaxation preconditioner, synchronized convergence strategy, finite volume method]</li>
<li class=""><strong>authors:</strong> Xin Yao, Yang Liu, Jin Jiang, Yesen Chen, Zhilong Chen, Hongkang Dong, Xiaofeng Wei, Teng Zhang, Dongyun Wang</li>
<li class=""><strong>institution:</strong> Zhejiang Normal University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.06824" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.06824</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a GPU-accelerated framework (GMAF) that uses Preconditioned Conjugate Gradient method with ASSOR preconditioner to efficiently simulate axial piston pump dynamics. The framework enables analysis of both smooth and textured pumps across multiple periods by accelerating pressure field computations and numerical integration. Results show that textured surfaces improve pressure capacity and torsion resistance, with pressure fields exhibiting step-like patterns corresponding to surface textures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251111] LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [trace-driven performance modeling, operator-level latency profiler, heterogeneous hardware integration, request routing, cache management, scheduling policies]</li>
<li class=""><strong>authors:</strong> Jaehong Cho, Hyunmin Choi, Jongse Park</li>
<li class=""><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07229</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLMServingSim2.0 introduces a unified simulator using trace-driven performance modeling and operator-level profiling to enable easy integration of heterogeneous hardware with modern LLM serving techniques. The system demonstrates 18.5× fewer lines of code for hardware integration and achieves only 1.9% error in reproducing GPU-based LLM serving. This makes it a comprehensive platform for both hardware developers and LLM service providers to evaluate heterogeneous systems efficiently.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 48</strong></p>
<ul>
<li class="">[arXiv251111] Revisiting Entropy in Reinforcement Learning for Large Reasoning Models <a href="https://arxiv.org/pdf/2511.05993" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Distributionally Robust Self Paced Curriculum Reinforcement Learning <a href="https://arxiv.org/pdf/2511.05694" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs <a href="https://arxiv.org/pdf/2511.05933" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling <a href="https://arxiv.org/pdf/2511.05589" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models <a href="https://arxiv.org/pdf/2511.05563" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] WAR-Re: Web API Recommendation with Semantic Reasoning <a href="https://arxiv.org/pdf/2511.05820" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Policy Gradient-Based EMT-in-the-Loop Learning to Mitigate Sub-Synchronous Control Interactions <a href="https://arxiv.org/pdf/2511.05822" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling <a href="https://arxiv.org/pdf/2511.05951" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph <a href="https://arxiv.org/pdf/2511.05849" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control <a href="https://arxiv.org/pdf/2511.05790" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration <a href="https://arxiv.org/pdf/2511.05965" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] ScRPO: From Errors to Insights <a href="https://arxiv.org/pdf/2511.06065" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Approximating Shapley Explanations in Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06094" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices <a href="https://arxiv.org/pdf/2511.06111" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] A Deep Learning Model for Predicting Transformation Legality <a href="https://arxiv.org/pdf/2511.06120" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs <a href="https://arxiv.org/pdf/2511.06134" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks <a href="https://arxiv.org/pdf/2511.06136" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning <a href="https://arxiv.org/pdf/2511.06142" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Deep Reinforcement Learning for Dynamic Origin-Destination Matrix Estimation in Microscopic Traffic Simulations Considering Credit Assignment <a href="https://arxiv.org/pdf/2511.06229" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios <a href="https://arxiv.org/pdf/2511.06252" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models <a href="https://arxiv.org/pdf/2511.06380" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization <a href="https://arxiv.org/pdf/2511.06411" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models <a href="https://arxiv.org/pdf/2511.06430" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Brain-Inspired Planning for Better Generalization in Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06470" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models <a href="https://arxiv.org/pdf/2511.06490" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Practical Policy Distillation for Reinforcement Learning in Radio Access Networks <a href="https://arxiv.org/pdf/2511.06563" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization <a href="https://arxiv.org/pdf/2511.06618" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View <a href="https://arxiv.org/pdf/2511.06722" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06745" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] OntoTune: Ontology-Driven Learning for Query Optimization with Convolutional Models <a href="https://arxiv.org/pdf/2511.06780" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Controllable Flow Matching for Online Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06816" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] On The Presence of Double-Descent in Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06895" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement Learning with Reward Function Optimization <a href="https://arxiv.org/pdf/2511.06937" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning <a href="https://arxiv.org/pdf/2511.06946" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Learning Quantized Continuous Controllers for Integer Hardware <a href="https://arxiv.org/pdf/2511.07046" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture <a href="https://arxiv.org/pdf/2511.07110" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving <a href="https://arxiv.org/pdf/2511.07155" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization <a href="https://arxiv.org/pdf/2511.07288" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search <a href="https://arxiv.org/pdf/2511.07312" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments <a href="https://arxiv.org/pdf/2511.07317" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation <a href="https://arxiv.org/pdf/2511.07322" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction <a href="https://arxiv.org/pdf/2511.07327" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training <a href="https://arxiv.org/pdf/2511.07328" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Grounding Computer Use Agents on Human Demonstrations <a href="https://arxiv.org/pdf/2511.07332" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training <a href="https://arxiv.org/pdf/2511.07372" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Robot Learning from a Physical World Model <a href="https://arxiv.org/pdf/2511.07416" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Convergence of Actor-Critic Learning for Mean Field Games and Mean Field Control in Continuous Spaces <a href="https://arxiv.org/pdf/2511.06812" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 21</strong></p>
<ul>
<li class="">[arXiv251111] GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery <a href="https://arxiv.org/pdf/2511.05726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation <a href="https://arxiv.org/pdf/2511.05879" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency Estimation <a href="https://arxiv.org/pdf/2511.05615" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Hilbert-Guided Block-Sparse Local Attention <a href="https://arxiv.org/pdf/2511.05832" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph <a href="https://arxiv.org/pdf/2511.05849" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] 10 Open Challenges Steering the Future of Vision-Language-Action Models <a href="https://arxiv.org/pdf/2511.05936" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction <a href="https://arxiv.org/pdf/2511.05596" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs <a href="https://arxiv.org/pdf/2511.06174" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Practical Policy Distillation for Reinforcement Learning in Radio Access Networks <a href="https://arxiv.org/pdf/2511.06563" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] SteganoSNN: SNN-Based Audio-in-Image Steganography with Encryption <a href="https://arxiv.org/pdf/2511.06573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality <a href="https://arxiv.org/pdf/2511.06597" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] ML-EcoLyzer: Quantifying the Environmental Cost of Machine Learning Inference Across Frameworks and Hardware <a href="https://arxiv.org/pdf/2511.06694" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations <a href="https://arxiv.org/pdf/2511.06767" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Neural-Initialized Newton: Accelerating Nonlinear Finite Elements via Operator Learning <a href="https://arxiv.org/pdf/2511.06802" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] DeepRWCap: Neural-Guided Random-Walk Capacitance Solver for IC Design <a href="https://arxiv.org/pdf/2511.06831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats <a href="https://arxiv.org/pdf/2511.06838" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] TNT: Improving Chunkwise Training for Test-Time Memorization <a href="https://arxiv.org/pdf/2511.07343" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Inference-Time Scaling of Diffusion Models for Infrared Data Generation <a href="https://arxiv.org/pdf/2511.07362" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] DigiData: Training and Evaluating General-Purpose Mobile Control Agents <a href="https://arxiv.org/pdf/2511.07413" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] AIRMap - AI-Generated Radio Maps for Wireless Digital Twins <a href="https://arxiv.org/pdf/2511.05522" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251111] Machine-Learning Accelerated Calculations of Reduced Density Matrices <a href="https://arxiv.org/pdf/2511.07367" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-12">2025-11-12<a href="#2025-11-12" class="hash-link" aria-label="Direct link to 2025-11-12" title="Direct link to 2025-11-12" translate="no">​</a></h2>
<p><strong>cs.DC total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251112] HyProv: Hybrid Provenance Management for Scientific Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [hybrid provenance management, centralized and federated querying, workflow-aware provenance, Airflow, Kubernetes]</li>
<li class=""><strong>authors:</strong> Vasilis Bountris, Lauritz Thamsen, Ulf Leser</li>
<li class=""><strong>institution:</strong> Humboldt-Universität zu Berlin, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07574</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyProv introduces a hybrid provenance management system that combines centralized and federated approaches to handle workflow provenance data. The system uses centralized components for workflow-specific provenance and federated querying for large-scale execution logs. Experiments show HyProv scales to large workflows, provides sub-second query latencies, and adds minimal overhead to cluster resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Network and Systems Performance Characterization of MCP-Enabled LLM Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Model Context Protocol, token efficiency, parallel tool calls, task abort mechanisms, measurement-based analysis]</li>
<li class=""><strong>authors:</strong> Zihao Ding, Mufeng Zhu, Yao Liu</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07426" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07426</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a comprehensive measurement-based analysis of MCP-enabled LLM interactions, revealing significant performance and cost trade-offs. The study shows that extensive contextual information in MCP workflows dramatically increases token usage, computational load, and monetary costs. The authors suggest optimizations like parallel tool calls and robust abort mechanisms to develop more efficient MCP-enabled workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Enhancing reliability in AI inference services: An empirical study on real production incidents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [incident taxonomy, traffic routing, node rebalancing, capacity-aware routing, per-endpoint isolation, auto-detection, hotfix]</li>
<li class=""><strong>authors:</strong> Bhala Ranganathan, Mickey Zhang, Kai Wu</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07424</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an empirical analysis of 156 high-severity LLM inference incidents from production systems, developing a taxonomy and methodology to classify failure modes and mitigation strategies. The study identifies dominant failure patterns and shows how systematic incident analysis enables targeted reliability improvements. The research demonstrates that empirical analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] An Evaluation of LLMs Inference on Popular Single-board Computers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [quantization, on-device inference, benchmarking, single-board computers, Ollama, Llamafile]</li>
<li class=""><strong>authors:</strong> Tung, Nguyen, Tuyen Nguyen</li>
<li class=""><strong>institution:</strong> BillulloNex, University of Technology Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07425" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07425</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks the performance of 25 quantized LLMs on three single-board computers using Ollama and Llamafile inference runtimes. The study evaluates generation throughput, memory usage, and power consumption under realistic workloads. Results show SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving significantly higher throughput and lower power consumption than Ollama.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Synera: Synergistic LLM Serving across Device and Cloud at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [device-cloud synergy, selective offloading, parallel inference, scalable batching]</li>
<li class=""><strong>authors:</strong> Genglin Wang, Liekang Zeng, Bufang Yang, Kaiwei Liu, Guoliang Xing, Chumin Sun, Li Zhou, Jie Sun, Zhenyu Yan</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Technologies Co. Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07423</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Synera is a device-cloud synergistic LLM serving system that uses an efficient SLM-LLM synergistic mechanism with communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. The system achieves 1.20-5.47× better generation quality compared to baselines while maintaining similar latency, and reduces cloud serving costs by 8.2-16.5% compared to existing cloud serving approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Parallel Sampling via Autospeculation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [speculative rejection sampling, autospeculation, parallel sampling, any-order autoregressive models, denoising diffusion models]</li>
<li class=""><strong>authors:</strong> Nima Anari, Carlo Baronio, CJ Chen, Alireza Haqi, Frederic Koehler, Anqi Li, Thuy-Duong Vuong</li>
<li class=""><strong>institution:</strong> Stanford University, University of Arizona, University of Chicago, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07869" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07869</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces speculative rejection sampling, a novel technique that uses autospeculation to accelerate sampling from autoregressive and diffusion models. By making sequence-level speculations using the same oracle that defines the target distribution, the method achieves parallel runtime of O(n^{1/2}). This improves upon previous bounds and provides the first parallel speedup for diffusion models in the high-accuracy regime.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KVCache management, cluster adaptation, flash management, memory virtualization, retrieval-based methods]</li>
<li class=""><strong>authors:</strong> Tuowei Wang, Minxing Huang, Fengzu Li, Ligeng Chen, Jinrui Zhang, Ju Ren</li>
<li class=""><strong>institution:</strong> Tsinghua University, Honor Device Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07427</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DynaKV is an adaptive KVCache management approach that improves long-sequence LLM decoding on smartphones through migration-free cluster adaptation, continuity-centric flash management, and memory-efficient cache design. It achieves significant improvements in retrieval accuracy (1.38×) and reduces end-to-end latency (1.47× speedups) compared to state-of-the-art solutions. The method&#x27;s insights extend to other long-context workloads and multi-tier memory hierarchies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [semantic knowledge graphs, constraint satisfaction, SMT solving, beam search, dual static-dynamic graphs]</li>
<li class=""><strong>authors:</strong> Wuyang Zhang, Chenkai Zhang, Zhen Luo, Jianming Ma, Wangming Yuan, Chuqiao Gu, Chenwei Feng</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Northeastern University, George Mason University, Carnegie Mellon University, Auckland University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07584</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SemanticForge introduces a repository-level code generation system that combines semantic knowledge graphs with constraint satisfaction to address LLM hallucinations. The system uses dual static-dynamic knowledge graphs, neural query generation, and SMT-guided beam search to ensure semantic correctness during code generation. Evaluation shows substantial improvements in code quality with 49.8% Pass@1 and significant reductions in both logical and schematic hallucinations while maintaining sub-3s latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] From Attention to Disaggregation: Tracing the Evolution of LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [disaggregated inference, service decomposition, resource disaggregation, workload partitioning, prefill phase, decode phase]</li>
<li class=""><strong>authors:</strong> Madabattula Rajesh Kumar, Srinivasa Rao Aravilli, Mustafa Saify, Shashank Srivastava</li>
<li class=""><strong>institution:</strong> Capital One</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07422" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07422</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes disaggregated inference as an architectural shift for LLM deployment, separating compute-intensive prefill and memory-intensive decode phases into independently scalable components. This approach addresses the distributed systems challenges of LLM inference by mitigating resource contention and enabling optimization of key metrics like Time to First Token and Inter Token Latency. The main conclusion is that disaggregation provides a solution to the multi-objective optimization problem of minimizing latency, maximizing throughput, and reducing costs for large-scale LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [locality-aware sampling, fine-grained parallelism scheduling, reinforcement learning, CPU-GPU heterogeneous platforms]</li>
<li class=""><strong>authors:</strong> Tong Qiao, Ao Zhou, Yingjie Qi, Yiou Wang, Han Wan, Jianlei Yang, Chunming Hu</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07421</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces A3GNN, a framework that optimizes GNN training on CPU-GPU heterogeneous platforms through locality-aware sampling and fine-grained parallelism scheduling, while using reinforcement learning to find optimal trade-offs between throughput, memory footprint, and accuracy. The framework enables affordable GNN training on resource-constrained devices by better utilizing available computational resources. Experiments demonstrate that A3GNN can outperform high-end GPUs, with seven Nvidia 2080Ti GPUs achieving up to 1.8× higher throughput than two A100 GPUs with minimal accuracy loss.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [probabilistic forecasting, analytical modeling, client selection, SLO compliance]</li>
<li class=""><strong>authors:</strong> Andrija Stanisic, Stefan Nastic</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08147</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ProbSelect, a stochastic client selection method for GPU-accelerated devices in federated learning systems that uses analytical modeling and probabilistic forecasting without requiring historical data. The approach significantly improves SLO compliance by 13.77% on average while reducing computational waste by 72.5% compared to baseline methods in the 3D compute continuum.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [transformer optimization, cross-platform deployment, compute-storage fusion, parallelism, FPGA adaptation]</li>
<li class=""><strong>authors:</strong> Zhuoheng Ran, Chong Wu, Renjie Xu, Maolin Che, Hong Yan</li>
<li class=""><strong>institution:</strong> City University of Hong Kong, Guizhou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08135</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UniFormer, a unified Transformer architecture designed for both general-purpose and custom computing platforms. By enabling higher parallelism and compute-storage fusion, it achieves state-of-the-art accuracy and latency on GPUs while maintaining strong adaptability on FPGAs. The work represents the first efficient Transformer approach that jointly optimizes for both general-purpose and customized computing architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Federated Learning, Proximal Policy Optimization, multi-agent reinforcement learning, client selection, energy efficiency, non-IID data]</li>
<li class=""><strong>authors:</strong> Anna Lackinger, Andrea Morichetta, Pantelis A. Frangoudis, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08142</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BIPPO, a budget-aware independent proximal policy optimization method for energy-efficient client selection in federated learning. This multi-agent reinforcement learning approach improves accuracy while consuming minimal budget in resource-constrained IoT environments. Results show BIPPO delivers performant, stable, scalable and sustainable client selection for federated learning services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph processing systems], [asynchronous execution, block-centric scheduling, out-of-core processing, hybrid storage format, pipelined I/O]</li>
<li class=""><strong>authors:</strong> Dechuang Chen, Sibo Wang, Qintian Guo</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07886" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07886</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACGraph introduces an asynchronous out-of-core graph processing framework that uses dynamic block-centric scheduling and pipelined I/O-computation execution to overcome limitations of synchronous systems. The system minimizes redundant disk accesses through an online asynchronous worklist and optimized hybrid storage format. Experimental results show ACGraph significantly outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Generic Algorithm for Universal TDM Communication Over Inter Satellite Links</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [TDM communication, federated learning, inter satellite links, time division multiplexing, peer data exchange, satellite constellations]</li>
<li class=""><strong>authors:</strong> Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Ilija Basicevic</li>
<li class=""><strong>institution:</strong> University of Novi Sad, RT-RK Institute for Computer Based Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a new generic algorithm for universal TDM communication that enables network nodes to communicate with multiple peers simultaneously, overcoming the limitation of pairwise communication in existing frameworks. The algorithm is designed for federated learning applications in satellite constellations and supports real-world TDM communications over inter-satellite links. The research validates that this approach enhances communication capabilities for satellites with multiple antennas in distributed edge systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Federated Learning, Segment Anything Model 2, Image Segmentation, Flower Framework, Fog Computing]</li>
<li class=""><strong>authors:</strong> Mehmet Batuhan Duman, Alejandro Carnero, Cristian Martín, Daniel Garrido, Manuel Díaz</li>
<li class=""><strong>institution:</strong> ITIS Software, University of Malaga</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08130" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08130</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework combining Federated Learning with Segment Anything Model 2 for foam segmentation in wastewater treatment plants. The method enables collaborative model training across multiple plants without sharing sensitive data by using SAM2&#x27;s pre-trained weights and Flower framework for federated aggregation. The research demonstrates that this approach provides a practical, privacy-preserving solution for industrial computer vision applications with distributed data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph algorithms], [alternating base trees, matching augmentation, maximum cardinality matching, structure theorem]</li>
<li class=""><strong>authors:</strong> Taisuke Izumi, Naoki Kitamura, Yutaro Yamaguchi</li>
<li class=""><strong>institution:</strong> Osaka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08210</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new framework for maximum matching that avoids the complex blossom structures used in previous approaches by employing alternating base trees and early alternation forgetting. The proposed algorithm is slightly slower than the Micali-Vazirani algorithm but is more implementable and easier to verify for correctness. The framework also enables improved deterministic approximation algorithms for distributed and semi-streaming settings with substantially better running time bounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] <u>LO</u>w-c<u>O</u>st yet High-<u>P</u>erformant <u>S</u>parse Matrix-Matrix Multiplication on Arm SME Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [SME, NEON, CSR, BCSR, two-level parallelization, performance modeling]</li>
<li class=""><strong>authors:</strong> Kelun Lei, Hailong Yang, Kaige Zhang, Kejie Ma, Yiqing Wang, Xin You, Yufan Xu, Enrique S. Quintana-Orti, Zhongzhi Luan, Yi Liu, Depei Qian</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08158</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LOOPS, a hybrid execution framework that combines row-wise CSR and vector-wise BCSR layouts to cooperatively utilize NEON vector instructions and Arm SME resources for sparse matrix multiplication. Experimental results show LOOPS achieves significant speedups over CPU baselines and GPU methods while delivering better energy efficiency on Apple M4Pro CPUs compared to NVIDIA A100 GPU implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [intelligence per watt, local inference, power efficiency, model-accelerator pairs, benchmarking]</li>
<li class=""><strong>authors:</strong> Jon Saad-Falcon, Avanika Narayan, Hakki Orhun Akengin, J. Wes Griffin, Herumb Shandilya, Adrian Gamarra Lafuente, Medhya Goel, Rebecca Joseph, Shlok Natarajan, Etash Kumar Guha, Shang Zhu, Ben Athiwaratkun, John Hennessy, Azalia Mirhoseini, Christopher Ré</li>
<li class=""><strong>institution:</strong> Stanford University, Together AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.07885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.07885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes intelligence per watt (IPW) as a metric to evaluate the efficiency and capability of local AI inference across different model-accelerator configurations. Through large-scale empirical analysis of 20+ local language models and 8 accelerators, the study demonstrates that local inference can accurately handle 88.7% of single-turn queries and has improved 5.3x in IPW from 2023-2025. The findings show local inference can meaningfully redistribute demand from centralized cloud infrastructure while revealing significant optimization headroom in local accelerators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [round-robin scheduler, vertex-transitive graphs, edge-transitive graphs, multiplicity detection, OBLOT model]</li>
<li class=""><strong>authors:</strong> Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra</li>
<li class=""><strong>institution:</strong> Università degli Studi dell&#x27;Aquila, Università degli Studi di Perugia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08222" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08222</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies the Gathering problem for oblivious robots moving on vertex- and edge-transitive graphs under round-robin scheduling without multiplicity detection. The authors develop time-optimal algorithms for specific topologies like infinite grids and hypercubes that exploit their structural properties. They conclude that no general algorithm likely exists for all solvable cases due to the heavy reliance on specific topological characteristics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251112] Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [constraint programming, pod packing, OR-Tools, Kubernetes scheduler]</li>
<li class=""><strong>authors:</strong> Henrik Daniel Christensen, Saverio Giallorenzo, Jacopo Mauro</li>
<li class=""><strong>institution:</strong> University of Southern Denmark, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08373" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08373</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes using constraint programming with OR-Tools to optimize Kubernetes pod scheduling by finding optimal allocations that satisfy priorities and resource requirements. As a plugin to the default scheduler, this approach significantly improves pod placement in scenarios where the default scheduler fails, achieving better allocations for higher-priority pods within practical time windows while certifying optimality when possible.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 31</strong></p>
<ul>
<li class="">[arXiv251112] From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training <a href="https://arxiv.org/pdf/2511.07738" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Partial Action Replacement: Tackling Distribution Shift in Offline MARL <a href="https://arxiv.org/pdf/2511.07629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] MURPHY: Multi-Turn GRPO for Self Correcting Code Generation <a href="https://arxiv.org/pdf/2511.07833" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models <a href="https://arxiv.org/pdf/2511.07581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] The Polite Liar: Epistemic Pathology in Language Models <a href="https://arxiv.org/pdf/2511.07477" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings <a href="https://arxiv.org/pdf/2511.07658" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Diffusion Guided Adversarial State Perturbations in Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07701" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems <a href="https://arxiv.org/pdf/2511.07707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning <a href="https://arxiv.org/pdf/2511.07483" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records <a href="https://arxiv.org/pdf/2511.07473" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework <a href="https://arxiv.org/pdf/2511.07702" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction <a href="https://arxiv.org/pdf/2511.07899" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Test-driven Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison <a href="https://arxiv.org/pdf/2511.07919" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] SERL: Self-Examining Reinforcement Learning on Open-Domain <a href="https://arxiv.org/pdf/2511.07922" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] SpeechJudge: Towards Human-Level Judgment for Speech Naturalness <a href="https://arxiv.org/pdf/2511.07931" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction <a href="https://arxiv.org/pdf/2511.07943" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning <a href="https://arxiv.org/pdf/2511.08024" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks <a href="https://arxiv.org/pdf/2511.08086" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] An Efficient Training Pipeline for Reasoning Graphical User Interface Agents <a href="https://arxiv.org/pdf/2511.08172" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore <a href="https://arxiv.org/pdf/2511.08241" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning <a href="https://arxiv.org/pdf/2511.08246" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress <a href="https://arxiv.org/pdf/2511.08325" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration <a href="https://arxiv.org/pdf/2511.08339" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games <a href="https://arxiv.org/pdf/2511.08412" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08436" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] The Path Not Taken: RLVR Provably Learns Off the Principals <a href="https://arxiv.org/pdf/2511.08567" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs <a href="https://arxiv.org/pdf/2511.08581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Shocks Under Control: Taming Transonic Compressible Flow over an RAE2822 Airfoil with Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.07564" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Distributionally Robust Online Markov Game with Linear Function Approximation <a href="https://arxiv.org/pdf/2511.07831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv251112] A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems <a href="https://arxiv.org/pdf/2511.07707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System <a href="https://arxiv.org/pdf/2511.07737" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Streaming Tensor Program: A streaming abstraction for dynamic parallelism <a href="https://arxiv.org/pdf/2511.07776" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization <a href="https://arxiv.org/pdf/2511.07836" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] A Self-Improving Architecture for Dynamic Safety in Large Language Models <a href="https://arxiv.org/pdf/2511.07645" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing <a href="https://arxiv.org/pdf/2511.07665" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Operational machine learning for remote spectroscopic detection of CH<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mn>4</mn></msub></mrow><annotation encoding="application/x-tex">_{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4511em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> point sources <a href="https://arxiv.org/pdf/2511.07719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning <a href="https://arxiv.org/pdf/2511.08003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics <a href="https://arxiv.org/pdf/2511.08185" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Dual-Kernel Graph Community Contrastive Learning <a href="https://arxiv.org/pdf/2511.08287" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration <a href="https://arxiv.org/pdf/2511.08339" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization <a href="https://arxiv.org/pdf/2511.08417" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Benchmarking Simulacra AI&#x27;s Quantum Accurate Synthetic Data Generation for Chemical Sciences <a href="https://arxiv.org/pdf/2511.07433" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Emulating Radiative Transfer in Astrophysical Environments <a href="https://arxiv.org/pdf/2511.08219" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications <a href="https://arxiv.org/pdf/2511.08416" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251112] Galactification: painting galaxies onto dark matter only simulations using a transformer-based model <a href="https://arxiv.org/pdf/2511.08438" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-13">2025-11-13<a href="#2025-11-13" class="hash-link" aria-label="Direct link to 2025-11-13" title="Direct link to 2025-11-13" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251113] Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain security], [vulnerability taxonomy, static analysis, dynamic analysis, learning-based detection, root-cause analysis, program structure]</li>
<li class=""><strong>authors:</strong> Parsa Hedayatnia, Tina Tavakkoli, Hadi Amini, Mohammad Allahbakhsh, Haleh Amintoosi</li>
<li class=""><strong>institution:</strong> Ferdowsi University of Mashhad</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09051" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09051</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an attack-centric program-structure taxonomy that organizes smart contract vulnerabilities into eight root-cause families. The taxonomy provides a unified framework for vulnerability detection, audit reproducibility, and security education. It enables more interpretable analysis by linking vulnerabilities to observable detection signals across different analysis tools.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] An MLIR pipeline for offloading Fortran to FPGAs via OpenMP</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [compiler systems], [MLIR, OpenMP, FPGA, High-Level Synthesis, Flang]</li>
<li class=""><strong>authors:</strong> Gabriel Rodriguez-Canal, David Katz, Nick Brown</li>
<li class=""><strong>institution:</strong> EPCC, The University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an MLIR-based compilation pipeline that enables Fortran code offloading to FPGAs using OpenMP target directives. The approach combines MLIR&#x27;s OpenMP dialect with a High-Level Synthesis dialect to create a portable FPGA compilation flow. The work demonstrates that building upon existing MLIR components significantly reduces development effort while providing a flexible path for directive-based FPGA acceleration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, second-order optimization, preconditioned mixing, local parameter updates, convergence analysis]</li>
<li class=""><strong>authors:</strong> Hiro Ishii, Kenta Niwa, Hiroshi Sawada, Akinori Fujino, Noboru Harada, Rio Yokota</li>
<li class=""><strong>institution:</strong> Institute of Science Tokyo, NTT Communication Science Laboratories</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09100</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedPM introduces a novel federated learning method that uses second-order optimization with preconditioned mixing of local parameters to address drift issues in local preconditioners. The approach decomposes ideal second-order updates into server-side parameter mixing and client-side local updates. Experimental results show significant improvements in test accuracy compared to conventional methods with simple mixing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Minimize Your Critical Path with Combine-and-Exchange Locks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrent programming], [Combine-and-Exchange Scheduling, coroutines, userspace synchronization, critical sections, cooperative multitasking]</li>
<li class=""><strong>authors:</strong> Simon König, Lukas Epple, Christian Becker</li>
<li class=""><strong>institution:</strong> University of Stuttgart</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09194</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Combine-and-Exchange Scheduling (CES), a novel scheduling approach for userspace tasks like coroutines that keeps contended critical sections on the same thread while distributing parallelizable work across other threads. The method addresses limitations in current userspace synchronization primitives that introduce unnecessary delays. The authors demonstrate 3-fold performance improvements in application benchmarks and 8-fold improvements in microbenchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [spatial data processing], [Apache Flink Stateful Functions, grid-based index, load balancing, streaming spatial queries]</li>
<li class=""><strong>authors:</strong> Jiaping Cao, Ting Sun, Man Lung Yiu, Xiao Yan, Bo Tang</li>
<li class=""><strong>institution:</strong> Hong Kong Polytechnic University, Southern University of Science and Technology, Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09262" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09262</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CheetahGIS is a scalable streaming spatial query processing system built on Apache Flink Stateful Functions with optimizations including grid-based indexing and load balancing. The system efficiently handles massive moving objects and real-time spatial queries through its modular architecture. Experimental results demonstrate its excellent scalability and performance for various streaming spatial query types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated learning, differential privacy, secure aggregation, confidential computing, hybrid cloud-HPC architecture]</li>
<li class=""><strong>authors:</strong> Zilinghan Li, Aditya Sinha, Yijiang Li, Kyle Chard, Kibaek Kim, Ravi Madduri</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, University of Chicago, University of Illinois at Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents the APPFL framework for enterprise-level privacy-preserving federated learning that enables collaborative model training without centralized data sharing. The framework bridges the gap between research prototypes and enterprise deployment by providing scalable local simulation, seamless transition to distributed deployment, and comprehensive privacy protection. It aims to enable scalable, reliable, and privacy-preserving AI for scientific applications across diverse computing infrastructures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] No Cords Attached: Coordination-Free Concurrent Lock-Free Queues</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrent data structures], [Cyclic Memory Protection, lock-free queues, linearizability, bounded reclamation, coordination-free]</li>
<li class=""><strong>authors:</strong> Yusuf Motiwala</li>
<li class=""><strong>institution:</strong> mesibo, PatANN</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09410</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Cyclic Memory Protection (CMP), a coordination-free concurrent lock-free queue that uses bounded protection windows to provide practical reclamation guarantees. CMP preserves strict FIFO ordering, unbounded capacity, and lock-free progress while outperforming state-of-the-art queues by 1.72-4x under high contention and scaling to hundreds of threads. The work demonstrates that highly concurrent queues can maintain fundamental simplicity without compromising queue semantics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Flex-MIG: Enabling Distributed Execution on MIG</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [MIG, GPU sharing, fragmentation reduction, host-shared-memory collectives, one-to-many allocation]</li>
<li class=""><strong>authors:</strong> Myungsu Kim, Ikjun Yeom, Younghoon Kim</li>
<li class=""><strong>institution:</strong> SungKyunKwan University, Ajou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09143</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Flex-MIG is a software framework that enables distributed execution across MIG instances by replacing the conventional one-to-one allocation model with a one-to-many model and implementing host-shared-memory collectives. This approach eliminates drain-required reconfigurations and reduces fragmentation without hardware modifications. The system improves cluster efficiency by up to 17% in makespan across diverse workload traces.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Distribution and Management of Datacenter Load Decoupling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [datacenter energy management], [load decoupling, energy resources, datacenter-grid cooperation, power capacity management]</li>
<li class=""><strong>authors:</strong> Liuzixuan Lin, Andrew A. Chien</li>
<li class=""><strong>institution:</strong> University of Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08936" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08936</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes decoupling datacenter power capacity from grid load using energy resources to create flexibility that improves renewable energy absorption. The study shows optimized distribution achieves 98% carbon reduction potential with 70% decoupling need, while DC-grid cooperation enables 1.4x greater carbon reduction. Economic analysis suggests decoupling can be viable with benefits exceeding local costs, though grid intervention may be needed due to site variations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing performance evaluation], [SPEC ACCEL, OpenMP workloads, virtualized cloud infrastructure, CPU architectures, instance pricing]</li>
<li class=""><strong>authors:</strong> Jay Tharwani, Shobhit Aggarwal, Arnab A Purkayastha</li>
<li class=""><strong>institution:</strong> University of North Carolina at Charlotte, The Citadel, Western New England University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.08948" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.08948</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates HPC-style CPU performance and cost across four major cloud providers using SPEC ACCEL OpenMP workloads on Intel, AMD, and ARM instances. The study found that AWS delivers the fastest runtimes but charges a premium, while OCI emerges as the most economical option despite slower performance. The results demonstrate that instance choices and provider selection significantly impact both runtime and cost, requiring careful consideration of workload priorities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [formal verification, CSP process algebra, model checking, deadlock freeness, successful termination]</li>
<li class=""><strong>authors:</strong> Miroslav Popovic, Marko Popovic, Pavle Vasiljevic, Miodrag Djukic</li>
<li class=""><strong>institution:</strong> University of Novi Sad, RT-RK Institute for Computer Based Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09485</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formally verifies a TDM communication algorithm for satellite constellations using CSP process algebra and the PAT model checker. The verification process involved creating a CSP model from Python code and proving deadlock freeness and successful termination properties. The approach successfully established the algorithm&#x27;s correctness for safety-critical edge systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] SPADA: A Spatial Dataflow Architecture Programming Language</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [spatial dataflow architectures, programming language, data placement, dataflow patterns, asynchronous operations, network-on-chip, compiler design, domain-specific languages]</li>
<li class=""><strong>authors:</strong> Lukas Gianinazzi, Tal Ben-Nun, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> ETH Zurich, Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09447</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SPADA is a programming language that provides precise control over data placement and dataflow patterns while abstracting low-level details for spatial dataflow architectures. The language enables 6-8x code reduction compared to existing approaches and demonstrates near-ideal weak scaling across three orders of magnitude. SPADA advances both theoretical foundations and practical usability for high-performance computing platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251113] LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [all-reduce, tensor parallelism, NVSHMEM, model parallelism, distributed inference]</li>
<li class=""><strong>authors:</strong> Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele</li>
<li class=""><strong>institution:</strong> University of Maryland, Lawrence Livermore National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09557" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09557</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces NVRAR, a hierarchical all-reduce algorithm using NVSHMEM to optimize communication bottlenecks in multi-node LLM inference. The method achieves up to 3.6x lower latency than NCCL and reduces end-to-end batch latency by 1.72x for large models like Llama 3.1 405B. The research demonstrates that optimized collective communication significantly improves performance in distributed inference workloads.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 21</strong></p>
<ul>
<li class="">[arXiv251113] Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning <a href="https://arxiv.org/pdf/2511.09109" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks <a href="https://arxiv.org/pdf/2511.09114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] History-Aware Reasoning for GUI Agents <a href="https://arxiv.org/pdf/2511.09127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] A Distributed Training Architecture For Combinatorial Optimization <a href="https://arxiv.org/pdf/2511.09261" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm <a href="https://arxiv.org/pdf/2511.09392" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations <a href="https://arxiv.org/pdf/2511.08832" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Iterated Population Based Training with Task-Agnostic Restarts <a href="https://arxiv.org/pdf/2511.09190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models <a href="https://arxiv.org/pdf/2511.08873" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Interpretable by Design: Query-Specific Neural Modules for Explainable Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08749" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Advancing Autonomous Emergency Response Systems: A Generative AI Perspective <a href="https://arxiv.org/pdf/2511.09044" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08922" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Efficient Reasoning via Reward Model <a href="https://arxiv.org/pdf/2511.09158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Structured Uncertainty guided Clarification for LLM Agents <a href="https://arxiv.org/pdf/2511.08798" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning <a href="https://arxiv.org/pdf/2511.08926" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization <a href="https://arxiv.org/pdf/2511.09219" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting <a href="https://arxiv.org/pdf/2511.09478" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Quasi-Newton Compatible Actor-Critic for Deterministic Policies <a href="https://arxiv.org/pdf/2511.09509" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] WMPO: World Model-based Policy Optimization for Vision-Language-Action Models <a href="https://arxiv.org/pdf/2511.09515" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Optimal Control of the Future via Prospective Foraging <a href="https://arxiv.org/pdf/2511.08717" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Practical considerations when designing an online learning algorithm for an app-based mHealth intervention <a href="https://arxiv.org/pdf/2511.08719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares <a href="https://arxiv.org/pdf/2511.08852" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv251113] Leveraging Large Language Models for Use Case Model Generation from Software Requirements <a href="https://arxiv.org/pdf/2511.09231" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Data reuse enables cost-efficient randomized trials of medical AI models <a href="https://arxiv.org/pdf/2511.08986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction <a href="https://arxiv.org/pdf/2511.09209" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation <a href="https://arxiv.org/pdf/2511.09180" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Tele-LLM-Hub: Building Context-Aware Multi-Agent LLM Systems for Telecom Networks <a href="https://arxiv.org/pdf/2511.09087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Enabling Agents to Communicate Entirely in Latent Space <a href="https://arxiv.org/pdf/2511.09149" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach <a href="https://arxiv.org/pdf/2511.09475" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Fluence Map Prediction with Deep Learning: A Transformer-based Approach <a href="https://arxiv.org/pdf/2511.08645" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251113] Bio AI Agent: A Multi-Agent Artificial Intelligence System for Autonomous CAR-T Cell Therapy Development with Integrated Target Discovery, Toxicity Prediction, and Rational Molecular Design <a href="https://arxiv.org/pdf/2511.08649" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-14">2025-11-14<a href="#2025-11-14" class="hash-link" aria-label="Direct link to 2025-11-14" title="Direct link to 2025-11-14" translate="no">​</a></h2>
<p><strong>cs.DC total: 18</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251114] MoFa: A Unified Performance Modeling Framework for LLM Pretraining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [performance modeling, hybrid parallelization, fault tolerance, checkpoint recovery, distributed pretraining]</li>
<li class=""><strong>authors:</strong> Lu Zhao, Rong Shi, Shaoqing Zhang, Shangchao Su, Ziqing Yin, Zhiyan Cui, Hongfeng Sun, Baoguo He, Yueqiang Chen, Liang Dong, Xiyuan Li, Lingbin Wang, Lijun Ma, Qiang Huang, Ting Liu, Chong Wang, Can Wei</li>
<li class=""><strong>institution:</strong> AIH Training Team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09837" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09837</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoFa is a unified performance modeling framework that incorporates multi-dimensional optimization features and fault tolerance mechanisms for LLM pretraining. It achieves high prediction accuracy across various scenarios and provides systematic guidance for pretraining system design by revealing key performance factors under different configurations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [supervised learning, sparse matrix reordering, automatic algorithm selection, matrix bandwidth minimization]</li>
<li class=""><strong>authors:</strong> Tao Tang, Youfu Jiang, Yingbo Cui, Jianbin Fang, Peng Zhang, Lin Peng, Chun Huang</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a supervised learning model that automatically selects optimal sparse matrix reordering algorithms based on matrix characteristics. The model learns correlations between matrix features and reordering algorithm performance. Experimental results show the approach reduces solution time by 55.37% compared to using only AMD algorithm, achieving an average speedup ratio of 1.45.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [transaction scheduling, poly-log approximation, doubling dimension graphs, distributed algorithms, fog-cloud computing]</li>
<li class=""><strong>authors:</strong> Ramesh Adhikari, Costas Busch, Pavan Poudel</li>
<li class=""><strong>institution:</strong> Augusta University, University of Houston-Clear Lake</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09776</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents transaction scheduling algorithms for fog-cloud computing networks that achieve poly-logarithmic approximation ratios for minimizing total communication cost. The algorithms handle both single and multiple shared objects and work in networks with constant doubling dimension. The authors also provide a fully distributed version that operates without global knowledge of transactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [cache abstraction, eviction sets, LLC contention-aware scheduling, virtual color-aware page management]</li>
<li class=""><strong>authors:</strong> Mani Tofigh, Edward Guo, Weiwei Jia, Xiaoning Ding, Jianchen Shan</li>
<li class=""><strong>institution:</strong> Hofstra University, Columbia University, University of Rhode Island, New Jersey Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09956</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CacheX, a solution that probes accurate cache abstraction within cloud VMs using eviction sets without requiring hardware or hypervisor support. It demonstrates that CacheX effectively improves cache utilization through LLC contention-aware task scheduling and virtual color-aware page cache management in public cloud environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [latency prediction, reliability-aware selection, hysteresis-based handover, passive measurements, exponentially modulated rational delay model]</li>
<li class=""><strong>authors:</strong> Jaime Sebastian Burbano, Arnova Abdullah, Eldiyar Zhantileuov, Mohan Liyanage, Rolf Schuster</li>
<li class=""><strong>institution:</strong> University of Applied Sciences and Arts, Dortmund</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10146</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MO-HAN, a lightweight server selection method that combines latency prediction with adaptive reliability and hysteresis-based handover for edge computing. Using passive measurements and a delay model, it balances predicted latency and reliability to minimize unnecessary handovers. Results show MO-HAN reduces mean and tail latencies while cutting handovers by nearly 50% compared to opportunistic selection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [pipeline parallelism, weight passing, topology-aware communication, hierarchical bandwidth, collective operations, peer-to-peer transfers]</li>
<li class=""><strong>authors:</strong> Houming Wu, Ling Chen</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09741</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TawPipe introduces a topology-aware weight pipeline parallelism method that optimizes communication by grouping devices based on cluster topology and transmitting weights instead of activations. It reduces cross-node traffic by confining most communication within nodes and overlapping communication with computation. Experiments show TawPipe achieves superior throughput and scalability compared to state-of-the-art baselines for long-context large model training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Revisit to the Bai-Galbraith signature scheme</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cryptography], [lattice-based cryptography, Fiat-Shamir paradigm, Learning with Errors, signature scheme, public key compression]</li>
<li class=""><strong>authors:</strong> Banhirup Sengupta, Peenal Gupta, Souvik Sengupta</li>
<li class=""><strong>institution:</strong> PinakashieldTech, Tata Institute Of Fundamental Research, IONOS SE</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09582" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09582</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper revisits the Bai-Galbraith lattice-based signature scheme which uses the Fiat-Shamir paradigm and Learning with Errors (LWE) approach. The scheme differs from Dilithium by avoiding public key compression while focusing on signature size reduction. The authors analyze this alternative lattice-based signature method for practical cryptographic applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [concurrent computation communication, thermal imbalance, power capping, power sloshing, performance modeling]</li>
<li class=""><strong>authors:</strong> Marco Kurzynski, Shaizeen Aga, Di Wu</li>
<li class=""><strong>institution:</strong> University of Central Florida, Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09861</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper identifies the &quot;Lit Silicon&quot; effect where thermal imbalance in multi-GPU systems causes performance variation during LLM training through concurrent computation communication. The authors propose detection methods and three power management solutions including GPU power capping and CPU power sloshing. Their approach achieves up to 6% performance and 4% power improvements with minimal implementation overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] SMoFi: Step-wise Momentum Fusion for Split Federated Learning on Heterogeneous Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split federated learning, momentum fusion, staleness-aware alignment, gradient divergence control]</li>
<li class=""><strong>authors:</strong> Mingkun Yang, Ran Zhu, Qing Wang, Jie Yang</li>
<li class=""><strong>institution:</strong> Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09828" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09828</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SMoFi, a framework that synchronizes momentum buffers across server-side optimizers in split federated learning to counteract gradient divergence from data heterogeneity. The method uses a staleness-aware alignment mechanism to constrain gradient updates during training. Experimental results show SMoFi improves model accuracy by up to 7.1% and convergence speed by up to 10.25×, particularly benefiting scenarios with more clients and deeper models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kalman Filter, Contextual Bandits, Gaussian Process Regression, Long Short Term Memory, Transformer Attention]</li>
<li class=""><strong>authors:</strong> Michael Dang&#x27;ana, Yuqiu Zhang, Hans-Arno Jacobsen</li>
<li class=""><strong>institution:</strong> University of Toronto</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.09766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.09766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Ksurf-Drone, which integrates a variance-minimizing Kalman filter estimator with contextual bandit optimization for cloud resource allocation. The method addresses workload variability and noise in containerized cloud environments. Results show significant improvements in latency variance reduction (41-47%), resource usage efficiency, and cost savings compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [industrial control system security], [programmable data plane, IOTA Tangle, certificate validation, P4, blockchain, OPC UA]</li>
<li class=""><strong>authors:</strong> Rinieri Lorenzo, Gori Giacomo, Melis Andrea, Girau Roberto, Prandini Marco, Callegati Franco</li>
<li class=""><strong>institution:</strong> University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10248" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10248</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pk-IOTA integrates programmable data plane switches for in-network certificate validation and uses the IOTA Tangle blockchain for decentralized certificate distribution to secure OPC UA communications. The system was evaluated on a physical industrial testbed and demonstrated minimal overhead while providing scalable, tamper-proof certificate management for Industry 4.0 deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Workload Schedulers -- Genesis, Algorithms and Differences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [scheduling algorithms], [list scheduling, largest processing time first, round robin, weighted round robin, fixed-priority pre-emptive scheduling]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Vladimir Getov</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10258</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a novel categorization approach for workload schedulers, examining three classes: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers, and Big Data Schedulers. It analyzes their evolution and algorithm features, highlighting both differences between scheduler classes and similarities in scheduling strategy design across local and distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [prefix-sum algorithms, parallel scan, Kalman filters, smoothers, temporal parallelization]</li>
<li class=""><strong>authors:</strong> Simo Särkkä, Ángel F. García-Fernández</li>
<li class=""><strong>institution:</strong> Aalto University, Universidad Politécnica de Madrid</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10363</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates parallel-in-time Kalman filters and smoothers using prefix-sum algorithms on GPUs, proposing a novel two-filter smoother. The study compares different parallel scan algorithms through operation counts and runtime measurements on actual GPU hardware. The results demonstrate that these parallel implementations significantly improve performance on GPU architectures compared to classical sequential approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [k-nearest neighbor, bin-partitioned approach, gradient-flow support, adaptive parameter tuning, CUDA]</li>
<li class=""><strong>authors:</strong> Aarush Agarwal, Raymond He, Jan Kieseler, Matteo Cremonesi, Shah Rukh Qasim</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Karlsruhe Institute of Technology, University of Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10442</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FastGraph introduces a GPU-optimized k-nearest neighbor algorithm using bin-partitioned approaches with gradient-flow support for efficient graph construction in low-dimensional spaces. The method achieves 20-40x speedup over existing libraries with minimal memory overhead. These improvements significantly benefit GNN workflows in applications like particle physics and object tracking.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, spatio-temporal graphs, dynamic dependencies, graph neural networks, client-server protocol, nonlinear computation decomposition, node embedding augmentation]</li>
<li class=""><strong>authors:</strong> Feng Wang, Tianxiang Chen, Shuyue Wei, Qian Chu, Yi Zhang, Yifan Sun, Zhiming Zheng</li>
<li class=""><strong>institution:</strong> Beihang University, Renmin University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10434</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedSTGD, a federated learning framework that models dynamic inter-client spatial dependencies for traffic flow forecasting using nonlinear computation decomposition and node embedding augmentation. The method achieves performance approaching centralized baselines while maintaining data privacy across distributed clients. Experimental results demonstrate superior performance over state-of-the-art methods on multiple real-world datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Massively Parallel Proof-Number Search for Impartial Games and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [game solving], [Proof-Number Search, parallelization, Grundy numbers, game tree reduction]</li>
<li class=""><strong>authors:</strong> Tomáš Čížek, Martin Balko, Martin Schmid</li>
<li class=""><strong>institution:</strong> Charles University, EquiLibre Technologies</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10339" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10339</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a massively parallel version of Proof-Number Search that uses two parallelized levels and shared information among workers to achieve efficient scaling on many CPUs. The enhanced solver, which incorporates Grundy numbers for game tree reduction, demonstrated a 332.9× speedup on 1024 cores and solved 42 new Sprouts positions, nearly doubling the number of known outcomes while outperforming the state-of-the-art solver by four orders of magnitude.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [symbolic tensor graphs, parallelization strategies, execution traces, distributed workload modeling, tensor-level accuracy]</li>
<li class=""><strong>authors:</strong> Changhai Man, Joongun Park, Hanjiang Wu, Huan Xu, Srinivas Sridharan, Tushar Krishna</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, Nvidia Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10480" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10480</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces STAGE, a framework that synthesizes high-fidelity execution traces using symbolic tensor graphs to model distributed LLM workloads. This approach enables systematic exploration of parallelization strategies and system configurations without requiring access to large-scale infrastructure. The method demonstrates scalability by accurately modeling LLM workloads across over 32K GPUs while preserving compute, memory, and communication accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251114] dHPR: A Distributed Halpern Peaceman--Rachford Method for Non-smooth Distributed Optimization Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [distributed optimization], [Halpern Peaceman-Rachford, symmetric Gauss-Seidel decomposition, proximal operators, non-smooth optimization, Karush-Kuhn-Tucker residual]</li>
<li class=""><strong>authors:</strong> Zhangcheng Feng, Defeng Sun, Yancheng Yuan, Guojun Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10069</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the distributed Halpern Peaceman-Rachford (dHPR) method for solving non-smooth distributed optimization problems. The algorithm achieves non-ergodic O(1/k) iteration complexity and maintains parallelizability through symmetric Gauss-Seidel decomposition. Numerical experiments demonstrate superior performance on distributed LASSO, group LASSO, and L1-regularized logistic regression problems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 28</strong></p>
<ul>
<li class="">[arXiv251114] Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey <a href="https://arxiv.org/pdf/2511.09586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Multi-agent In-context Coordination via Decentralized Memory Retrieval <a href="https://arxiv.org/pdf/2511.10030" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09693" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09873" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09681" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Tree-Based Stochastic Optimization for Solving Large-Scale Urban Network Security Games <a href="https://arxiv.org/pdf/2511.10072" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning <a href="https://arxiv.org/pdf/2511.10087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO <a href="https://arxiv.org/pdf/2511.09780" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models <a href="https://arxiv.org/pdf/2511.09864" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning <a href="https://arxiv.org/pdf/2511.09998" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning <a href="https://arxiv.org/pdf/2511.09792" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy <a href="https://arxiv.org/pdf/2511.09737" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard <a href="https://arxiv.org/pdf/2511.09727" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Optimistic Reinforcement Learning with Quantile Objectives <a href="https://arxiv.org/pdf/2511.09652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Improved Offline Reinforcement Learning via Quantum Metric Encoding <a href="https://arxiv.org/pdf/2511.10187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Heuristic Transformer: Belief Augmented In-Context Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10251" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search <a href="https://arxiv.org/pdf/2511.10264" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access <a href="https://arxiv.org/pdf/2511.10291" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns <a href="https://arxiv.org/pdf/2511.10390" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] AgentEvolver: Towards Efficient Self-Evolving Agent System <a href="https://arxiv.org/pdf/2511.10395" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Explaining Decentralized Multi-Agent Reinforcement Learning Policies <a href="https://arxiv.org/pdf/2511.10409" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Reasoning About Intent for Ambiguous Requests <a href="https://arxiv.org/pdf/2511.10453" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling <a href="https://arxiv.org/pdf/2511.10501" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Towards Emotionally Intelligent and Responsible Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Instella: Fully Open Language Models with Stellar Performance <a href="https://arxiv.org/pdf/2511.10628" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Robot Crash Course: Learning Soft and Stylized Falling <a href="https://arxiv.org/pdf/2511.10635" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Operator Models for Continuous-Time Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10383" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity <a href="https://arxiv.org/pdf/2511.10626" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251114] A General Anchor-Based Framework for Scalable Fair Clustering <a href="https://arxiv.org/pdf/2511.09889" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads <a href="https://arxiv.org/pdf/2511.10010" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] On the Convergence of Overparameterized Problems: Inherent Properties of the Compositional Structure of Neural Networks <a href="https://arxiv.org/pdf/2511.09810" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Steering Pretrained Drafters during Speculative Decoding <a href="https://arxiv.org/pdf/2511.09844" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Efficient Hyperdimensional Computing with Modular Composite Representations <a href="https://arxiv.org/pdf/2511.09708" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics <a href="https://arxiv.org/pdf/2511.10271" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training <a href="https://arxiv.org/pdf/2511.10333" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Simulating Misinformation Propagation in Social Networks using Large Language Models <a href="https://arxiv.org/pdf/2511.10384" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs <a href="https://arxiv.org/pdf/2511.10590" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] Solvaformer: an SE(3)-equivariant graph transformer for small molecule solubility prediction <a href="https://arxiv.org/pdf/2511.09774" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251114] MATAI: A Generalist Machine Learning Framework for Property Prediction and Inverse Design of Advanced Alloys <a href="https://arxiv.org/pdf/2511.10108" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-23T08:17:13.000Z" itemprop="dateModified">Dec 23, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251103-20251109"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251103-20251109</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251117-20251123"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251117-20251123</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-10" class="table-of-contents__link toc-highlight">2025-11-10</a></li><li><a href="#2025-11-11" class="table-of-contents__link toc-highlight">2025-11-11</a></li><li><a href="#2025-11-12" class="table-of-contents__link toc-highlight">2025-11-12</a></li><li><a href="#2025-11-13" class="table-of-contents__link toc-highlight">2025-11-13</a></li><li><a href="#2025-11-14" class="table-of-contents__link toc-highlight">2025-11-14</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251110/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>