<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251117-20251123" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251117-20251123 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251117-20251123"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251117-20251123 | AI头条"><meta data-rh="true" name="description" content="2025-11-17"><meta data-rh="true" property="og:description" content="2025-11-17"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251117-20251123"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251117-20251123" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251117-20251123" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251117-20251123","item":"https://jokebear666.github.io/ai_toutiao/daily/20251117-20251123"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.97dd4fba.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.d5bed893.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251117-20251123</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251117-20251123</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-17">2025-11-17<a href="#2025-11-17" class="hash-link" aria-label="Direct link to 2025-11-17" title="Direct link to 2025-11-17" translate="no">​</a></h2>
<p><strong>cs.DC total: 9</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251117] HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent LLM, OpenMP, MPI, unit test generation, parallel computing, critique loop]</li>
<li class=""><strong>authors:</strong> Rabimba Karanjai, Lei Xu, Weidong Shi</li>
<li class=""><strong>institution:</strong> University of Houston, Kent State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10860</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HPCAgentTester, a multi-agent LLM framework that uses specialized agents working collaboratively through a critique loop to generate unit tests for HPC software. The system specifically targets parallel execution constructs in OpenMP and MPI applications. Evaluation shows it significantly improves test compilation rates and correctness compared to standalone LLMs, effectively identifying subtle bugs in parallel software.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semi-decentralized federated learning, device-to-device communication, local SGD, model aggregation, convergence analysis]</li>
<li class=""><strong>authors:</strong> Angelo Rodio, Giovanni Neglia, Zheng Chen, Erik G. Larsson</li>
<li class=""><strong>institution:</strong> Linköping University, Inria Université Côte d&#x27;Azur</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11560</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes two communication strategies (sampled-to-sampled vs sampled-to-all) in semi-decentralized federated learning where devices primarily use device-to-device communication with periodic server interactions. The unified convergence framework reveals that the optimal strategy depends primarily on data heterogeneity across devices, providing concrete design guidelines for practical deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high performance computing], [Domain Translation, shallow-water equations, stencil computations, finite difference methods, cluster computing]</li>
<li class=""><strong>authors:</strong> Tomas Oppelstrup, Nicholas Giamblanco, Delyan Z. Kalchev, Ilya Sharapov, Mark Taylor, Dirk Van Essendelft, Sivasankaran Rajamanickam, Michael James</li>
<li class=""><strong>institution:</strong> Cerebras Systems, Sandia National Laboratories, National Energy Technology Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11542</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Domain Translation algorithm to overcome performance limitations of traditional domain decomposition methods in physical system simulations. The method achieves 1.6 million time steps per second and 84 PFLOP/s while maintaining 90% of peak performance on Cerebras CS-3 clusters. The approach was demonstrated by modeling planetary-scale tsunami simulations at 460m-resolution using shallow-water equations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph neural networks, large language models, parallel discrete event simulation, hybrid simulation, Dragonfly networks]</li>
<li class=""><strong>authors:</strong> Xin Wang, Pietro Lodi Rizzini, Sourav Medya, Zhiling Lan</li>
<li class=""><strong>institution:</strong> University of Illinois Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11111</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents SMART, a surrogate model that combines graph neural networks and large language models to predict application runtime in Dragonfly network systems. The model captures both spatial and temporal patterns from router data to address workload interference challenges. SMART outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] Cascading Bandits With Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [cascade bandits, explore-then-commit, action elimination, lower confidence bound, thompson sampling, edge inference]</li>
<li class=""><strong>authors:</strong> R Sri Prakash, Nikhil Karamchandani, Sharayu Moharir</li>
<li class=""><strong>institution:</strong> IIITDM Kancheepuram, IIT Bombay</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes four bandit algorithms for optimizing cascade model ordering in edge inference systems. The study finds that LCB and Thompson Sampling achieve constant regret by continuously adapting to feedback, while Explore-then-Commit and Action Elimination incur suboptimal regret due to fixed ordering commitments. Adaptive policies are shown to be crucial for efficient edge inference under uncertainty.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semantic codec, bit error rate-aware decoder, soft quantization, feature-augmentation learning, XAI-based asymmetry compensation]</li>
<li class=""><strong>authors:</strong> Jiaming Huang, Yi Gao, Fuchang Pan, Renjie Li, Wei Dong</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11038</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SemanticNN, a semantic codec system that tolerates bit-level transmission errors while maintaining semantic-level correctness for collaborative inference offloading on weak IoT devices. The method incorporates BER-aware decoding, soft quantization encoding, and novel training strategies to handle asymmetric device-edge capabilities. Experimental results show SemanticNN reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy under varying error conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-task inference, ground-station query scheduler, dynamic filter ordering, distributed decision problem, resource-aware adaptive decisions]</li>
<li class=""><strong>authors:</strong> Ansel Kaplan Erol, Seungjun Lee, Divya Mahajan</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EarthSight introduces a distributed framework that coordinates satellite constellations and ground stations to perform intelligent image analysis. The system uses multi-task inference, query scheduling, and dynamic filtering to prioritize valuable images while conserving onboard resources. Evaluation shows it reduces compute time by 1.9x and cuts 90th percentile latency from 51 to 21 minutes compared to state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] FengHuang: Next-Generation Memory Orchestration for AI Inferencing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [memory disaggregation, multi-tier shared-memory, active tensor paging, near-memory compute]</li>
<li class=""><strong>authors:</strong> Jiamin Li, Lei Qu, Tao Zhang, Grigory Chirkov, Shuotao Xu, Peng Cheng, Lidong Zhou</li>
<li class=""><strong>institution:</strong> Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.10753" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.10753</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FengHuang proposes a disaggregated AI infrastructure platform with multi-tier shared-memory architecture and active tensor paging to overcome memory and communication scaling limits for AI inference. The platform achieves significant reductions in local memory capacity (up to 93%) and GPU requirements (up to 50%) while maintaining performance across large language models like GPT-3 and Grok-1. This provides a scalable, cost-effective solution for AI inference infrastructure with open, heterogeneous design that eliminates vendor lock-in.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251117] UFO<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>: Weaving the Digital Agent Galaxy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [TaskConstellation, distributed DAG, asynchronous execution, adaptive recovery, dynamic optimization]</li>
<li class=""><strong>authors:</strong> Chaoyun Zhang, Liqun Li, He Huang, Chiming Ni, Bo Qiao, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</li>
<li class=""><strong>institution:</strong> Microsoft, ZJU-UIUC Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11332" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11332</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UFO3 introduces a system that models user requests as mutable TaskConstellations - distributed DAGs of atomic subtasks executed across heterogeneous devices. The system enables asynchronous execution, adaptive recovery, and dynamic optimization through its Constellation Orchestrator and Agent Interaction Protocol. Evaluation shows UFO3 achieves efficient cross-device task orchestration with 70.9% task success rate and 31% latency reduction while maintaining graceful degradation under failures.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 18</strong></p>
<ul>
<li class="">[arXiv251117] Understanding the Nature of Depth-1 Equivariant Quantum Circuit <a href="https://arxiv.org/pdf/2511.10756" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11402" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2511.10788" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Context-aware Adaptive Visualizations for Critical Decision Making <a href="https://arxiv.org/pdf/2511.11476" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models <a href="https://arxiv.org/pdf/2511.11233" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag <a href="https://arxiv.org/pdf/2511.11190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation <a href="https://arxiv.org/pdf/2511.11500" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations <a href="https://arxiv.org/pdf/2511.10872" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms <a href="https://arxiv.org/pdf/2511.11323" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving <a href="https://arxiv.org/pdf/2511.11079" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets <a href="https://arxiv.org/pdf/2511.10985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Scalable Population Training for Zero-Shot Coordination <a href="https://arxiv.org/pdf/2511.11083" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis <a href="https://arxiv.org/pdf/2511.11020" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] VIDEOP2R: Video Understanding from Perception to Reasoning <a href="https://arxiv.org/pdf/2511.11113" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping <a href="https://arxiv.org/pdf/2511.11551" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism <a href="https://arxiv.org/pdf/2511.11373" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning <a href="https://arxiv.org/pdf/2511.10843" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Robust and Efficient Communication in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11393" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv251117] Virtual Width Networks <a href="https://arxiv.org/pdf/2511.11238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Benchmarking Quantum Kernels Across Diverse and Complex Data <a href="https://arxiv.org/pdf/2511.10831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy <a href="https://arxiv.org/pdf/2511.11558" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LiteAttention: A Temporal Sparse Attention for Diffusion Transformers <a href="https://arxiv.org/pdf/2511.11062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices <a href="https://arxiv.org/pdf/2511.10680" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models <a href="https://arxiv.org/pdf/2511.11505" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization <a href="https://arxiv.org/pdf/2511.11118" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery <a href="https://arxiv.org/pdf/2511.11257" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251117] MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores <a href="https://arxiv.org/pdf/2511.10909" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-18">2025-11-18<a href="#2025-11-18" class="hash-link" aria-label="Direct link to 2025-11-18" title="Direct link to 2025-11-18" translate="no">​</a></h2>
<p><strong>cs.DC total: 53</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251118] The Anatomy of a Triton Attention Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [triton, paged attention, auto-tuning, domain-specific languages, cross-platform kernels]</li>
<li class=""><strong>authors:</strong> Burkhard Ringlein, Jan van Lunteren, Radu Stoica, Thomas Parnell</li>
<li class=""><strong>institution:</strong> IBM Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a paged attention kernel using only the Triton domain-specific language to achieve cross-platform LLM inference. The kernel achieves 105.9% of state-of-the-art performance on both NVIDIA and AMD GPUs through algorithmic improvements and parameter auto-tuning. The work demonstrates that open-source domain-specific languages can enable portable, efficient LLM inference across different hardware vendors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [deep reinforcement learning, neural networks, multi-agent systems, hybrid architectures, edge computing]</li>
<li class=""><strong>authors:</strong> Deep Bodra, Sushil Khairnar</li>
<li class=""><strong>institution:</strong> Harrisburg University of Science and Technology, Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11603" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11603</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares machine learning algorithms for cloud resource allocation, evaluating 10 approaches across four categories including deep reinforcement learning and neural networks. The analysis shows that hybrid AI/ML architectures consistently outperform single-method approaches in performance metrics like cost optimization and energy efficiency. Edge computing environments demonstrate the highest deployment readiness for these next-generation resource allocation strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [workload mapping, scheduling optimization, constraint reasoning, makespan calculation, natural language processing]</li>
<li class=""><strong>authors:</strong> Aasish Kumar Sharma, Julian Kunkel</li>
<li class=""><strong>institution:</strong> Georg-August-Universität Göttingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11612</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study evaluates 21 large language models on their ability to solve HPC workload mapping and scheduling problems from natural language descriptions. The results show that while three models achieved optimal scheduling solutions, most struggled with precise timing calculations and dependency enforcement. The findings suggest LLMs are better suited as explainable co-pilots for optimization tasks rather than autonomous solvers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [device-edge co-inference, pipeline parallelism, data parallelism, adaptive scheduling, system-aware optimization, batch inference]</li>
<li class=""><strong>authors:</strong> Ao Zhou, Jianlei Yang, Tong Qiao, Yingjie Qi, Xinming Wei, Cenlin Duan, Weisheng Zhao, Chunming Hu</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11586</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACE-GNN introduces an adaptive co-inference framework that dynamically schedules between pipeline and data parallelism to optimize GNN inference in edge environments. It uses system-level abstraction and prediction methods to maintain stable performance under network fluctuations and multi-device access. Experiments show it achieves up to 12.7× speedup and 82.3% energy savings compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differential testing, automated pipeline, model synthesis, operator implementation, numerical discrepancies, compilation failures]</li>
<li class=""><strong>authors:</strong> Elliott Wen, Sean Ma, Ewan Tempero, Jens Dietrich, Daniel Luo, Jiaxing Shen, Kaiqi Zhao, Bruce Sham, Yousong Song, Jiayi Hua, Jia Hong</li>
<li class=""><strong>institution:</strong> The University of Auckland, Hong Kong Polytechnic University, Victoria University of Wellington, Harbin Institute of Technology, Lingnan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11601" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11601</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an empirical study using differential testing with an automated pipeline that synthesizes over 100,000 variant models from real-world examples. The study reveals significant inconsistencies across heterogeneous AI accelerators, showing newer platforms support fewer operators and exhibit higher output discrepancy rates. These findings highlight challenges in achieving consistent machine learning behavior across diverse hardware ecosystems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [FPGA, hardware reconfiguration, parallel pipelines, deterministic timing, energy efficiency, hardware-algorithm co-design, partial reconfiguration]</li>
<li class=""><strong>authors:</strong> Arturo Urías Jiménez</li>
<li class=""><strong>institution:</strong> Instituto Tecnológico y de Estudios Superiores de Monterrey</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11614</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using Field-Programmable Gate Arrays (FPGAs) as reconfigurable hardware platforms that can directly map AI algorithms into device logic for improved performance. The method enables parallel processing of convolutions and attention mechanisms with deterministic timing and reduced power consumption. The main conclusion is that FPGAs offer strategic advantages over GPUs for AI workloads requiring predictable performance, deep customization, and edge deployment near sensors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [intermediate feature compression, asymmetric top-K filtering, magnitude-splitting, adaptive bit quantization, model partitioning]</li>
<li class=""><strong>authors:</strong> Mingyu Sung, Suhwan Im, Daeho Bang, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</li>
<li class=""><strong>institution:</strong> Kyungpook National University, Queen&#x27;s University, Pukyong National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SLICER, a training-free framework that compresses intermediate features in split computing using sparsification and adaptive quantization techniques. This approach reduces communication volume by up to 10x and server GPU time by up to 4.4x while maintaining task quality within 0-3 percentage points of baseline. The method works with off-the-shelf models without retraining, providing scalable distributed inference for both vision and language tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] PACE Solver Description: twin_width_fmi</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph-algorithms], [greedy-heuristic, simulated-annealing, iterative-greedy, local-search, graph-reductions]</li>
<li class=""><strong>authors:</strong> David Balaban, Adrian Miclăuş</li>
<li class=""><strong>institution:</strong> International Computer High School of Bucharest, University of Bucharest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents twin_width_fmi&#x27;s solver for the Minimum Dominating Set problem, featuring three approaches: a greedy baseline, greedy with simulated annealing, and their best-performing hedom5 method that combines iterative greedy construction with pruning and local improvement. The hedom5 algorithm consistently achieved the smallest dominating sets in PACE 2025 benchmark tests, outperforming the other methods under competition constraints. This demonstrates that iterative greedy approaches with focused repair strategies are more effective than single-pass greedy methods or basic local search for this problem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, LoRA, parameter-efficient fine-tuning, client adapters]</li>
<li class=""><strong>authors:</strong> Kabir Khan, Manju Sarkar, Anita Kar, Suresh Ghosh</li>
<li class=""><strong>institution:</strong> San Francisco State University, University of Lakhimpur, Manipur University, Lakshadweep University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11585" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11585</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedGen-Edge, a federated learning framework that uses Low-Rank Adaptation (LoRA) to train only lightweight client adapters while keeping the pre-trained backbone frozen. This approach reduces communication costs by over 99%, handles non-IID data effectively, and enables personalization through local adapter tuning. The method achieves better performance metrics and faster convergence than full-model federated averaging while maintaining a simple server architecture.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Q-learning, distributed algorithms, shortest-path tree, IoT routing, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Van-Vi Vo, Tien-Dung Nguyen, Duc-Tai Le, Hyunseung Choo</li>
<li class=""><strong>institution:</strong> Sungkyunkwan University, Hanoi University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11598" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11598</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed Q-learning framework for constructing shortest-path trees in IoT sensor networks, where nodes independently learn optimal routing decisions using local information. The method achieves near-optimal routing accuracy (over 99% for large networks) while reducing communication overhead compared to traditional centralized approaches. The framework demonstrates superior scalability, energy efficiency, and adaptability to topology changes in resource-constrained IoT environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [software-defined architecture, VRAM-aware allocation, heterogeneous GPU support, load balancing, dynamic reallocation]</li>
<li class=""><strong>authors:</strong> Pedro Antunes, Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luís Frazão, Nuno Costa, António Pereira</li>
<li class=""><strong>institution:</strong> Polytechnic University of Leiria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11621</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AIvailable introduces a software-defined architecture for LLM-as-a-Service that enables efficient inference across heterogeneous and legacy GPU nodes through dynamic VRAM-aware allocation. The system abstracts GPU-specific details and provides a unified client interface while ensuring full GPU acceleration without CPU fallbacks. This approach allows resource-constrained organizations to repurpose legacy hardware for generative AI workloads, democratizing access to LLM services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [CPU-GPU heterogeneous computing, static timing analysis, delay calculation models, graph-based timing queries, path-based timing queries, flattened API, SDC constraints]</li>
<li class=""><strong>authors:</strong> Zizheng Guo, Haichuan Liu, Xizhe Shi, Shenglu Hua, Zuodong Zhang, Chunyuan Zhao, Runsheng Wang, Yibo Lin</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11660" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11660</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HeteroSTA is a CPU-GPU heterogeneous static timing analysis engine that provides end-to-end GPU acceleration for both graph-based and path-based timing queries. It supports industrial design formats and offers versatile delay calculation models without requiring external tools. The system demonstrates remarkable runtime speed-up while maintaining comparable quality in various integration scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [speculative backpropagation, FPGA acceleration, OpenMP parallelization, MNIST dataset]</li>
<li class=""><strong>authors:</strong> Sed Centeno, Christopher Sprague, Arnab A Purkayastha, Ray Simar, Neeraj Magotra</li>
<li class=""><strong>institution:</strong> Western New England University, Rice University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11640" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11640</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements speculative backpropagation using OpenMP to overlap forward and backward passes in neural network training. The method achieved up to 24% speedup in execution time while maintaining accuracy within 3-4% of baseline on MNIST dataset. The work demonstrates the potential for hardware acceleration and is planned for FPGA synthesis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [elastic tensor parallelism, state preservation, continuous minimal migration, bandwidth-aware planning, P2P transfer pipelining]</li>
<li class=""><strong>authors:</strong> Wendong Xu, Chujie Chen, He Xiao, Kuan Li, Jing Xiong, Chen Zhang, Wenyong Zhou, Chaofan Tao, Yang Bai, Bei Yu, Ngai Wong</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Chinese Academy of Sciences, Hong Kong University of Science and Technology, Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AnchorTP introduces a state-preserving elastic tensor parallelism framework that enables resilient LLM inference by preserving model parameters and KV caches during GPU failures. It uses a bandwidth-aware planner with continuous minimal migration and execution scheduler to minimize data movement during recovery. The system reduces Time to First Success by up to 11x and Time to Peak by up to 59% compared to restart-and-reload approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [adaptive scheduling, machine learning model, time-weighted probability voting, sched_ext framework, workload pattern recognition]</li>
<li class=""><strong>authors:</strong> Xinbo Wang, Shian Jia, Ziyang Huang, Jing Cao, Mingli Song</li>
<li class=""><strong>institution:</strong> Zhejiang University, Hangzhou City University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11628</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an Adaptive Scheduling Agent (ASA) that uses a machine learning model to dynamically select optimal scheduling policies from a portfolio of expert schedulers based on workload patterns. This approach combines offline training of a hardware-agnostic model with runtime decision-making using probability voting and mapping tables. Evaluation shows ASA outperforms the default Linux scheduler in 86.4% of test scenarios and achieves near-optimal performance in most cases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [zero-knowledge proofs, IPFS, peer-to-peer, decentralized identity, Rust SDK, Noir, DID-Key, Libp2p, Iroh]</li>
<li class=""><strong>authors:</strong> Yuanjie Liu, Wenpeng Xing, Ye Zhou, Gaowei Chang, Changting Lin, Meng Han</li>
<li class=""><strong>institution:</strong> Zhejiang University, Binjiang Institute of Zhejiang University, GenTel.io, ANP Open Source Community</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11619" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11619</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DIAP proposes a decentralized agent identity protocol that binds agent identities to IPFS content identifiers and uses zero-knowledge proofs for ownership verification without record updates. The framework includes a Rust SDK with a hybrid P2P stack combining Libp2p and Iroh, and eliminates external ZKP toolchain dependencies through precompiled Noir circuits. This establishes a practical foundation for trustless, privacy-preserving autonomous agent ecosystems and A2A economies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [energy efficiency, GPU acceleration, memory bandwidth, model architecture, edge computing]</li>
<li class=""><strong>authors:</strong> Md Romyull Islam, Bobin Deng, Nobel Dhar, Tu N. Nguyen, Selena He, Yong Shi, Kun Suo</li>
<li class=""><strong>institution:</strong> Kennesaw State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11624</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study empirically evaluates the energy efficiency of five small language models on various edge devices including Raspberry Pi 5 and Jetson platforms. The research found that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, while Llama 3.2 provides the best balance of accuracy and power efficiency. GPU acceleration, memory bandwidth, and model architecture were identified as key factors in optimizing inference energy efficiency for edge deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Range Asymmetric Numeral Systems, asymmetric integer quantization, sparse tensor representation, GPU-accelerated implementation, split computing]</li>
<li class=""><strong>authors:</strong> Mingyu Sung, Suhwan Im, Vikas Palakonda, Jae-Mo Kang</li>
<li class=""><strong>institution:</strong> Kyungpook National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11664</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a lightweight compression framework using rANS encoding with asymmetric integer quantization and sparse tensor representation to reduce intermediate feature transmission in split computing. The method achieves significant bandwidth reduction while maintaining near-baseline accuracy across various neural architectures and tasks. Extensive evaluations demonstrate the framework&#x27;s effectiveness in both computer vision and natural language processing applications without requiring network modifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed data engine, OS replicas, parallelization, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Zengyi Qin, Jinyuan Chen, Yunze Man, Shengcao Cao, Ziqi Pang, Zhuoyuan Wang, Xin Sun, Gen Lin, Han Fang, Ling Zhu, Zixin Xie, Zibu Wei, Tianshu Ran, Haoran Geng, Xander Wu, Zachary Bright, Qizhen Sun, Rui Wang, Yuyang Cai, Song Wang, Jiace Zhao, Han Cao, Yeyang Zhou, Tianrui Liu, Ray Pan, Chongye Yang, Xiang Ren, Bo Zhang, Yutong Ban, Jitendra Malik, Brian Anthony, Pieter Abbeel</li>
<li class=""><strong>institution:</strong> MIT, UIUC, CMU, USC, UV A, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11672" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11672</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OSGym is a super-scalable distributed data engine that efficiently runs over a thousand OS replicas for training computer agents across diverse tasks. It enables comprehensive pipelines for data collection, supervised fine-tuning, and reinforcement learning at low cost. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance agent research scalability and universality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [co-tuning, knowledge transfer, distilled proxy models, structure-agnostic mutual learning, cloud-edge systems]</li>
<li class=""><strong>authors:</strong> Yuze Liu, Yunhan Wang, Tiehua Zhang, Zhishu Shen, Cheng Peng, Libing Wu, Feng Xia, Jiong Jin</li>
<li class=""><strong>institution:</strong> Swinburne University of Technology, Tongji University, Wuhan University of Technology, INFLY TECH, Wuhan University, RMIT University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Co-PLMs, a structure-agnostic co-tuning framework that enables collaborative training between large language models on cloud servers and small language models on edge devices using distilled proxy models as bridges. This approach facilitates knowledge exchange between heterogeneous models while preserving domain-specific insights. Experimental results show Co-PLMs outperforms state-of-the-art methods with average improvements of 5.38% in Rouge-L and 4.88% in EM scores.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, chest x-ray classification, non-IID data, privacy-preserving training]</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11714</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses federated learning to enable multiple hospitals to collaboratively train a pneumonia detection model from chest X-rays without sharing patient data. The method achieved significant performance improvements (47.5% accuracy gain and 50.0% ROC-AUC gain) compared to single-hospital models. The results demonstrate that federated learning provides a secure and effective approach for medical AI collaboration while maintaining data privacy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Meta-Heuristic Load Balancer for Cloud Computing Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [genetic algorithm, meta-heuristic, load balancing, resource allocation, service migration]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Vladimir Getov</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11721</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a meta-heuristic load balancing strategy for cloud computing systems that uses a novel genetic algorithm seeded with outputs from other meta-heuristic algorithms. The approach focuses on allocating services without overloading nodes while maintaining system stability with minimum cost, considering multiple resource types and migration costs. Experimental results demonstrate the effectiveness of this strategy for managing dynamic cloud environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [predictive analytics, reinforcement learning, data replication, adaptive replication mechanisms]</li>
<li class=""><strong>authors:</strong> Almond Kiruthu Murimi</li>
<li class=""><strong>institution:</strong> Kabarak University, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes machine learning-driven data replication strategies using predictive analytics and reinforcement learning to enhance fault tolerance in distributed systems. The research demonstrates that these adaptive mechanisms can forecast failures and optimize data placement in real-time, outperforming traditional static approaches. The findings highlight both the potential and implementation challenges of ML-driven solutions for creating resilient, self-optimizing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [parameter-efficient finetuning, unified memory allocator, two-stage latency predictor, QoS-guaranteed scheduler, co-location]</li>
<li class=""><strong>authors:</strong> Ao Xu, Han Zhao, Weihao Cui, Quan Chen, Yukang Chen, Shulai Zhang, Shuang Chen, Jiemin Jiang, Zhibin Yu, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Zhejiang University, Chinese Academy of Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11729</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Harli improves GPU utilization in LLM serving by co-locating parameter-efficient finetuning tasks with decode instances. The system uses a unified memory allocator, latency predictor, and QoS-aware scheduler to address memory and interference challenges. Experimental results show Harli increases finetune throughput by 46.2% on average while maintaining strict QoS guarantees for inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, decentralized inference, parallel verification, adaptive verification, communication optimization]</li>
<li class=""><strong>authors:</strong> Jingwei Song, Wanyi Chen, Xinyuan Song, Chris Tong, Gufeng Chen, Tianyi Zhao, Eric Yang, Bill Shi, Lynn Ai</li>
<li class=""><strong>institution:</strong> Gradient Network, The University of Hong Kong, Soochow University, Emory University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11733</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Decentralized Speculative Decoding (DSD), a framework that converts network latency into computation throughput by verifying multiple candidate tokens in parallel across distributed nodes. It uses an adaptive verification strategy that adjusts acceptance thresholds based on token-level semantic importance. The method achieves up to 2.59× speedup on benchmarks while preserving accuracy, enabling faster distributed LLM inference without model retraining.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [task-data orchestration, distributed push-pull, load balancing]</li>
<li class=""><strong>authors:</strong> Yiwei Zhao, Qiushi Lin, Hongbo Kang, Guy E. Blelloch, Laxman Dhulipala, Charles McGuffey, Phillip B. Gibbons</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Tsinghua University, University of Maryland, Reed College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TD-Orch is a distributed orchestration framework that uses a bidirectional push-pull technique to co-locate tasks with their required data while achieving scalable load balancing. The system demonstrates up to 2.7x speedup over existing distributed scheduling baselines and enables TDO-GP, a graph processing system that achieves 4.1x average speedup over prior distributed graph systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [edge-based statistical summarization, cloud-based query planning, context-rich prompts, behavioral data clustering, data reduction]</li>
<li class=""><strong>authors:</strong> Kausar Patherya, Ashutosh Dhekne, Francisco Romero</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Flash-Fusion is an edge-cloud system that uses statistical summarization at the edge to reduce IoT data volume and intelligent query planning in the cloud to assemble context-rich prompts for LLMs. The system achieves 95% latency reduction and 98% decrease in token usage while maintaining response quality, enabling efficient natural language queries on IoT sensor streams without manual preprocessing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimization, distributional analysis, pairwise divergence metrics, clustering, noise-aware decision-making]</li>
<li class=""><strong>authors:</strong> Christina Schenk, Miguel Hernández-del-Valle, Luis Calero-Lumbreras, Marcus Noack, Maciej Haranczyk</li>
<li class=""><strong>institution:</strong> IMDEA Materials Institute, Universidad Carlos III de Madrid, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11739</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a noise-aware optimization framework that uses distributional analysis and pairwise divergence metrics with clustering to select between single-device and robust multi-device Bayesian optimization strategies. The method explicitly models device-specific noise profiles to manage variability in high-throughput manufacturing systems. Experimental results with nominally identical 3D printers demonstrate reduced redundancy, lower resource usage, and improved reliability compared to conventional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Modular GPU Programming with Typed Perspectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [typed perspectives, collective operations, thread coordination, Bundl calculus, modular programming]</li>
<li class=""><strong>authors:</strong> Manya Bansal, Daniel Sainati, Joseph W. Cutler, Saman Amarasinghe, Jonathan Ragan-Kelley</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology, University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11939</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Prism, a new GPU programming language that uses typed perspectives to materialize thread granularity at the type level. This approach enables modular programming while maintaining low-level control over collective operations. The authors demonstrate that Prism provides safety guarantees for writing modular GPU code without sacrificing performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache offloading, disk-aware optimization, preloading prediction, hardware-aware disk access]</li>
<li class=""><strong>authors:</strong> Huawei Zhang, Chunwei Xia, Zheng Wang</li>
<li class=""><strong>institution:</strong> University of Leeds</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11907</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KVSwap is a software framework that offloads KV cache to disk storage for long-context on-device inference. It uses compact metadata to predict critical entries for preloading and optimizes disk access patterns. The method achieves higher throughput under memory constraints while maintaining generation quality compared to existing offloading schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel computing], [iterative DFS, GPU shared memory, bank conflict avoidance, parallel optimization]</li>
<li class=""><strong>authors:</strong> Guangchao Yao, Yali Li</li>
<li class=""><strong>institution:</strong> XiaoPeng Motors</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12009</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an iterative depth-first search algorithm optimized for GPU parallel computing with zero bank conflicts to solve the N-Queens problem. Using eight RTX 5090 GPUs, they verified the 27-Queens solution in 28.4 days and achieved over 10x speedup compared to state-of-the-art methods, confirming previous results while making larger problems computationally feasible.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [CUDA acceleration, memory control, chunked execution, GPU-accelerated filtering, volumetric data segmentation]</li>
<li class=""><strong>authors:</strong> Camila Machado de Araujo, Egon P. B. S. Borges, Ricardo Marcelo Canteiro Grangeiro, Allan Pinto</li>
<li class=""><strong>institution:</strong> Brazilian Synchrotron Light Laboratory (LNLS), Brazilian Center for Research in Energy and Materials (CNPEM)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11890" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11890</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Harpia, a CUDA-accelerated library that enhances Annotat3D with strict memory control, native chunked execution, and GPU-accelerated tools for large-scale volumetric data segmentation. The system enables reliable operation on datasets exceeding single-GPU memory capacity and demonstrates significant improvements in processing speed and scalability compared to existing frameworks. The combination of interactive human-in-the-loop interfaces with efficient GPU resource management makes it suitable for collaborative scientific imaging workflows in HPC environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Seasonal Temporal Pattern Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [distributed hierarchical lookup hash structures, seasonal temporal patterns, time series mining]</li>
<li class=""><strong>authors:</strong> Van Ho-Long, Nguyen Ho, Anh-Vu Dinh-Duc, Ha Manh Tran, Ky Trung Nguyen, Tran Dung Pham, Quoc Viet Hung Nguyen</li>
<li class=""><strong>institution:</strong> International University, Vietnam National University, Loyola University Maryland, Griffith University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12216" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12216</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DSTPM, the first distributed framework for mining seasonal temporal patterns from time series data using distributed hierarchical lookup hash structures. The method significantly outperforms sequential baselines in runtime and memory usage while effectively scaling to very large datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Quick and Exact Method for Distributed Quantile Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [Greenwald-Khanna Sketch, GK Select, tree-reduce, exact quantile computation, Spark]</li>
<li class=""><strong>authors:</strong> Ivan Cao, Jaromir J. Saloni, David A. G. Harrison</li>
<li class=""><strong>institution:</strong> University of Mississippi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12025</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents GK Select, a distributed algorithm that computes exact quantiles in Spark by leveraging the GK Sketch to identify candidate values and then tree-reducing them. This approach avoids expensive global sorting operations while maintaining exact results. Empirical results show GK Select achieves approximately 10.5x speedup over Spark&#x27;s full sort method while matching the time complexity of approximate sketching methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, KV cache optimization, memory-compute tradeoff, in-place updates]</li>
<li class=""><strong>authors:</strong> Arun Ramachandran, Ramaswamy Govindarajan, Murali Annavaram, Prakash Raghavendra, Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Indian Institute of Science, University of Southern California</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12031</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BMC, a KV cache allocation mechanism that balances memory and compute by allocating redundant rows periodically to enable in-place updates without copy overhead. This approach reduces memory management costs while allowing redundant computation to be repurposed for speculative decoding. BMC achieves significant throughput improvements over baseline methods and state-of-the-art inference servers on both CPUs and GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [serverless computing, high-performance computing, distributed data frames, NAT traversal, TCP hole punching, AWS Lambda, Cylon]</li>
<li class=""><strong>authors:</strong> Mills Staylor, Arup Kumar Sarker, Gregor von Laszewski, Geoffrey Fox, Yue Cheng, Judy Fox</li>
<li class=""><strong>institution:</strong> University of Virginia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12185" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12185</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Cylon, a high-performance distributed data frame solution that combines serverless and HPC paradigms using a serverless communicator with NAT traversal TCP hole punching. The research demonstrates that AWS Lambda performance falls below 1% of serverful AWS (EC2) and HPC systems in strong scaling experiments. The work addresses communication and performance challenges in serverless functions for data-intensive ML applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [sequence parallelism, task pipelining, model decoupling, attention co-processing]</li>
<li class=""><strong>authors:</strong> Sijie Wang, Qiang Wang, Shaohuai Shi</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12056</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PipeDiT introduces a pipelining framework with three innovations: pipelined sequence parallelism, decoupled diffusion-VAE execution, and attention co-processing to accelerate video generation. The method achieves 1.06x to 4.02x speedups over existing frameworks while maintaining video quality through system-level optimizations rather than algorithmic changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] SEE++: Evolving Snowpark Execution Environment for Modern Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gVisor, sandboxing, syscall filtering, virtual warehouse, secure execution environment]</li>
<li class=""><strong>authors:</strong> Gaurav Jain, Brandon Baker, Joe Yin, Chenwei Xie, Zihao Ye, Sidh Kulkarni, Sara Abdelrahman, Nova Qi, Urjeet Shrestha, Mike Halcrow, Dave Bailey, Yuxiong He</li>
<li class=""><strong>institution:</strong> Snowflake, Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12457</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper describes Snowflake&#x27;s transition from an in-house sandboxing solution to gVisor for their Snowpark Execution Environment to handle diverse Data Engineering and AI/ML workloads. The upgraded architecture incorporates targeted optimizations and standardized base images to improve functionality, performance, and maintainability. The new system successfully provides enhanced security and flexibility for supporting modern workloads while maintaining performance advantages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [knowledge adaptation, edge-cloud collaboration, distributed inference, Pareto optimization, model compression]</li>
<li class=""><strong>authors:</strong> Mohammad Mahdi Kamani, Zhongwei Cheng, Lin Chen</li>
<li class=""><strong>institution:</strong> Wyze Labs, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.11719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.11719</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ECCENTRIC, a framework that uses knowledge adaptation between edge and cloud models to create Pareto optimal models with different computation-communication-performance trade-offs. This approach reduces both computation and communication costs during inference while maintaining high performance. Empirical results on classification and object detection tasks demonstrate the framework&#x27;s effectiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Decentralized Root Cause Localization Approach for Edge Computing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [Personalized PageRank, decentralized clustering, anomaly scoring, peer-to-peer coordination]</li>
<li class=""><strong>authors:</strong> Duneesha Fernando, Maria A. Rodriguez, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12486" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12486</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a decentralized root cause localization method using Personalized PageRank with communication-aware microservice clustering and inter-cluster coordination. The approach achieves comparable or higher accuracy than centralized methods while reducing localization time by up to 34%. The results demonstrate that decentralized graph-based RCL provides an efficient solution for anomaly diagnosis in resource-constrained edge environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] The Time to Consensus in a Blockchain: Insights into Bitcoin&#x27;s &quot;6 Blocks Rule&#x27;&#x27;</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [queueing theory, Laplace transform, simulation, Nakamoto consensus]</li>
<li class=""><strong>authors:</strong> Partha S. Dey, Aditya S. Gopalan, Vijay G. Subramanian</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes consensus time in blockchain systems using queueing techniques to model competing honest and adversarial growth processes. The authors compute the Laplace transform for time to consensus in a stylized Bitcoin model and validate their approach through simulations, providing insights into Bitcoin&#x27;s &quot;6 blocks rule&quot; for achieving permanent consensus.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Iris: First-Class Multi-GPU Programming Experience in Triton</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [multi-GPU programming, tile-based symmetric memory, compute-communication overlap, Triton, fused kernels]</li>
<li class=""><strong>authors:</strong> Muhammad Awad, Muhammad Osama, Brandon Potter</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12500" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12500</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Iris is a multi-GPU communication library implemented in Python and Triton that provides tile-based symmetric memory abstractions for seamless computation-communication overlap. It enables developers to write single-source kernels with minimal code changes while achieving high performance. The evaluation shows Iris delivers up to 1.79x speedup over existing libraries while dramatically simplifying multi-GPU programming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [hardware acceleration], [FPGA, SVD, Hestenes method, Dataflow Architecture, Jacobi method, BRAM optimization]</li>
<li class=""><strong>authors:</strong> Fangqiang Du, Sixuan Chong, Zixuan Huang, Rui Qin, Fengnan Mi, Caibao Hu, Jiangang Chen</li>
<li class=""><strong>institution:</strong> East China Normal University, Zhejiang Hospital, Shanghai Publishing and Printing College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12461</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Data Stream-Based SVD processing algorithm (DSB Jacobi) implemented on FPGA that reduces on-chip memory usage while improving computational speed. Experimental results show the method reduces on-chip RAM consumption by 41.5% and improves computational efficiency by 23 times compared to previous works, providing a practical solution for real-time SVD computation of large-scale data streams.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [Kubernetes, cloud design patterns, data-sharing pipelines, automatic pattern injection, energy metrics]</li>
<li class=""><strong>authors:</strong> Sepideh Masoudi, Mark Edward Michael Daly, Jannis Kiesel</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12667" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12667</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a Kubernetes-based tool called SnapPattern that enables non-intrusive, deferred integration of cloud design patterns into data-sharing pipelines without modifying service code. The tool automates pattern injection and collects energy metrics to support energy-aware decisions. This approach preserves transformation service reusability across different pipeline structures while improving energy efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [federated learning, personalized fine-tuning, linear probing, feature distortion, domain shift]</li>
<li class=""><strong>authors:</strong> Minghui Chen, Hrad Ghoukasian, Ruinan Jin, Zehua Wang, Sai Praneeth Karimireddy, Xiaoxiao Li</li>
<li class=""><strong>institution:</strong> University of British Columbia, Vector Institute, McMaster University, University of Southern California</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12695</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper adapts Linear Probing followed by full Fine-Tuning (LP-FT) to federated learning settings to address personalized model training. The method demonstrates superior performance in balancing personalization and generalization across diverse datasets by mitigating federated feature distortion. The analysis provides theoretical characterization and practical guidelines for deploying robust personalization in heterogeneous federated environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] QPU Micro-Kernels for Stencil Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [QPU micro-kernels, stencil computation, Monte Carlo estimation, shallow quantum circuits, Bernoulli micro-kernel, branching micro-kernel]</li>
<li class=""><strong>authors:</strong> Stefano Markidis, Luca Pennati, Marco Pasquale, Gilbert Netzer, Ivy Peng</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces QPU micro-kernels - shallow quantum circuits that perform stencil node updates and return Monte Carlo estimates from repeated measurements to solve PDEs. The approach treats quantum processors as sampling accelerators for local grid updates while maintaining classical time loops. Experimental results show the Bernoulli micro-kernel achieves lower errors than the branching approach on quantum hardware, with execution time dominated by QPU micro-kernel operations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] On the Fundamental Limits of LLMs at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [diagonalization, finite description length, positional curricula, sparse attention, hierarchical attention, bounded-oracle retrieval]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, The University of Oklahoma, Emory University, Purdue University, UC Riverside, UC Berkeley, National University of Sciences and Technology, Zoox, Meta, Google DeepMind, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.12869" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.12869</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a unified theoretical framework analyzing the fundamental limits of Large Language Model scaling. It demonstrates through computability theory, information theory, and geometric analysis that scaling inevitably encounters irreducible errors, compression constraints, and reasoning degradation. The work concludes that while scaling helps in some areas, it cannot overcome certain theoretical ceilings, and proposes mitigation strategies like positional curricula and sparse attention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sparse matrix-vector multiplication, GPU optimization, unstructured pruning, memory reduction, kernel co-design]</li>
<li class=""><strong>authors:</strong> Vladimír Macko, Vladimír Boža</li>
<li class=""><strong>institution:</strong> Comenius University Bratislava, GrizzlyTech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13061</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MACKO-SpMV, a GPU-optimized format and kernel co-designed for efficient sparse matrix-vector multiplication at low sparsity levels. This approach enables significant memory reduction (1.5×) and faster inference (1.2-1.5× speedup) over dense representations without requiring specialized hardware. The method makes unstructured pruning at 50% sparsity practical for real-world LLM workloads, outperforming existing SpMV libraries by substantial margins.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Learning Process Energy Profiles from Node-Level Power Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [eBPF, perf, power distribution unit, regression model, process-level resource metrics]</li>
<li class=""><strong>authors:</strong> Jonathan Bader, Julius Irion, Jannis Kappel, Joel Witzke, Niklas Fomin, Diellza Sherifi, Odej Kao</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13155" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13155</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method that uses eBPF and perf to collect process-level resource metrics synchronized with node-level power measurements, then applies regression modeling to learn energy profiles for individual processes. This enables fine-grained per-process energy predictions without hardware-specific limitations. The approach addresses the need for hardware-agnostic process-level energy monitoring in data centers to improve energy efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Asymptotic analysis of cooperative censoring policies in sensor networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless sensor networks], [Markov Decision Process, constant-threshold rules, centralized algorithm]</li>
<li class=""><strong>authors:</strong> Jesus Fernandez-Bes, Rocío Arroyo-Valles, Jesús Cid-Sueiro</li>
<li class=""><strong>institution:</strong> Universidad Carlos III de Madrid, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13492</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper models cooperative data censoring in sensor networks using a joint Markov Decision Process to find optimal energy-saving policies. The computationally prohibitive optimal rules are approximated by constant-threshold rules computed via a centralized algorithm. Experimental results show cooperative censoring policies are more energy-efficient than non-cooperative schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [container-based virtualization, service discovery, lightweight orchestration, single-board computers, edge computing]</li>
<li class=""><strong>authors:</strong> Mordechai Guri</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13253" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13253</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pico-Cloud introduces a micro-edge cloud architecture using ultra-minimal hardware platforms like Raspberry Pi Zero to deliver container-based virtualization and lightweight orchestration. The system enables local operation with low latency and power consumption without relying on centralized data centers. The results demonstrate it as a cost-effective, decentralized platform for lightweight distributed workloads at the network edge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split inference, information decomposition, privacy protection, data reconstruction attacks, differential privacy]</li>
<li class=""><strong>authors:</strong> Ruijun Deng, Zhihui Lu, Qiang Duan</li>
<li class=""><strong>institution:</strong> Fudan University, Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13365" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13365</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes InfoDecom, a defense framework that decomposes and removes redundant information from smashed data before injecting calibrated noise to protect against privacy leakage in split inference. This approach addresses the utility degradation problem in existing defenses by targeting only non-redundant information. Experiments show that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251118] Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [DeepSets, distributed hierarchical learning, permutation equivariance, slack-aware normalization, mixed-integer nonlinear programming, network slicing]</li>
<li class=""><strong>authors:</strong> Sulaiman Muhammad Rashid, Ibrahim Aliyu, Jaehyung Park, Jinsul Kim</li>
<li class=""><strong>institution:</strong> Chonnam National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13313</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a distributed hierarchical DeepSets-based model (DeepSets-S) that decomposes joint resource allocation and slice selection into three sub-problems. The method achieves near-optimal performance with 86.1% faster execution time compared to exact solvers while maintaining permutation equivariance over variable-size device sets. Experimental results show high tolerance-based accuracies (over 95% for allocation sub-problems) and improved resource utilization across COIN/MEC systems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 50</strong></p>
<ul>
<li class="">[arXiv251118] Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL <a href="https://arxiv.org/pdf/2511.11592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.11607" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection <a href="https://arxiv.org/pdf/2511.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Convergence of Multiagent Learning Systems for Traffic control <a href="https://arxiv.org/pdf/2511.11654" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom <a href="https://arxiv.org/pdf/2511.11703" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction <a href="https://arxiv.org/pdf/2511.11770" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing <a href="https://arxiv.org/pdf/2511.11780" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Conformal Constrained Policy Optimization for Cost-Effective LLM Agents <a href="https://arxiv.org/pdf/2511.11828" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Better LLM Reasoning via Dual-Play <a href="https://arxiv.org/pdf/2511.11881" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization <a href="https://arxiv.org/pdf/2511.11896" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression <a href="https://arxiv.org/pdf/2511.11973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams <a href="https://arxiv.org/pdf/2511.11992" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation <a href="https://arxiv.org/pdf/2511.12033" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization <a href="https://arxiv.org/pdf/2511.12060" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies <a href="https://arxiv.org/pdf/2511.12075" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12123" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning <a href="https://arxiv.org/pdf/2511.12344" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach <a href="https://arxiv.org/pdf/2511.12351" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation <a href="https://arxiv.org/pdf/2511.12417" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Tailored Primitive Initialization is the Secret Key to Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12429" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mitigating Length Bias in RLHF through a Causal Lens <a href="https://arxiv.org/pdf/2511.12573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] NFQ2.0: The CartPole Benchmark Revisited <a href="https://arxiv.org/pdf/2511.12644" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs <a href="https://arxiv.org/pdf/2511.12706" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL <a href="https://arxiv.org/pdf/2511.12755" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation <a href="https://arxiv.org/pdf/2511.12779" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization <a href="https://arxiv.org/pdf/2511.12792" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Maximizing the efficiency of human feedback in AI alignment: a comparative analysis <a href="https://arxiv.org/pdf/2511.12796" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Expressive Temporal Specifications for Reward Monitoring <a href="https://arxiv.org/pdf/2511.12808" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback <a href="https://arxiv.org/pdf/2511.12844" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making <a href="https://arxiv.org/pdf/2511.12876" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning <a href="https://arxiv.org/pdf/2511.12908" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning Branching Policies for MILPs with Proximal Policy Optimization <a href="https://arxiv.org/pdf/2511.12986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training <a href="https://arxiv.org/pdf/2511.13016" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection <a href="https://arxiv.org/pdf/2511.13027" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow <a href="https://arxiv.org/pdf/2511.13035" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization <a href="https://arxiv.org/pdf/2511.13091" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions <a href="https://arxiv.org/pdf/2511.13103" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13133" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition <a href="https://arxiv.org/pdf/2511.13137" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play <a href="https://arxiv.org/pdf/2511.13186" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks <a href="https://arxiv.org/pdf/2511.13214" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning <a href="https://arxiv.org/pdf/2511.13322" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Finding Kissing Numbers with Game-theoretic Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13391" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction <a href="https://arxiv.org/pdf/2511.13565" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] P1: Mastering Physics Olympiads with Reinforcement Learning <a href="https://arxiv.org/pdf/2511.13612" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries <a href="https://arxiv.org/pdf/2511.12176" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles <a href="https://arxiv.org/pdf/2511.12260" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Discovering autonomous quantum error correction via deep reinforcement learning <a href="https://arxiv.org/pdf/2511.12482" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Accelerated Distributional Temporal Difference Learning with Linear Function Approximation <a href="https://arxiv.org/pdf/2511.12688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 34</strong></p>
<ul>
<li class="">[arXiv251118] Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines <a href="https://arxiv.org/pdf/2511.11613" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs <a href="https://arxiv.org/pdf/2511.11576" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates <a href="https://arxiv.org/pdf/2511.11615" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion <a href="https://arxiv.org/pdf/2511.11627" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding <a href="https://arxiv.org/pdf/2511.11634" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection <a href="https://arxiv.org/pdf/2511.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling <a href="https://arxiv.org/pdf/2511.11688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Diffusion Models: A Mathematical Introduction <a href="https://arxiv.org/pdf/2511.11746" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Towards autonomous quantum physics research using LLM agents with access to intelligent tools <a href="https://arxiv.org/pdf/2511.11752" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers <a href="https://arxiv.org/pdf/2511.12041" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] D<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo stretchy="false">{</mo></msup><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">^\{3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs <a href="https://arxiv.org/pdf/2511.12280" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing <a href="https://arxiv.org/pdf/2511.12286" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration <a href="https://arxiv.org/pdf/2511.12544" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] PID-controlled Langevin Dynamics for Faster Sampling of Generative Models <a href="https://arxiv.org/pdf/2511.12603" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions <a href="https://arxiv.org/pdf/2511.12628" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs <a href="https://arxiv.org/pdf/2511.12706" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers <a href="https://arxiv.org/pdf/2511.12764" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees <a href="https://arxiv.org/pdf/2511.12846" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] An approach of deep reinforcement learning for maximizing the net present value of stochastic projects <a href="https://arxiv.org/pdf/2511.12865" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning <a href="https://arxiv.org/pdf/2511.12976" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MeanFlow Transformers with Representation Autoencoders <a href="https://arxiv.org/pdf/2511.13019" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Learning Time-Scale Invariant Population-Level Neural Representations <a href="https://arxiv.org/pdf/2511.13022" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications <a href="https://arxiv.org/pdf/2511.13131" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Warm-starting active-set solvers using graph neural networks <a href="https://arxiv.org/pdf/2511.13174" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] KForge: Program Synthesis for Diverse AI Hardware Accelerators <a href="https://arxiv.org/pdf/2511.13274" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Hardware optimization on Android for inference of AI models <a href="https://arxiv.org/pdf/2511.13453" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models <a href="https://arxiv.org/pdf/2511.13526" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping <a href="https://arxiv.org/pdf/2511.13587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Data-driven Acceleration of MPC with Guarantees <a href="https://arxiv.org/pdf/2511.13588" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization <a href="https://arxiv.org/pdf/2511.13676" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention <a href="https://arxiv.org/pdf/2511.13679" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] From Black Box to Insight: Explainable AI for Extreme Event Preparedness <a href="https://arxiv.org/pdf/2511.13712" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] The Singularity Warfare: The metatheoretical Framework <a href="https://arxiv.org/pdf/2511.11674" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251118] Discovering autonomous quantum error correction via deep reinforcement learning <a href="https://arxiv.org/pdf/2511.12482" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-19">2025-11-19<a href="#2025-11-19" class="hash-link" aria-label="Direct link to 2025-11-19" title="Direct link to 2025-11-19" translate="no">​</a></h2>
<p><strong>cs.DC total: 19</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251119] What happens when nanochat meets DiLoCo?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [DiLoCo, Distributed Data Parallelism, low-communication training, representation drift, asynchronous updates]</li>
<li class=""><strong>authors:</strong> Alexander Acker, Soeren Becker, Sasho Nedelkoski, Dominik Scheinert, Odej Kao, Philipp Wiesner</li>
<li class=""><strong>institution:</strong> Team exalsius, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13761</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements DiLoCo as a communication-efficient distributed training method for LLMs using the nanochat framework. While DiLoCo achieves stable pretraining convergence, it causes irreversible representation drift that impairs downstream task performance. The study reveals fundamental trade-offs between communication efficiency and model quality in distributed LLM training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [multi-GPU communication, compute-communication overlap, CUDA framework, data parallelism, tensor parallelism, sequence parallelism, expert parallelism]</li>
<li class=""><strong>authors:</strong> Stuart H. Sul, Simran Arora, Benjamin F. Spector, Christopher Ré</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13940" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13940</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ParallelKittens is a minimal CUDA framework that simplifies multi-GPU kernel development through eight core primitives and a unified programming template. It enables compute-communication overlap for AI workloads across different parallelism strategies. The framework achieves significant speedups (up to 4.08×) with fewer than 50 lines of device code on modern GPU architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Boosting performance: Gradient Clock Synchronisation with two-way measured links</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [gradient clock synchronization, two-way measurement, frequency sources, estimation error, skew bounds]</li>
<li class=""><strong>authors:</strong> Sophie Wenning</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis extends the Gradient Clock Synchronization algorithm by replacing one-way measurements with two-way measurements, creating a more practical model. The new approach removes restrictions on link lengths, formally models frequency sources, and significantly reduces estimation error by multiple orders of magnitude. The method achieves uncertainty contributions of only 0.1-10% of link delay compared to previous implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Inside VOLT: Designing an Open-Source GPU Compiler</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [GPU compiler], [SIMT execution, compiler transformations, divergence management, hierarchical design, middle-end optimizations]</li>
<li class=""><strong>authors:</strong> Shinnung Jeong, Chihyo Ahn, Huanzhi Pu, Jisheng Zhao, Hyesoon Kim, Blaise Tine</li>
<li class=""><strong>institution:</strong> Georgia Tech, University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13751" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13751</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VOLT is an open-source GPU compiler framework that uses a hierarchical design with centralized SIMT analyses and optimizations in the middle-end to support multiple front-end languages and open GPU hardware. The compiler enables efficient SIMT code generation and optimization while maintaining extensibility for evolving GPU architectures. The paper demonstrates VOLT&#x27;s effectiveness through case studies on ISA extensions and host-runtime API support.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU-as-a-service, execution mode identifier, dynamic function runtime, SLO-aware scheduling, hybrid hardware acceleration]</li>
<li class=""><strong>authors:</strong> Maximilian Reisecker, Cynthia Marcelino, Thomas Pusztai, Stefan Nastic</li>
<li class=""><strong>institution:</strong> TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13728</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gaia introduces a hybrid hardware acceleration architecture that combines a lightweight Execution Mode Identifier with a Dynamic Function Runtime to continuously optimize between CPU and GPU backends based on workload requirements. The system dynamically selects the best hardware acceleration while maintaining service level objectives across heterogeneous environments. Evaluation shows Gaia reduces end-to-end latency by up to 95% while ensuring cost-efficient acceleration for serverless AI workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Ozaki decompositions, Automatic Dynamic Precision, Exponent Span Capacity, unsigned integer slicing, FP64 emulation]</li>
<li class=""><strong>authors:</strong> Angelika Schwarz, Anton Anders, Cole Brower, Harun Bayraktar, John Gunnels, Kate Clark, RuQing G. Xu, Samuel Rodriguez, Sebastien Cayrols, Paweł Tabaszewski, Victor Podlozhnyuk</li>
<li class=""><strong>institution:</strong> NVIDIA Corporation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13778" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13778</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Automatic Dynamic Precision (ADP), a GPU-resident framework that uses Ozaki decompositions and Exponent Span Capacity estimation to achieve FP64-level accuracy in matrix multiplication using low-precision Tensor Cores. The method includes unsigned integer slicing for improved efficiency and automatic fallback mechanisms. Results show the approach maintains FP64 fidelity with minimal overhead while achieving significant speedups over native FP64 computation on modern NVIDIA GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [tensor-train decomposition, hardware-software co-design, SVD optimization, GEMM accelerator, RISC-V processor]</li>
<li class=""><strong>authors:</strong> Hyunseok Kwak, Kyeongwon Lee, Kyeongpil Min, Chaebin Jung, Woojoo Lee</li>
<li class=""><strong>institution:</strong> Chung-Ang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13738</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TT-Edge is a hardware-software co-designed framework that optimizes Tensor-Train Decomposition for edge AI by splitting SVD computations and integrating a specialized TTD engine with existing GEMM accelerators. The system achieves 1.7× speedup and 40.2% energy reduction for ResNet-32 compression while maintaining minimal hardware overhead through resource reuse. Experimental results demonstrate effective resolution of latency and energy bottlenecks for on-device model compression in edge environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC communication], [MPI derived datatypes, manual packing/unpacking, MPI_Type_vector, MPI_Type_create_subarray, MPI_Neighbor_alltoallw, persistent operations, cross-implementation benchmarking]</li>
<li class=""><strong>authors:</strong> Temitayo Adefemi</li>
<li class=""><strong>institution:</strong> University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13804" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13804</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares MPI derived datatypes against manual packing approaches across three 2D applications and four MPI implementations using various communication semantics. The study found no consistent performance winner - DDTs can be fastest or slowest depending on the specific MPI implementation and application. The authors recommend profiling both approaches with target MPI stacks since performance portability for derived datatypes is not guaranteed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Semantic Multiplexing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [semantic multiplexing, task multiplexing, semantic communication, MIMO, edge computing, Jetson Orin Nano, millimeter-wave radios]</li>
<li class=""><strong>authors:</strong> Mohammad Abdi, Francesca Meneghello, Francesco Restuccia</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.13779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.13779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Semantic Multiplexing, a method that shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. The approach enables multiplexing more tasks than physical channels without adding hardware resources while maintaining task accuracy. Experimental results show significant reductions in latency (8×), energy consumption (25×), and communication load (54×) compared to baselines while keeping comparable performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative execution, expert offloading, prefetching, quantization, amortization roofline model]</li>
<li class=""><strong>authors:</strong> Wenfeng Wang, Jiacheng Liu, Xiaofeng Hou, Xinfeng Xia, Peng Tang, Mingxuan Zhang, Chao Li, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14102</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoE-SpeQ introduces a speculative execution system that uses a small draft model to predict future expert requirements, enabling proactive prefetching and offloading to hide I/O latency. The system employs an adaptive governor guided by an Amortization Roofline Model to dynamically optimize speculation strategies. This approach achieves up to 2.34x speedup over existing offloading frameworks, making MoE inference more efficient on memory-constrained hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Overview and Prospects of Using Integer Surrogate Keys for Data Warehouse Performance Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [database optimization], [integer surrogate keys, indexing, aggregation, compression, batching]</li>
<li class=""><strong>authors:</strong> Sviatoslav Stumpf, Vladislav Povyshev</li>
<li class=""><strong>institution:</strong> ITMO University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14502" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14502</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes replacing standard DATE/TIMESTAMP types with 32-bit and 64-bit integer formats for time-series data in data warehouses. The method demonstrates significant performance improvements, reducing storage requirements by 30-60% and speeding up query execution by 25-40% across various real-world applications including finance, telecommunications, and IoT.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] 10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [tensor caching, tensor migration, memory offloading, prefetch policies, pinned memory allocation]</li>
<li class=""><strong>authors:</strong> Sabiha Afroz, Redwan Ibne Seraj Khan, Hadeel Albahar, Jingoo Han, Ali R. Butt</li>
<li class=""><strong>institution:</strong> Virginia Tech, Kuwait University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14124" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14124</a></li>
<li class=""><strong>Simple LLM Summary:</strong> 10Cache is a resource-aware tensor caching and migration system that coordinates memory usage across GPU, CPU, and NVMe tiers to accelerate LLM training. It profiles tensor execution order for prefetching and optimizes memory buffer allocation to reduce overhead. The system achieves up to 2× training speedup and significantly improves memory utilization compared to existing offloading methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated learning, cross-silo, participant failure, model quality, data skew]</li>
<li class=""><strong>authors:</strong> Fabian Stricker, David Bermbach, Christian Zirpins</li>
<li class=""><strong>institution:</strong> Hochschule Karlsruhe – University of Applied Sciences, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14456</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes how participant failures affect model quality in cross-silo federated learning systems. The study examines factors like failure timing and data distribution, showing that high data skews lead to optimistic evaluations that mask real impacts. The results provide insights for building more robust federated learning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hierarchical scheduling, model partitioning, dynamic programming, adaptive task scheduling, multi-tier networks, pipelined inference]</li>
<li class=""><strong>authors:</strong> Mulei Ma, Minrui Xu, Zihan Chen, Yang Yang, Tony Q.S.Quek</li>
<li class=""><strong>institution:</strong> Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14450</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hyperion introduces a hierarchical two-stage framework that jointly optimizes inter-tier model partitioning and intra-tier request scheduling for LLM inference in multi-tier networks. The framework combines offline partitioning using dynamic programming with online adaptive scheduling to minimize end-to-end latency. Experimental results show Hyperion reduces latency by up to 52.1% compared to baselines while maintaining high GPU utilization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Hapax Locks : Value-Based Mutual Exclusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrency control], [mutual exclusion, FIFO admission, constant-time operations, cache coherence]</li>
<li class=""><strong>authors:</strong> Dave Dice, Alex Kogan</li>
<li class=""><strong>institution:</strong> Oracle Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hapax Locks is a novel mutual exclusion algorithm that provides constant-time arrival and unlock operations with FIFO admission order while generating minimal coherence traffic. The lock offers performance comparable to state-of-the-art alternatives while being easier to integrate into existing systems due to reduced environmental dependencies. A key innovation is that no pointers transfer ownership between threads during lock operations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [MPI, CUDA-Q, multi-GPU simulation, interconnect benchmarking]</li>
<li class=""><strong>authors:</strong> W. Michael Brown, Anurag Ramesh, Thomas Lubinski, Thien Nguyen, David E. Bernal Neira</li>
<li class=""><strong>institution:</strong> NVIDIA, Purdue University, QED-C Technical Advisory Committee, Quantum Circuits Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14664" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14664</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MPI into quantum circuit simulation benchmarks to evaluate multi-GPU performance across different interconnect technologies. The research demonstrates that while GPU architecture improvements provide 4.5X speedups, interconnect advances deliver over 16X performance gains for multi-GPU quantum simulations. The findings highlight network performance as the critical bottleneck in large-scale quantum circuit simulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] FailSafe: High-performance Resilient Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [tensor parallelism, cyclic KVCache placement, hybrid attention, fine-grained load-aware routing, proactive KVCache backup, on-demand weight recovery]</li>
<li class=""><strong>authors:</strong> Ziyi Xu, Zhiqiang Xie, Swapnil Gandhi, Christos Kozyrakis</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Stanford University, NVIDIA Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14116</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FailSafe introduces a fault-tolerant tensor parallelism system using cyclic KVCache placement, hybrid attention, and load-aware routing to maintain high-performance LLM serving during GPU failures. It achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard approaches. The system demonstrates robust performance even with multiple GPU failures while maintaining balanced utilization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [divided rollout, context-aware scheduling, adaptive grouped speculative decoding, dynamic load balancing]</li>
<li class=""><strong>authors:</strong> Ruoyu Qin, Weiran He, Weixiao Huang, Yangkun Zhang, Yikai Zhao, Bo Pang, Xinran Xu, Yingdi Shan, Yongwei Wu, Mingxing Zhang</li>
<li class=""><strong>institution:</strong> Moonshot AI, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Seer introduces an online context learning system that uses divided rollout, context-aware scheduling, and adaptive grouped speculative decoding to optimize synchronous LLM reinforcement learning. The system addresses workload imbalance in the rollout phase by exploiting similarities in output patterns among requests sharing the same prompt. Evaluations show Seer improves rollout throughput by 74-97% and reduces long-tail latency by 75-93% compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251119] FLARE: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [reputation-based framework, multi-dimensional reputation score, adaptive threshold, reputation-weighted aggregation, local differential privacy, statistical mimicry attack]</li>
<li class=""><strong>authors:</strong> Abolfazl Younesi, Leon Kiss, Zahra Najafabadi Samani, Juan Aznar Poveda, Thomas Fahringer</li>
<li class=""><strong>institution:</strong> University of Innsbruck</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLARE introduces an adaptive reputation-based framework that evaluates client reliability through multi-dimensional scoring and reputation-weighted aggregation with soft exclusion. It maintains model accuracy and converges faster than existing methods under various Byzantine attacks while preserving privacy through local differential privacy. The framework demonstrates improved robustness by up to 16% and maintains convergence within 30% of non-attacked baselines.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 14</strong></p>
<ul>
<li class="">[arXiv251119] GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards <a href="https://arxiv.org/pdf/2511.14045" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Quantifying Distribution Shift in Traffic Signal Control with Histogram-Based GEH Distance <a href="https://arxiv.org/pdf/2511.13785" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Deep reinforcement learning-based spacecraft attitude control with pointing keep-out constraint <a href="https://arxiv.org/pdf/2511.13746" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Beat the long tail: Distribution-Aware Speculative Decoding for RL Training <a href="https://arxiv.org/pdf/2511.13841" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation <a href="https://arxiv.org/pdf/2511.14135" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Parallelizing Tree Search with Twice Sequential Monte Carlo <a href="https://arxiv.org/pdf/2511.14220" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Object-Centric World Models for Causality-Aware Reinforcement Learning <a href="https://arxiv.org/pdf/2511.14262" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning <a href="https://arxiv.org/pdf/2511.14427" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language <a href="https://arxiv.org/pdf/2511.14565" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents <a href="https://arxiv.org/pdf/2511.14584" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Failure to Mix: Large language models struggle to answer according to desired probability distributions <a href="https://arxiv.org/pdf/2511.14630" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration <a href="https://arxiv.org/pdf/2511.14730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mo stretchy="false">{</mo></msup><mo>∗</mo><msub><mo stretchy="false">}</mo><mo stretchy="false">{</mo></msub><mn>0.6</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">π^\{*\}_\{0.6\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2432em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span></span></span></span></span><span class="mord">∗</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">{</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mord">0.6</span><span class="mclose">}</span></span></span></span>: a VLA That Learns From Experience <a href="https://arxiv.org/pdf/2511.14759" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization <a href="https://arxiv.org/pdf/2511.14710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 13</strong></p>
<ul>
<li class="">[arXiv251119] Can Artificial Intelligence Accelerate Technological Progress? Researchers&#x27; Perspectives on AI in Manufacturing and Materials Science <a href="https://arxiv.org/pdf/2511.14007" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation <a href="https://arxiv.org/pdf/2511.14057" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Review of Passenger Flow Modelling Approaches Based on a Bibliometric Analysis <a href="https://arxiv.org/pdf/2511.13742" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] nuCarla: A nuScenes-Style Bird&#x27;s-Eye View Perception Dataset for CARLA Simulation <a href="https://arxiv.org/pdf/2511.13744" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Beat the long tail: Distribution-Aware Speculative Decoding for RL Training <a href="https://arxiv.org/pdf/2511.13841" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling <a href="https://arxiv.org/pdf/2511.14142" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Parallelizing Tree Search with Twice Sequential Monte Carlo <a href="https://arxiv.org/pdf/2511.14220" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Algebraformer: A Neural Approach to Linear Systems <a href="https://arxiv.org/pdf/2511.14263" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning <a href="https://arxiv.org/pdf/2511.14427" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge <a href="https://arxiv.org/pdf/2511.14744" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Look-Ahead Reasoning on Learning Platforms <a href="https://arxiv.org/pdf/2511.14745" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] Principled Coarse-Grained Acceptance for Speculative Decoding in Speech <a href="https://arxiv.org/pdf/2511.13732" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251119] QUASAR: An Evolutionary Algorithm to Accelerate High-Dimensional Optimization <a href="https://arxiv.org/pdf/2511.13843" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-20">2025-11-20<a href="#2025-11-20" class="hash-link" aria-label="Direct link to 2025-11-20" title="Direct link to 2025-11-20" translate="no">​</a></h2>
<p><strong>cs.DC total: 12</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251120] PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingkun Yu, Heming Zhong, Dan Huang, Yutong Lu, Jiazhi Jiang</li>
<li class=""><strong>institution:</strong></li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14852</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [fault-tolerance], [Byzantine fault detection, bounded-time recovery, network measurement protocols]</li>
<li class=""><strong>authors:</strong> Yifan Cai, Linh Thi Xuan Phan</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15031" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15031</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GeoShield introduces Byzantine fault detection and bounded-time recovery mechanisms for geo-distributed cyber-physical systems, using protocols for resilient network measurement and omission fault detection. It guarantees safety through timely recovery rather than fault masking, requiring fewer resources than traditional approaches. Evaluation shows it significantly outperforms existing methods in both effectiveness and resource efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] Towards a Formal Verification of Secure Vehicle Software Updates</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [automotive cybersecurity], [formal verification, ProVerif, symbolic execution, security analysis]</li>
<li class=""><strong>authors:</strong> Martin Slind Hagen, Emil Lundqvist, Alex Phu, Yenan Wang, Kim Strandberg, Elad Michael Schiller</li>
<li class=""><strong>institution:</strong> Chalmers University of Technology, Volvo Car Corporation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15479</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper performs a formal security analysis of the Unified Software Update Framework (UniSUF) using ProVerif-based symbolic execution. The authors model UniSUF&#x27;s architecture and verify its compliance with essential security requirements including confidentiality, integrity, and authenticity. Their results demonstrate that UniSUF adheres to specified security guarantees, ensuring the correctness and reliability of its security framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] Proving there is a leader without naming it</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [local certification, leader election, graph theory, sublogarithmic certification, chordal graphs]</li>
<li class=""><strong>authors:</strong> Laurent Feuilloley, Josef Erik Sedláček, Martin Slávik</li>
<li class=""><strong>institution:</strong> Unknown (no explicit affiliations provided)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15491" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15491</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies local certification methods for proving the existence of a unique leader in networks without explicitly naming it. The authors demonstrate that graph structure significantly impacts certification complexity, achieving sublogarithmic certification in chordal graphs and grids while showing constant-diameter graphs still require logarithmic certificates. Their results reveal how network topology and sparsity affect leader certification requirements across different graph classes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain networks], [peer-to-peer measurement, network crawling, connectivity probes, internet-wide scans, discovery protocols]</li>
<li class=""><strong>authors:</strong> Lucianna Kiffer, Lioba Heimbach, Dennis Trautwein, Yann Vonlanthen, Oliver Gasser</li>
<li class=""><strong>institution:</strong> IMDEA Networks, Category Labs, University of Göttingen, Ethereum Foundation, IPinfo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15388" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15388</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a comprehensive measurement study of 36 blockchain networks using active crawlers, community data, connectivity probes, and internet-wide scanning. The research reveals dramatic variations in network size, resilience, and decentralization across different cryptocurrencies. The methodology provides a general framework for large-scale measurement of decentralized networks, enabling continued monitoring and benchmarking of blockchain infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] A Tensor Compiler for Processing-In-Memory Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [data-centric compiler, joint optimization, data rearrangement, PIM abstraction, performance prediction model]</li>
<li class=""><strong>authors:</strong> Peiming Yang, Sankeerth Durvasula, Ivan Fernandez, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko, Christina Giannoula</li>
<li class=""><strong>institution:</strong> University of Toronto, Barcelona Supercomputing Center, ETH Zurich, Max Planck Institute for Software Systems</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15503</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DCC, a data-centric compiler that jointly optimizes data rearrangements and compute code for Processing-In-Memory architectures. It uses a unified tuning process with PIM-specific abstractions and performance modeling to handle diverse PIM backends. Evaluation shows significant speedups for ML kernels and LLM inference compared to GPU-only execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] Privacy-Preserving IoT in Connected Aircraft Cabin</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [IoT security], [Differential Privacy, additive secret sharing, Privacy-Enhancing Technologies, CSMIM protocol]</li>
<li class=""><strong>authors:</strong> Nilesh Vyas, Benjamin Zhao, Aygün Baltaci, Gustavo de Carvalho Bertoli, Hassan Asghar, Markus Klügel, Gerrit Schramm, Martin Kubisch, Dali Kaafar</li>
<li class=""><strong>institution:</strong> Airbus Central R&amp;T, Macquarie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework integrating Privacy-Enhancing Technologies (PETs) including Differential Privacy and additive secret sharing atop CSMIM architecture for aircraft cabin IoT systems. The research demonstrates that PET computational overhead is negligible compared to network latencies, and architectural choices have greater impact on performance than the privacy techniques themselves. The findings provide practical guidance for implementing trustworthy collaborative IoT ecosystems in aviation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] BlueBottle: Fast and Robust Blockchains through Subsystem Specialization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [two-layer architecture, n=5f+1 protocol, DAG-based consensus, synchronous recovery, Byzantine fault tolerance]</li>
<li class=""><strong>authors:</strong> Preston Vander Vos, Alberto Sonnino, Giorgos Tsimos, Philipp Jovanovic, Lefteris Kokoris-Kogias</li>
<li class=""><strong>institution:</strong> Mysten Labs, pod Network, University College London (UCL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15361" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15361</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BlueBottle introduces a two-layer consensus architecture with BB-Core for high-performance transaction processing and BB-Guard for decentralized monitoring and recovery. This subsystem specialization achieves sub-second finality latency while maintaining strong security and liveness guarantees. Experimental results show 20-25% latency reduction compared to Mysticeti while tolerating Byzantine failures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain security], [blockchain forks, prospect theory, network delay simulation, transaction confirmation probability]</li>
<li class=""><strong>authors:</strong> Ethan Hicks, Joseph Oglio, Mikhail Nesterenko, Gokarna Sharma</li>
<li class=""><strong>institution:</strong> Kent State University, Rio Grande University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15421" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15421</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a value-dependent approach to determine Bitcoin transaction finality by analyzing blockchain forks through network delay simulations and actual Bitcoin data. The study establishes a relationship between block depth and confirmation revocation probability, then applies prospect theory to relate confirmation probability to transaction amount and user risk tolerance. The main conclusion is that transaction confirmation wait time should be dynamically adjusted based on transaction value and user risk preferences rather than using fixed block depths like the conventional six-block rule.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [optimization systems], [RemoteOptiGraph, OptiGraph, Benders decomposition, distributed memory, hypergraphs, InterWorkerEdges]</li>
<li class=""><strong>authors:</strong> David L. Cole, Jordan Jalving, Jonah Langlieb, Jesse D. Jenkins</li>
<li class=""><strong>institution:</strong> Princeton University, Atomic Machines Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.14966" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.14966</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RemoteOptiGraph, a graph-based modeling abstraction for distributed optimization that extends the OptiGraph model with InterWorkerEdges to handle linking constraints across distributed memory systems. This approach enables unified modeling and supports decomposition algorithms like Benders decomposition. The method achieved a 7.5x speedup when solving a large-scale capacity expansion problem compared to non-decomposed approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] GPU-Initiated Networking for NCCL</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU-Initiated Networking, NCCL Device API, RDMA, one-sided communication, DOCA GPUNetIO, DeepEP integration]</li>
<li class=""><strong>authors:</strong> Khaled Hamidouche, John Bachan, Pak Markthub, Peter-Jan Gootzen, Elena Agostini, Sylvain Jeaugey, Aamir Shafi, Georgios Theodorakis, Manjunath Gorentla Venkata</li>
<li class=""><strong>institution:</strong> NVIDIA Corporation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15076" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15076</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GPU-Initiated Networking (GIN) as part of NCCL 2.28&#x27;s Device API, enabling GPUs to directly initiate network communication without CPU involvement through dual backend implementations. The method provides low-latency device-initiated communication using either direct GPU-to-NIC communication via DOCA GPUNetIO or proxy-based approaches over standard RDMA networks. Results demonstrate that GIN successfully integrates device-initiated communication within NCCL&#x27;s unified runtime, combining low-latency operations with collective algorithms for improved AI workload performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251120] Beluga: Block Synchronization for BFT Consensus Protocols</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [block synchronization, BFT consensus, push-pull mechanism, reliable broadcast, adversarial resilience]</li>
<li class=""><strong>authors:</strong> Tasos Kichidis, Lefteris Kokoris-Kogias, Arun Koshy, Ilya Sergey, Alberto Sonnino, Mingwei Tian, Jianting Zhang</li>
<li class=""><strong>institution:</strong> Mysten Labs, University College London (UCL), National University of Singapore, Purdue University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15517" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15517</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Beluga, a block synchronizer for BFT consensus protocols that provides resource-aware block exchange and incremental retrieval. The system maintains optimal performance during normal operation while bounding recovery costs under faults and adversarial attacks. Experimental results show Beluga delivers up to 3x higher throughput and 25x lower latency than prior designs under attack conditions.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 23</strong></p>
<ul>
<li class="">[arXiv251120] Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization <a href="https://arxiv.org/pdf/2511.15055" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation <a href="https://arxiv.org/pdf/2511.14993" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation <a href="https://arxiv.org/pdf/2511.14768" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments <a href="https://arxiv.org/pdf/2511.15284" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis <a href="https://arxiv.org/pdf/2511.14900" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents <a href="https://arxiv.org/pdf/2511.15378" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone <a href="https://arxiv.org/pdf/2511.14887" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs <a href="https://arxiv.org/pdf/2511.15137" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control <a href="https://arxiv.org/pdf/2511.15248" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones <a href="https://arxiv.org/pdf/2511.15208" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15175" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15256" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15002" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization <a href="https://arxiv.org/pdf/2511.14846" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking <a href="https://arxiv.org/pdf/2511.15199" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Learning Interestingness in Automated Mathematical Theory Formation <a href="https://arxiv.org/pdf/2511.14778" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment <a href="https://arxiv.org/pdf/2511.15032" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges <a href="https://arxiv.org/pdf/2511.15652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] VisPlay: Self-Evolving Vision-Language Models from Images <a href="https://arxiv.org/pdf/2511.15661" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models <a href="https://arxiv.org/pdf/2511.15669" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] The Impact of Quantization on Large Reasoning Model Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15694" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution <a href="https://arxiv.org/pdf/2511.15262" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251120] Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs <a href="https://arxiv.org/pdf/2511.15300" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments <a href="https://arxiv.org/pdf/2511.15284" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] D2D Power Allocation via Quantum Graph Neural Network <a href="https://arxiv.org/pdf/2511.15246" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification <a href="https://arxiv.org/pdf/2511.14977" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information <a href="https://arxiv.org/pdf/2511.14765" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning <a href="https://arxiv.org/pdf/2511.15190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] NTK-Guided Implicit Neural Teaching <a href="https://arxiv.org/pdf/2511.15487" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index <a href="https://arxiv.org/pdf/2511.15557" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity <a href="https://arxiv.org/pdf/2511.15593" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Fully Differentiable dMRI Streamline Propagation in PyTorch <a href="https://arxiv.org/pdf/2511.14807" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251120] Particle Monte Carlo methods for Lattice Field Theory <a href="https://arxiv.org/pdf/2511.15196" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-21">2025-11-21<a href="#2025-11-21" class="hash-link" aria-label="Direct link to 2025-11-21" title="Direct link to 2025-11-21" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251121] Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [quantization, streaming, federated learning, message compression, memory management]</li>
<li class=""><strong>authors:</strong> Ziyue Xu, Zhihong Zhang, Holger R. Roth, Chester Chen, Yan Cheng, Andrew Feng</li>
<li class=""><strong>institution:</strong> Nvidia Corp.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16450</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes message quantization and container/file streaming techniques to optimize federated learning for large language models. These methods reduce communication overhead and improve memory management during distributed training. The enhancements significantly boost the robustness and efficiency of federated learning with LLMs in real-world scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Digital Agriculture Sandbox for Collaborative Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, differential privacy, privacy-preserving framework]</li>
<li class=""><strong>authors:</strong> Osama Zafar, Rosemarie Santa González, Alfonso Morales, Erman Ayday</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Georgia Institute of Technology, University of Wisconsin–Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a Digital Agriculture Sandbox platform that uses federated learning and differential privacy to enable secure collaboration between farmers and researchers. This approach allows data analysis without exposing private farm information while maintaining research utility. The platform successfully bridges the gap between farm data privacy and agricultural research needs for addressing global food challenges.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [NorthPole neural inference accelerator, 4-bit integer quantization, vertical integration, containerized inference pipeline]</li>
<li class=""><strong>authors:</strong> Michael V. DeBole, Rathinakumar Appuswamy, Neil McGlohon, Brian Taba, Steven K. Esser, Filipp Akopyan, John V. Arthur, Arnon Amir, Alexander Andreopoulos, Peter J. Carlson, Andrew S. Cassidy, Pallab Datta, Myron D. Flickner, Rajamohan Gandhasri, Guillaume J. Garreau, Megumi Ito, Jennifer L. Klamo, Jeffrey A. Kusnitz, Nathaniel J. McClatchey, Jeffrey L. McKinstry, Tapan K. Nayak, Carlos Ortega Otero, Hartmut Penner, William P. Risk, Jun Sawada, Jay Sivagnaname, Daniel F. Smith, Rafael Sousa, Ignacio Terrizzano, Takanori Ueda, Trent Gray-Donald, David Cox, Dharmendra S. Modha</li>
<li class=""><strong>institution:</strong> IBM Research, IBM Software</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15950" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15950</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a vertically integrated system using 288 NorthPole neural inference accelerator cards to enable scalable and energy-efficient LLM inference. The system achieves high performance with low latency while consuming minimal power, making it suitable for enterprise AI applications in existing data center environments. The prototype demonstrates the ability to run multiple model instances simultaneously with configurable model sizes and context lengths.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [symbolic regression, polynomial regression, Knapsack problem, dynamic scheduling, static scheduling, task packing]</li>
<li class=""><strong>authors:</strong> Daniel Mas Montserrat, Ray Verma, Míriam Barrabés, Francisco M. de la Vega, Carlos D. Bustamante, Alexander G. Ioannidis</li>
<li class=""><strong>institution:</strong> Galatea Bio, Stanford University, New York University, University of California, Santa Cruz</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15977</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes adaptive scheduling methods for genomic workflows that use regression models to predict per-chromosome memory requirements and optimize task batching. The approaches include both dynamic and static schedulers that treat task packing as a Knapsack problem to minimize memory overruns. These methods achieve faster execution times and better resource utilization in large-scale genomic pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [blockchain, decentralized identity, zero-knowledge proofs, TEE-based attestations, verifiable credentials, intent proofs]</li>
<li class=""><strong>authors:</strong> Vivek Acharya</li>
<li class=""><strong>institution:</strong> Unknown (no explicit affiliation provided)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a blockchain-based framework that uses decentralized identity standards, zero-knowledge proofs, and secure execution environments to authenticate AI agents and verify transaction intent. The hybrid on-chain/off-chain architecture creates an immutable audit trail linking user instructions to payment outcomes. The framework demonstrates strong security against impersonation and unauthorized transactions, enabling verifiable trust in autonomous agent payments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Fast LLM Post-training via Decoupled and Best-of-N Speculation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [speculative decoding, dynamic decoupled speculation, Best-of-N speculation]</li>
<li class=""><strong>authors:</strong> Rongxin Cheng, Kai Zhou, Xingda Wei, Siyuan Liu, Mingcong Han, Mingjing Ai, Yeju Zhou, Baoquan Zhong, Wencong Xiao, Xin Liu, Rong Chen, Haibo Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16193" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16193</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SpecActor, a system that accelerates LLM post-training rollout using speculative decoding with dynamic decoupled execution and Best-of-N speculation methods. These approaches maximize GPU efficiency for large-batch execution and improve speculation accuracy without requiring extra resources. SpecActor achieves 1.3-1.7× speedup over common post-training baselines and 1.3-1.5× speedup compared to naive speculative decoding adoption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Pipelined Dense Symmetric Eigenvalue Decomposition on Multi-GPU Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [pipelined two-stage eigenvalue decomposition, multi-GPU optimization, symmetric eigenvalue decomposition, tridiagonalization, band reduction, bulge chasing]</li>
<li class=""><strong>authors:</strong> Hansheng Wang, Ruiyi Zhan, Dajun Huang, Xingchen Liu, Qiao Li, Hancong Duan, Dingwen Tao, Guangming Tan, Shaoshuai Zhang</li>
<li class=""><strong>institution:</strong> University of Electronic Science and Technology of China, Chinese Academy of Sciences, Xiamen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16174</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a pipelined two-stage eigenvalue decomposition algorithm with substantial optimizations for multi-GPU architectures. The implementation achieves significant performance improvements over state-of-the-art libraries, delivering mean speedups of 5.74× and 6.59× over cuSOLVERMp and MAGMA baselines on an 8×A100 platform with better scalability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Can Asymmetric Tile Buffering Be Beneficial?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [asymmetric tile buffering, tiling strategies, performance modeling, arithmetic intensity, kernel switching]</li>
<li class=""><strong>authors:</strong> Chengyue Wang, Wesley Pang, Xinrui Wu, Gregory Jun, Luis Romero, Endri Taka, Diana Marculescu, Tony Nowatzki, Pranathi Vasireddy, Joseph Melber, Deming Chen, Jason Cong</li>
<li class=""><strong>institution:</strong> UCLA, UIUC, EPFL, UT Austin, AMD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16041" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16041</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces asymmetric tile buffering (ATB), a technique that decouples input and output tile dimensions in GEMM operations to improve arithmetic intensity. The authors develop a performance model balancing ATB&#x27;s benefits against kernel switching overheads and demonstrate its effectiveness on AMD&#x27;s XDNA2 AI Engine, achieving up to 4.54× speedup and setting new performance records for mixed-precision GEMM.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Mitigating Shared Storage Congestion Using Control Theory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC storage systems], [control theory, I/O rate regulation, runtime system load metrics, congestion mitigation]</li>
<li class=""><strong>authors:</strong> Thomas Collignon, Kouds Halitim, Raphaël Bleuse, Sophie Cerf, Bogdan Robu, Éric Rutten, Lionel Seinturier, Alexandre van Kempen</li>
<li class=""><strong>institution:</strong> Qarnot Computing, University of Lille, Inria, CNRS, Centrale Lille, University of Grenoble Alpes, CNRS, LIG, GIPSA-lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16177" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16177</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a control theory-based approach to dynamically regulate client-side I/O rates in shared HPC environments using runtime system load metrics. The method effectively mitigates storage congestion by adapting I/O behavior in real-time. Experimental results show the approach reduces total runtime by up to 20% and lowers tail latency while maintaining stable performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus protocols], [Byzantine agreement, atomic broadcast, asynchronous networks, communication complexity, prioritized provable-broadcast]</li>
<li class=""><strong>authors:</strong> Nasit S Sony, Xianzhong Ding</li>
<li class=""><strong>institution:</strong> University of California, Merced, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.15957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.15957</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Slim-HBBFT, an atomic broadcast protocol that reduces communication complexity by considering requests from only a fraction of parties using a prioritized provable-broadcast mechanism. This approach improves communication efficiency by a factor of O(n) compared to conventional protocols. Security analysis confirms that Slim-HBBFT satisfies Asynchronous Common Subset protocol properties while maintaining robust security and reliability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Distributed MIS Algorithms for Rational Agents using Games</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [maximal independent set, rational agents, game theory, symmetry breaking, trembling-hand perfect equilibrium]</li>
<li class=""><strong>authors:</strong> Nithin Salevemula, Shreyas Pai</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Madras</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16533" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16533</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two distributed algorithms for computing Maximal Independent Sets in networks where nodes are rational agents with utility functions. The algorithms use pairwise interactions between neighboring nodes as simple games to generate randomness and break symmetry while ensuring no agent can benefit from unilateral deviation. The approach guarantees correct MIS computation with positive participation probability for all nodes and terminates in O(log n) rounds with high probability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [power systems optimization], [relax-and-round, mixed integer programming, unit commitment, continuous solvers, GPU parallel computation]</li>
<li class=""><strong>authors:</strong> Shaked Regev, Eve Tsybina, Slaven Peles</li>
<li class=""><strong>institution:</strong> UT-Battelle, LLC (Oak Ridge National Laboratory)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16420</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a relax-and-round approach for unit commitment that allows generators to be fractionally on, enabling the use of continuous solvers followed by rounding to obtain feasible solutions. This method reduces solution time from 10 hours to less than a second for a 276-unit system with minor accuracy degradation, and scales efficiently to tens of thousands of generators for large-scale power system applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251121] Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [speculative decoding, reinforcement learning, adaptive drafter, CUDA graphs, long-tail distribution]</li>
<li class=""><strong>authors:</strong> Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han</li>
<li class=""><strong>institution:</strong> MIT, NVIDIA, ETH Zurich, MIT-IBM Watson AI Lab, UMass Amherst</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16665" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16665</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes TLT, a system that accelerates reasoning RL training using adaptive speculative decoding with two key components: an Adaptive Drafter trained on idle GPUs and an Adaptive Rollout Engine. This approach addresses the efficiency bottleneck caused by long-tail response generation distributions in RL training. Evaluations show TLT achieves over 1.7× speedup while preserving model accuracy and producing a high-quality draft model as a byproduct.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 22</strong></p>
<ul>
<li class="">[arXiv251121] A Mathematical Framework for Custom Reward Functions in Job Application Evaluation using Reinforcement Learning <a href="https://arxiv.org/pdf/2511.16073" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming <a href="https://arxiv.org/pdf/2511.16027" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning <a href="https://arxiv.org/pdf/2511.16043" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management <a href="https://arxiv.org/pdf/2511.16075" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy <a href="https://arxiv.org/pdf/2511.15974" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn <a href="https://arxiv.org/pdf/2511.15738" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective <a href="https://arxiv.org/pdf/2511.16231" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding <a href="https://arxiv.org/pdf/2511.15716" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob <a href="https://arxiv.org/pdf/2511.16248" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent <a href="https://arxiv.org/pdf/2511.16108" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Optimizing Operation Recipes with Reinforcement Learning for Safe and Interpretable Control of Chemical Processes <a href="https://arxiv.org/pdf/2511.16297" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe <a href="https://arxiv.org/pdf/2511.16334" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs <a href="https://arxiv.org/pdf/2511.15921" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms <a href="https://arxiv.org/pdf/2511.16475" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Limitations of Scalarisation in MORL: A Comparative Study in Discrete Environments <a href="https://arxiv.org/pdf/2511.16476" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense <a href="https://arxiv.org/pdf/2511.16483" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Green Resilience of Cyber-Physical Systems: Doctoral Dissertation <a href="https://arxiv.org/pdf/2511.16593" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization <a href="https://arxiv.org/pdf/2511.16602" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Stabilizing Policy Gradient Methods via Reward Profiling <a href="https://arxiv.org/pdf/2511.16629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations <a href="https://arxiv.org/pdf/2511.16661" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation <a href="https://arxiv.org/pdf/2511.16671" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] A Primer on Quantum Machine Learning <a href="https://arxiv.org/pdf/2511.15969" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 13</strong></p>
<ul>
<li class="">[arXiv251121] VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference <a href="https://arxiv.org/pdf/2511.16449" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Early science acceleration experiments with GPT-5 <a href="https://arxiv.org/pdf/2511.16072" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming <a href="https://arxiv.org/pdf/2511.16027" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] From Performance to Understanding: A Vision for Explainable Automated Algorithm Design <a href="https://arxiv.org/pdf/2511.16201" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response <a href="https://arxiv.org/pdf/2511.15755" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization <a href="https://arxiv.org/pdf/2511.15915" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] The Future of Food: How Artificial Intelligence is Transforming Food Manufacturing <a href="https://arxiv.org/pdf/2511.15728" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] LLM4EO: Large Language Model for Evolutionary Optimization in Flexible Job Shop Scheduling <a href="https://arxiv.org/pdf/2511.16485" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution <a href="https://arxiv.org/pdf/2511.16541" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] An Exterior-Embedding Neural Operator Framework for Preserving Conservation Laws <a href="https://arxiv.org/pdf/2511.16573" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions <a href="https://arxiv.org/pdf/2511.16575" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] Almost Sure Convergence Analysis of Differentially Private Stochastic Gradient Methods <a href="https://arxiv.org/pdf/2511.16587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251121] gfnx: Fast and Scalable Library for Generative Flow Networks in JAX <a href="https://arxiv.org/pdf/2511.16592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-29T11:30:55.000Z" itemprop="dateModified">Dec 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251110-20251116"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251110-20251116</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251124-20251130"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251124-20251130</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-17" class="table-of-contents__link toc-highlight">2025-11-17</a></li><li><a href="#2025-11-18" class="table-of-contents__link toc-highlight">2025-11-18</a></li><li><a href="#2025-11-19" class="table-of-contents__link toc-highlight">2025-11-19</a></li><li><a href="#2025-11-20" class="table-of-contents__link toc-highlight">2025-11-20</a></li><li><a href="#2025-11-21" class="table-of-contents__link toc-highlight">2025-11-21</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251117/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251117/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>