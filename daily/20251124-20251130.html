<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251124-20251130" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251124-20251130 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251124-20251130"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251124-20251130 | AI头条"><meta data-rh="true" name="description" content="2025-11-24"><meta data-rh="true" property="og:description" content="2025-11-24"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251124-20251130"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251124-20251130" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251124-20251130" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251124-20251130","item":"https://jokebear666.github.io/ai_toutiao/daily/20251124-20251130"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.84a25bac.css">
<script src="/ai_toutiao/assets/js/runtime~main.36d5e77f.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.e61b1501.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251124-20251130</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251124-20251130</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-24">2025-11-24<a href="#2025-11-24" class="hash-link" aria-label="Direct link to 2025-11-24" title="Direct link to 2025-11-24" translate="no">​</a></h2>
<p><strong>cs.DC total: 4</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251124] Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [Stochastic Reward Nets, anomaly detection, precision, recall, inspection frequency]</li>
<li class=""><strong>authors:</strong> Gabriel Job Antunes Grabher, Fumio Machida, Thomas Ropars</li>
<li class=""><strong>institution:</strong> Université Grenoble-Alpes, University of Tsukuba</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17119" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17119</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses Stochastic Reward Nets to model cloud services with performance anomaly detection. The study analyzes how detector characteristics like precision, recall, and inspection frequency affect latency and resource consumption. The main finding is that high precision alone suffices for good performance-cost trade-off with frequent detection, while recall becomes more important with infrequent detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251124] Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [multi-agent systems, LLM-based optimization, PyTorch inference, explore-exploit tradeoff, error-fixing agents]</li>
<li class=""><strong>authors:</strong> Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu</li>
<li class=""><strong>institution:</strong> Northwestern University, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16964" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16964</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using LLM-based multi-agent systems to optimize PyTorch inference performance on GPUs. The research shows that exploit-heavy strategies combined with error-fixing agents achieve the best results, with the optimal implementation delivering an average 2.88x speedup on H100 GPUs across diverse machine learning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251124] MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [token scheduling, expert parallelism, micro-batch optimization]</li>
<li class=""><strong>authors:</strong> Chenqi Zhao, Wenfei Wu, Linhai Song, Yuchen Xu</li>
<li class=""><strong>institution:</strong> Peking University, Institute of Computing Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.16947" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.16947</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MicroEP, a parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts systems through token scheduling across GPUs. Experimental results show that their MicroMoE system improves training throughput by up to 47.6% compared to state-of-the-art systems while maintaining optimal load balance among GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251124] Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [mixture-of-experts, transformer, context parallelism, compressed convolutional attention, fault-tolerance, checkpoint-reshaping, microbenchmarks]</li>
<li class=""><strong>authors:</strong> Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge</li>
<li class=""><strong>institution:</strong> Zyphra, IBM, AMD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents the first large-scale mixture-of-experts pretraining study on pure AMD hardware using MI300X GPUs with Pollara interconnect. The authors developed ZAYA1-base model with optimized transformer sizing rules and system design including fault-tolerance mechanisms and detailed training recipes. The results demonstrate that AMD&#x27;s hardware, networking, and software stack are mature enough for competitive large-scale pretraining, achieving performance comparable to leading base models.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251124] Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems <a href="https://arxiv.org/pdf/2511.17435" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection <a href="https://arxiv.org/pdf/2511.16929" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components <a href="https://arxiv.org/pdf/2511.17112" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving <a href="https://arxiv.org/pdf/2511.16916" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Convergence and stability of Q-learning in Hierarchical Reinforcement Learning <a href="https://arxiv.org/pdf/2511.17351" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Predicting Talent Breakout Rate using Twitter and TV data <a href="https://arxiv.org/pdf/2511.16905" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward <a href="https://arxiv.org/pdf/2511.17165" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle <a href="https://arxiv.org/pdf/2511.17171" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability <a href="https://arxiv.org/pdf/2511.17367" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Intrinsic preservation of plasticity in continual quantum learning <a href="https://arxiv.org/pdf/2511.17228" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization <a href="https://arxiv.org/pdf/2511.17489" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">[arXiv251124] Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration <a href="https://arxiv.org/pdf/2511.17123" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates <a href="https://arxiv.org/pdf/2511.16815" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Generating transition states of chemical reactions via distance-geometry-based flow matching <a href="https://arxiv.org/pdf/2511.17229" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes <a href="https://arxiv.org/pdf/2511.17399" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation <a href="https://arxiv.org/pdf/2511.16807" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation <a href="https://arxiv.org/pdf/2511.16757" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search <a href="https://arxiv.org/pdf/2511.16681" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251124] Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation <a href="https://arxiv.org/pdf/2511.17031" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-25">2025-11-25<a href="#2025-11-25" class="hash-link" aria-label="Direct link to 2025-11-25" title="Direct link to 2025-11-25" translate="no">​</a></h2>
<p><strong>cs.DC total: 18</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251125] A novel strategy for multi-resource load balancing in agent-based systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent systems], [load balancing, multi-resource management, agent self-assessment, social behavior modeling, adaptation abilities]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Aleksander Zgrzywa</li>
<li class=""><strong>institution:</strong> Wroclaw University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17580" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17580</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a multi-resource load balancing strategy that utilizes agent social behavior and adaptation capabilities to optimize enterprise system architectures. The approach enables agents to perform self-assessment for determining optimal configurations. Experimental results demonstrate the effectiveness of the implemented agent system for load balancing tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud resource management], [CloudSim Plus, spot instances, HLEM-VMP, Google Cluster Trace, virtual machine allocation]</li>
<li class=""><strong>authors:</strong> Christoph Goldgruber, Benedikt Pittl, Erich Schikuta</li>
<li class=""><strong>institution:</strong> University of Vienna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18137</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the CloudSim Plus simulation framework to model spot instance lifecycle management and adapts the HLEM-VMP allocation algorithm for dynamic cloud markets. The enhanced framework demonstrates reduced spot instance interruptions and shorter maximum interruption durations compared to baseline strategies. This contributes to more robust and cost-effective resource management in volatile cloud computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Pier: Efficient Large Language Model pretraining with Relaxed Global Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [momentum warmup, momentum decay, data parallel, tensor parallel, DiLoCo, global communication optimization]</li>
<li class=""><strong>authors:</strong> Shuyuan Fan, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17849</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pier introduces an efficient LLM pretraining optimizer that reduces global communication bottlenecks through momentum warmup and momentum decay techniques. The system achieves significant speedups (up to 3.7x on 256 A100s) while maintaining model performance across GPT model variants. Experimental results demonstrate Pier&#x27;s effectiveness with various parallelization strategies including data parallel and tensor parallel configurations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [network algorithms], [mobile agents, monotone decontamination, dynamic graphs, edge-search, node-search, mixed-search]</li>
<li class=""><strong>authors:</strong> Rajashree Bar, Daibik Barik, Adri Bhattacharya, Partha Sarathi Mandal</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Guwahati, Indian Statistical Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18315</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies monotone decontamination in arbitrary dynamic graphs using mobile agents, proposing two models based on edge reappearance timing. The authors establish both lower and upper bounds on the number of agents required for complete decontamination while maintaining monotonicity. The results demonstrate the challenges posed by dynamic edge changes and optimize agent requirements for network decontamination.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [cascaded conditional learning, hypergraph modeling, root cause localization, failure type identification]</li>
<li class=""><strong>authors:</strong> Shuaiyu Xie, Hanbin He, Jian Wang, Bing Li</li>
<li class=""><strong>institution:</strong> Wuhan University, Zhongguancun Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17566</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CCLH, a root cause analysis framework that uses cascaded conditional learning and heterogeneous hypergraphs to model group-level relationships between microservice instances. The method addresses limitations in existing approaches by capturing causal dependencies between diagnostic tasks and simulating failure propagation. Experimental results show CCLH outperforms state-of-the-art methods in both root cause localization and failure type identification.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, decentralized federated learning, low-rank adaptation, alternating optimization, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Xiaoyu Wang, Xiaotian Li, Zhixiang Zhou, Chen Li, Yong Liu</li>
<li class=""><strong>institution:</strong> New York University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18291</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ADF-LoRA, a decentralized federated learning method that alternates updates between low-rank matrices and mixes both matrices to maintain parameter consistency. The approach addresses challenges of phase-state mismatch and block-wise divergence in peer-to-peer communication settings. Experimental results show ADF-LoRA achieves faster convergence and higher accuracy than existing LoRA variants in decentralized federated learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [PagedAttention, throughput optimization, latency optimization, GPU memory utilization, batch processing]</li>
<li class=""><strong>authors:</strong> Saicharan Kolluru</li>
<li class=""><strong>institution:</strong> Independent researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17593</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two LLM serving frameworks - vLLM and HuggingFace TGI - through empirical evaluation of throughput, latency, and resource utilization. The study found that vLLM achieves significantly higher throughput using its novel PagedAttention mechanism, while TGI performs better for latency-sensitive interactive applications. The choice between frameworks should depend on specific workload requirements, with vLLM excelling in high-throughput scenarios and TGI better for low-latency use cases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC I/O optimization], [adaptive middleware, consistent hashing, power-of-d sampling, cooperative caching, self-stabilizing control loop, namespace-aware load balancing]</li>
<li class=""><strong>authors:</strong> Sangam Ghimire, Nigam Niraula, Nirjal Bhurtel, Paribartan Timalsina, Bishal Neupane, James Bhattarai, Sudan Jha</li>
<li class=""><strong>institution:</strong> Kathmandu University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18124" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18124</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MIDAS introduces an adaptive proxy middleware that uses namespace-aware load balancing, cooperative caching, and self-stabilizing control loops to mitigate metadata hotspots in HPC systems. The system operates transparently between clients and metadata servers without requiring kernel or backend modifications. Experimental results show it reduces average queue lengths by 23% and worst-case hotspots by up to 80% compared to round-robin scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [split computing, dual-stream architecture, adaptive compression, embodied self-awareness, edge-cloud computing]</li>
<li class=""><strong>authors:</strong> Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt</li>
<li class=""><strong>institution:</strong> University of California, Irvine, Kookmin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18151" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18151</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AVERY introduces an adaptive split computing framework for Vision-Language Models that uses a cognitive-inspired dual-stream approach separating real-time context processing from deep analysis. The system dynamically adjusts compression based on network conditions and operator intent through a lightweight onboard controller. This approach achieves 11.2% higher accuracy than raw compression and 93.98% lower energy consumption than full-edge execution, enabling efficient VLM deployment on resource-constrained UAVs in disaster scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [real-time systems], [schedule-abstraction graph, response-time analysis, hybrid-triggered jobs, non-preemptive systems]</li>
<li class=""><strong>authors:</strong> Ruide Cao, Zhuyun Qi, Qinyang He, Chenxi Ling, Yi Wang, Guoming Tang</li>
<li class=""><strong>institution:</strong> SUSTech, Tsinghua University, Nankai University, Peng Cheng Laboratory, The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.17882" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.17882</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SAGkit, a Python toolkit implementing the schedule-abstraction graph framework for exact response-time analysis of hybrid-triggered jobs in distributed control systems. The toolkit addresses limitations of conventional methods by allowing job absence modeling and handling release jitter and execution time variations. Experimental results show SAGkit achieves exact timing analysis with acceptable runtime and memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [low-rank approximation, FP8 precision, SVD, randomized SVD, memory bandwidth optimization]</li>
<li class=""><strong>authors:</strong> Alfredo Metere</li>
<li class=""><strong>institution:</strong> Metere Consulting, LLC</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18674</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Low-Rank GEMM, which uses low-rank matrix approximations with FP8 precision to achieve sub-quadratic complexity in matrix multiplication. The system automatically selects optimal decomposition methods and precision levels based on hardware capabilities. On NVIDIA RTX 4090, it achieves up to 378 TFLOPS with 75% memory savings and 7.8× speedup over traditional approaches for large matrices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split learning, cyclical updates, feature resampling, alternating block coordinate descent, aggregation-free training]</li>
<li class=""><strong>authors:</strong> Mengdi Wang, Efe Bozkir, Enkelejda Kasneci</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Munich Center for Machine Learning (MCML)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18611" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18611</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CycleSL introduces a cyclical update framework for split learning where the server model is optimized first using resampled client features, followed by client updates using the updated server. This aggregation-free approach addresses scalability issues and performance degradation in existing split learning methods. Empirical results demonstrate improved model performance across multiple datasets with non-IID data distribution and partial client participation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Federated style aware transformer aggregation of representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [personalized federated learning, disentangled representation, transformer attention, prototype aggregation, style-content separation]</li>
<li class=""><strong>authors:</strong> Mincheol Jeon, Euinam Huh</li>
<li class=""><strong>institution:</strong> Kyunghee University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18841" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18841</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedSTAR, a federated learning framework that disentangles client-specific style factors from shared content representations and uses Transformer-based attention for prototype aggregation. This approach reduces communication overhead by exchanging compact prototypes and style vectors instead of full model parameters. Experimental results show improved personalization and robustness in heterogeneous environments without increasing communication costs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [multi-instance GPU, fragmentation-aware scheduling, greedy algorithm, resource allocation, GPU partitioning]</li>
<li class=""><strong>authors:</strong> Marco Zambianco, Lorenzo Fasol, Roberto Doriguzzi-Corin</li>
<li class=""><strong>institution:</strong> Fondazione Bruno Kessler (FBK)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18906" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18906</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a fragmentation-aware GPU scheduling framework for MIG-based cloud environments that uses a fragmentation metric and greedy algorithm to minimize resource inefficiency. The method achieves higher workload acceptance rates compared to baseline strategies, increasing scheduled workloads by an average of 10% under heavy load conditions while using similar numbers of GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [vector database, hardware-aware matrix pipeline, workload-aware scheduling, on-chip memory optimization, concurrent query processing]</li>
<li class=""><strong>authors:</strong> Xinkui Zhao, Qingyu Ma, Yifan Zhang, Hengxuan Lou, Guanjie Cheng, Shuiguang Deng, Jianwei Yin</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19192" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19192</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AME is an on-device agentic memory engine that introduces a hardware-aware matrix pipeline and workload-aware scheduling scheme to optimize vector database operations on smartphones. The system addresses mobile SoC constraints and continuous learning workloads through efficient compute utilization and coordinated task management. Experimental results show significant improvements in query throughput, index construction speed, and insertion performance compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [branch-and-bound, distributed computing, sliding-window algorithm, MPI, deterministic solving]</li>
<li class=""><strong>authors:</strong> Longfei Wang, Junyan Liu, Fan Zhang, Jiangwen Wei, Yuanhua Tang, Jie Sun, Xiaodong Luo</li>
<li class=""><strong>institution:</strong> Shenzhen Research Institute of Big Data, Huawei Technologies Co., Ltd., Chinese University of Hong Kong, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.18723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.18723</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes N2N, a parallel framework that maps branch-and-bound nodes to distributed computing nodes for solving large-scale MILP problems. It supports both deterministic and nondeterministic modes and integrates with existing solvers like SCIP and HiGHS. Experimental results show N2N-SCIP achieves significant speedups over ParaSCIP, demonstrating superior performance in distributed memory environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [local certification, proof labelling schemes, self-stabilizing algorithms, chordal graphs, dismantlable graphs, leader election, spanning tree construction]</li>
<li class=""><strong>authors:</strong> Jérémie Chalopin, Maria Kokkou</li>
<li class=""><strong>institution:</strong> Aix Marseille University, CNRS, LIS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19208" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19208</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents constant-size local certification schemes for leader election in chordal and K4-free dismantlable graphs, and for spanning tree construction in dismantlable graphs. The certification enables each node to verify solution correctness using only local neighborhood information. Additionally, the authors propose a method to transform any certification scheme into a silent self-stabilizing algorithm by adding just one extra state.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251125] IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computer architecture], [IOMMU, SMMU, DMA, virtual address translation, kernel modules, ARMv8]</li>
<li class=""><strong>authors:</strong> Antonis Psistakis</li>
<li class=""><strong>institution:</strong> University of Crete, Foundation for Research and Technology - Hellas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19258" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19258</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis implemented custom kernel modules to test ARM&#x27;s System Memory Management Unit (SMMU) for virtual-to-physical address translation in DMA operations. The research successfully demonstrated SMMU functionality by mapping virtual addresses and configuring page tables for dynamic translation. The work confirmed that the IOMMU approach can support virtual-address remote DMA in ARMv8 environments.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 47</strong></p>
<ul>
<li class="">[arXiv251125] Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning <a href="https://arxiv.org/pdf/2511.17598" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning <a href="https://arxiv.org/pdf/2511.17927" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] LLM Reasoning for Cold-Start Item Recommendation <a href="https://arxiv.org/pdf/2511.18261" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction <a href="https://arxiv.org/pdf/2511.17879" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning <a href="https://arxiv.org/pdf/2511.18181" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning <a href="https://arxiv.org/pdf/2511.18000" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change <a href="https://arxiv.org/pdf/2511.17630" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment <a href="https://arxiv.org/pdf/2511.18055" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization <a href="https://arxiv.org/pdf/2511.17568" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch <a href="https://arxiv.org/pdf/2511.17826" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization <a href="https://arxiv.org/pdf/2511.18093" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation <a href="https://arxiv.org/pdf/2511.17579" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building <a href="https://arxiv.org/pdf/2511.17654" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Deep Gaussian Process Proximal Policy Optimization <a href="https://arxiv.org/pdf/2511.18214" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks <a href="https://arxiv.org/pdf/2511.17506" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems <a href="https://arxiv.org/pdf/2511.18223" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization <a href="https://arxiv.org/pdf/2511.17963" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently <a href="https://arxiv.org/pdf/2511.17852" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Tail Distribution of Regret in Optimistic Reinforcement Learning <a href="https://arxiv.org/pdf/2511.18247" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization <a href="https://arxiv.org/pdf/2511.17938" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation <a href="https://arxiv.org/pdf/2511.17765" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production <a href="https://arxiv.org/pdf/2511.17632" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models <a href="https://arxiv.org/pdf/2511.17876" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design <a href="https://arxiv.org/pdf/2511.17595" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference <a href="https://arxiv.org/pdf/2511.17931" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] General Agentic Memory Via Deep Research <a href="https://arxiv.org/pdf/2511.18423" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints <a href="https://arxiv.org/pdf/2511.18450" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints <a href="https://arxiv.org/pdf/2511.18606" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition <a href="https://arxiv.org/pdf/2511.18671" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Reinforcement Learning for Self-Healing Material Systems <a href="https://arxiv.org/pdf/2511.18728" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion <a href="https://arxiv.org/pdf/2511.18742" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning <a href="https://arxiv.org/pdf/2511.18871" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals <a href="https://arxiv.org/pdf/2511.18878" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation <a href="https://arxiv.org/pdf/2511.18958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning <a href="https://arxiv.org/pdf/2511.18977" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Dynamic Mixture of Experts Against Severe Distribution Shifts <a href="https://arxiv.org/pdf/2511.18987" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning <a href="https://arxiv.org/pdf/2511.19168" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization <a href="https://arxiv.org/pdf/2511.19218" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization <a href="https://arxiv.org/pdf/2511.19253" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Leveraging LLMs for reward function design in reinforcement learning control tasks <a href="https://arxiv.org/pdf/2511.19355" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems <a href="https://arxiv.org/pdf/2511.19368" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research <a href="https://arxiv.org/pdf/2511.19399" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Learning Robust Social Strategies with Large Language Models <a href="https://arxiv.org/pdf/2511.19405" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning <a href="https://arxiv.org/pdf/2511.19422" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Prequential posteriors <a href="https://arxiv.org/pdf/2511.17721" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19 <a href="https://arxiv.org/pdf/2511.18035" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons <a href="https://arxiv.org/pdf/2511.18076" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 27</strong></p>
<ul>
<li class="">[arXiv251125] Learning Rate Scheduling with Matrix Factorization for Private Training <a href="https://arxiv.org/pdf/2511.17994" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices <a href="https://arxiv.org/pdf/2511.17754" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Gate-level boolean evolutionary geometric attention neural networks <a href="https://arxiv.org/pdf/2511.17550" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning <a href="https://arxiv.org/pdf/2511.17885" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python <a href="https://arxiv.org/pdf/2511.18157" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery <a href="https://arxiv.org/pdf/2511.18298" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Energy-based Autoregressive Generation for Neural Population Dynamics <a href="https://arxiv.org/pdf/2511.17606" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Accelerating Time Series Foundation Models with Speculative Decoding <a href="https://arxiv.org/pdf/2511.18191" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] ProHD: Projection-Based Hausdorff Distance Approximation <a href="https://arxiv.org/pdf/2511.18207" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models <a href="https://arxiv.org/pdf/2511.17511" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction <a href="https://arxiv.org/pdf/2511.18150" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data <a href="https://arxiv.org/pdf/2511.18260" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations <a href="https://arxiv.org/pdf/2511.18793" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories <a href="https://arxiv.org/pdf/2511.18834" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals <a href="https://arxiv.org/pdf/2511.18878" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Skeletons Matter: Dynamic Data Augmentation for Text-to-Query <a href="https://arxiv.org/pdf/2511.18934" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Understanding, Accelerating, and Improving MeanFlow Training <a href="https://arxiv.org/pdf/2511.19065" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks <a href="https://arxiv.org/pdf/2511.19265" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] CDLM: Consistency Diffusion Language Models For Faster Sampling <a href="https://arxiv.org/pdf/2511.19269" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model <a href="https://arxiv.org/pdf/2511.19272" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems <a href="https://arxiv.org/pdf/2511.19368" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Flow Map Distillation Without Data <a href="https://arxiv.org/pdf/2511.19428" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation <a href="https://arxiv.org/pdf/2511.19114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] TorchQuantumDistributed <a href="https://arxiv.org/pdf/2511.19291" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] High-throughput validation of phase formability and simulation accuracy of Cantor alloys <a href="https://arxiv.org/pdf/2511.19335" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers <a href="https://arxiv.org/pdf/2511.19347" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251125] Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design <a href="https://arxiv.org/pdf/2511.19423" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-26">2025-11-26<a href="#2025-11-26" class="hash-link" aria-label="Direct link to 2025-11-26" title="Direct link to 2025-11-26" translate="no">​</a></h2>
<p><strong>cs.DC total: 24</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251126] Systemic approach for modeling a generic smart grid</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [smart grid simulation], [systemic approach, distributed optimization, agent-based models, JADE, microgrid]</li>
<li class=""><strong>authors:</strong> Sofiane Ben Amor, Guillaume Guerard, Loup-Noé Levy</li>
<li class=""><strong>institution:</strong> Laboratory LI-PaRAD, Pole Universitaire Léonard de Vinci, De Vinci Research Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19460</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a systemic modeling approach that uses distributed optimization of subsystems to simulate smart grid operations. The method enables production and consumption scheduling while maintaining flexibility and scalability for testing alternative grid scenarios. The backbone model serves as a foundation for developing additional modules to address specific smart grid management needs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [reinforcement learning, temporal workload forecasting, safe-PPO, deterministic inference, shard allocation]</li>
<li class=""><strong>authors:</strong> M. Zeeshan Haider, Tayyaba Noreen, M. D. Assuncao, Kaiwen Zhang</li>
<li class=""><strong>institution:</strong> École de technologie supérieure (ÉTS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19450" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19450</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PSAP, a predictive shard allocation protocol that combines temporal workload forecasting with safety-constrained reinforcement learning to dynamically allocate blockchain shards. The system achieves up to 2x throughput improvement and 35% lower latency while maintaining Byzantine safety through deterministic inference and safety constraints. Experimental results demonstrate significant performance gains over existing dynamic sharding approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical storage, hot-cold tiering, modality-aware compression, metadata indexing, embedded hardware validation]</li>
<li class=""><strong>authors:</strong> Yuxin Wang, Yuankai He, Weisong Shi</li>
<li class=""><strong>institution:</strong> University of Delaware</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19453" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19453</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AVS, a computational storage system for autonomous vehicles that uses hierarchical design with modality-aware compression and hot-cold tiering. The system was validated on embedded hardware using real autonomous driving traces and demonstrated predictable real-time data ingestion with fast selective retrieval. The work concludes that storage should be treated as a first-class component in autonomous vehicle stacks to handle massive heterogeneous sensor data efficiently.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hybrid scheduling, sparsity, computational intensity, reinforcement learning, asynchronous execution, batch optimization]</li>
<li class=""><strong>authors:</strong> Ziyang Zhang, Jie Liu, Luca Mottola</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, Harbin Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19457</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SparOA is a CPU-GPU hybrid inference framework that optimizes DNN operator scheduling using sparsity and computational intensity analysis with reinforcement learning. It achieves significant performance improvements, delivering 1.22-1.31x speedup over baselines and reducing energy consumption by 7-16% compared to state-of-the-art co-execution methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [directed acyclic graphs, code generation, static scheduling, domain-specific optimization, Julia programming]</li>
<li class=""><strong>authors:</strong> Anton Reinhard, Simeon Ehrig, René Widera, Michael Bussmann, Uwe Hernandez Acosta</li>
<li class=""><strong>institution:</strong> Helmholtz-Zentrum Dresden-Rossendorf, Center for Advanced Systems Understanding</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19456</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a Julia framework that uses directed acyclic graphs with domain-specific information to automatically generate statically scheduled and compiled code for complex computational problems. The approach enables graph-level optimizations that combine hardware-aware scheduling with domain knowledge. The method is demonstrated through quantum electrodynamics calculations, showing improved efficiency for heterogeneous computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel optimization], [shared-memory parallelism, asynchronous optimization, single-trajectory parallel adaptation, iteration-based parallelism]</li>
<li class=""><strong>authors:</strong> Luca Accorsi, Demetrio Laganà, Federico Michelotto, Roberto Musmanno, Daniele Vigo</li>
<li class=""><strong>institution:</strong> Google, University of Calabria, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FILO2^x, a parallel shared-memory approach that asynchronously optimizes multiple solution areas of Capacitated Vehicle Routing Problems with minimal synchronization. This single-trajectory parallel adaptation of the FILO2 algorithm enables concurrent optimization by multiple solvers. Computational results show FILO2^x significantly reduces resolution time while maintaining similar solution quality compared to the original single-threaded approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [urban energy modeling], [EnergyPlus, HPC, LiDAR, geospatial data, building simulation]</li>
<li class=""><strong>authors:</strong> Aldo Canfora, Eleonora Bergamaschi, Riccardo Mioli, Federico Battini, Mirko Degli Esposti, Giorgio Pedrazzi, Chiara Dellacasa</li>
<li class=""><strong>institution:</strong> University of Bologna, Cineca, IFAB Foundation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19463" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19463</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an Urban Building Energy Modeling pipeline that integrates EnergyPlus simulations with high-performance computing and open geospatial datasets to estimate building energy demand in Bologna. The method combines building geometry from open data portals with construction attributes from regulatory databases. Using the Leonardo supercomputer, the system successfully simulated approximately 25,000 buildings in under 30 minutes, demonstrating efficient city-scale energy consumption analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [GPTQ 4-bit quantization, shared memory buffering, vectorized memory loading, inline assembly, vLLM]</li>
<li class=""><strong>authors:</strong> Yaozheng Zhang, Wei Wang, Jie Kong, Jiehan Zhou, Huanqing Cui</li>
<li class=""><strong>institution:</strong> Shandong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19438" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19438</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Opt4GPTQ, a platform-level optimization method for 4-bit GPTQ quantized LLM inference that integrates three strategies: shared memory buffering, vectorized memory loading, and inline assembly optimization. The method achieves up to 84.42% throughput improvement and 51.35% latency reduction across different models. This demonstrates the critical importance of platform-level engineering optimizations for efficient LLM inference on heterogeneous AI accelerators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [temperature hyperparameter, small language models, on-premises deployment, GPU architectures, execution time analysis, incident categorization]</li>
<li class=""><strong>authors:</strong> Marcio Pohlmann, Alex Severo, Gefté Almeida, Diego Kreutz, Tiago Heinrich, Lourenço Pereira</li>
<li class=""><strong>institution:</strong> AI Horizon Labs, Federal University of Pampa, Max Planck Institute for Informatics, Instituto Tecnológico de Aeronáutica</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19464" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19464</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates 21 small language models (1B-20B parameters) for automated cybersecurity incident categorization in on-premises environments. The study systematically analyzes the impact of temperature hyperparameter across different model architectures and sizes. Results show that temperature has minimal influence on performance, while model size and GPU capacity are the decisive factors for effective local deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Towards a future space-based, highly scalable AI infrastructure system design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [satellite clusters, free-space optical links, formation flight, radiation-hardened TPUs, solar power, constellation control]</li>
<li class=""><strong>authors:</strong> Blaise Agüera y Arcas, Travis Beals, Maria Biggs, Jessica V. Bloom, Thomas Fischbacher, Konstantin Gromov, Urs Köster, Rishiraj Pravahan, James Manyika</li>
<li class=""><strong>institution:</strong> Google</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19468" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19468</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a space-based AI infrastructure system using solar-powered satellite clusters with Google TPU accelerators and free-space optical inter-satellite links. The system employs formation flight of satellites in close proximity and ML-based constellation control for scalable computing. The authors conclude this approach could provide highly scalable AI compute by directly harnessing solar energy in space, with radiation testing showing TPUs can survive 5-year missions and launch costs potentially dropping below $200/kg by the mid-2030s.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated learning, heterogeneous systems, resource scheduling, fault tolerance, non-IID data]</li>
<li class=""><strong>authors:</strong> Sangam Ghimire, Paribartan Timalsina, Nirjal Bhurtel, Bishal Neupane, Bigyan Byanju Shrestha, Subarna Bhattarai, Prajwal Gaire, Jessica Thapa, Sudan Jha</li>
<li class=""><strong>institution:</strong> Kathmandu University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19479" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19479</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a federated learning framework designed to operate efficiently across mixed HPC and cloud environments, addressing challenges like system heterogeneity and communication overhead. The framework demonstrates strong performance in scalability, fault tolerance, and convergence under non-IID data distributions and varied hardware. The results show federated learning is a practical approach for building scalable AI systems in distributed computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [uncertainty-aware distillation, asynchronous federated learning, model aggregation, heterogeneous data]</li>
<li class=""><strong>authors:</strong> Yujia Wang, Fenglong Ma, Jinghui Chen</li>
<li class=""><strong>institution:</strong> Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19966" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19966</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedEcho, a framework that uses uncertainty-aware distillation to dynamically assess the reliability of predictions from straggler clients in asynchronous federated learning. This approach mitigates the negative impacts of outdated updates and data heterogeneity by prioritizing more certain predictions while leveraging diverse client information. Experimental results show FedEcho outperforms existing asynchronous FL baselines without requiring access to private client data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [database systems], [dual-layer compression, in-storage compression, PolarCSD hardware, software compression, compression-aware scheduling]</li>
<li class=""><strong>authors:</strong> Qingda Hu, Xinjun Yang, Feifei Li, Junru Li, Ya Lin, Yuqi Zhou, Yicong Zhu, Junwei Zhang, Rongbiao Xie, Ling Zhou, Bin Wu, Wenchao Zhou</li>
<li class=""><strong>institution:</strong> Alibaba Cloud Computing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19949</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PolarStore introduces a dual-layer compression system combining hardware-based in-storage compression with software-based lightweight compression for cloud-native databases. The system achieves a 3.55 compression ratio and reduces storage costs by approximately 60% while maintaining performance comparable to uncompressed clusters. It has been deployed at scale in PolarDB, managing over 100 PB of data across thousands of storage servers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [batch denoising, STACKING algorithm, bandwidth allocation, joint optimization]</li>
<li class=""><strong>authors:</strong> Jinghang Xu, Kun Guo, Wei Teng, Chenxi Liu, Wei Feng</li>
<li class=""><strong>institution:</strong> East China Normal University, Shaanxi Normal University, Beijing University of Posts and Telecommunications, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19847" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19847</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a batch denoising framework called STACKING that optimizes AI-generated content service provisioning in wireless edge networks. The method jointly optimizes content generation and transmission by leveraging batch processing to reduce latency while maintaining quality. Simulation results demonstrate the algorithm achieves superior performance in delivering high-quality, lower-latency AIGC services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed graph algorithms], [mobile agents, minimal dominating set, synchronous model, dispersion algorithm, spanning tree construction, leader election]</li>
<li class=""><strong>authors:</strong> Prabhat Kumar Chand, Anisur Rahaman Molla</li>
<li class=""><strong>institution:</strong> Indian Statistical Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents two new algorithms that use mobile agents to compute minimal dominating sets in anonymous graphs. By building on optimal dispersion algorithms in the synchronous mobile agent model, the methods achieve linear-time solutions using only logarithmic memory per agent without prior global knowledge. The algorithms also construct spanning trees and elect leaders as by-products, improving upon previous complexity results in the literature.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, block coordinate descent, communication-computation parallelism, large language models]</li>
<li class=""><strong>authors:</strong> Yujia Wang, Yuanpu Cao, Jinghui Chen</li>
<li class=""><strong>institution:</strong> Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19959" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19959</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ParaBlock introduces a communication-computation parallel approach for federated block coordinate descent in large language model training. The method establishes parallel threads for communication and computation to reduce latency while maintaining the same convergence rate as standard methods. Empirical results show ParaBlock significantly improves communication efficiency while preserving strong performance on instruction following and mathematical reasoning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC workflow scheduling], [NUMA-aware scheduling, workflow execution runtime, simulation models, data locality optimization]</li>
<li class=""><strong>authors:</strong> Aurelio Vivas, Harold Castro</li>
<li class=""><strong>institution:</strong> Universidad de los Andes</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19832" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19832</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables modeling, execution, and simulation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system addresses the gap in traditional workflow schedulers by accounting for complex memory hierarchies and data locality within individual computing nodes. nFlows facilitates the study of NUMA effects, design of NUMA-aware algorithms, and identification of performance bottlenecks in scientific workflow execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed storage systems], [programmable switches, asynchronous metadata updating, in-network data visibility, ordered writes]</li>
<li class=""><strong>authors:</strong> Junru Li, Qing Wang, Zhe Yang, Shuo Liu, Jiwu Shu, Youyou Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Nanjing University, Huawei Technologies Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19978" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19978</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SwitchDelta accelerates distributed storage write operations by moving metadata updates out of the critical path using programmable switches to buffer in-flight metadata updates. This approach maintains strong consistency while enabling data visibility in the network. Evaluation shows it reduces write latency by up to 52.4% and increases throughput by up to 126.9% under write-heavy workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hybrid split and federated learning, block coordinate descent, batch size optimization, delay minimization]</li>
<li class=""><strong>authors:</strong> Kun Guo, Xuefei Li, Xijun Wang, Howard H. Yang, Wei Feng, Tony Q. S. Quek</li>
<li class=""><strong>institution:</strong> East China Normal University, Sun Yat-sen University, Zhejiang University, Tsinghua University, Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.19851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.19851</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid split and federated learning (HSFL) approach that combines the advantages of both paradigms to accelerate wireless distributed learning. The authors develop a two-stage optimization method using block coordinate descent and rounding algorithms to jointly optimize learning modes, batch sizes, and resource allocation. Experimental results show their approach significantly accelerates convergence to target accuracy compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Macro Thinking Micro Coding, reinforcement learning, LLM-based code generation, staged optimization]</li>
<li class=""><strong>authors:</strong> Xinguo Zhu, Shaohui Peng, Jiaming Guo, Yunji Chen, Qi Guo, Yuanbo Wen, Hang Qin, Ruizhi Chen, Qirui Zhou, Ke Gao, Yanjun Wu, Chen Zhao, Ling Li</li>
<li class=""><strong>institution:</strong> Institute of Software Chinese Academy of Sciences, Institute of Computing Technology Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20100" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20100</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a hierarchical framework called Macro Thinking Micro Coding (MTMC) that decouples optimization strategy from implementation details for GPU kernel generation. Macro Thinking uses reinforcement learning to explore optimization strategies, while Micro Coding employs LLMs for incremental implementation. This approach achieves superior accuracy and performance, with up to 7.3× speedup over existing LLM methods and 2.2× over expert-optimized kernels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [CXL, KVCache management, memory architecture, disaggregated memory, vLLM]</li>
<li class=""><strong>authors:</strong> Xinjun Yang, Qingda Hu, Junru Li, Feifei Li, Yuqi Zhou, Yicong Zhu, Qiuru Lin, Jian Dai, Yang Kong, Jiayu Zhang, Guoqiang Xu, Qiang Liu</li>
<li class=""><strong>institution:</strong> Alibaba Cloud Computing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20172" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20172</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Beluga introduces a CXL-based memory architecture that enables GPUs to directly access large-scale memory pools through CXL switches for efficient KVCache management in LLM inference. This approach achieves near-local memory latency while reducing programming complexity compared to RDMA-based solutions. The system demonstrates 89.6% reduction in Time-To-First-Token and 7.35x throughput improvement in vLLM inference engine.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain education], [proof-of-work consensus, ethereum, raspberry pi, lcd visualization, peer-to-peer network]</li>
<li class=""><strong>authors:</strong> Anton Ivashkevich, Matija Piškorec, Claudio J. Tessone</li>
<li class=""><strong>institution:</strong> University of Zurich, Ruđer Bošković Institute, UZH Blockchain Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20391</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an educational prototype system that runs a fully functional Ethereum Proof-of-Work blockchain network on multiple Raspberry Pi computers with LCD screens for visualization. The system demonstrates blockchain consensus mechanisms and allows observation of consensus degradation through configurable parameters via a web interface. The setup provides an accessible way to teach fundamental blockchain concepts to diverse audiences without requiring technical expertise.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Parallel simulation and adaptive mesh refinement for 3D elastostatic contact mechanics problems between deformable bodies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computational mechanics], [adaptive mesh refinement, finite element method, MPI parallelization, node-to-node contact pairing, penalization technique, super-parametric elements, Hertzian contact]</li>
<li class=""><strong>authors:</strong> Alexandre Epalle, Isabelle Ramière, Guillaume Latu, Frédéric Lebon</li>
<li class=""><strong>institution:</strong> CEA, Aix-Marseille Université</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20142</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a parallel adaptive mesh refinement method for 3D elastostatic contact problems that combines node-to-node contact pairing with penalization techniques and non-conforming mesh refinement. The approach ensures contact-paired nodes reside on the same MPI tasks to minimize communication overhead and uses super-parametric elements to maintain geometric accuracy during refinement. The method demonstrates strong scalability up to 1024 cores and effectively handles Hertzian contact problems in both 2D and 3D simulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251126] Efficient Parallel Implementation of the Pilot Assignment Problem in Massive MIMO Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [genetic algorithm, k-means clustering, FPGA, high-level synthesis, loop unrolling, pipelining, function inlining]</li>
<li class=""><strong>authors:</strong> Eman Alqudah, Ashfaq Khokhar</li>
<li class=""><strong>institution:</strong> Iowa State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20511" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20511</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid K-means clustering and Genetic Algorithm approach for pilot assignment in massive MIMO systems, with a parallel FPGA implementation using Vivado High-Level Synthesis tools. The implementation achieves significant speedup through optimization techniques like loop unrolling and pipelining, reducing convergence time from 82 seconds to 3.5 milliseconds. This makes the method highly suitable for low-latency real-time wireless networks such as 6G systems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 28</strong></p>
<ul>
<li class="">[arXiv251126] Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma <a href="https://arxiv.org/pdf/2511.19504" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories <a href="https://arxiv.org/pdf/2511.19528" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] HunyuanOCR Technical Report <a href="https://arxiv.org/pdf/2511.19575" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Learning Massively Multitask World Models for Continuous Control <a href="https://arxiv.org/pdf/2511.19584" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs <a href="https://arxiv.org/pdf/2511.19773" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Learning to Clean: Reinforcement Learning for Noisy Label Correction <a href="https://arxiv.org/pdf/2511.19808" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception <a href="https://arxiv.org/pdf/2511.19820" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Reinforcement Learning with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">ω</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">ω</span></span></span></span>-Regular Objectives and Constraints <a href="https://arxiv.org/pdf/2511.19849" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Complex Instruction Following with Diverse Style Policies in Football Games <a href="https://arxiv.org/pdf/2511.19885" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning <a href="https://arxiv.org/pdf/2511.19900" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities <a href="https://arxiv.org/pdf/2511.19930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning <a href="https://arxiv.org/pdf/2511.19941" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning <a href="https://arxiv.org/pdf/2511.19942" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Energy Costs and Neural Complexity Evolution in Changing Environments <a href="https://arxiv.org/pdf/2511.20018" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] SOMBRL: Scalable and Optimistic Model-Based RL <a href="https://arxiv.org/pdf/2511.20066" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] From data to concepts via wiring diagrams <a href="https://arxiv.org/pdf/2511.20138" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025 <a href="https://arxiv.org/pdf/2511.20200" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Leveraging weights signals - Predicting and improving generalizability in reinforcement learning <a href="https://arxiv.org/pdf/2511.20234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis <a href="https://arxiv.org/pdf/2511.20237" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] NNGPT: Rethinking AutoML with Large Language Models <a href="https://arxiv.org/pdf/2511.20333" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Soft Adaptive Policy Optimization <a href="https://arxiv.org/pdf/2511.20347" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning <a href="https://arxiv.org/pdf/2511.20349" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs <a href="https://arxiv.org/pdf/2511.20468" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning <a href="https://arxiv.org/pdf/2511.20549" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.20591" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models <a href="https://arxiv.org/pdf/2511.20629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] CycleChemist: A Dual-Pronged Machine Learning Framework for Organic Photovoltaic Discovery <a href="https://arxiv.org/pdf/2511.19500" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Optimization and Regularization Under Arbitrary Objectives <a href="https://arxiv.org/pdf/2511.19628" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 25</strong></p>
<ul>
<li class="">[arXiv251126] stable-pretraining-v1: Foundation Model Research Made Simple <a href="https://arxiv.org/pdf/2511.19484" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression <a href="https://arxiv.org/pdf/2511.19493" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning <a href="https://arxiv.org/pdf/2511.19518" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Learning to Solve Weighted Maximum Satisfiability with a Co-Training Architecture <a href="https://arxiv.org/pdf/2511.19544" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.19562" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Learning Massively Multitask World Models for Continuous Control <a href="https://arxiv.org/pdf/2511.19584" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Agint: Agentic Graph Compilation for Software Engineering Agents <a href="https://arxiv.org/pdf/2511.19635" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Synthetic Data: AI&#x27;s New Weapon Against Android Malware <a href="https://arxiv.org/pdf/2511.19649" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Training-Free Active Learning Framework in Materials Science with Large Language Models <a href="https://arxiv.org/pdf/2511.19730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] CAMformer: Associative Memory is All You Need <a href="https://arxiv.org/pdf/2511.19740" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions <a href="https://arxiv.org/pdf/2511.19749" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements <a href="https://arxiv.org/pdf/2511.19794" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Designing Reputation Systems for Manufacturing Data Trading Markets: A Multi-Agent Evaluation with Q-Learning and IRL-Estimated Utilities <a href="https://arxiv.org/pdf/2511.19930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning <a href="https://arxiv.org/pdf/2511.19941" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices <a href="https://arxiv.org/pdf/2511.19986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation <a href="https://arxiv.org/pdf/2511.20222" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] MXtalTools: A Toolkit for Machine Learning on Molecular Crystals <a href="https://arxiv.org/pdf/2511.20327" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] 3D Motion Perception of Binocular Vision Target with PID-CNN <a href="https://arxiv.org/pdf/2511.20332" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning <a href="https://arxiv.org/pdf/2511.20349" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Block Cascading: Training Free Acceleration of Block-Causal Video Models <a href="https://arxiv.org/pdf/2511.20426" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features <a href="https://arxiv.org/pdf/2511.20469" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning <a href="https://arxiv.org/pdf/2511.20549" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent <a href="https://arxiv.org/pdf/2511.20584" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] DiFR: Inference Verification Despite Nondeterminism <a href="https://arxiv.org/pdf/2511.20621" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251126] Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model <a href="https://arxiv.org/pdf/2511.20636" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-11-27">2025-11-27<a href="#2025-11-27" class="hash-link" aria-label="Direct link to 2025-11-27" title="Direct link to 2025-11-27" translate="no">​</a></h2>
<p><strong>cs.DC total: 14</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251127] Privacy in Federated Learning with Spiking Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, spiking neural networks, gradient inversion attacks, surrogate gradients, neuromorphic computation]</li>
<li class=""><strong>authors:</strong> Dogukan Aksu, Jesus Martinez del Rincon, Ihsen Alouani</li>
<li class=""><strong>institution:</strong> Queen&#x27;s University Belfast</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21181</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates gradient leakage attacks in federated learning systems using spiking neural networks (SNNs) with surrogate gradient training. The study demonstrates that SNNs produce noisy, temporally inconsistent gradient reconstructions compared to conventional neural networks. The results show that SNNs have inherent privacy-preserving advantages due to their event-driven dynamics and surrogate gradient methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [data redundancy, locality metric, memory access dispersion, cache behavior, GPU optimization]</li>
<li class=""><strong>authors:</strong> Morteza Sadeghi</li>
<li class=""><strong>institution:</strong> University of Tehran</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21535" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21535</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces data redundancy to improve spatial locality in MLFMA near-field computations on GPUs by reducing memory access dispersion. The approach achieves up to 7× kernel speedup through improved cache behavior, though end-to-end application speedup is limited to 1.04× due to data restructuring overheads. The proposed analytical model reliably predicts performance trends across different problem sizes without hardware-specific profiling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [Stochastic Petri Nets, redundancy strategies, availability analysis, virtual machine redundancy, host-level redundancy]</li>
<li class=""><strong>authors:</strong> Alison Silva, Gustavo Callou</li>
<li class=""><strong>institution:</strong> Universidade de Pernambuco, Universidade Federal Rural de Pernambuco</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20780" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20780</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a methodology using Stochastic Petri Nets to evaluate redundancy strategies for improving availability in private cloud file servers. The study models four architectural configurations including baseline, host-level redundancy, VM redundancy, and combined approaches. Results show that implementing redundancy at both host and VM levels significantly enhances system availability and reduces expected downtime.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [vLLM, Kubernetes, Slurm, HPC, dynamic scaling]</li>
<li class=""><strong>authors:</strong> Tim Trappen, Robert Keßler, Roland Pabel, Viktor Achter, Stefan Wesner</li>
<li class=""><strong>institution:</strong> Ruhr University Bochum, University of Cologne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21413" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21413</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a solution for serving LLMs by integrating vLLM, Slurm and Kubernetes on the RAMSES supercomputer. The architecture enables automated dynamic AI inference scaling on HPC infrastructure. Benchmark results show the system scales efficiently for up to 1000 concurrent requests with only approximately 500 ms end-to-end latency overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [memory-aware scheduling, chunked recomputation, dynamic token routing, load balancing, expert capacity]</li>
<li class=""><strong>authors:</strong> Lu Zhao, Rong Shi, Shaoqing Zhang, Yueqiang Chen, Baoguo He, Hongfeng Sun, Ziqing Yin, Shangchao Su, Zhiyan Cui, Liang Dong, Xiyuan Li, Lingbin Wang, Jianwei He, Jiesong Ma, Weikang Huang, Jianglei Tong, Dongdong Gao, Jian Zhang, Hong Tian</li>
<li class=""><strong>institution:</strong> AIH Training Team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21431</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MemFine introduces a memory-aware fine-grained scheduling framework that decomposes token distribution and expert computation into chunks with optimized recomputation. This approach balances memory efficiency and throughput while maintaining model accuracy. Experiments show it reduces activation memory by 48.03% and improves throughput by 4.42%, enabling stable large-scale MoE training on memory-limited GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Readout-Side Bypass for Residual Hybrid Quantum-Classical Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [residual hybrid architecture, quantum-classical interface, federated learning, privacy preservation, measurement bottleneck, membership inference attacks]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan, Hongyang He, Hailong Jiang</li>
<li class=""><strong>institution:</strong> George Washington University, University of Warwick, Kent State University, Youngstown State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20922" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20922</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a residual hybrid quantum-classical architecture that concatenates raw inputs with quantum features before classification, bypassing the quantum measurement bottleneck. Experiments show this method achieves up to 55% accuracy improvement over quantum baselines while maintaining low communication costs and enhanced privacy robustness. The approach provides a practical pathway for integrating quantum models into privacy-sensitive federated edge learning settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Handling of Memory Page Faults during Virtual-Address RDMA</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [RDMA, page fault handling, SMMU, DMA engine, hardware-software co-design]</li>
<li class=""><strong>authors:</strong> Antonis Psistakis</li>
<li class=""><strong>institution:</strong> University of Crete</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21018" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21018</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements a hardware-software page fault handling mechanism integrated with the DMA engine that detects faults via ARM SMMU and resolves them through retransmission requests. The approach eliminates the need for memory pinning while maintaining RDMA performance. The evaluation shows advantages over traditional methods like pinning and pre-faulting by providing fault tolerance without programming complexity and memory utilization limitations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [just-in-time model routing, configuration selection, per-stage scheduling, accuracy-preserving configurations]</li>
<li class=""><strong>authors:</strong> Yinwei Dai, Zhuofu Chen, Anand Iyer, Ravi Netravali</li>
<li class=""><strong>institution:</strong> Princeton University, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20975" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20975</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Aragog introduces a just-in-time model routing system that dynamically adapts workflow configurations during execution using a two-step approach: one-time routing to identify accuracy-preserving configurations and per-stage scheduling using real-time system observations. This approach significantly improves serving performance, increasing throughput by 50.0-217.0% and reducing median latency by 32.5-78.9% while maintaining accuracy comparable to expensive static configurations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefill-decoding disaggregation, dynamic resource allocation, request scheduling, load monitoring, KV caching]</li>
<li class=""><strong>authors:</strong> Junhan Liao, Minxian Xu, Wanyi Zheng, Yan Wang, Kejiang Ye, Rajkumar Buyya, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Southern University of Science and Technology, Inner Mongolia University, University of Melbourne, University of Macau</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20982" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20982</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DOPD, a dynamic LLM inference system that adjusts prefill-to-decoding instance ratios based on real-time load monitoring to address workload imbalance. Combined with intelligent request scheduling, it resolves resource allocation mismatches under high concurrency. Experimental results show DOPD improves goodput by up to 1.5× while reducing latency metrics and achieving over 99% SLO attainment with fewer resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [sparse convolution, voxel indexing, kernel map construction, one-shot search, packed-native processing, dual-dataflow execution, network-wide parallelization]</li>
<li class=""><strong>authors:</strong> Dionysios Adamopoulos, Anastasia Poulopoulou, Georgios Goumas, Christina Giannoula</li>
<li class=""><strong>institution:</strong> Max Planck Institute for Software Systems, National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.20834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.20834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Spira, a voxel-property-aware sparse convolution engine for GPUs that exploits integer-valued, bounded, and geometrically continuous voxel coordinates. Spira uses a one-shot search algorithm, packed-native processing, dual-dataflow execution, and network-wide parallelization to accelerate kernel map construction and feature computation. Evaluation shows Spira outperforms prior sparse convolution engines by 1.71-2.31× for end-to-end inference and 2.13-3.32× for layer-wise execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] MAD-DAG: Protecting Blockchain Consensus from MEV</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [Markov Decision Process, selfish mining, DAG, security threshold, MEV protection]</li>
<li class=""><strong>authors:</strong> Roi Bar-Zur, Aviv Tamar, Ittay Eyal</li>
<li class=""><strong>institution:</strong> Technion</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21552</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MAD-DAG, a blockchain protocol that counters selfish mining under adverse conditions like MEV and rushing using a novel ledger function that discards contents of competing chains. By modeling selfish mining with Markov Decision Processes, the authors show MAD-DAG maintains security thresholds between 11-31% where existing protocols fail completely. This represents the first practical DAG-based protocol resilient to selfish mining under realistic network conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed databases], [scaling plane, diagonal scaling, local-search algorithm, horizontal scaling, vertical scaling, multi-dimensional resource model]</li>
<li class=""><strong>authors:</strong> Shahir Abdullah, Syed Rohit Zaman</li>
<li class=""><strong>institution:</strong> Bangladesh University of Engineering and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21612</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-dimensional Scaling Plane model and DIAGONALSCALE algorithm that jointly optimizes horizontal and vertical scaling decisions for distributed databases. The method demonstrates that diagonal scaling trajectories outperform traditional binary scaling approaches by reducing latency by up to 40%, lowering costs by up to 37%, and decreasing rebalancing overhead by 2-5 times. This provides a foundation for next-generation autoscaling in cloud database systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [distributed speculative decoding, adaptive window control, edge-cloud deployment]</li>
<li class=""><strong>authors:</strong> Fengze Yu, Leshu Li, Brad McDanel, Saiqian Zhang</li>
<li class=""><strong>institution:</strong> New York University, University of Minnesota Twin Cities, Franklin &amp; Marshall College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21669" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21669</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DSD, a distributed speculative decoding framework that extends speculative decoding to multi-device deployments through coordinated draft-target execution across edge-cloud environments. The system includes DSD-Sim simulator for evaluation and an Adaptive Window Control policy to dynamically optimize throughput. Experimental results show DSD achieves up to 1.1x speedup and 9.7% higher throughput compared to existing baselines, enabling scalable LLM serving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251127] AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Model Context Protocol, REST interface, dynamic model cards, AI accountability]</li>
<li class=""><strong>authors:</strong> Beth Plale, Neelesh Karthikeyan, Isuru Gamage, Joe Stubbs, Sachith Withana</li>
<li class=""><strong>institution:</strong> University of Oregon, Indiana University, University of Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21661</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies dynamic AI/ML model cards within the ICICLE AI Institute ecosystem, evaluating the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative analysis shows MCP introduces overhead compared to REST interfaces, while qualitative assessment focuses on active sessions enabled by MCP for dynamic model management. The research concludes that MCP&#x27;s session capabilities provide important benefits for dynamic model cards despite performance tradeoffs.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 18</strong></p>
<ul>
<li class="">[arXiv251127] Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs <a href="https://arxiv.org/pdf/2511.21050" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning <a href="https://arxiv.org/pdf/2511.21075" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.21149" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning <a href="https://arxiv.org/pdf/2511.20993" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] SPHINX: A Synthetic Environment for Visual Perception and Reasoning <a href="https://arxiv.org/pdf/2511.20814" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Monet: Reasoning in Latent Visual Space Beyond Images and Language <a href="https://arxiv.org/pdf/2511.21395" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning <a href="https://arxiv.org/pdf/2511.21011" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance <a href="https://arxiv.org/pdf/2511.21356" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning <a href="https://arxiv.org/pdf/2511.21005" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems <a href="https://arxiv.org/pdf/2511.20977" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment <a href="https://arxiv.org/pdf/2511.20913" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Predictive Safety Shield for Dyna-Q Reinforcement Learning <a href="https://arxiv.org/pdf/2511.21531" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation <a href="https://arxiv.org/pdf/2511.21135" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] BAMAS: Structuring Budget-Aware Multi-Agent Systems <a href="https://arxiv.org/pdf/2511.21572" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Escaping the Verifier: Learning to Reason via Demonstrations <a href="https://arxiv.org/pdf/2511.21667" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration <a href="https://arxiv.org/pdf/2511.21689" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms <a href="https://arxiv.org/pdf/2511.20678" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Morality in AI. A plea to embed morality in LLM architectures and frameworks <a href="https://arxiv.org/pdf/2511.20689" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 15</strong></p>
<ul>
<li class="">[arXiv251127] Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning <a href="https://arxiv.org/pdf/2511.21075" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities <a href="https://arxiv.org/pdf/2511.20730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving <a href="https://arxiv.org/pdf/2511.20720" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation <a href="https://arxiv.org/pdf/2511.20709" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models <a href="https://arxiv.org/pdf/2511.21320" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection <a href="https://arxiv.org/pdf/2511.21325" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning <a href="https://arxiv.org/pdf/2511.21011" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] A Brief History of Digital Twin Technology <a href="https://arxiv.org/pdf/2511.20695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media <a href="https://arxiv.org/pdf/2511.20685" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference <a href="https://arxiv.org/pdf/2511.21513" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Conversational no-code and multi-agentic disease module identification and drug repurposing prediction with ChatDRex <a href="https://arxiv.org/pdf/2511.21438" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI <a href="https://arxiv.org/pdf/2511.21232" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining <a href="https://arxiv.org/pdf/2511.21613" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] AI4X Roadmap: Artificial Intelligence for the advancement of scientific pursuit and its future directions <a href="https://arxiv.org/pdf/2511.20976" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251127] Lattice-to-total thermal conductivity ratio: a phonon-glass electron-crystal descriptor for data-driven thermoelectric design <a href="https://arxiv.org/pdf/2511.21213" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-25T03:58:05.000Z" itemprop="dateModified">Dec 25, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251117-20251123"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251117-20251123</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251201-20251207"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251201-20251207</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-11-24" class="table-of-contents__link toc-highlight">2025-11-24</a></li><li><a href="#2025-11-25" class="table-of-contents__link toc-highlight">2025-11-25</a></li><li><a href="#2025-11-26" class="table-of-contents__link toc-highlight">2025-11-26</a></li><li><a href="#2025-11-27" class="table-of-contents__link toc-highlight">2025-11-27</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251124/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>