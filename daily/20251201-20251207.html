<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251201-20251207" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251201-20251207 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251201-20251207"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251201-20251207 | AI头条"><meta data-rh="true" name="description" content="2025-12-01"><meta data-rh="true" property="og:description" content="2025-12-01"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251201-20251207"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251201-20251207" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251201-20251207" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251201-20251207","item":"https://jokebear666.github.io/ai_toutiao/daily/20251201-20251207"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.24c28e12.css">
<script src="/ai_toutiao/assets/js/runtime~main.19b952c5.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.fbe58e21.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251201-20251207</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251201-20251207</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-01">2025-12-01<a href="#2025-12-01" class="hash-link" aria-label="Direct link to 2025-12-01" title="Direct link to 2025-12-01" translate="no">​</a></h2>
<p><strong>cs.DC total: 23</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251201] An Empirical Study of Cross-Language Interoperability in Replicated Data Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [replicated data libraries, foreign-function interface, common data format, cross-language interoperability, empirical study]</li>
<li class=""><strong>authors:</strong> Provakar Mondal, Eli Tilevich</li>
<li class=""><strong>institution:</strong> Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22010</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper empirically compares two strategies for cross-language interoperability in replicated data systems: foreign-function interface (FFI) and common data format (CDF). The study found that CDF-based integration provides better software quality, lower latency, reduced memory consumption, and higher throughput. The authors validated their findings by implementing a CDF-based replicated data library that supports mixed language environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefix-aware attention, multi-tile kernel, KV cache optimization, pack-forward-merge, vLLM integration]</li>
<li class=""><strong>authors:</strong> Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li</li>
<li class=""><strong>institution:</strong> Tianjin University, Stevens Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22333</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PAT introduces a prefix-aware attention kernel that organizes execution using a pack-forward-merge paradigm to reduce redundant KV cache loading. It employs multi-tile kernels and query packing to optimize resource utilization during LLM decoding. Evaluation shows PAT reduces attention latency by 67.4% on average and improves throughput compared to state-of-the-art attention kernels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [latency-disaggregated architecture, bottleneck-based scheduler, Roofline-based performance model, fast preemption mechanism, Prefill/Decode disaggregation]</li>
<li class=""><strong>authors:</strong> Siyu Wu, Zihan Tang, Yuting Zeng, Hui Chen, Guiguang Ding, Tongxuan Liu, Ke Zhang, Hailong Yang</li>
<li class=""><strong>institution:</strong> Beihang University, Tsinghua University, University of Science and Technology of China, JD Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21862" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21862</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a latency-disaggregated architecture that separates cluster resources into latency-strict and latency-relaxed pools for co-locating online and offline LLM workloads. It introduces a bottleneck-based scheduler with Roofline modeling and fast preemption mechanism to maintain online SLOs while improving resource utilization. Experiments show the method achieves up to 3× higher offline throughput while preserving online performance compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, weighted momentum, sharpness-aware minimization, non-IID convergence, cosine-similarity adaptive rule]</li>
<li class=""><strong>authors:</strong> Tianle Li, Yongzhi Huang, Linshan Jiang, Chang Liu, Qipeng Xie, Wenfeng Du, Lu Wang, Kaishun Wu</li>
<li class=""><strong>institution:</strong> Shenzhen University, The Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22080</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedWMSAM, a federated learning method that combines weighted momentum and sharpness-aware minimization to address local-global curvature misalignment and momentum-echo oscillation. It introduces momentum-guided global perturbation and a two-phase training schedule to improve optimization. Experimental results demonstrate the method&#x27;s effectiveness in achieving fast convergence and robust generalization across non-IID data distributions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [epistemic logic, knowledge-based programs, simultaneous agreement, crash failures, limited information exchange]</li>
<li class=""><strong>authors:</strong> Kaya Alpturer, Ron van der Meyden, Sushmita Ruj, Godfrey Wong</li>
<li class=""><strong>institution:</strong> Princeton University, UNSW Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22380</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops optimal fault-tolerant simultaneous consensus protocols using epistemic logic and knowledge-based programming with limited information exchange. The authors introduce a new information exchange approach that achieves decisions at most one round later than the optimal Dwork-Moses protocol while reducing computation cost and space requirements. They derive protocols that are optimal for various limited information exchanges from the literature, including FloodSet variants and failure-counting approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [biophotonics simulation], [ray-tracing, GPU acceleration, Monte Carlo method, hardware RT-cores, OptiX platform]</li>
<li class=""><strong>authors:</strong> Shijie Yan, Douglas Dwyer, David R. Kaeli, Qianqian Fang</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RT-MMC, a mesh-based Monte Carlo method that leverages modern GPU ray-tracing hardware for accelerated light transport simulations. By using NVIDIA&#x27;s OptiX platform and RT-cores, the approach eliminates complex mesh generation while achieving 1.5x to 4.5x speed improvements over traditional methods. The hardware-based ray-tracing significantly simplifies simulation workflows and enhances practicality for biophotonics applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [Heard-Of model, asynchronous message-passing, model equivalence, colorless tasks, colored tasks, crash failures, message omissions, silencing]</li>
<li class=""><strong>authors:</strong> Hagit Attiya, Armando Castañeda, Dhrubajyoti Ghosh, Thomas Nowak</li>
<li class=""><strong>institution:</strong> Technion – Israel Institute of Technology, Instituto de Matemáticas, Universidad Nacional Autónoma de México, Université Paris-Saclay, CNRS, ENS Paris-Saclay, Laboratoire Méthodes Formelles, Institut Universitaire de France</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21859" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21859</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper analyzes the relationship between the asynchronous message-passing model (AMP_f) and the Heard-Of model (HO_f) through bidirectional simulations and an intermediate model. It concludes that the models are equivalent for solving colorless tasks when n &gt; 2f, but for colored tasks, equivalence holds only for f=1 due to the issue of silenced processes in HO_f.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimization, deep learning, active learning, design space exploration, numerical simulation]</li>
<li class=""><strong>authors:</strong> Ahmad Tarraf, Koutaiba Kassem-Manthey, Seyed Ali Mohammadi, Philipp Martin, Lukas Moj, Semih Burak, Enju Park, Christian Terboven, Felix Wolf</li>
<li class=""><strong>institution:</strong> Technical University of Darmstadt, GNS Gesellschaft für numerische Simulation mbH, RWTH Aachen University, GNS Systems GmbH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22302</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an AI-assisted workflow that combines deep learning and Bayesian optimization to automate the tuning of design parameters in sheet metal forming simulations. The method reduces expert involvement and accelerates design space exploration by using a deep learning model for initial parameter estimation followed by iterative refinement. The approach demonstrates significant potential to shorten design iterations and lower computational costs in simulation-driven industrial processes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Areon: Latency-Friendly and Resilient Multi-Proposer Consensus</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [directed acyclic graph, proof-of-stake, multi-proposer, fork choice rule, VRF-based eligibility, sliding window]</li>
<li class=""><strong>authors:</strong> Álvaro Castro-Castilla, Marcin Pawlowski, Hong-Sheng Zhou</li>
<li class=""><strong>institution:</strong> Nomos Institute of Free Technology, Jagiellonian University, Virginia Commonwealth University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23025</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Areon introduces a multi-proposer consensus protocol that organizes blocks into a directed acyclic graph (DAG) with a sliding window reference mechanism and weighted fork choice rule. The protocol achieves bounded-latency finality with lower reorganization frequency compared to chain-based baselines. Experimental results show consistent performance improvements across various adversarial conditions and network delays.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [retry policy, distributed framework, analytical model, microservices, auto-scaling]</li>
<li class=""><strong>authors:</strong> Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RetryGuard, a distributed framework that uses an analytical model to manage retry policies across microservices and prevent retry storms. Experimental results show RetryGuard significantly reduces resource usage and costs compared to AWS retry policies in both standard and complex Kubernetes deployments with Istio service mesh.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [network security], [history covert channels, timing patterns, relative pointers, covert amplification factor, silent history protocol]</li>
<li class=""><strong>authors:</strong> Christoph Weissenborn, Steffen Wendzel</li>
<li class=""><strong>institution:</strong> Federal Office for Information Security, Ulm University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22259</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel covert communication method called History Covert Channels (HCC), which embeds hidden messages by using relative pointers to past network timing patterns instead of directly manipulating traffic. This approach reduces reliance on centralized timekeeping and aims to evade standard detection tools. The authors&#x27; experiments demonstrate that their method achieves a higher bitrate compared to prior work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Closing the Generalization Gap in Parameter-efficient Federated Edge Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [model pruning, client selection, joint resource management, generalization analysis, alternating optimization]</li>
<li class=""><strong>authors:</strong> Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong (Shenzhen), KTH Royal Institute of Technology, Shenzhen University, University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23282</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a parameter-efficient federated edge learning framework that jointly optimizes model pruning and client selection with communication-computation resources. The method formulates a generalization-aware optimization problem solved via alternating optimization. Experiments show the approach achieves superior learning performance compared to state-of-the-art baselines by coupling generalization analysis with system-level optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [content-oblivious communication, leader election, topology knowledge, graph symmetry, message complexity]</li>
<li class=""><strong>authors:</strong> Yi-Jun Chang, Lyuting Chen, Haoran Zhou</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23297" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23297</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies leader election in content-oblivious networks where nodes can only send asynchronous, content-less pulses. It shows that with topology knowledge, leader election is possible in many non-2-edge-connected graphs like asymmetric trees, but impossible in graphs symmetric about an edge or when topology knowledge is insufficient. The work provides both impossibility results and algorithms with specific message complexity bounds for different graph classes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed ledger technology], [ZipperChain, atomic broadcast, trustless, third-party services, distributed consensus, immutability, agreement, availability, pipeline, fast data center network]</li>
<li class=""><strong>authors:</strong> Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie</li>
<li class=""><strong>institution:</strong> BLOCKY, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21969</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ZipperChain is a blockchain that replaces distributed consensus with a pipeline of specialized services on a small number of nodes, transferring trust from established third-party services to provide correctness guarantees. This approach enables high transaction throughput near network line speeds and block finality around 500 ms, without needing a native token for incentives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [storage systems], [Clock2Q+, cache replacement algorithm, metadata cache, correlated references, S3-FIFO, three queues, correlation window]</li>
<li class=""><strong>authors:</strong> Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Bandung Institute of Technology, Broadcom Inc., Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21958</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Clock2Q+, a cache replacement algorithm designed for metadata caches that uses three queues and a correlation window to mitigate the negative impact of correlated references. It demonstrates superior performance, achieving up to a 28.5% lower miss ratio than S3-FIFO on metadata traces, while maintaining low overhead and ease of implementation for large-scale storage systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [blockchain, proof-of-work, trust rating, Bayesian-time-decay, high-performance cluster computing, reward incentive]</li>
<li class=""><strong>authors:</strong> Murat Yaslioglu</li>
<li class=""><strong>institution:</strong> Istanbul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21844" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21844</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel blockchain-based framework that integrates high-performance cluster computing with AI, using an evolved proof-of-work consensus linked to computational effort and a dynamic Bayesian-time-decay trust rating for node selection. This mechanism aims to optimize resource use, incentivize broad participation, and create a merit-based system for block generation. The main conclusion is that this approach fosters a more sustainable, equitable, and energy-efficient environment for AI development by balancing computational power with inclusivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts scheduling, sparse attention acceleration, prefill-decode disaggregation, KV-cache reuse, continuous batching, load-aware scheduling]</li>
<li class=""><strong>authors:</strong> Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang</li>
<li class=""><strong>institution:</strong> Huawei Technologies Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22481" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22481</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OmniInfer is a system-level acceleration framework built on vLLM that integrates three components—OmniPlacement, OmniAttn, and OmniProxy—to optimize LLM serving through expert placement, sparse attention, and disaggregation-aware scheduling. It achieves performance gains by adaptively disaggregating resources, exploiting sparsity, and coordinating prefill and decode phases. Evaluated on a 10-node cluster, it significantly reduces time-per-output-token and time-to-first-token while increasing query throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] DisCEdge: Distributed Context Management for Large Language Models at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [distributed context management, tokenization, geo-distributed storage, edge computing, data replication]</li>
<li class=""><strong>authors:</strong> Mohammadreza Malekabbasi, Minghe Wang, David Bermbach</li>
<li class=""><strong>institution:</strong> TU Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22599</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DisCEdge, a system for managing LLM user context by storing and replicating it in tokenized form across distributed edge nodes. This approach reduces redundant computation and enables efficient synchronization. The evaluation shows it improves response times, lowers synchronization overhead, and significantly reduces client request sizes compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, aggregation methods, personalization, optimization, robustness, heterogeneity, privacy-preserving, IID, non-IID]</li>
<li class=""><strong>authors:</strong> Meriem Arbaoui, Mohamed-el-Amine Brahmia, Abdellatif Rahmoun, Mourad Zghal</li>
<li class=""><strong>institution:</strong> LabRi-SBA Laboratory, CESI LINEACT UR 7527</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22616</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper provides a multi-level taxonomy of Federated Learning (FL) aggregation techniques, combining bibliometric analysis and systematic review to classify research in personalization, optimization, and robustness. It concludes by comparing aggregation methods under IID and non-IID data distributions and outlines future research directions to advance the field.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [LoRA, dynamic adapter placement, GPU Direct RDMA, multi-tenant serving, rank heterogeneity]</li>
<li class=""><strong>authors:</strong> Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor Rühle, Josep Torrellas</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Microsoft, National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents LoRAServe, a framework that dynamically places and routes heterogeneous LoRA adapters across GPUs to address performance skew in multi-tenant LLM inference. It uses workload-aware rebalancing and GPU Direct RDMA for remote access to improve resource utilization. The evaluation shows LoRAServe achieves higher throughput and lower latency while using fewer GPUs compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split learning, pipeline parallelism, wireless edge networks, communication-computation overlap, alternating optimization, micro-batches]</li>
<li class=""><strong>authors:</strong> Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23167</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes C²P²SL, a method that applies pipeline parallelism to split learning in wireless edge networks to overlap communication and computation processes, thereby reducing training latency. It formulates a joint optimization problem for task split and resource allocation, solved via alternating optimization. Experiments show the method reduces system training time by over 38% while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Probabilistic Forward Pass, Bayesian Neural Networks, Gaussian propagation, TVM compiler, code generation, operator tuning, uncertainty estimation]</li>
<li class=""><strong>authors:</strong> Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fröning</li>
<li class=""><strong>institution:</strong> Heidelberg University, Graz University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23440</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an end-to-end pipeline for deploying Bayesian Neural Networks (BNNs) using a Probabilistic Forward Pass (PFP), which approximates inference by propagating Gaussian distributions through the network in a single deterministic pass. The method is implemented via custom operators in the TVM compiler and optimized for ARM CPUs, achieving speedups of up to 4200x compared to traditional sampling-based methods while maintaining comparable accuracy and uncertainty estimation. The results demonstrate that combining Bayesian approximations with code generation enables efficient BNN deployment on resource-constrained embedded systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A lasso-alternative to Dijkstra&#x27;s algorithm for identifying short paths in networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph theory and optimization], [lasso, LARS algorithm, ADMM, bi-directional Dijkstra, ℓ1-regularized regression]</li>
<li class=""><strong>authors:</strong> Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, University of Washington, University of California, Irvine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22745</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formulates the shortest path problem in graphs as an ℓ1-regularized regression (lasso). It connects this formulation, solved via the LARS algorithm, to the bi-directional Dijkstra algorithm and highlights the applicability of ADMM for efficient updates to network topology changes.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 33</strong></p>
<ul>
<li class="">[arXiv251201] Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation <a href="https://arxiv.org/pdf/2511.21934" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba <a href="https://arxiv.org/pdf/2511.22101" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Factors That Support Grounded Responses in LLM Conversations: A Rapid Review <a href="https://arxiv.org/pdf/2511.21762" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information <a href="https://arxiv.org/pdf/2511.22176" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Representative Action Selection for Large Action Space: From Bandits to MDPs <a href="https://arxiv.org/pdf/2511.22104" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.22105" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning <a href="https://arxiv.org/pdf/2511.22210" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning <a href="https://arxiv.org/pdf/2511.22226" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis <a href="https://arxiv.org/pdf/2511.22018" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation <a href="https://arxiv.org/pdf/2511.22235" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks <a href="https://arxiv.org/pdf/2511.21726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs <a href="https://arxiv.org/pdf/2511.21928" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface <a href="https://arxiv.org/pdf/2511.22108" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents <a href="https://arxiv.org/pdf/2511.22076" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices <a href="https://arxiv.org/pdf/2511.22138" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GPS: General Per-Sample Prompter <a href="https://arxiv.org/pdf/2511.21714" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions <a href="https://arxiv.org/pdf/2511.22406" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning <a href="https://arxiv.org/pdf/2511.22570" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering <a href="https://arxiv.org/pdf/2511.22715" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ORION: Teaching Language Models to Reason Efficiently in the Language of Thought <a href="https://arxiv.org/pdf/2511.22891" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Switching-time bioprocess control with pulse-width-modulated optogenetics <a href="https://arxiv.org/pdf/2511.22893" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Language-conditioned world model improves policy generalization by reading environmental descriptions <a href="https://arxiv.org/pdf/2511.22904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary <a href="https://arxiv.org/pdf/2511.22963" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Evolutionary Discovery of Heuristic Policies for Traffic Signal Control <a href="https://arxiv.org/pdf/2511.23122" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23148" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection <a href="https://arxiv.org/pdf/2511.23158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging <a href="https://arxiv.org/pdf/2511.23193" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning <a href="https://arxiv.org/pdf/2511.23262" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23315" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts <a href="https://arxiv.org/pdf/2511.23442" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ThetaEvolve: Test-time Learning on Open Problems <a href="https://arxiv.org/pdf/2511.23473" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks <a href="https://arxiv.org/pdf/2511.22321" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23310" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 19</strong></p>
<ul>
<li class="">[arXiv251201] MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation <a href="https://arxiv.org/pdf/2511.22102" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models <a href="https://arxiv.org/pdf/2511.21759" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ResearchArcade: Graph Interface for Academic Tasks <a href="https://arxiv.org/pdf/2511.22036" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization <a href="https://arxiv.org/pdf/2511.21736" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance <a href="https://arxiv.org/pdf/2511.21901" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics <a href="https://arxiv.org/pdf/2511.21848" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Co-Evolving Agents: Learning from Failures as Hard Negatives <a href="https://arxiv.org/pdf/2511.22254" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code <a href="https://arxiv.org/pdf/2511.21920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis <a href="https://arxiv.org/pdf/2511.22293" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Cacheback: Speculative Decoding With Nothing But Cache <a href="https://arxiv.org/pdf/2511.21699" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE <a href="https://arxiv.org/pdf/2511.22434" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction <a href="https://arxiv.org/pdf/2511.22460" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization <a href="https://arxiv.org/pdf/2511.22586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration <a href="https://arxiv.org/pdf/2511.23203" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI <a href="https://arxiv.org/pdf/2511.23274" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking <a href="https://arxiv.org/pdf/2511.21747" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Automated Statistical and Machine Learning Platform for Biological Research <a href="https://arxiv.org/pdf/2511.21770" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Generative models for crystalline materials <a href="https://arxiv.org/pdf/2511.22652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Escaping Barren Plateaus in Variational Quantum Algorithms Using Negative Learning Rate in Quantum Internet of Things <a href="https://arxiv.org/pdf/2511.22861" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-02">2025-12-02<a href="#2025-12-02" class="hash-link" aria-label="Direct link to 2025-12-02" title="Direct link to 2025-12-02" translate="no">​</a></h2>
<p><strong>cs.DC total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251202] An optimization framework for task allocation in the edge/hub/cloud paradigm</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing optimization], [binary integer linear programming, task flow graph, latency optimization, energy optimization]</li>
<li class=""><strong>authors:</strong> Andreas Kouloumpris, Georgios L. Stavrinides, Maria K. Michael, Theocharis Theocharides</li>
<li class=""><strong>institution:</strong> University of Cyprus</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00029</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a binary integer linear programming (BILP) framework for optimal task allocation in edge/hub/cloud architectures, aiming to minimize either latency or energy consumption. The method is evaluated with real-world and synthetic benchmarks, showing it provides optimal and scalable results for design space exploration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [k-core decomposition, parallel computing, shared-memory, Rust, message passing]</li>
<li class=""><strong>authors:</strong> Davide Rucci, Sebastian Parfeniuc, Matteo Mordacchini, Emanuele Carlini, Alfredo Cuzzocrea, Patrizio Dazzi</li>
<li class=""><strong>institution:</strong> ISTI-CNR, University of Pisa, IIT-CNR, University of Calabria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00233</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper adapts a distributed k-core decomposition algorithm for shared-memory systems and implements three optimized versions in Rust. The fastest version, FastK, significantly reduces synchronization overhead and outperforms baseline sequential and parallel implementations, achieving up to an 11x speedup on 16 threads and being orders of magnitude faster than a reference Python implementation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, semantic communication, joint source-channel coding, global representation alignment, domain-aware aggregation]</li>
<li class=""><strong>authors:</strong> Loc X. Nguyen, Ji Su Yoon, Huy Q. Le, Yu Qiao, Avi Deb Raha, Eui-Nam Huh, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong</li>
<li class=""><strong>institution:</strong> Kyung Hee University, Virginia Tech, Nanyang Technological University, University of Houston</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00711</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel federated learning framework to train semantic communication models across different data domains. It introduces global representation alignment to preserve domain semantics and a domain-aware aggregation method to prevent bias from dominant clients. The approach outperforms existing methods in image reconstruction quality, especially as channel conditions improve.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless networks], [multi-hop clustering, mobility-aware clustering, energy-centric CH selection, GS-assisted cluster maintenance]</li>
<li class=""><strong>authors:</strong> Basilis Mamalis, Marios Perlitis</li>
<li class=""><strong>institution:</strong> University of West Attica, Democritus University of Thrace</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00623</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel multi-hop clustering algorithm for FANETs that constructs stable clusters by selecting cluster heads based on high stability and energy, and employs a ground station-assisted maintenance mechanism. The method aims to enhance cluster longevity and communication efficiency. Experimental results demonstrate that the approach significantly outperforms existing schemes in terms of cluster stability, communication overhead, and security resilience.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [dynamic random walks, rejection sampling, reservoir sampling, runtime adaptation, cost model, compile-time specialization]</li>
<li class=""><strong>authors:</strong> Seongyeon Park, Jaeyong Song, Changmin Shin, Sukjin Kim, Junguk Hong, Jinho Lee</li>
<li class=""><strong>institution:</strong> Seoul National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexiWalker is a GPU framework that introduces high-performance kernels for rejection and reservoir sampling to efficiently execute dynamic random walks. It uses a lightweight runtime cost model to select the optimal sampling kernel per node and a compile-time component to specialize user logic. The framework significantly outperforms existing CPU and GPU baselines and can handle workloads prior systems cannot support.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [GPU parallelization, memory management, multi-threaded framework, pipeline parallelism]</li>
<li class=""><strong>authors:</strong> Bingzheng Xia, Zujie Ren, Kuang Ma, Xiaoqian Li, Wenda Li, Shuibing He</li>
<li class=""><strong>institution:</strong> University of Chinese Academy of Sciences, Zhejiang Lab, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00398" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00398</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Heimdall++, an optimized GPU-accelerated tool for single-pulse detection in radio astronomy. It improves upon the original Heimdall by implementing fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple processing stages and reduce GPU stalls. The results show that Heimdall++ achieves up to 2.66x speedup in processing while maintaining result consistency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [multi-objective orchestration, agent-based routing, tiered island groups, typed placeholder sanitization, reversible anonymization, data locality routing]</li>
<li class=""><strong>authors:</strong> Bala Siva Sai Akhil Malepati</li>
<li class=""><strong>institution:</strong> Independent researcher (based on email domain)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00595</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents IslandRun, a system for distributed AI inference that treats computational resources as autonomous &quot;islands&quot; and uses agent-based routing and reversible anonymization to orchestrate tasks across personal devices, edge servers, and the cloud. Its core method involves policy-constrained multi-objective optimization to balance performance, privacy, cost, and trust. The main conclusion is that this establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous computing ecosystems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperparameter optimization, genetic algorithms, optuna, apache spark, transformer embeddings, roberta, bayesian target encoding, svm, logistic regression]</li>
<li class=""><strong>authors:</strong> González Trigueros Jesús Eduardo, Alonso Sánchez Alejandro, Muñoz Rivera Emilio, Peñarán Prieto Mariana Jaqueline, Mendoza González Camila Natalia</li>
<li class=""><strong>institution:</strong> Universidad de Guanajuato</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00298</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares classification strategies for structured and unstructured big data, using evolutionary/Bayesian hyperparameter optimization for numerical data and distributed Apache Spark processing for text. It finds a &quot;complexity paradox&quot; where optimized linear models outperform complex ones on structured data, while for text, robust feature engineering with Transformer embeddings allows simpler models to generalize better. The work provides a framework for algorithm selection based on data nature and infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [cache arbitration, thread throttling, MSHR contention, KV Cache, hybrid simulation]</li>
<li class=""><strong>authors:</strong> Zhongchun Zhou, Chengtao Lai, Wei Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00083</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LLaMCAT, a method that optimizes LLM inference by combining Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to reduce stalls in KV Cache access. It demonstrates significant speedups over baselines, particularly when systems are bottlenecked by miss handling throughput or limited cache size, offering a practical hardware-level solution for accelerating LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cryptographic audit trails], [post-quantum signatures, hash-and-sign, QROM, hybrid signatures, re-signing, Merkle-root anchoring, Q-Audit Integrity, Q-Non-Equivocation, Q-Binding]</li>
<li class=""><strong>authors:</strong> Leo Kao</li>
<li class=""><strong>institution:</strong> Codebat Technologies Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00110" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00110</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formalizes quantum-adversary-resilient security notions for constant-size cryptographic evidence structures used in AI audit trails and analyzes a post-quantum hash-and-sign instantiation. It proposes and evaluates three migration strategies—hybrid signatures, re-signing, and Merkle-root anchoring—for transitioning existing logs. The study concludes that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can extend the evidentiary lifetime of deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [functional connectivity, anomaly detection, root cause analysis, microservices, time-varying dependencies]</li>
<li class=""><strong>authors:</strong> Giles Winchester, George Parisis, Luc Berthouze</li>
<li class=""><strong>institution:</strong> University of Sussex</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00844" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00844</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FC-ADL, a method for microservice anomaly detection and localization by analyzing time-varying functional connectivity between service metrics. It shows this approach can effectively detect faults and identify root causes while being more scalable than causal inference methods. The method is validated on large-scale real-world deployments like Alibaba&#x27;s.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sampling, decision plane, tensor parallelism, pipeline parallelism, sequence-parallel sampling, speculative hot-vocab sampling, CPU-based algorithm]</li>
<li class=""><strong>authors:</strong> Bohan Zhao, Zane Cao, Yongchao He</li>
<li class=""><strong>institution:</strong> Not explicitly stated in the provided text. Author affiliations are not included.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00719</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SIMPLE, a method that disaggregates the sampling step from GPU inference by moving it to a CPU-side service. This approach uses sequence-parallel sampling and speculative hot-vocab sampling to reduce the bottleneck caused by sampling in distributed LLM serving. The evaluation shows that SIMPLE significantly improves throughput and reduces latency while being compatible with existing data-plane optimizations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Delta Sum Learning: an approach for fast and global convergence in Gossip Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gossip learning, delta sum learning, open application model, decentralized orchestration, federated averaging]</li>
<li class=""><strong>authors:</strong> Tom Goethals, Merlijn Sebrechts, Stijn De Schrijver, Filip De Turck, Bruno Volckaert</li>
<li class=""><strong>institution:</strong> Ghent University - imec, IDLab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01549" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01549</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Delta Sum Learning, a new aggregation method to improve global convergence in decentralized Gossip Learning. It also implements the method within a decentralized orchestration framework based on the Open Application Model. Evaluation shows that Delta Sum Learning maintains strong global convergence and scales better than alternatives, with a logarithmic versus linear accuracy loss as the network size increases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, LoRA, mixture of experts, knowledge reuse, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Yebo Wu, Jingguang Li, Zhijiang Guo, Li Li</li>
<li class=""><strong>institution:</strong> University of Macau, HKUST, HKUST (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00902</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SmartFed, a federated fine-tuning framework that reuses knowledge from existing LoRA modules via a Mixture of Rank-Wise Experts (MoRE) and an Elastic Expert Quota Allocation (EEQA) mechanism to reduce computational and communication costs. It demonstrates that this approach significantly improves model performance and training efficiency compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] StarDist: A Code Generator for Distributed Graph Algorithms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed graph processing], [MPI, Remote Memory Access (RMA), code generation, communication aggregation, bulk-reduction substrate]</li>
<li class=""><strong>authors:</strong> Barenya Kumar Nandy, Rupesh Nasre</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Madras</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01646" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01646</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents StarDist, an analysis-transformation framework that optimizes the distributed backend of the StarPlat DSL compiler for graph algorithms. It aggregates communication by reordering neighborhood accesses and uses an optimized bulk-reduction substrate built on Open MPI&#x27;s passive RMA. The optimized system outperforms d-Galois and DRONE in Single Source Shortest Path computations on large graphs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [GPU memory reuse, unified GPU memory pool, on-demand KV cache allocation, GPU-affinity-aware scheduling, cold-start optimization]</li>
<li class=""><strong>authors:</strong> Wenbin Zhu, Zhaoyan Shen, Zili Shao, Hongjun Dai, Feng Chen</li>
<li class=""><strong>institution:</strong> Shandong University, The Chinese University of Hong Kong, Indiana University Bloomington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01357" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01357</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Tangram, a system that accelerates serverless LLM loading by reusing GPU memory to retain model parameters, reducing cold-start latency. Its key techniques include a unified memory pool for tensor sharing, dynamic KV cache management, and affinity-aware scheduling. Experiments show Tangram achieves up to 6.2x faster loading and reduces Time-To-First-Token by 23–55% compared to state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, energy harvesting, client scheduling, version age of information, semantics-aware communication, feature-based proxy, non-IID data]</li>
<li class=""><strong>authors:</strong> Eunjeong Jeong, Giovanni Perin, Howard H. Yang, Nikolaos Pappas</li>
<li class=""><strong>institution:</strong> Linköping University, University of Brescia, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01983" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01983</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a lightweight client scheduling framework for Energy-Harvesting Federated Learning that uses a feature-based proxy to efficiently estimate the Version Age of Information, a semantics-aware metric. This approach reduces the computational cost of predicting update significance, avoiding redundant local training. Experiments show the method achieves superior learning performance and energy reduction compared to baseline policies under extreme non-IID data and scarce energy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic graph re-partitioning, model-aware capacity profiling, distributed split inference, adaptive orchestration, runtime-resolved placement]</li>
<li class=""><strong>authors:</strong> Aladin Djuhera, Fernando Koch, Alecio Binotto</li>
<li class=""><strong>institution:</strong> Technical University of Munich, Florida Atlantic University, Carl Zeiss AG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01039" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01039</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a framework for dynamically partitioning and placing foundation model layers across heterogeneous edge nodes at runtime, formalized as a constrained optimization problem. It aims to adapt to fluctuating network and compute resources by integrating model-aware profiling with reactive graph re-partitioning. The main conclusion is that this approach enables efficient, low-latency inference for large models in volatile edge environments like 6G MEC, overcoming the limitations of static split inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Morphling: Fast, Fused, and Flexible GNN Training at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [domain-specific code synthesis, architecture-aware primitives, sparsity-aware execution, OpenMP, CUDA, MPI]</li>
<li class=""><strong>authors:</strong> Anubhab, Rupesh Nasre</li>
<li class=""><strong>institution:</strong> IIT Madras</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Morphling, a domain-specific code synthesizer that compiles high-level GNN specifications into optimized, portable implementations for CPUs and GPUs. It uses a library of architecture-aware primitives and a runtime engine to dynamically choose dense or sparse execution paths. The results show that Morphling significantly outperforms existing frameworks in training throughput and memory efficiency on diverse datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] Trace-based, time-resolved analysis of MPI application performance using standard metrics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [MPI performance analysis], [trace-based analysis, time-resolved metrics, Paraver, critical path reconstruction, load balance, serialisation efficiency, transfer efficiency]</li>
<li class=""><strong>authors:</strong> Kingshuk Haldar</li>
<li class=""><strong>institution:</strong> High Performance Computing Center Stuttgart (HLRS), University of Stuttgart</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.01764" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.01764</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for analyzing MPI application performance by calculating time-resolved standard metrics (like load balance and transfer efficiency) from execution traces, using fixed or adaptive time windows. It robustly processes Paraver traces to reconstruct critical paths and handle event anomalies. The approach reveals transient performance bottlenecks that are hidden by global, time-aggregated metrics, offering a scalable alternative to full trace visualization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251202] UNIQ: Communication-Efficient Distributed Quantum Computing via Unified Nonlinear Integer Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed quantum computing], [non-linear integer programming, greedy algorithm, JIT (Just-In-Time), EPR pair generation, qubit allocation, network scheduling]</li>
<li class=""><strong>authors:</strong> Hui Zhong, Jiachen Shen, Lei Fan, Xinyue Zhang, Hao Wang, Miao Pan, Zhu Han</li>
<li class=""><strong>institution:</strong> University of Houston</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.00401" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.00401</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes UNIQ, a unified optimization framework for distributed quantum computing that integrates qubit allocation, entanglement management, and network scheduling into a single non-linear integer programming model. It uses a greedy algorithm for qubit mapping and a JIT approach for parallel EPR pair generation to minimize communication costs and runtime. The method is shown to substantially outperform existing approaches across diverse circuits and topologies.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 61</strong></p>
<ul>
<li class="">[arXiv251202] NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration <a href="https://arxiv.org/pdf/2512.00119" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00545" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches <a href="https://arxiv.org/pdf/2512.00049" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Gradient Inversion in Federated Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00303" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] List Replicable Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00553" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs <a href="https://arxiv.org/pdf/2512.00319" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00352" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Learning Causal States Under Partial Observability and Perturbation <a href="https://arxiv.org/pdf/2512.00357" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00351" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization <a href="https://arxiv.org/pdf/2512.00601" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge <a href="https://arxiv.org/pdf/2512.00048" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00079" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization <a href="https://arxiv.org/pdf/2512.00499" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] G-KV: Decoding-Time KV Cache Eviction with Global Attention <a href="https://arxiv.org/pdf/2512.00504" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning <a href="https://arxiv.org/pdf/2512.00062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF <a href="https://arxiv.org/pdf/2512.00709" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions <a href="https://arxiv.org/pdf/2512.00042" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00030" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations <a href="https://arxiv.org/pdf/2512.00249" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control <a href="https://arxiv.org/pdf/2512.00050" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking <a href="https://arxiv.org/pdf/2512.00724" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] AI Agent for Source Finding by SoFiA-2 for SKA-SDC2 <a href="https://arxiv.org/pdf/2512.00769" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] What Is Preference Optimization Doing, How and Why? <a href="https://arxiv.org/pdf/2512.00778" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning <a href="https://arxiv.org/pdf/2512.00831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs <a href="https://arxiv.org/pdf/2512.00908" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments <a href="https://arxiv.org/pdf/2512.00915" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning <a href="https://arxiv.org/pdf/2512.00961" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search <a href="https://arxiv.org/pdf/2512.00968" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning <a href="https://arxiv.org/pdf/2512.01034" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids <a href="https://arxiv.org/pdf/2512.01046" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Automating the Refinement of Reinforcement Learning Specifications <a href="https://arxiv.org/pdf/2512.01047" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs <a href="https://arxiv.org/pdf/2512.01054" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] World Model Robustness via Surprise Recognition <a href="https://arxiv.org/pdf/2512.01119" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Mode-Conditioning Unlocks Superior Test-Time Scaling <a href="https://arxiv.org/pdf/2512.01127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems <a href="https://arxiv.org/pdf/2512.01167" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach for Joint Optimization <a href="https://arxiv.org/pdf/2512.01202" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions <a href="https://arxiv.org/pdf/2512.01224" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] On the Tension Between Optimality and Adversarial Robustness in Policy Optimization <a href="https://arxiv.org/pdf/2512.01228" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning <a href="https://arxiv.org/pdf/2512.01282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL <a href="https://arxiv.org/pdf/2512.01311" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Extending NGU to Multi-Agent RL: A Preliminary Study <a href="https://arxiv.org/pdf/2512.01321" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Directed evolution algorithm drives neural prediction <a href="https://arxiv.org/pdf/2512.01362" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud <a href="https://arxiv.org/pdf/2512.01366" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices <a href="https://arxiv.org/pdf/2512.01374" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Multi-Path Collaborative Reasoning via Reinforcement Learning <a href="https://arxiv.org/pdf/2512.01485" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems <a href="https://arxiv.org/pdf/2512.01661" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] How Does RL Post-training Induce Skill Composition? A Case Study on Countdown <a href="https://arxiv.org/pdf/2512.01775" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation <a href="https://arxiv.org/pdf/2512.01801" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning <a href="https://arxiv.org/pdf/2512.01878" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles <a href="https://arxiv.org/pdf/2512.01882" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Rectifying LLM Thought from Lens of Optimization <a href="https://arxiv.org/pdf/2512.01925" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Agentic Policy Optimization via Instruction-Policy Co-Evolution <a href="https://arxiv.org/pdf/2512.01945" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment <a href="https://arxiv.org/pdf/2512.01952" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Learned-Rule-Augmented Large Language Model Evaluators <a href="https://arxiv.org/pdf/2512.01958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Forecasting in Offline Reinforcement Learning for Non-stationary Environments <a href="https://arxiv.org/pdf/2512.01987" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies <a href="https://arxiv.org/pdf/2512.01993" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Learning Sim-to-Real Humanoid Locomotion in 15 Minutes <a href="https://arxiv.org/pdf/2512.01996" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A Diffusion Model Framework for Maximum Entropy Reinforcement Learning <a href="https://arxiv.org/pdf/2512.02019" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach <a href="https://arxiv.org/pdf/2512.00243" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] How do trout regulate patterns of muscle contraction to optimize propulsive efficiency during steady swimming <a href="https://arxiv.org/pdf/2512.01218" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Formal Verification of Noisy Quantum Reinforcement Learning Policies <a href="https://arxiv.org/pdf/2512.01502" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 29</strong></p>
<ul>
<li class="">[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches <a href="https://arxiv.org/pdf/2512.00049" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors <a href="https://arxiv.org/pdf/2512.00113" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement <a href="https://arxiv.org/pdf/2512.00396" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A Comprehensive Survey on Surgical Digital Twin <a href="https://arxiv.org/pdf/2512.00019" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning <a href="https://arxiv.org/pdf/2512.00044" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A Rosetta Stone for AI Benchmarks <a href="https://arxiv.org/pdf/2512.00193" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators <a href="https://arxiv.org/pdf/2512.00059" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework <a href="https://arxiv.org/pdf/2512.00406" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization <a href="https://arxiv.org/pdf/2512.00499" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning <a href="https://arxiv.org/pdf/2512.00062" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays <a href="https://arxiv.org/pdf/2512.00055" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Faster Verified Explanations for Neural Networks <a href="https://arxiv.org/pdf/2512.00164" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs <a href="https://arxiv.org/pdf/2512.00722" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy <a href="https://arxiv.org/pdf/2512.00829" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study <a href="https://arxiv.org/pdf/2512.00931" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Diffusion Model in Latent Space for Medical Image Segmentation Task <a href="https://arxiv.org/pdf/2512.01292" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] The Necessity of Imperfection<!-- -->:Reversing<!-- --> Model Collapse via Simulating Cognitive Boundedness <a href="https://arxiv.org/pdf/2512.01354" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices <a href="https://arxiv.org/pdf/2512.01374" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] RE-LLM: Integrating Large Language Models into Renewable Energy Systems <a href="https://arxiv.org/pdf/2512.01392" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware <a href="https://arxiv.org/pdf/2512.01463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations <a href="https://arxiv.org/pdf/2512.01702" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion <a href="https://arxiv.org/pdf/2512.01881" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets <a href="https://arxiv.org/pdf/2512.01888" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling <a href="https://arxiv.org/pdf/2512.02010" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI <a href="https://arxiv.org/pdf/2512.02020" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network <a href="https://arxiv.org/pdf/2512.00299" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Building Trustworthy AI for Materials Discovery: From Autonomous Laboratories to Z-scores <a href="https://arxiv.org/pdf/2512.01080" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding <a href="https://arxiv.org/pdf/2512.01565" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251202] Cuffless Blood Pressure Estimation from Six Wearable Sensor Modalities in Multi-Motion-State Scenarios <a href="https://arxiv.org/pdf/2512.01653" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-03">2025-12-03<a href="#2025-12-03" class="hash-link" aria-label="Direct link to 2025-12-03" title="Direct link to 2025-12-03" translate="no">​</a></h2>
<p><strong>cs.DC total: 9</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251203] Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPUDirect Async, vector similarity search, GPU cluster, HNSW, CAGRA, pipelining, overlapping computation and communication]</li>
<li class=""><strong>authors:</strong> Yi Liu, Chen Qian</li>
<li class=""><strong>institution:</strong> University of California Santa Cruz</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Fantasy, a system for large-scale vector similarity search that pipelines search and data transfer across a GPU cluster using GPUDirect Async. It overlaps computation and network communication to improve throughput and support large query batches for graphs that exceed single-GPU memory. The main conclusion is that this approach significantly enhances search performance for massive vector datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [memory disaggregation], [data object level offloading, dual-buffer prefetch, remote memory access, quantitative analysis]</li>
<li class=""><strong>authors:</strong> Haoyu Zheng, Shouwei Gao, Jie Ren, Wenqian Dong</li>
<li class=""><strong>institution:</strong> Oregon State University, College of William &amp; Mary</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02300" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02300</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DOLMA is a framework that performs memory disaggregation for HPC applications by intelligently offloading data objects to remote memory and using a dual-buffer design for prefetching. It balances local and remote memory usage to minimize performance impact. The evaluation shows it reduces local memory usage by up to 63% while keeping performance degradation under 16%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Theoretical analysis of beaconless geocast protocols in 1D</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless networking], [beaconless geocast, routing protocols, mobile ad-hoc networks, wireless sensor networks, probabilistic analysis, network load]</li>
<li class=""><strong>authors:</strong> Joachim Gudmundsson, Irina Kostitsyna, Maarten Löffler, Tobias Müller, Vera Sacristán, Rodrigo I. Silveira</li>
<li class=""><strong>institution:</strong> University of Sydney, Eindhoven University of Technology, Universiteit Utrecht, Groningen University, Universitat Politècnica de Catalunya</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02663" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02663</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a formal theoretical analysis of six beaconless geocast routing protocols in one-dimensional mobile ad-hoc networks, focusing on the maximum number of messages a node can receive as a measure of network load. The analysis, which includes involved probabilistic methods for some protocols, confirms behaviors previously observed only through simulation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Solutions for Distributed Memory Access Mechanism on HPC Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC and distributed systems], [MPI, Infiniband, Slingshot, RDMA, LD_PRELOAD, Lustre, remote memory access]</li>
<li class=""><strong>authors:</strong> Jan Meizner, Maciej Malawski</li>
<li class=""><strong>institution:</strong> Sano Centre for Computational Medicine, AGH University of Krakow</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02546" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02546</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates mechanisms for remote memory access in HPC clusters, comparing solutions based on shared storage and MPI (over Infiniband and Slingshot) to local access. It proposes using LD_PRELOAD to intercept memory calls for distributed access without kernel modifications. The main finding is that remote access performance, especially when backed by MPI, is similar to local memory access.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] TokenPowerBench: Benchmarking the Power Consumption of LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [power consumption measurement, phase-aligned metrics, prefill and decode stages, batch size, context length, parallelism, quantization]</li>
<li class=""><strong>authors:</strong> Chenxu Niu, Wei Zhang, Jie Li, Yongjian Zhao, Tongyang Wang, Xi Wang, Yong Chen</li>
<li class=""><strong>institution:</strong> Texas Tech University, Texas Advanced Computing Center, Southeast University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03024</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces TokenPowerBench, a benchmark for measuring the power consumption of LLM inference. It provides a configurable interface, a measurement layer for GPU/node/system power, and a pipeline to attribute energy to prefill and decode stages. The authors demonstrate its use on several model series and release it as open-source to help forecast operating expenses and meet sustainability goals.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Sampling on Metric Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic processes], [Euler-Maruyama discretization, timestep splitting, CUDA kernels, Langevin diffusion, metric graphs]</li>
<li class=""><strong>authors:</strong> Rajat Vadiraj Dwaraknath, Lexing Ying</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02175" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02175</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the first algorithm for simulating Brownian motions and sampling on metric graphs using a timestep splitting Euler-Maruyama discretization of the corresponding stochastic differential equation. It provides theoretical convergence guarantees and demonstrates a highly parallelizable custom CUDA kernel implementation that achieves speedups of up to ~8000x over a PyTorch GPU baseline on simple graphs and ~1500x on a real vascular network model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high performance computing], [FAIR principles, workflow components, metadata, EOSC, HPC infrastructure]</li>
<li class=""><strong>authors:</strong> Sean R. Wilkinson, Patrick Widener, Sarp Oral, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory (OLCF)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02818" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02818</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a component-based model for designing FAIR (Findable, Accessible, Interoperable, Reusable) workflows tailored for High Performance Computing centers, building on the architecture of the European Open Science Cloud. It concludes that HPC centers should foster cross-disciplinary FAIR ecosystems by focusing on making individual workflow components reusable, rather than entire workflows, to maximize long-term value and reduce duplication of effort.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Distributed and Autonomic Minimum Spanning Trees</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [spanning tree, VCube, fault-tolerance, broadcast, autonomic algorithm]</li>
<li class=""><strong>authors:</strong> Luiz A. Rodrigues, Elias P. Duarte Jr., Luciana Arantes</li>
<li class=""><strong>institution:</strong> Western Parana State University (UNIOESTE), Federal University of Parana (UFPR), Sorbonne Université – CNRS/LIP6</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02683</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an autonomic algorithm for building and maintaining a scalable spanning tree in a distributed system, using the VCube virtual topology as a failure detector. The algorithm ensures logarithmic degree and depth, supports dynamic reconstruction under process failures and recoveries, and enables efficient broadcast. Simulation results demonstrate its effectiveness compared to other alternatives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251203] Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [active storage, computing continuum, dataClay, workload offloading, distributed systems]</li>
<li class=""><strong>authors:</strong> Alex Barceló, Sebastián A. Cajas Ordoñez, Jaydeep Samanta, Andrés L. Suárez-Cetrulo, Romila Ghosh, Ricardo Simón Carbajo, Anna Queralt</li>
<li class=""><strong>institution:</strong> Barcelona Supercomputing Center (BSC), Ireland’s Centre for Artificial Intelligence (CeADAR), Universitat Politècnica de Catalunya (UPC)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.02646" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.02646</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a software architecture that uses active storage systems, specifically the dataClay platform, to offload and distribute AI workloads across the computing continuum (IoT, edge, cloud). The method embeds computation into storage to reduce data movement overhead. The results show that this approach significantly improves memory efficiency and training speed while maintaining model accuracy, making distributed AI deployments more scalable and resource-efficient.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 19</strong></p>
<ul>
<li class="">[arXiv251203] Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts <a href="https://arxiv.org/pdf/2512.02486" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection <a href="https://arxiv.org/pdf/2512.02386" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Synthetic Error Injection Fails to Elicit Self-Correction In Language Models <a href="https://arxiv.org/pdf/2512.02389" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies <a href="https://arxiv.org/pdf/2512.02581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Deep Research: A Systematic Survey <a href="https://arxiv.org/pdf/2512.02038" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data <a href="https://arxiv.org/pdf/2512.02350" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Zero-Shot Instruction Following in RL via Structured LTL Representations <a href="https://arxiv.org/pdf/2512.02633" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games <a href="https://arxiv.org/pdf/2512.02358" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Improved Training Mechanism for Reinforcement Learning via Online Model Selection <a href="https://arxiv.org/pdf/2512.02214" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.02406" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Modelling the Doughnut of social and planetary boundaries with frugal machine learning <a href="https://arxiv.org/pdf/2512.02200" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization <a href="https://arxiv.org/pdf/2512.02631" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles <a href="https://arxiv.org/pdf/2512.02409" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms <a href="https://arxiv.org/pdf/2512.02810" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering <a href="https://arxiv.org/pdf/2512.02435" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach <a href="https://arxiv.org/pdf/2512.02834" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning <a href="https://arxiv.org/pdf/2512.02835" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning <a href="https://arxiv.org/pdf/2512.02551" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] VACoT: Rethinking Visual Data Augmentation with VLMs <a href="https://arxiv.org/pdf/2512.02361" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 14</strong></p>
<ul>
<li class="">[arXiv251203] Model Recovery at the Edge under Resource Constraints for Physical AI <a href="https://arxiv.org/pdf/2512.02283" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity <a href="https://arxiv.org/pdf/2512.02403" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] The Impact of Artificial Intelligence on Enterprise Decision-Making Process <a href="https://arxiv.org/pdf/2512.02048" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements <a href="https://arxiv.org/pdf/2512.03035" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models <a href="https://arxiv.org/pdf/2512.02636" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] CONFIDE: Hallucination Assessment for Reliable Biomolecular Structure Prediction and Design <a href="https://arxiv.org/pdf/2512.02033" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification <a href="https://arxiv.org/pdf/2512.02337" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients <a href="https://arxiv.org/pdf/2512.02342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning <a href="https://arxiv.org/pdf/2512.02444" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents <a href="https://arxiv.org/pdf/2512.02812" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles <a href="https://arxiv.org/pdf/2512.02409" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] Fast Gaussian Process Approximations for Autocorrelated Data <a href="https://arxiv.org/pdf/2512.02925" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links <a href="https://arxiv.org/pdf/2512.02465" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251203] EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis <a href="https://arxiv.org/pdf/2512.02932" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-04">2025-12-04<a href="#2025-12-04" class="hash-link" aria-label="Direct link to 2025-12-04" title="Direct link to 2025-12-04" translate="no">​</a></h2>
<p><strong>cs.DC total: 12</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251204] A Chronological Analysis of the Evolution of SmartNICs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [network hardware], [SmartNIC, DPU, FPGA, HPC, network offloading, data processing unit, system-on-chip]</li>
<li class=""><strong>authors:</strong> Olasupo Ajayi, Ryan Grant</li>
<li class=""><strong>institution:</strong> Queen&#x27;s University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04054</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a chronological analysis of SmartNICs (SNICs) by reviewing 370 articles published between 2010 and 2024. The main conclusion is to provide insights into the evolution, manufacturers, use cases, and application domains of SNICs, which are designed to offload tasks from host CPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC energy efficiency], [energy profiling, Likwid, Nvidia profiling tools, MPI parallelism, synthetic benchmarks, Gromacs]</li>
<li class=""><strong>authors:</strong> Rafael Ravedutti Lucio Machado, Jan Eitzinger, Georg Hager, Gerhard Wellein</li>
<li class=""><strong>institution:</strong> Erlangen National High Performance Computing Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03697" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03697</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the energy efficiency of HPC systems by evaluating synthetic benchmarks and the Gromacs application on CPU and GPU clusters using profiling tools like Likwid. It identifies challenges and pitfalls in conducting such energy analysis. The study concludes by suggesting best practices for future energy efficiency research in HPC.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [SIMD vectorization, dynamic tuning, molecular dynamics, AutoPas, force calculation, neighbor identification]</li>
<li class=""><strong>authors:</strong> Luis Gall, Samuel James Newcome, Fabio Alexander Gratl, Markus Mühlhäußer, Manish Kumar Mishra, Hans-Joachim Bungartz</li>
<li class=""><strong>institution:</strong> Technical University of Munich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03565</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the AutoPas particle simulation library by introducing a dynamic tuning mechanism to select the optimal SIMD vectorization order for pairwise force calculations in Molecular Dynamics simulations. The method considers runtime parameters like particle density and neighbor identification algorithms. Benchmarks show this approach yields significant performance improvements over the library&#x27;s previous static method.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, multi-frequency learning, head-worn sensors, human activity recognition, privacy-aware machine learning]</li>
<li class=""><strong>authors:</strong> Dario Fenoglio, Mohan Li, Davide Casnici, Matias Laporte, Shkurta Gashi, Silvia Santini, Martin Gjoreski, Marc Langheinrich</li>
<li class=""><strong>institution:</strong> Università della Svizzera italiana, ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03287</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-frequency Federated Learning (FL) approach for Human Activity Recognition (HAR) using head-worn sensors to enable privacy-aware model training across devices with different sampling rates. It demonstrates that this method outperforms frequency-specific approaches on two datasets, showing promise for multi-frequency FL in HAR tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [asynchronous checkpointing, state management, fast failover, surplus network capacity]</li>
<li class=""><strong>authors:</strong> Bohan Zhao, Yuanhong Wang, Chenglin Liu, Jiagi Pan, Guang Yang, Ruitao Liu, Tingrui Zhang, Kai Luo, Wei Xu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03644" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03644</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FFTrainer, a system that leverages surplus network capacity to quickly save and load training states, enabling fast failover in large language model training. This approach aims to prevent costly rollbacks and accelerate recovery from node failures. The main conclusion is that FFTrainer significantly reduces recovery time and mitigates GPU utilization loss compared to prior checkpointing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [computation offloading, resource allocation, alternating optimization, layered approach, multi-access edge computing]</li>
<li class=""><strong>authors:</strong> Zhen Wang, Bin Lin, Qiang</li>
<li class=""><strong>institution:</strong> Dalian Maritime University, Dalian Neusoft University of Information, University of Calgary</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03487</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a double-edge-assisted scheme for space-air-marine integrated networks, using an alternating optimization method and a layered approach to jointly optimize offloading mode, volume, and resource allocation. It concludes that the proposed algorithm effectively minimizes system energy consumption under latency constraints compared to benchmark methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computational physics], [Parallel Tempering, Metropolis-Hastings, OpenMP, CUDA, Markov Chain Monte Carlo]</li>
<li class=""><strong>authors:</strong> Aingeru Ramos, Jose A Pascual, Javier Navaridas, Ivan Coluzza</li>
<li class=""><strong>institution:</strong> Basque Center on Material, Applications and Nanostructures (BCMaterials), University of the Basque Country, Rice University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03825" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03825</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a parallel implementation of the Metropolis-Hastings algorithm with Parallel Tempering to accelerate Markov Chain Monte Carlo sampling. The implementation uses OpenMP for CPU and CUDA for GPU parallelization, achieving speed-ups of up to 52x and 986x, respectively. The results provide a benchmark for future quantum implementations of the algorithm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [on-demand expert loading, edge-distributed inference, expert activation prediction, cacheless MoE, expert offloading]</li>
<li class=""><strong>authors:</strong> Liujianfu Wang, Yuyang Du, Yuchen Pan, Soung Chang Liew, Jiacheng Liu, Kexin Chen</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03927" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03927</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces OD-MoE, a distributed inference framework for Mixture-of-Experts models that eliminates the need for GPU expert caches by using on-demand expert loading. Its key innovations are parallelizing expert loading/computation across edge nodes and an accurate predictor to forecast expert activations in advance. The system achieves high prediction accuracy and enables efficient MoE inference on edge devices with very limited GPU memory.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Token Velocity, Convertible Decoders, autoscaling, prefill/decode disaggregation]</li>
<li class=""><strong>authors:</strong> Ruiqi Lai, Hongrui Liu, Chengzhi Lu, Zonghao Liu, Siyu Cao, Siyang Shao, Yixin Zhang, Luo Mai, Dmitrii Ustiugov</li>
<li class=""><strong>institution:</strong> Nanyang Technological University (NTU Singapore), Georgia Institute of Technology, Alibaba Group, University of Edinburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03416" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03416</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TokenScale, an autoscaling framework for disaggregated LLM serving that uses a novel predictive metric called Token Velocity and a flexible resource design with Convertible Decoders. This approach enables proactive scaling and rapid absorption of traffic bursts. The evaluation shows that TokenScale significantly improves SLO attainment and reduces costs compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, adaptive layer-freezing, encoder freezing, Green AI, MRI-to-CT conversion]</li>
<li class=""><strong>authors:</strong> Ciro Benito Raggio, Lucia Migliorelli, Nils Skupien, Mathias Krohmer Zabaleta, Oliver Blanck, Francesco Cicone, Giuseppe Lucio Cascini, Paolo Zaffino, Maria Francesca Spadea</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Università Degli Studi Di Teramo, University Medical Center Schleswig-Holstein, Magna Graecia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03054</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive encoder-freezing strategy for Federated Learning to reduce the computational and energy costs of training models for MRI-to-CT image conversion. The method selectively freezes encoder weights based on minimal updates between training rounds, achieving up to 23% reductions in training time, energy, and emissions. The core conclusion is that this Green AI approach maintains model performance while significantly improving the sustainability and accessibility of federated healthcare AI.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum computing], [distributed quantum computing, multipartite entanglement, GHZ states, fan-out operations, qudits, global gates, GMS gates]</li>
<li class=""><strong>authors:</strong> Seng W. Loke</li>
<li class=""><strong>institution:</strong> Deakin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03685</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper explores the use of multipartite entanglement resources like GHZ states and qudits to implement distributed fan-out operations, specifically for realizing challenging distributed global gates (GMS gates). It concludes that such an approach can lead to more efficient distributed quantum circuit execution compared to using only distributed two-qubit gates, with implications for quantum circuit compilation and data center design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251204] Integrating High Performance In-Memory Data Streaming and In-Situ Visualization in Hybrid MPI+OpenMP PIC MC Simulations Towards Exascale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high performance computing], [openPMD, ADIOS2, MPI+OpenMP, in-memory data streaming, in-situ visualization, Sustainable Staging Transport (SST), Particle-in-Cell Monte Carlo (PIC MC)]</li>
<li class=""><strong>authors:</strong> Jeremy J. Williams, Stefan Costea, Daniel Medeiros, Jordy Trilaksono, Pratibha Hegde, David Tskhakaya, Leon Kos, Ales Podolnik, Jakub Hromadka, Kevin A. Huck, Allen D. Malony, Frank Jenko, Erwin Laure, Stefano Markidis</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, University of Ljubljana, Max Planck Institute for Plasma Physics, University of Oregon, University of Vienna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.03914" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.03914</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances the BIT1 plasma simulation code by integrating OpenMP task-based parallelism, the openPMD API, and ADIOS2&#x27;s in-memory SST engine for data streaming. The core method replaces traditional file I/O with in-memory streaming and in-situ visualization to reduce bottlenecks. The main conclusion is that this approach significantly improves runtime, data accessibility, and enables real-time analysis for exascale plasma simulations.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 26</strong></p>
<ul>
<li class="">[arXiv251204] Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">λ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span>,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">λ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span>))-GA <a href="https://arxiv.org/pdf/2512.03805" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Better World Models Can Lead to Better Post-Training Performance <a href="https://arxiv.org/pdf/2512.03400" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Autonomous Reinforcement Learning Robot Control with Intel&#x27;s Loihi 2 Neuromorphic Hardware <a href="https://arxiv.org/pdf/2512.03911" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration <a href="https://arxiv.org/pdf/2512.03102" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training <a href="https://arxiv.org/pdf/2512.03847" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Hierarchical Vision Language Action Model Using Success and Failure Demonstrations <a href="https://arxiv.org/pdf/2512.03913" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Adaptive sampling using variational autoencoder and reinforcement learning <a href="https://arxiv.org/pdf/2512.03525" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Multi-Agent Reinforcement Learning with Communication-Constrained Priors <a href="https://arxiv.org/pdf/2512.03528" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models <a href="https://arxiv.org/pdf/2512.03882" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] A Learning-based Control Methodology for Transitioning VTOL UAVs <a href="https://arxiv.org/pdf/2512.03548" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2512.03973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding <a href="https://arxiv.org/pdf/2512.03194" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning <a href="https://arxiv.org/pdf/2512.03783" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2512.03891" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations <a href="https://arxiv.org/pdf/2512.03429" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] A Multi-Agent, Policy-Gradient approach to Network Routing <a href="https://arxiv.org/pdf/2512.03211" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective <a href="https://arxiv.org/pdf/2512.03759" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition <a href="https://arxiv.org/pdf/2512.03794" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning <a href="https://arxiv.org/pdf/2512.03244" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control <a href="https://arxiv.org/pdf/2512.03736" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving <a href="https://arxiv.org/pdf/2512.03795" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Multimodal Reinforcement Learning with Agentic Verifier for AI Agents <a href="https://arxiv.org/pdf/2512.03438" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing <a href="https://arxiv.org/pdf/2512.03729" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL <a href="https://arxiv.org/pdf/2512.03059" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] SkillFactory: Self-Distillation For Learning Cognitive Behaviors <a href="https://arxiv.org/pdf/2512.04072" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback <a href="https://arxiv.org/pdf/2512.03208" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 14</strong></p>
<ul>
<li class="">[arXiv251204] Robust Tabular Foundation Models <a href="https://arxiv.org/pdf/2512.03307" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing <a href="https://arxiv.org/pdf/2512.03394" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms <a href="https://arxiv.org/pdf/2512.03476" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra <a href="https://arxiv.org/pdf/2512.03127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Scalable Decision Focused Learning via Online Trainable Surrogates <a href="https://arxiv.org/pdf/2512.03861" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Adaptive Identification and Modeling of Clinical Pathways with Process Mining <a href="https://arxiv.org/pdf/2512.03787" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control <a href="https://arxiv.org/pdf/2512.03736" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling <a href="https://arxiv.org/pdf/2512.03050" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers <a href="https://arxiv.org/pdf/2512.03451" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] The promising potential of vision language models for the generation of textual weather forecasts <a href="https://arxiv.org/pdf/2512.03623" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Irresponsible AI: big tech&#x27;s influence on AI research and associated impacts <a href="https://arxiv.org/pdf/2512.03077" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] AtomDisc: An Atom-level Tokenizer that Boosts Molecular LLMs and Reveals Structure--Property Associations <a href="https://arxiv.org/pdf/2512.03080" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] An AI Implementation Science Study to Improve Trustworthy Data in a Large Healthcare System <a href="https://arxiv.org/pdf/2512.03098" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251204] Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs <a href="https://arxiv.org/pdf/2512.04047" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-05">2025-12-05<a href="#2025-12-05" class="hash-link" aria-label="Direct link to 2025-12-05" title="Direct link to 2025-12-05" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251205] Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [data-sharing systems], [energy profiling, pipeline modeling, simulation experiments, stage reuse, configuration estimation]</li>
<li class=""><strong>authors:</strong> Sepideh Masoudi, Sebastian Werner, Pierluigi Plebani, Stefan Tai</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin, Politecnico di Milano</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines and identifies reuse potential in shared stages to reduce energy. The method is validated through simulation experiments, showing promising potential for cross-organizational pipeline optimization and establishing a foundation for energy-conscious execution strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Toward Sustainability-Aware LLM Inference on Edge Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [edge computing, carbon-aware routing, latency-aware routing, benchmarking, batch processing]</li>
<li class=""><strong>authors:</strong> Kolichala Rajashekar, Nafiseh Sharghivand, Radu Prodan, Reza Farahani</li>
<li class=""><strong>institution:</strong> University of Innsbruck, University of Klagenfurt</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04088</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes sustainability-aware LLM inference on edge clusters using carbon- and latency-aware routing strategies based on empirical benchmarking of energy and execution time. It finds that a batch size of four prompts offers a good trade-off between throughput and energy efficiency, while larger batches risk GPU memory saturation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [analytical performance model, JIT compilation, cache hierarchy modeling, matrix multiplication, Triton]</li>
<li class=""><strong>authors:</strong> Ryan Swann, Muhammad Osama, Xiaohu Guo, Bryant Nelson, Lixun Zhang, Alex Brown, Yen Ong, Ali Yazdani, Sean Siddens, Ganesh Dasika, Alex Underwood</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04226</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces tritonBLAS, a GEMM library that uses an analytical model of GPU architecture and algorithmic blocking to predict and generate high-performance kernels without autotuning. It achieves over 95% of the performance of autotuned solutions while eliminating tuning time, making it a practical replacement for empirical methods in HPC and ML workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Scaling MPI Applications on Aurora</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [MPI, Slingshot interconnect, dragonfly topology, HPL, HPCG, Graph500]</li>
<li class=""><strong>authors:</strong> Huda Ibeid, Anthony-Trung Nguyen, Aditya Nishtala, Premanand Sakarda, Larry Kaplan, Nilakantan Mahadevan, Michael Woodacre, Victor Anisimov, Kalyan Kumaran, JaeHyuk Kwack, Vitali Morozov, Servesh Muralidharan, Scott Parker</li>
<li class=""><strong>institution:</strong> Intel Corporation, Hewlett Packard Enterprise, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04291" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04291</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper details the system design and network validation of the Aurora exascale supercomputer, focusing on its HPE Slingshot fabric and dragonfly topology. It demonstrates the system&#x27;s performance through MPI and application benchmarks, concluding that Aurora provides the necessary throughput, latency, and bandwidth to scale applications to large node counts for breakthrough science.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] A Structure-Aware Irregular Blocking Method for Sparse LU Factorization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [sparse LU factorization, irregular blocking, load balancing, dependency tree, diagonal block-based feature]</li>
<li class=""><strong>authors:</strong> Zhen Hu, Dongliang Xiong, Kai Huang, Changjun Wu, Xiaowen Jiang</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04389" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04389</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a structure-aware irregular blocking method for sparse LU factorization, which introduces a diagonal block-based feature to characterize local nonzero distribution and adjusts block sizes accordingly to balance workloads. The method uses fine-grained blocks in dense regions and coarse-grained blocks in sparse regions. Experiments show it achieves significant speedups over existing methods like PanguLU and SuperLU_DIST on NVIDIA A100 GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed messaging systems], [Remote Procedure Call, Remote Memory Access, file-based delivery, ordered delivery, at-least-once delivery]</li>
<li class=""><strong>authors:</strong> Sushant Kumar Gupta, Anil Raghunath Iyer, Chang Yu, Neel Bagora, Olivier Pomerleau, Vivek Kumar, Prunthaban Kanthakumar</li>
<li class=""><strong>institution:</strong> Google LLC</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04096</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Fast ACS, a file-based ordered message delivery system that uses Remote Procedure Call for inter-cluster and Remote Memory Access for intra-cluster communication to achieve low-latency, scalable delivery. The system successfully provides ordering and at-least-once delivery guarantees to thousands of consumers across global clusters with sub-second p99 latency at low resource cost.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [service placement, resource provisioning, task scheduling, resource allocation, instance selection, AI-driven optimization, quantum computing, serverless computing]</li>
<li class=""><strong>authors:</strong> Ali Akbar Vali, Sadoon Azizi, Mohammad Shojafar, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> University of Kurdistan, University of Surrey, The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04093" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04093</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper surveys state-of-the-art energy-efficient resource management strategies in microservices-based fog and edge computing, reviewing and classifying over 136 studies from 2020-2024. It identifies a lack of synergy among core resource management components and outlines future research directions, including AI-driven optimization and quantum computing, to develop more integrated and sustainable solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless computing], [WebAssembly, WASI, AOT compilation, JIT compilation, cold-start, warm-start, serverless workflow]</li>
<li class=""><strong>authors:</strong> Mario Colosi, Reza Farahani, Lauri Loven, Radu Prodan, Massimo Villari</li>
<li class=""><strong>institution:</strong> University of Messina, University of Klagenfurt, University of Oulu, University of Innsbruck</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04089" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04089</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the performance of serverless workflows using WebAssembly (Wasm) across browser, edge, and cloud environments. It finds that Ahead-of-Time (AOT) compilation and instance warming significantly reduce startup latency, and while browsers perform well with small payloads, edge and cloud nodes outperform them for compute- and memory-intensive tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Counting Without Running: Evaluating LLMs&#x27; Reasoning About Code Complexity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [gpuFLOPBench, CUDA kernels, FLOP counting, static performance analysis, HeCBench]</li>
<li class=""><strong>authors:</strong> Gregory Bolet, Giorgis Georgakoudis, Konstantinos Parasyris, Harshitha Menon, Niranjan Hasabnis, Kirk W. Cameron, Gal Oren</li>
<li class=""><strong>institution:</strong> Virginia Tech, Lawrence Livermore National Laboratory, Codemetal, Stanford University, Technion</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04355" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04355</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces gpuFLOPBench, a benchmark for evaluating Large Language Models&#x27; ability to statically predict the floating-point operation (FLOP) counts of CUDA kernels without executing them. The results show that while modern LLMs perform well on straightforward kernels, they make significant errors when FLOPs depend on implicit compiler or runtime behaviors like division or intrinsic functions. This reveals a core limitation in current LLMs&#x27; reasoning about hardware-specific performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] VLCs: Managing Parallelism with Virtualized Libraries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Virtual Library Contexts, resource partitioning, library virtualization, OpenMP, OpenBLAS, LibTorch]</li>
<li class=""><strong>authors:</strong> Yineng Yan, William Ruys, Hochan Lee, Ian Henriksen, Arthur Peters, Sean Stephens, Bozhi You, Henrique Fingler, Martin Burtscher, Milos Gligoric, Keshav Pingali, Mattan Erez, George Biros, Christopher J. Rossbach</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Texas State University, Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04320" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04320</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Virtual Library Contexts (VLCs), a method to manage parallelism by creating process subunits that encapsulate libraries and their resource allocations without modifying library code. This allows users to partition resources between libraries to prevent contention or load multiple library copies for parallel execution of thread-unsafe code. Experiments with C++ and Python prototypes show VLCs can achieve speedups of up to 2.85x on benchmarks using libraries like OpenMP, OpenBLAS, and LibTorch.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [VLSI physical design acceleration], [FPGA-CPU synergy, task partition, multi-granularity pipelining, cell shifting optimization, mixed-cell-height legalization]</li>
<li class=""><strong>authors:</strong> Xingyu Liu, Jiawei Liang, Linfeng Du, Yipu Zhang, Chaofang Ma, Hanwei Fan, Jiang Xu, Wei Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04527" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04527</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents FLEX, an accelerator for VLSI legalization that uses an optimized FPGA-CPU task partition and a multi-granularity pipelining technique to speed up the critical cell placement process. It demonstrates significant speedups over CPU-GPU and multi-threaded CPU baselines while also improving legalization quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Offloading to CXL-based Computational Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [memory systems], [CXL, computational memory, near-memory processing, asynchronous back-streaming, offloading, KAI]</li>
<li class=""><strong>authors:</strong> Suyeon Lee, Kangkyu Park, Kwangsik Shin, Ada Gavrilovska</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, SK hynix</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04449" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04449</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel &#x27;Asynchronous Back-Streaming&#x27; protocol and a system called KAI to improve operation offloading to CXL-based Computational Memory (CCM). KAI enables asynchronous data movement and lightweight pipelining between the host and CCM. The system significantly reduces end-to-end runtime and idle times, demonstrating improved performance and efficiency for disaggregated memory systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251205] Federated Learning for Terahertz Wireless Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, terahertz communications, multicarrier stochastic framework, beam squint, molecular absorption, SNR-weighted aggregation]</li>
<li class=""><strong>authors:</strong> O. Tansel Baydas, Ozgur B. Akan</li>
<li class=""><strong>institution:</strong> University of Cambridge, Koç University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.04984" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.04984</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a multicarrier stochastic framework to analyze the impact of realistic Terahertz channel impairments, like beam squint and molecular absorption, on Federated Learning optimization dynamics. It identifies a critical &quot;diversity trap&quot; where convergence error is driven by the harmonic mean of subcarrier SNRs, and proposes an SNR-weighted aggregation strategy to recover convergence in high-squint regimes where standard averaging fails.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 27</strong></p>
<ul>
<li class="">[arXiv251205] Toward Virtuous Reinforcement Learning <a href="https://arxiv.org/pdf/2512.04246" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning <a href="https://arxiv.org/pdf/2512.04368" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] The Geometry of Benchmarks: A New Path Toward AGI <a href="https://arxiv.org/pdf/2512.04276" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Towards better dense rewards in Reinforcement Learning Applications <a href="https://arxiv.org/pdf/2512.04302" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism <a href="https://arxiv.org/pdf/2512.04341" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Data-regularized Reinforcement Learning for Diffusion Models at Scale <a href="https://arxiv.org/pdf/2512.04332" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning <a href="https://arxiv.org/pdf/2512.04359" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Learning to Orchestrate Agents in Natural Language with the Conductor <a href="https://arxiv.org/pdf/2512.04388" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order <a href="https://arxiv.org/pdf/2512.04277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models <a href="https://arxiv.org/pdf/2512.04124" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] MARL Warehouse Robots <a href="https://arxiv.org/pdf/2512.04463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] GTM: Simulating the World of Tools for AI Agents <a href="https://arxiv.org/pdf/2512.04535" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS <a href="https://arxiv.org/pdf/2512.04552" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control <a href="https://arxiv.org/pdf/2512.04653" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] TRINITY: An Evolved LLM Coordinator <a href="https://arxiv.org/pdf/2512.04695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting <a href="https://arxiv.org/pdf/2512.04752" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions <a href="https://arxiv.org/pdf/2512.04773" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance <a href="https://arxiv.org/pdf/2512.04779" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases <a href="https://arxiv.org/pdf/2512.04793" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty <a href="https://arxiv.org/pdf/2512.04918" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent <a href="https://arxiv.org/pdf/2512.04949" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning <a href="https://arxiv.org/pdf/2512.04958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards <a href="https://arxiv.org/pdf/2512.05098" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Structured Document Translation via Format Reinforcement Learning <a href="https://arxiv.org/pdf/2512.05100" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning <a href="https://arxiv.org/pdf/2512.05105" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm <a href="https://arxiv.org/pdf/2512.04405" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Continuous-time reinforcement learning for optimal switching over multiple regimes <a href="https://arxiv.org/pdf/2512.04697" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 11</strong></p>
<ul>
<li class="">[arXiv251205] Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint <a href="https://arxiv.org/pdf/2512.04283" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning <a href="https://arxiv.org/pdf/2512.04252" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures <a href="https://arxiv.org/pdf/2512.04273" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Towards better dense rewards in Reinforcement Learning Applications <a href="https://arxiv.org/pdf/2512.04302" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Decoding Large Language Diffusion Models with Foreseeing Movement <a href="https://arxiv.org/pdf/2512.04135" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models <a href="https://arxiv.org/pdf/2512.04562" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning <a href="https://arxiv.org/pdf/2512.04632" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting <a href="https://arxiv.org/pdf/2512.04752" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Arbitrage: Efficient Reasoning via Advantage-Aware Speculation <a href="https://arxiv.org/pdf/2512.05033" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics <a href="https://arxiv.org/pdf/2512.04716" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251205] Meta-Learning for Quantum Optimization via Quantum Sequence Model <a href="https://arxiv.org/pdf/2512.05058" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-19T10:48:32.000Z" itemprop="dateModified">Dec 19, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251124-20251130"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251124-20251130</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251208-20251214"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251208-20251214</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-01" class="table-of-contents__link toc-highlight">2025-12-01</a></li><li><a href="#2025-12-02" class="table-of-contents__link toc-highlight">2025-12-02</a></li><li><a href="#2025-12-03" class="table-of-contents__link toc-highlight">2025-12-03</a></li><li><a href="#2025-12-04" class="table-of-contents__link toc-highlight">2025-12-04</a></li><li><a href="#2025-12-05" class="table-of-contents__link toc-highlight">2025-12-05</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251201/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>