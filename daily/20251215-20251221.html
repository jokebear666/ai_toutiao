<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 | AI头条"><meta data-rh="true" name="description" content="2025-12-15"><meta data-rh="true" property="og:description" content="2025-12-15"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251215-20251221","item":"https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.db8e67fa.css">
<script src="/ai_toutiao/assets/js/runtime~main.fc3de9b6.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.d4776e6c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/jokebear666" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-15">2025-12-15<a href="#2025-12-15" class="hash-link" aria-label="Direct link to 2025-12-15" title="Direct link to 2025-12-15" translate="no">​</a></h2>
<p><strong>cs.DC total: 15</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251215] Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed network algorithms], [multi-packet messaging, pruning, closeness centrality, decentralized computation, message efficiency]</li>
<li class=""><strong>authors:</strong> Patrick D. Manya, Eugene M. Mbuyi, Gothy T. Ngoie, Jordan F. Masakuna</li>
<li class=""><strong>institution:</strong> University of Kinshasa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances a distributed pruning method for closeness centrality by using multi-packet messaging to batch data into larger blocks. This reduces the number of exchanged messages and improves communication efficiency, particularly for large networks, with a manageable trade-off in local memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Seamless Transitions: A Comprehensive Review of Live Migration Technologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [virtualization], [live migration, container migration, virtual machine migration, migration techniques, migration units, infrastructure characteristics]</li>
<li class=""><strong>authors:</strong> Sima Attar-Khorasani, Lincoln Sherpa, Matthias Lieber, Siavash Ghiasvand</li>
<li class=""><strong>institution:</strong> TUD Dresden University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10979</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a comprehensive review of live migration technologies, focusing on container-based and virtual machine-based approaches. It analyzes migration techniques, units, and infrastructure, concluding that practical challenges and resource demands can sometimes outweigh the benefits, making implementation difficult to justify.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An Efficient Approach for Energy Conservation in Cloud Computing Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [task scheduling, resource utilization, fitness value, multi-criteria energy-efficient task scheduling (MCEETS), MaxUtil]</li>
<li class=""><strong>authors:</strong> Sohan Kumar Pande, Sanjaya Kumar Panda, Preeti Ranjan Sahu</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10974</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-criteria energy-efficient task scheduling (MCEETS) algorithm for cloud computing, which uses a fitness value based on CPU, disk, I/O utilization, and task processing time to improve resource utilization. Simulation results show that the proposed algorithm consumes less energy than the existing MaxUtil algorithm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [dynamic scheduling, multi-objective scheduling, hybrid priority scheduler, predictive backfill, smart batch, fragmentation reduction, job starvation, GPU utilization]</li>
<li class=""><strong>authors:</strong> Akhmadillo Mamirov</li>
<li class=""><strong>institution:</strong> The College of Wooster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces three dynamic multi-objective schedulers (Hybrid Priority, Predictive Backfill, and Smart Batch) designed to reduce fragmentation and starvation in GPU clusters. Through simulation, these schedulers significantly outperform static baselines in utilization, throughput, and fairness, demonstrating that adaptive scheduling can meaningfully improve GPU efficiency in heterogeneous AI clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An LLVM-Based Optimization Pipeline for SPDZ</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [secure multiparty computation], [LLVM, SPDZ, secret sharing, batching, GPU kernels, non-blocking scheduler]</li>
<li class=""><strong>authors:</strong> Tianye Dai, Hammurabi Mendes, Heuichan Lim</li>
<li class=""><strong>institution:</strong> Davidson College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11112" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11112</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an LLVM-based compiler pipeline for the SPDZ MPC protocol, which uses C with privacy annotations and LLVM IR to automatically batch operations and a runtime scheduler to overlap communication and computation, including GPU kernel mapping. The evaluation shows significant speedups over MP-SPDZ, indicating that leveraging LLVM with protocol-aware scheduling is effective for extracting parallelism without sacrificing usability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hybrid parallelism, data parallelism, pipeline parallelism, model partitioning, network scheduling, runtime adaptation]</li>
<li class=""><strong>authors:</strong> Jianli Jin, Ziyang Lin, Qianli Dong, Yi Chen, Jayanth Srinivasa, Myungjin Lee, Zhaowei Tan, Fan Lai</li>
<li class=""><strong>institution:</strong> UIUC, Northwestern University, University of California, Riverside, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dora is a framework that uses heterogeneity-aware model partitioning, contention-aware network scheduling, and a runtime adapter to achieve QoE-aware hybrid parallelism for distributed edge AI. It demonstrates significant improvements in execution speed and energy efficiency while maintaining quality of experience under dynamic runtime conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [GPU-native compilation, parallel traditional compilation, neural compilation, sequence-to-sequence translation, probabilistic verification, hybrid architecture, in-VRAM iteration]</li>
<li class=""><strong>authors:</strong> Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva</li>
<li class=""><strong>institution:</strong> Institute of Information Technology, Kyrgyz State Technical University named after I. Razzakov</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11200</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes theoretical foundations for three GPU-native compilation approaches—parallel traditional, neural, and hybrid—to eliminate CPU-GPU data transfers during code iteration cycles. It demonstrates that these methods can achieve 10-100x speedups by keeping compilation and execution entirely in GPU memory. The main conclusion is that GPU-native compilation offers a path to drastically reduce latency and energy consumption in AI code generation systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [cluster scheduling, disaggregated architecture, co-execution group, two-tier scheduling, round-robin, warm-start context switching]</li>
<li class=""><strong>authors:</strong> Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, UIUC, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RollMux, a cluster scheduling framework that improves efficiency in disaggregated RL post-training by multiplexing jobs to utilize idle phases. Its core method involves a two-tier scheduler and a co-execution group abstraction to orchestrate cross-cluster execution. The evaluation shows RollMux significantly improves cost efficiency over standard disaggregated and co-located baselines while maintaining service-level objectives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical federated learning, aggregated federated learning, continual federated learning, decentralized aggregation, gossip-based protocols]</li>
<li class=""><strong>authors:</strong> Sumit Chongder</li>
<li class=""><strong>institution:</strong> Maharashtra Institute of Technology - Art, Design and Technology University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10987</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an evaluation framework to compare centralized and decentralized aggregation algorithms in federated learning systems. It finds that decentralized methods (Aggregated and Continual Federated Learning) outperform centralized Hierarchical Federated Learning in metrics like precision and recall on standard datasets, highlighting the advantages of distributed computation and aggregation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Agentic Operator Generation for ML ASICs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Triton, ATen kernels, PyTorch OpInfo, JIT compilation, large language models, agentic AI, kernel generation]</li>
<li class=""><strong>authors:</strong> Alec M. Hammond, Aram Markosyan, Aman Dontula, Simon Mahns, Zacharias Fisches, Dmitrii Pedchenko, Keyur Muzumdar, Natacha Supper, Mark Saroufim, Joe Isaacson, Laura Wang, Warren Hunt, Kaustubh Gondkar, Roman Levenstein, Gabriel Synnaeve, Richard Li, Jacob Kahn, Ajit Mathews</li>
<li class=""><strong>institution:</strong> Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10977</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents TritorX, an agentic AI system that uses large language models combined with a custom linter, JIT compilation, and a PyTorch OpInfo test harness to automatically generate functionally correct Triton ATen kernels for ML accelerators like MTIA. The system prioritizes broad operator coverage over performance for a limited set, successfully generating kernels for 481 unique operators that pass over 20,000 tests. This approach enables the rapid overnight generation of complete PyTorch ATen backends for new accelerator platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [runtime parallelization, branch-aware memory management, adaptive scheduling, DAG partitioning, buffer reuse, heterogeneous inference]</li>
<li class=""><strong>authors:</strong> Chong Tang, Hao Dai, Jagmohan Chauhan</li>
<li class=""><strong>institution:</strong> University of Southampton, University College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax is a framework that accelerates mobile DNN inference by partitioning the computation graph to expose parallelism and using branch-aware memory management with adaptive scheduling to handle operator fallbacks. It reduces latency by up to 46% and energy consumption by up to 30% compared to state-of-the-art frameworks, while controlling memory overhead, without requiring model refactoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [continuous learning, camera grouping, GPU allocation, transmission control, cross-camera correlation]</li>
<li class=""><strong>authors:</strong> Yuze He, Ferdi Kossmann, Srinivasan Seshan, Peter Steenkiste</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ECCO is a video analytics framework that improves resource efficiency by grouping cameras with similar data drift patterns to share retrained models. It uses a dynamic grouping algorithm, GPU allocator, and transmission controller to reduce compute and communication costs. The system increases retraining accuracy by 6.7%-18.1% or supports 3.3x more cameras at the same accuracy compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [Snowflake algorithm, network-derived identity, private IPv4 address, bit-allocation scheme (1-41-16-6), stateless microservices, container orchestration]</li>
<li class=""><strong>authors:</strong> Manideep Reddy Chinthareddy</li>
<li class=""><strong>institution:</strong> Independent researcher (based on email domain)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11643</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a stateless, cloud-agnostic distributed ID generator that eliminates the need for explicit worker IDs by deriving node uniqueness from a container&#x27;s private IPv4 address. It introduces a modified bit-allocation scheme to incorporate this network-derived entropy while preserving monotonicity. The method demonstrates performance comparable to traditional stateful generators while offering improved scalability and operational simplicity in containerized environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC resource access], [RESTful API, performance testing, proxy-based APIs, I/O bottlenecks, architectural redesign, security, authorization]</li>
<li class=""><strong>authors:</strong> Elia Palme, Juan Pablo Dorsch, Ali Khosravi, Giovanni Pizzi, Francesco Pagnamenta, Andrea Ceriani, Eirini Koutsaniti, Rafael Sarmiento, Ivano Bonesana, Alejandro Dabin</li>
<li class=""><strong>institution:</strong> CSCS – Swiss National Supercomputing Centre, PSI Center for Scientific Computing, Theory, and Data</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11634</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents FirecREST v2, a redesigned RESTful API for programmatic access to HPC resources, focusing on integrating enhanced security and high throughput. Through systematic performance testing, the authors identified and addressed bottlenecks in proxy-based APIs, achieving a ~100x performance improvement. The key conclusion is that a ground-up architectural redesign was necessary to meet growing user demands for scalable and secure HPC resource access.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Hypergraph based Multi-Party Payment Channel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain scalability], [hypergraph, payment channel networks, multi-party channels, off-chain scaling, DAG updates]</li>
<li class=""><strong>authors:</strong> Ayush Nainwal, Atharva Kamble, Nitin Awathare</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Jodhpur; Indian Institute of Technology, Bombay</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11775" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11775</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces traditional bilateral channels with collectively funded hyperedges to enable leaderless, concurrent payments. This design addresses liquidity fragmentation and channel depletion in existing payment networks. An implementation demonstrates a high transaction success rate of approximately 94%, highlighting the robustness of the approach.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv251215] Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11179" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Motif-2-12.7B-Reasoning: A Practitioner&#x27;s Guide to RL Training Recipes <a href="https://arxiv.org/pdf/2512.11463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Rethinking Expert Trajectory Utilization in LLM Post-training <a href="https://arxiv.org/pdf/2512.11470" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] In-Context Multi-Objective Optimization <a href="https://arxiv.org/pdf/2512.11114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents <a href="https://arxiv.org/pdf/2512.11277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits <a href="https://arxiv.org/pdf/2512.11345" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance <a href="https://arxiv.org/pdf/2512.11421" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control <a href="https://arxiv.org/pdf/2512.11247" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Three methods, one problem: Classical and AI approaches to no-three-in-line <a href="https://arxiv.org/pdf/2512.11469" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound <a href="https://arxiv.org/pdf/2512.11169" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation <a href="https://arxiv.org/pdf/2512.11270" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization <a href="https://arxiv.org/pdf/2512.11391" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry <a href="https://arxiv.org/pdf/2512.11558" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Agile Flight Emerges from Multi-Agent Competitive Racing <a href="https://arxiv.org/pdf/2512.11781" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Marti-5: A Mathematical Model of &quot;Self in the World&quot; as a First Step Toward Self-Awareness <a href="https://arxiv.org/pdf/2512.10985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 6</strong></p>
<ul>
<li class="">[arXiv251215] A Scalable Multi-GPU Framework for Encrypted Large-Model Inference <a href="https://arxiv.org/pdf/2512.11269" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling <a href="https://arxiv.org/pdf/2512.11187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee <a href="https://arxiv.org/pdf/2512.11127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise <a href="https://arxiv.org/pdf/2512.11282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration <a href="https://arxiv.org/pdf/2512.11587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-16T02:41:54.000Z" itemprop="dateModified">Dec 16, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251208-20251214"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251208-20251214</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-15" class="table-of-contents__link toc-highlight">2025-12-15</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 jokebear666, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>