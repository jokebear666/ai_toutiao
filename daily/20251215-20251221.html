<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 | AI头条"><meta data-rh="true" name="description" content="2025-12-15"><meta data-rh="true" property="og:description" content="2025-12-15"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251215-20251221","item":"https://jokebear666.github.io/ai_toutiao/daily/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.38d649e8.css">
<script src="/ai_toutiao/assets/js/runtime~main.fd2ba062.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.919db729.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-15">2025-12-15<a href="#2025-12-15" class="hash-link" aria-label="Direct link to 2025-12-15" title="Direct link to 2025-12-15" translate="no">​</a></h2>
<p><strong>cs.DC total: 15</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251215] Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed network algorithms], [multi-packet messaging, pruning, closeness centrality, decentralized computation, message efficiency]</li>
<li class=""><strong>authors:</strong> Patrick D. Manya, Eugene M. Mbuyi, Gothy T. Ngoie, Jordan F. Masakuna</li>
<li class=""><strong>institution:</strong> University of Kinshasa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances a distributed pruning method for closeness centrality by using multi-packet messaging to batch data into larger blocks. This reduces the number of exchanged messages and improves communication efficiency, particularly for large networks, with a manageable trade-off in local memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Seamless Transitions: A Comprehensive Review of Live Migration Technologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [virtualization], [live migration, container migration, virtual machine migration, migration techniques, migration units, infrastructure characteristics]</li>
<li class=""><strong>authors:</strong> Sima Attar-Khorasani, Lincoln Sherpa, Matthias Lieber, Siavash Ghiasvand</li>
<li class=""><strong>institution:</strong> TUD Dresden University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10979</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a comprehensive review of live migration technologies, focusing on container-based and virtual machine-based approaches. It analyzes migration techniques, units, and infrastructure, concluding that practical challenges and resource demands can sometimes outweigh the benefits, making implementation difficult to justify.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An Efficient Approach for Energy Conservation in Cloud Computing Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [task scheduling, resource utilization, fitness value, multi-criteria energy-efficient task scheduling (MCEETS), MaxUtil]</li>
<li class=""><strong>authors:</strong> Sohan Kumar Pande, Sanjaya Kumar Panda, Preeti Ranjan Sahu</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10974</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-criteria energy-efficient task scheduling (MCEETS) algorithm for cloud computing, which uses a fitness value based on CPU, disk, I/O utilization, and task processing time to improve resource utilization. Simulation results show that the proposed algorithm consumes less energy than the existing MaxUtil algorithm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [dynamic scheduling, multi-objective scheduling, hybrid priority scheduler, predictive backfill, smart batch, fragmentation reduction, job starvation, GPU utilization]</li>
<li class=""><strong>authors:</strong> Akhmadillo Mamirov</li>
<li class=""><strong>institution:</strong> The College of Wooster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces three dynamic multi-objective schedulers (Hybrid Priority, Predictive Backfill, and Smart Batch) designed to reduce fragmentation and starvation in GPU clusters. Through simulation, these schedulers significantly outperform static baselines in utilization, throughput, and fairness, demonstrating that adaptive scheduling can meaningfully improve GPU efficiency in heterogeneous AI clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An LLVM-Based Optimization Pipeline for SPDZ</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [secure multiparty computation], [LLVM, SPDZ, secret sharing, batching, GPU kernels, non-blocking scheduler]</li>
<li class=""><strong>authors:</strong> Tianye Dai, Hammurabi Mendes, Heuichan Lim</li>
<li class=""><strong>institution:</strong> Davidson College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11112" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11112</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an LLVM-based compiler pipeline for the SPDZ MPC protocol, which uses C with privacy annotations and LLVM IR to automatically batch operations and a runtime scheduler to overlap communication and computation, including GPU kernel mapping. The evaluation shows significant speedups over MP-SPDZ, indicating that leveraging LLVM with protocol-aware scheduling is effective for extracting parallelism without sacrificing usability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hybrid parallelism, data parallelism, pipeline parallelism, model partitioning, network scheduling, runtime adaptation]</li>
<li class=""><strong>authors:</strong> Jianli Jin, Ziyang Lin, Qianli Dong, Yi Chen, Jayanth Srinivasa, Myungjin Lee, Zhaowei Tan, Fan Lai</li>
<li class=""><strong>institution:</strong> UIUC, Northwestern University, University of California, Riverside, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dora is a framework that uses heterogeneity-aware model partitioning, contention-aware network scheduling, and a runtime adapter to achieve QoE-aware hybrid parallelism for distributed edge AI. It demonstrates significant improvements in execution speed and energy efficiency while maintaining quality of experience under dynamic runtime conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [GPU-native compilation, parallel traditional compilation, neural compilation, sequence-to-sequence translation, probabilistic verification, hybrid architecture, in-VRAM iteration]</li>
<li class=""><strong>authors:</strong> Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva</li>
<li class=""><strong>institution:</strong> Institute of Information Technology, Kyrgyz State Technical University named after I. Razzakov</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11200</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes theoretical foundations for three GPU-native compilation approaches—parallel traditional, neural, and hybrid—to eliminate CPU-GPU data transfers during code iteration cycles. It demonstrates that these methods can achieve 10-100x speedups by keeping compilation and execution entirely in GPU memory. The main conclusion is that GPU-native compilation offers a path to drastically reduce latency and energy consumption in AI code generation systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [cluster scheduling, disaggregated architecture, co-execution group, two-tier scheduling, round-robin, warm-start context switching]</li>
<li class=""><strong>authors:</strong> Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, UIUC, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RollMux, a cluster scheduling framework that improves efficiency in disaggregated RL post-training by multiplexing jobs to utilize idle phases. Its core method involves a two-tier scheduler and a co-execution group abstraction to orchestrate cross-cluster execution. The evaluation shows RollMux significantly improves cost efficiency over standard disaggregated and co-located baselines while maintaining service-level objectives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical federated learning, aggregated federated learning, continual federated learning, decentralized aggregation, gossip-based protocols]</li>
<li class=""><strong>authors:</strong> Sumit Chongder</li>
<li class=""><strong>institution:</strong> Maharashtra Institute of Technology - Art, Design and Technology University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10987</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an evaluation framework to compare centralized and decentralized aggregation algorithms in federated learning systems. It finds that decentralized methods (Aggregated and Continual Federated Learning) outperform centralized Hierarchical Federated Learning in metrics like precision and recall on standard datasets, highlighting the advantages of distributed computation and aggregation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Agentic Operator Generation for ML ASICs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Triton, ATen kernels, PyTorch OpInfo, JIT compilation, large language models, agentic AI, kernel generation]</li>
<li class=""><strong>authors:</strong> Alec M. Hammond, Aram Markosyan, Aman Dontula, Simon Mahns, Zacharias Fisches, Dmitrii Pedchenko, Keyur Muzumdar, Natacha Supper, Mark Saroufim, Joe Isaacson, Laura Wang, Warren Hunt, Kaustubh Gondkar, Roman Levenstein, Gabriel Synnaeve, Richard Li, Jacob Kahn, Ajit Mathews</li>
<li class=""><strong>institution:</strong> Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10977</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents TritorX, an agentic AI system that uses large language models combined with a custom linter, JIT compilation, and a PyTorch OpInfo test harness to automatically generate functionally correct Triton ATen kernels for ML accelerators like MTIA. The system prioritizes broad operator coverage over performance for a limited set, successfully generating kernels for 481 unique operators that pass over 20,000 tests. This approach enables the rapid overnight generation of complete PyTorch ATen backends for new accelerator platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [runtime parallelization, branch-aware memory management, adaptive scheduling, DAG partitioning, buffer reuse, heterogeneous inference]</li>
<li class=""><strong>authors:</strong> Chong Tang, Hao Dai, Jagmohan Chauhan</li>
<li class=""><strong>institution:</strong> University of Southampton, University College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax is a framework that accelerates mobile DNN inference by partitioning the computation graph to expose parallelism and using branch-aware memory management with adaptive scheduling to handle operator fallbacks. It reduces latency by up to 46% and energy consumption by up to 30% compared to state-of-the-art frameworks, while controlling memory overhead, without requiring model refactoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [continuous learning, camera grouping, GPU allocation, transmission control, cross-camera correlation]</li>
<li class=""><strong>authors:</strong> Yuze He, Ferdi Kossmann, Srinivasan Seshan, Peter Steenkiste</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ECCO is a video analytics framework that improves resource efficiency by grouping cameras with similar data drift patterns to share retrained models. It uses a dynamic grouping algorithm, GPU allocator, and transmission controller to reduce compute and communication costs. The system increases retraining accuracy by 6.7%-18.1% or supports 3.3x more cameras at the same accuracy compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [Snowflake algorithm, network-derived identity, private IPv4 address, bit-allocation scheme (1-41-16-6), stateless microservices, container orchestration]</li>
<li class=""><strong>authors:</strong> Manideep Reddy Chinthareddy</li>
<li class=""><strong>institution:</strong> Independent researcher (based on email domain)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11643</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a stateless, cloud-agnostic distributed ID generator that eliminates the need for explicit worker IDs by deriving node uniqueness from a container&#x27;s private IPv4 address. It introduces a modified bit-allocation scheme to incorporate this network-derived entropy while preserving monotonicity. The method demonstrates performance comparable to traditional stateful generators while offering improved scalability and operational simplicity in containerized environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC resource access], [RESTful API, performance testing, proxy-based APIs, I/O bottlenecks, architectural redesign, security, authorization]</li>
<li class=""><strong>authors:</strong> Elia Palme, Juan Pablo Dorsch, Ali Khosravi, Giovanni Pizzi, Francesco Pagnamenta, Andrea Ceriani, Eirini Koutsaniti, Rafael Sarmiento, Ivano Bonesana, Alejandro Dabin</li>
<li class=""><strong>institution:</strong> CSCS – Swiss National Supercomputing Centre, PSI Center for Scientific Computing, Theory, and Data</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11634</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents FirecREST v2, a redesigned RESTful API for programmatic access to HPC resources, focusing on integrating enhanced security and high throughput. Through systematic performance testing, the authors identified and addressed bottlenecks in proxy-based APIs, achieving a ~100x performance improvement. The key conclusion is that a ground-up architectural redesign was necessary to meet growing user demands for scalable and secure HPC resource access.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Hypergraph based Multi-Party Payment Channel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain scalability], [hypergraph, payment channel networks, multi-party channels, off-chain scaling, DAG updates]</li>
<li class=""><strong>authors:</strong> Ayush Nainwal, Atharva Kamble, Nitin Awathare</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Jodhpur; Indian Institute of Technology, Bombay</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11775" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11775</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces traditional bilateral channels with collectively funded hyperedges to enable leaderless, concurrent payments. This design addresses liquidity fragmentation and channel depletion in existing payment networks. An implementation demonstrates a high transaction success rate of approximately 94%, highlighting the robustness of the approach.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv251215] Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11179" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Motif-2-12.7B-Reasoning: A Practitioner&#x27;s Guide to RL Training Recipes <a href="https://arxiv.org/pdf/2512.11463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Rethinking Expert Trajectory Utilization in LLM Post-training <a href="https://arxiv.org/pdf/2512.11470" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] In-Context Multi-Objective Optimization <a href="https://arxiv.org/pdf/2512.11114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents <a href="https://arxiv.org/pdf/2512.11277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits <a href="https://arxiv.org/pdf/2512.11345" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance <a href="https://arxiv.org/pdf/2512.11421" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control <a href="https://arxiv.org/pdf/2512.11247" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Three methods, one problem: Classical and AI approaches to no-three-in-line <a href="https://arxiv.org/pdf/2512.11469" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound <a href="https://arxiv.org/pdf/2512.11169" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation <a href="https://arxiv.org/pdf/2512.11270" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization <a href="https://arxiv.org/pdf/2512.11391" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry <a href="https://arxiv.org/pdf/2512.11558" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Agile Flight Emerges from Multi-Agent Competitive Racing <a href="https://arxiv.org/pdf/2512.11781" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Marti-5: A Mathematical Model of &quot;Self in the World&quot; as a First Step Toward Self-Awareness <a href="https://arxiv.org/pdf/2512.10985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 6</strong></p>
<ul>
<li class="">[arXiv251215] A Scalable Multi-GPU Framework for Encrypted Large-Model Inference <a href="https://arxiv.org/pdf/2512.11269" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling <a href="https://arxiv.org/pdf/2512.11187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee <a href="https://arxiv.org/pdf/2512.11127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise <a href="https://arxiv.org/pdf/2512.11282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration <a href="https://arxiv.org/pdf/2512.11587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<p><strong>cs.DC total: 3</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Shapley-CMI, Private Set Intersection, Conditional Mutual Information, Vertical Federated Learning, data valuation]</li>
<li class=""><strong>authors:</strong> Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza</li>
<li class=""><strong>institution:</strong> Mondragon Unibertsitatea</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14767</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach&#x27;s correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [8-bit training, activation checkpointing, offloading, copy-engine collectives, dynamic tensor-level scaling, ZeRO-1]</li>
<li class=""><strong>authors:</strong> Erik Schultheis, Dan Alistarh</li>
<li class=""><strong>institution:</strong> IST Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Dynamic Rebatching for Efficient Early-Exit Inference with DREX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [early-exit, dynamic rebatching, copy-free buffer, SLA-aware scheduler, KV cache, state-copying]</li>
<li class=""><strong>authors:</strong> Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 19</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251218] A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Latent Class Reinforcement Learning (LCRL), Variational Bayes, Bayesian estimation, discrete choice models]</li>
<li class=""><strong>authors:</strong> Georges Sfeir, Stephane Hess, Thomas O. Hancock, Filipe Rodrigues, Jamal Amani Rad, Michiel Bliemer, Matthew Beck, Fayyaz Khan</li>
<li class=""><strong>institution:</strong> University of Leeds, Technical University of Denmark, University of Sydney, Al Yamamah University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Latent Class Reinforcement Learning (LCRL) model, estimated using Variational Bayes, to capture how travelers adapt their preferences through experience. The application to a driving simulator dataset reveals three distinct behavioral classes, showing heterogeneity in how individuals balance exploration and exploitation in their travel decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, entanglement, interference, offline reinforcement learning, Decision Transformer]</li>
<li class=""><strong>authors:</strong> Abraham Itzhak Weinberg</li>
<li class=""><strong>institution:</strong> AI-WEINBERG, AI Experts</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Quantum Decision Transformer (QDT), a novel architecture that integrates quantum-inspired attention with entanglement and quantum feedforward networks with interference to improve offline reinforcement learning. It demonstrates a dramatic performance improvement over standard Decision Transformers and shows that the synergy between its quantum-inspired components is crucial for its success.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [machine learning theory], [Bregman projection, entropy reservoir, information geometry, model collapse, self-referential learning]</li>
<li class=""><strong>authors:</strong> Jingwei Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14879" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14879</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Entropy-Reservoir Bregman Projection (ERBP) framework, which models self-referential learning as a stochastic Bregman projection sequence in distribution space. It shows that injecting an entropy reservoir stabilizes the dynamics and prevents model collapse, unifying various empirical fixes into a single quantitative design rule.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Puzzle Curriculum GRPO for Vision-Centric Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [PC-GRPO, RL with Verifiable Rewards, self-supervised puzzle environments, difficulty-aware curriculum, reasoning-answer consistency]</li>
<li class=""><strong>authors:</strong> Ahmadreza Jeddi, Hakki Can Karaimer, Hue Nguyen, Zhongling Wang, Ke Zhao, Javad Rajabi, Ran Zhang, Raghav Goyal, Babak Taati, Radek Grzeszczuk</li>
<li class=""><strong>institution:</strong> Samsung Electronics, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14944</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Puzzle Curriculum GRPO (PC-GRPO), a supervision-free reinforcement learning method that uses self-supervised puzzle environments and a difficulty-aware curriculum to improve visual reasoning in vision language models. It enhances reasoning quality, training stability, and downstream accuracy without relying on annotations or external verifiers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [adaptive partitioning, model-based algorithm, diffusion processes, regret bounds, zooming dimension]</li>
<li class=""><strong>authors:</strong> Hanqing Jin, Renyuan Xu, Yanzhao Yang</li>
<li class=""><strong>institution:</strong> University of Oxford, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14991</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a model-based reinforcement learning algorithm that adaptively partitions the state-action space for controlled diffusion processes with unbounded continuous states. It refines partitions based on estimation bias versus statistical confidence, achieving regret bounds that extend to unbounded domains. The approach is validated through numerical experiments, including high-dimensional portfolio selection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Spectral Representation-based Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [spectral representations, spectral decomposition, transition operator, partially observable MDPs, model-free, model-based]</li>
<li class=""><strong>authors:</strong> Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai</li>
<li class=""><strong>institution:</strong> Georgia Tech, Harvard University, Google DeepMind, University of Alberta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces spectral representations, derived from the spectral decomposition of the transition operator, as a framework for reinforcement learning to address issues like theoretical ambiguity and optimization instability. It shows how to construct these representations for different system structures and extends the approach to partially observable environments. The proposed algorithms achieve performance comparable to or better than state-of-the-art methods on over 20 challenging control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [cognitive-inspired elastic reasoning, markov decision process, reinforcement learning, chain-of-thought, tool-assisted reasoning]</li>
<li class=""><strong>authors:</strong> Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo Chen, Bin Xiao, Yuanqing Li, Mingkui Tan</li>
<li class=""><strong>institution:</strong> South China University of Technology, Pazhou Laboratory, Peng Cheng Laboratory, Chongqing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15089" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15089</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CogER, a framework that dynamically selects reasoning strategies for LLMs based on query complexity, modeled as a Markov Decision Process and trained with reinforcement learning. It introduces Cognitive Tool-Assisted Reasoning for autonomous tool use within reasoning chains. Experiments show CogER outperforms state-of-the-art methods, improving exact match scores by at least 13% on in-domain and 8% on out-of-domain tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automatic Reward Shaping from Multi-Objective Human Heuristics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reward shaping, multi-objective optimization, bi-level optimization, stochastic exploration]</li>
<li class=""><strong>authors:</strong> Yuqing Xie, Jiayu Chen, Wenhao Tang, Ya Zhang, Chao Yu, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15120</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MORSE, a framework that automatically combines multiple human-designed heuristic rewards into a unified reward function using bi-level optimization with stochastic exploration. It effectively balances conflicting objectives in robotic tasks, achieving performance comparable to manually tuned rewards in MuJoCo and Isaac Sim environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [BEV-grounding, Chain-of-Thought, reinforcement learning, determinantal point process, keyframe selection, spatial reward, pose querying]</li>
<li class=""><strong>authors:</strong> Jiaxu Wan, Xu Wang, Mengwei Xie, Hang Zhang, Mu Xu, Yang Han, Hong Zhang, Ding Yuan, Yifan Yang</li>
<li class=""><strong>institution:</strong> BUAA (Beihang University)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15160</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EagleVision introduces a dual-stage framework for spatial reasoning, combining a macro perception stage for selecting keyframes and a micro verification stage that uses BEV-grounded pose querying and reinforcement learning. It achieves state-of-the-art performance on VSI-Bench, demonstrating strong and generalizable spatial understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [reinforcement learning with verifiable rewards, progressive prefix-token policy optimization, beginning lock-in effect, prefix optimization, continuation accumulated reward]</li>
<li class=""><strong>authors:</strong> Yiliu Sun, Zicheng Zhao, Yang Wei, Yanfang Zhang, Chen Gong</li>
<li class=""><strong>institution:</strong> Nanjing University of Science and Technology, North University of China, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15274" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15274</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Progressive Prefix-token Policy Optimization (PPPO), a reinforcement learning method that focuses on optimizing the initial prefix tokens of an LLM&#x27;s reasoning output, based on the identified Beginning Lock-in Effect. It introduces strategies like Progressive Prefix Retention and Continuation Accumulated Reward to improve training efficiency. The method achieves significant accuracy improvements on reasoning tasks while using far fewer training tokens compared to standard approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [formal methods], [reinforcement learning, graph neural networks, labeled transition system, exploration policy]</li>
<li class=""><strong>authors:</strong> Toshihide Ubukata, Enhong Mu, Takuto Yamauchi, Mingyue Zhang, Jialong Li, Kenji Tei</li>
<li class=""><strong>institution:</strong> Waseda University, Southwest University, Institute of Science Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15295</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GCRL, a method that enhances reinforcement learning for controller synthesis by integrating Graph Neural Networks to encode exploration history into a graph for broader context. It demonstrates superior learning efficiency and generalization compared to state-of-the-art methods in most benchmark domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bayesian reinforcement learning, epistemic uncertainty, minimax-optimal regret, sample complexity, infinite-horizon MDPs]</li>
<li class=""><strong>authors:</strong> Jianfei Ma, Wee Sun Lee</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15405</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EUBRL, a Bayesian reinforcement learning algorithm that uses epistemic uncertainty to guide exploration and reduce per-step regret. It provides theoretical guarantees for regret and sample complexity and demonstrates superior sample efficiency and scalability in sparse-reward, long-horizon tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [feature model-based reinforcement learning, actor-critic, Dyna-Q, model-based RL, model-free RL, multi-task control]</li>
<li class=""><strong>authors:</strong> Quanxi Zhou, Wencan Mao, Manabu Tsukada, John C.S. Lui, Yusheng Ji</li>
<li class=""><strong>institution:</strong> The University of Tokyo, National Institute of Informatics, The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15430</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FM-EAC, a feature model-based enhanced actor-critic algorithm that integrates planning, acting, and learning for multi-task control. It combines model-based and model-free reinforcement learning to improve generalizability across tasks. Simulations show it outperforms state-of-the-art methods in urban and agricultural applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Double Horizon Model-Based Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [model-based reinforcement learning, policy optimization, distribution rollout, training rollout, double horizon]</li>
<li class=""><strong>authors:</strong> Akihiro Kubo, Paavo Parmas, Shin Ishii</li>
<li class=""><strong>institution:</strong> Advanced Telecommunications Research Institute, Kyoto University, The University of Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15439</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Double Horizon Model-Based Policy Optimization (DHMBPO), a method that separates the model rollout process into a long &quot;distribution rollout&quot; to mitigate distribution shift and a short &quot;training rollout&quot; for stable gradient estimation. This approach balances model bias and gradient variance. The method demonstrates superior sample efficiency and runtime compared to existing MBRL methods on continuous-control benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [language models], [autoregressive models, energy-based models, soft Bellman equation, maximum entropy reinforcement learning, distillation]</li>
<li class=""><strong>authors:</strong> Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet</li>
<li class=""><strong>institution:</strong> Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes a formal bijection between autoregressive language models (ARMs) and energy-based models (EBMs) in function space, showing this equivalence corresponds to a special case of the soft Bellman equation in maximum entropy RL. It demonstrates the equivalence of their supervised learning and provides theoretical error bounds for distilling an EBM into an ARM. The results explain how ARMs, despite being trained on next-token prediction, can implicitly perform lookahead planning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reasoning and self-critique], [stepwise think-critique, reinforcement learning, self-critique, chain of thought, process reward models]</li>
<li class=""><strong>authors:</strong> Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan LU</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Microsoft Research Asia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15662</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single large language model, trained using a hybrid reinforcement learning objective. Experiments on mathematical reasoning benchmarks show that STC enhances critical thinking capabilities and produces more interpretable reasoning traces.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient-guided reinforcement learning, G2RL, PPO, KL control, final-layer sensitivity]</li>
<li class=""><strong>authors:</strong> Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu</li>
<li class=""><strong>institution:</strong> Tencent AI Lab, University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces G2RL, a gradient-guided reinforcement learning framework that uses the model&#x27;s own gradient directions to guide exploration, rather than external heuristics like entropy bonuses. It shows that G2RL improves reasoning performance across multiple benchmarks by encouraging diverse and orthogonal update directions. The results indicate that a policy&#x27;s internal update geometry provides a more effective basis for exploration in LLM reinforcement learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical deep reinforcement learning, double deep Q-network, constrained soft actor-critic, Lagrangian multipliers, centralized training and decentralized execution]</li>
<li class=""><strong>authors:</strong> Jiayang Wan, Ke He, Yafei Wang, Fan Liu, Wenjin Wang, Shi Jin</li>
<li class=""><strong>institution:</strong> Southeast University, Purple Mountain Laboratories, University of Luxembourg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15119" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15119</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-level hierarchical deep reinforcement learning framework for joint link selection and trajectory optimization in UAV mobility management within SAGIN. The method uses a top-level DDQN for discrete link selection and a lower-level constrained SAC with Lagrangian multipliers for continuous trajectory optimization under QoS constraints. Simulation results show the proposed scheme outperforms benchmarks in throughput, link switching frequency, and QoS satisfaction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, deep learning surrogate model, digital twin, autonomous control, pressure control]</li>
<li class=""><strong>authors:</strong> Guillermo Rodriguez-Llorente, Galo Gallardo, Rodrigo Morant Navascués, Nikita Khvatkin Petrovsky, Anderson Sabogal, Roberto Gómez-Espinosa Martín</li>
<li class=""><strong>institution:</strong> HI Iberia, Universidad Carlos III de Madrid, IFMIF-DONES Spain</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15521</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a fully data-driven approach for autonomous pressure control in a particle accelerator prototype. It uses a deep learning surrogate model as a digital twin to emulate system dynamics and trains a deep reinforcement learning agent within this simulation. The agent successfully learns a control policy to maintain pressure within strict operational limits, advancing intelligent autonomous control for next-generation accelerator facilities.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251218] A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [graph neural network, k-nearest neighbor, random forest, hyperparameter optimization, feature selection]</li>
<li class=""><strong>authors:</strong> Mahmuda Sharmin, Taihao Han, Jie Huang, Narayanan Neithalath, Gaurav Sant, Aditya Kumar</li>
<li class=""><strong>institution:</strong> Missouri University of Science and Technology, Arizona State University, University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14855" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14855</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for applying Graph Neural Networks (GNNs) to numerical data in cementitious materials research by converting tabular data into graph representations using the k-nearest neighbor approach. The study systematically optimizes model hyperparameters and feature selection, finding that the GNN&#x27;s performance is comparable to a benchmark random forest model. The work establishes a foundational roadmap for transitioning to advanced, multi-modal, and physics-informed AI architectures for material design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network activation functions], [Fourier Analysis Network, Dual-Activation Layer, sine activation, gradient analysis, dying-ReLU problem]</li>
<li class=""><strong>authors:</strong> Sam Jeong, Hae Yong Kim</li>
<li class=""><strong>institution:</strong> University of São Paulo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14873" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14873</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the mechanism of the Fourier Analysis Network (FAN) and finds its benefit stems from the sine function&#x27;s non-zero derivative near zero, which mitigates vanishing gradients and the dying-ReLU problem. Based on this insight, the authors propose a new Dual-Activation Layer (DAL) that accelerates convergence and achieves equal or higher accuracy than conventional activations in evaluated tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PIP<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Net: Physics-informed Partition Penalty Deep Operator Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [operator learning, DeepONet, partition-of-unity, physics-informed neural networks, regularization, Burgers equation, Allen-Cahn equation]</li>
<li class=""><strong>authors:</strong> Hongjin Mi, Huiqiang Lun, Changhong Mou, Yeyu Zhang</li>
<li class=""><strong>institution:</strong> Shanghai University of Finance and Economics, York University, Utah State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PIP² Net, a physics-informed operator learning network that introduces a simplified partition penalty based on partition-of-unity methods to regularize and stabilize trunk-network features in DeepONet. The method is evaluated on nonlinear PDEs like Burgers and Allen-Cahn equations, where it consistently outperforms baseline models in prediction accuracy and robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational catalysis], [deep generative model, periodic Brownian-bridge, equivariant graph neural network, DFT relaxation, adsorption energy, outlier detection]</li>
<li class=""><strong>authors:</strong> Songze Huo, Xiao-Ming Cao</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15228" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15228</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DBCata, a deep generative model that combines a periodic Brownian-bridge framework with an equivariant graph neural network to directly generate equilibrium adsorption structures from unrelaxed ones, bypassing explicit energy or force calculations. It achieves high-fidelity geometry generation and improves DFT accuracy, enabling accelerated high-throughput screening for catalysts like those in the oxygen reduction reaction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [long-context fine-tuning, sequential bucketed strategy, tool-integrated reasoning, multi-mode supervision, dataset distillation]</li>
<li class=""><strong>authors:</strong> Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15489" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15489</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Nemotron-Math, a large-scale mathematical reasoning dataset generated using the multi-mode capabilities of gpt-oss-120b, featuring diverse reasoning styles and Python tool integration. The authors also propose a sequential bucketed strategy to accelerate long-context fine-tuning. The dataset enables state-of-the-art performance on mathematical benchmarks, achieving 100% accuracy on AIME 2024/2025 with tool-integrated reasoning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [decision theory], [restless multi-armed bandits, Whittle index, Markov chains, Thompson Sampling, Bayesian UCB, epsilon-Greedy]</li>
<li class=""><strong>authors:</strong> Jaume Anguera Peris, Songtao Cheng, Hanzhao Zhang, Wei Ouyang, Joakim Jaldén</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, SciLifeLab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14930" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14930</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Restless Multi-Process Multi-Armed Bandit (RMPMAB) framework, which models experimental regions as ensembles of Markov chains to capture biological heterogeneity. It derives closed-form expressions for process behavior and designs scalable Whittle index policies. The method significantly outperforms existing bandit algorithms in simulations and live-cell imaging, capturing more biological events and reducing regret, demonstrating its potential for autonomous smart microscopy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, deep learning surrogate model, digital twin, autonomous control, pressure control]</li>
<li class=""><strong>authors:</strong> Guillermo Rodriguez-Llorente, Galo Gallardo, Rodrigo Morant Navascués, Nikita Khvatkin Petrovsky, Anderson Sabogal, Roberto Gómez-Espinosa Martín</li>
<li class=""><strong>institution:</strong> HI Iberia, Universidad Carlos III de Madrid, IFMIF-DONES Spain</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15521</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a fully data-driven approach for autonomous pressure control in a particle accelerator prototype. It uses a deep learning surrogate model as a digital twin to emulate system dynamics and trains a deep reinforcement learning agent within this simulation. The agent successfully learns a control policy to maintain pressure within strict operational limits, advancing intelligent autonomous control for next-generation accelerator facilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Photonics-Enhanced Graph Convolutional Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [photonic positional embeddings, graph convolutional networks, synthetic frequency lattices, intensity correlation matrices, hybrid photonic-electronic workflow]</li>
<li class=""><strong>authors:</strong> Yuan Wang, Oleksandr Kyriienko</li>
<li class=""><strong>institution:</strong> University of Sheffield</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15549" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15549</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid workflow that enhances Graph Convolutional Networks (GCNs) by using photonic positional embeddings derived from simulating light propagation on synthetic frequency lattices that match the input graph structure. The method generates internode intensity correlation matrices to provide global structural information to the GCN. The results show that these photonic embeddings outperform baseline Laplacian-based embeddings on molecular graph benchmarks, improving regression and classification performance, and support the potential for optical acceleration in graph machine learning.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-19">2025-12-19<a href="#2025-12-19" class="hash-link" aria-label="Direct link to 2025-12-19" title="Direct link to 2025-12-19" translate="no">​</a></h2>
<p><strong>cs.DC total: 9</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251219] SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [vector quantization, spline networks, memory optimization, cache optimization, hardware-aware compilation]</li>
<li class=""><strong>authors:</strong> Jeff Smith</li>
<li class=""><strong>institution:</strong> 2nd Set AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Coupled with a hardware-aware compiler called LUTHAM, it achieves an 88x reduction in runtime memory while matching baseline accuracy, effectively decoupling the workload from DRAM bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]</li>
<li class=""><strong>authors:</strong> Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai</li>
<li class=""><strong>institution:</strong> Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative tool calls, tool cache, vLLM, prefix-caching]</li>
<li class=""><strong>authors:</strong> Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon</li>
<li class=""><strong>institution:</strong> Lawrence Livermore National Laboratory, University of Maryland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new &quot;tool cache&quot; API to facilitate adoption of these optimizations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [staggered batch scheduling, load-aware global allocation, DP+EP, time-to-first-token, throughput, data parallelism, expert parallelism]</li>
<li class=""><strong>authors:</strong> Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen</li>
<li class=""><strong>institution:</strong> Baidu Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16134</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Staggered Batch Scheduling (SBS), a method that buffers requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. It also introduces a Load-Aware Global Allocation strategy to balance computational load. The system reduces Time-to-First-Token by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sparse attention, dynamic programming, top-k selection, anchor layers, reuse layers, FlashAttention-3]</li>
<li class=""><strong>authors:</strong> Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee</li>
<li class=""><strong>institution:</strong> Microsoft Research India</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16391</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated cloud platform, service catalogue, interactive development environments, GPU resources, annotation tools, experiment tracking, federated learning, model deployment, traceability, reproducibility]</li>
<li class=""><strong>authors:</strong> Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez</li>
<li class=""><strong>institution:</strong> Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16455</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform integrates distributed e-Infrastructures to provide consistent access to development environments, GPU training, annotation tools, and deployment options. Its main conclusion is that this integrated, customizable platform lowers adoption barriers and facilitates reproducible AI workflows for the scientific community.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [mixed integer linear programming, heuristic algorithm, task offloading, server deployment, budget constraint]</li>
<li class=""><strong>authors:</strong> Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham</li>
<li class=""><strong>institution:</strong> Sebelas Maret University, Curtin University, University of Wollongong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16792</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework, solved via an optimal Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). The main conclusion is that the heuristic solution performs close to optimal for small networks and significantly outperforms alternative strategies in large-scale networks, improving task satisfaction by up to 21.57% under budget and demand growth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, QMIX, reactive jamming, channel hopping, power control, Upper Confidence Bound]</li>
<li class=""><strong>authors:</strong> Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella</li>
<li class=""><strong>institution:</strong> Nexcepta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16813</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, immunofluorescence microscopy, collagen VI-related dystrophies, rare disease diagnosis, decentralized training]</li>
<li class=""><strong>authors:</strong> Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 27</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251219] DSO: Direct Steering Optimization for Bias Mitigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [fairness and bias mitigation], [activation steering, reinforcement learning, linear transformations, inference-time control]</li>
<li class=""><strong>authors:</strong> Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff</li>
<li class=""><strong>institution:</strong> Apple, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15926</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [supervised fine-tuning, tool calling, small language models, OPT-350M, TRL, ToolBench]</li>
<li class=""><strong>authors:</strong> Polaris Jhandi, Owais Kazi, Shreyas Subramanian, Neel Sendas</li>
<li class=""><strong>institution:</strong> Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15943</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper fine-tunes a small language model (OPT-350M) using supervised fine-tuning to perform agentic tool-calling tasks. The results show that this targeted fine-tuning enables the small model to significantly outperform larger baseline models like ChatGPT on the ToolBench evaluation, demonstrating that SLMs can be a cost-effective alternative for specific production workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [reinforcement learning, low-rank approximation, dynamic rank selection, matrix perturbation theory, singular value decomposition]</li>
<li class=""><strong>authors:</strong> Caner Erden</li>
<li class=""><strong>institution:</strong> Sakarya University of Applied Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15973" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15973</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Dynamic Rank Reinforcement Learning (DR-RL), a framework that uses a reinforcement learning agent to dynamically select low-rank approximations for Multi-Head Self-Attention in LLMs during inference, balancing accuracy and computational cost. It employs online matrix perturbation theory for efficient updates. Experiments show the method maintains accuracy equivalent to full-rank attention while significantly reducing FLOPs, especially for long sequences.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, Gaussian processes, multi-layer perceptrons, surrogate models, levelized cost of electricity, techno-economic analysis]</li>
<li class=""><strong>authors:</strong> Paul Seurin, Dean Price, Luis Nunez</li>
<li class=""><strong>institution:</strong> Idaho National Laboratory, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16032" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16032</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a reinforcement learning-based optimization framework using surrogate models (Gaussian processes and multi-layer perceptrons) to minimize the levelized cost of electricity for a heat-pipe microreactor design. The method successfully reduces costs by over 57% by adjusting design parameters while satisfying key physical and safety constraints. The main cost drivers identified are operation and maintenance costs, capital costs, and specifically the costs of axial reflectors and control drum materials.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, group relative policy optimization, margin-based cosine similarity, semantic-driven reinforcement learning, large vision-language model]</li>
<li class=""><strong>authors:</strong> Pengyu Wang, Shuchang Ye, Usman Naseem, Jinman Kim</li>
<li class=""><strong>institution:</strong> The University of Sydney, Macquarie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16145" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16145</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a semantic-driven reinforcement learning method (MRG-R1) for medical report generation, which uses a report-level reward based on clinical findings similarity and Group Relative Policy Optimization to improve clinical correctness over token-level objectives. It achieves state-of-the-art performance on benchmark datasets, demonstrating that optimizing for semantic alignment meaningfully enhances clinical accuracy in generated reports.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] INTELLECT-3: Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [mixture-of-experts, reinforcement learning, large-scale training, prime-rl, verifiers library, environments hub, SFT, RL training]</li>
<li class=""><strong>authors:</strong> Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian Müller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann</li>
<li class=""><strong>institution:</strong> Prime Intellect, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16144</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces INTELLECT-3, a 106B-parameter Mixture-of-Experts model trained using large-scale reinforcement learning on a custom end-to-end RL infrastructure stack. It achieves state-of-the-art performance for its size across multiple benchmarks by leveraging the open-source prime-rl framework and scaling training to 512 H200 GPUs. The authors open-source the model and the full training infrastructure, including the RL frameworks and environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, group-relative proximal optimization, visual alignment, radiology report generation, medical vision-language models]</li>
<li class=""><strong>authors:</strong> Sarosij Bose, Ravi K. Rajendran, Biplob Debnath, Konstantinos Karydis, Amit K. Roy-Chowdhury, Srimat Chakradhar</li>
<li class=""><strong>institution:</strong> NEC Laboratories America, University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16201" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16201</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VALOR, a reinforcement learning-based framework using Group-Relative Proximal Optimization to align medical vision-language models. It improves report generation by first enhancing clinical terminology and then aligning visual features with disease findings. The method achieves better factual accuracy and visual grounding compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hypernetworks That Evolve Themselves</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neuroevolution], [hypernetworks, graph hypernetworks, self-referential systems, evolutionary algorithms, adaptive mutation rates]</li>
<li class=""><strong>authors:</strong> Joachim Winther Pedersen, Erwan Plantec, Eleni Nisioti, Marcello Barylli, Milton Montero, Kathrin Korte, Sebastian Risi</li>
<li class=""><strong>institution:</strong> IT University of Copenhagen, Sakana AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16406" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16406</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Self-Referential Graph HyperNetworks, which embed evolutionary mechanisms like mutation and inheritance directly within neural networks, enabling them to evolve autonomously without external optimizers. The method demonstrates swift adaptation to environmental shifts and emergent population dynamics in reinforcement learning benchmarks, supporting the idea that evolvability can emerge from neural self-reference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [nested dual-agent reinforcement learning, water stress factor, nitrogen stress factor, dssat simulation, hierarchical decision making]</li>
<li class=""><strong>authors:</strong> Ruifeng Xu, Liang He</li>
<li class=""><strong>institution:</strong> Xinjiang University, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16408" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16408</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Nested Dual-Agent Reinforcement Learning (NDRL) method to optimize cotton irrigation and nitrogen fertilization, using a parent agent for macro-action selection and a child agent incorporating quantified stress factors for daily dynamic control. The method, validated with DSSAT simulations using 2023-2024 field data, achieved increased simulated yield, irrigation water productivity, and nitrogen partial factor productivity compared to baselines. It provides a new approach for improving precision and sustainability in agricultural resource management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [StarCraft II battle arena, adversarial PyMARL, dual-algorithm paired adversary, multi-algorithm mixed adversary, benchmark]</li>
<li class=""><strong>authors:</strong> Yadong Li, Tong Zhang, Bo Huang, Zhen Cui</li>
<li class=""><strong>institution:</strong> Nanjing University of Science and Technology, Zaozhuang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16444</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces StarCraft II Battle Arena (SC2BA), a new multi-agent algorithm-vs-algorithm environment, and an Adversarial PyMARL (APyMARL) library to benchmark MARL algorithms in an adversary paradigm. The extensive experiments reveal thought-provoking observations about the effectivity, sensibility, and scalability of classic MARL algorithms when pitted against each other.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [reinforcement learning, perception-reasoning cascade, self-consistent reasoning, ROUGE-1, human annotations]</li>
<li class=""><strong>authors:</strong> Yuan Li, Yahan Yu, Youyuan Lin, Yong-Hao Yang, Chenhui Chu, Shin&#x27;ya Nishida</li>
<li class=""><strong>institution:</strong> Kyoto University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16484" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16484</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to make blind image quality assessment (BIQA) models more human-like by using reinforcement learning guided by human annotations to align the model&#x27;s perception-reasoning process. The approach introduces a reward to encourage self-consistent reasoning from self-generated descriptions. The results show the model achieves competitive score prediction and significantly improves alignment with human reasoning chains, as measured by ROUGE-1.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ParamExplorer: A framework for exploring parameters in generative art</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative art], [reinforcement learning, parameter exploration, human-in-the-loop, p5.js]</li>
<li class=""><strong>authors:</strong> Julien Gachadoat, Guillaume Lagarde</li>
<li class=""><strong>institution:</strong> University of Bordeaux</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16529" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16529</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ParamExplorer, an interactive and modular framework inspired by reinforcement learning to help explore the high-dimensional parameter spaces of generative art algorithms. It allows for exploration guided by human feedback and integrates with existing p5.js projects. The framework implements and evaluates several automated exploration strategies, referred to as agents, to discover aesthetically compelling outputs more efficiently than manual trial-and-error.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [preference optimization], [Stackelberg game, sequential-move game, inference-time refinement, preference models, Nash equilibrium, Bradley-Terry model]</li>
<li class=""><strong>authors:</strong> Barna Pásztor, Thomas Kleine Buening, Andreas Krause</li>
<li class=""><strong>institution:</strong> ETH Zürich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16626</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework that frames preference optimization as a sequential-move game between a Leader and a Follower policy. It demonstrates that this approach offers advantages in consistency, data sensitivity, and robustness to intransitive preferences compared to RLHF and NLHF. Experiments on large language models show that SLHF achieves strong alignment and enables inference-time refinements that transfer across model families.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, q-learning, sentence-transformers, natural language processing, crisp-dm, flask, flutter]</li>
<li class=""><strong>authors:</strong> Wisnu Uriawan, Aria Octavian Hamza, Ade Ripaldi Nuralim, Adi Purnama, Ahmad Juaeni Yunus, Anissya Auliani Supriadi Putri</li>
<li class=""><strong>institution:</strong> UIN Sunan Gunung Djati Bandung</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16644" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16644</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements a Sharia-compliant chatbot using Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic matching on a curated Islamic QA dataset. The prototype, built with Flask and Flutter, achieved 87% semantic accuracy, demonstrating its potential as a digital consultation tool for verified Islamic knowledge.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Olaf: Bringing an Animated Character to Life in the Physical World</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, imitation rewards, thermal actuator modeling, spherical linkages, planar linkages, asymmetric leg design]</li>
<li class=""><strong>authors:</strong> David Müller, Espen Knoop, Dario Mylonopoulos, Agon Serifi, Michael A. Hopkins, Ruben Grandia, Moritz Bächer</li>
<li class=""><strong>institution:</strong> Disney Research Imagineering</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper brings the animated character Olaf to life as a physical robot using reinforcement learning guided by animation references for control. Key innovations include a compact mechanical design with hidden asymmetric legs and linkages, and a policy that incorporates actuator temperature inputs to prevent overheating. The approach successfully achieves a high level of believability for the costumed robotic character.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Meta-RL Induces Exploration in Language Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [meta-reinforcement learning], [meta-rl, cross-episode training, in-context policy adaptation, exploration, large language model agents]</li>
<li class=""><strong>authors:</strong> Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic</li>
<li class=""><strong>institution:</strong> EPFL, ETH Zurich, Idiap Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16848" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16848</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LaMer, a Meta-RL framework designed to enhance exploration in LLM-based agents through cross-episode training and in-context policy adaptation via reflection. The method enables agents to actively learn from environmental feedback without gradient updates. Experiments show LaMer outperforms RL baselines and generalizes better to novel or challenging tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [imitation learning, reinforcement learning, task decomposition, motion planning, data generation, skill policies]</li>
<li class=""><strong>authors:</strong> Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, Georgia Institute of Technology, NVIDIA Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16861" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16861</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ReinforceGen, a hybrid system for long-horizon robot manipulation that first decomposes tasks into skills trained via imitation learning on generated data, and then refines them using reinforcement learning. It achieves an 80% success rate on benchmark tasks, with fine-tuning contributing to an 89% average performance increase.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [RePlan, plan-then-execute, vision-language planner, diffusion editor, attention-region injection, GRPO-based reinforcement learning, IV-Edit benchmark]</li>
<li class=""><strong>authors:</strong> Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia</li>
<li class=""><strong>institution:</strong> Tencent AI Lab, CUHK, HKUST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16864</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RePlan, a framework for complex instruction-based image editing that uses a vision-language planner to decompose instructions and ground them to specific regions, followed by a diffusion-based editor to apply the changes. The method employs a training-free attention-region injection mechanism for precise multi-region edits and uses GRPO-based RL to improve planning. It demonstrates superior performance on complex instruction-visual tasks compared to existing baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [behavioral cloning, posterior behavioral cloning, policy pretraining, RL finetuning, generative models]</li>
<li class=""><strong>authors:</strong> Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine</li>
<li class=""><strong>institution:</strong> UC Berkeley, Stanford</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16911" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16911</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Posterior Behavioral Cloning (PostBC), a method that pretrains a policy by modeling the posterior distribution of demonstrator behavior from a dataset, rather than exactly matching it. This ensures better coverage of demonstrator actions, which serves as a more effective initialization for subsequent reinforcement learning finetuning. The method is shown to improve RL finetuning performance on robotic control benchmarks and real-world manipulation tasks compared to standard behavioral cloning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [scene graph, vision-language model, reinforcement learning, task planning, graph-then-plan, state-aware representation]</li>
<li class=""><strong>authors:</strong> Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, University of Maryland, College Park, University of Toronto</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16909</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MomaGraph, a unified scene graph representation for embodied agents that integrates spatial, functional, and part-level information, and develops a 7B vision-language model (MomaGraph-R1) trained with reinforcement learning to predict task-oriented graphs for planning. The model, evaluated on a new dataset and benchmark, achieves state-of-the-art performance among open-source models for task planning and generalizes to real-robot experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [RLVR, GRPO, policy entropy, spurious rewards, clipping bias, exploration-exploitation trade-off]</li>
<li class=""><strong>authors:</strong> Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</li>
<li class=""><strong>institution:</strong> Columbia University, The Chinese University of Hong Kong, Shenzhen, Alibaba DAMO Academy, New York University Stern School of Business</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16912" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16912</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the exploration-exploitation trade-off in Reinforcement Learning with Verifiable Rewards (RLVR) for improving LLM reasoning. It finds that spurious rewards, combined with clipping bias, reduce policy entropy to produce more confident outputs, which enhances performance, while entropy minimization alone is insufficient. The authors propose a reward-misalignment model to explain why spurious rewards can be beneficial beyond simple data contamination.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adversarial reinforcement learning, process reward models, step-level rewards, reasoning chain partitioning, joint training, discriminator]</li>
<li class=""><strong>authors:</strong> Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille</li>
<li class=""><strong>institution:</strong> Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16917</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Generative Adversarial Reasoner, a framework that jointly trains an LLM reasoner and an LLM-based discriminator using adversarial reinforcement learning to provide dense, step-level rewards for improving reasoning. This method enhances sample efficiency and reasoning quality by co-evolving the models to detect and correct process errors. It demonstrates consistent performance gains on mathematical benchmarks over standard RL post-training baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AdaTooler-V: Adaptive Tool-Use for Images and Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [AT-GRPO, reinforcement learning, adaptive tool-use, multimodal chain-of-thought, vision tools]</li>
<li class=""><strong>authors:</strong> Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue</li>
<li class=""><strong>institution:</strong> MMLab (CUHK), THU, SJTU, DB Group (CUHK), UCF, Sangfor, JMU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16918</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AdaTooler-V, a multimodal large language model that uses adaptive tool-use to decide when to invoke vision tools, reducing unnecessary inference overhead. It employs a reinforcement learning algorithm called AT-GRPO, which adjusts rewards based on a Tool Benefit Score to encourage tool-use only when beneficial. Experiments show AdaTooler-V outperforms existing methods, achieving 89.8% accuracy on the V* benchmark, surpassing models like GPT-4o and Gemini 1.5 Pro.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, model auditing, capability gap discovery, counterfactual generation]</li>
<li class=""><strong>authors:</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</li>
<li class=""><strong>institution:</strong> Google, Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16921</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AuditDM, a framework that uses reinforcement learning to fine-tune a multimodal LLM as an &quot;auditor&quot; to generate challenging questions and counterfactual images that expose model weaknesses. This automated auditing process discovers interpretable failure cases, which are then used as annotation-free data for rectification. The method successfully identifies over 20 failure types and improves model performance, enabling a smaller 3B model to surpass a larger 28B model, suggesting targeted auditing is an effective path for model improvement as data scaling yields diminishing returns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] The Red Queen&#x27;s Trap: Limits of Deep Evolution in High-Frequency Trading</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, evolutionary computation, LSTM, Transformer, genetic algorithm, high-frequency trading, multi-agent simulation]</li>
<li class=""><strong>authors:</strong> Yijia Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the failure of a hybrid trading system called &quot;Galaxy Empire,&quot; which combines deep learning (LSTM/Transformer) for perception with evolutionary algorithms for agent survival in a high-frequency cryptocurrency market. Despite promising training results, the system suffered catastrophic live performance losses due to overfitting, survivor bias, and microstructure friction. The main conclusion is that increasing model complexity without true information asymmetry leads to systemic fragility in adaptive markets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, adaptive dynamic programming, extended state observer, event-triggered control, lyapunov analysis]</li>
<li class=""><strong>authors:</strong> Ningwei Bai, Chi Pui Chan, Qichen Yin, Tengyang Gong, Yunda Yan, Zezhi Tang</li>
<li class=""><strong>institution:</strong> University College London, University of Manchester</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15735</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a control method combining reinforcement learning (via Adaptive Dynamic Programming) with an Extended State Observer for disturbance rejection and an Event-Triggered Mechanism to reduce computational updates. The approach maintains robust performance for uncertain nonlinear systems while significantly lowering the sampling and processing effort compared to standard time-triggered schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Non-Asymptotic Global Convergence of PPO-Clip</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [PPO-Clip, f-divergence regularization, Łojasiewicz inequality, non-asymptotic convergence, softmax policy]</li>
<li class=""><strong>authors:</strong> Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen</li>
<li class=""><strong>institution:</strong> Peking University, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16565" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16565</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of the deterministic actor-only PPO-Clip algorithm with f-divergence regularization under softmax parameterization. It establishes non-uniform Lipschitz smoothness and a Łojasiewicz inequality, proving non-asymptotic linear global convergence for forward KL regularization and local linear convergence for reverse KL regularization.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 18</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251219] TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [speculative decoding, reinforcement learning, knowledge distillation, transformer, temporal adaptivity]</li>
<li class=""><strong>authors:</strong> Ye Li, Jiahe Feng, Yuan Meng, Kangye Ji, Chen Tang, Xinwan Wen, Shutao Xia, Zhi Wang, Wenwu Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15773" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15773</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TS-DP, a framework that accelerates Diffusion Policy inference using speculative decoding. It employs a distilled Transformer-based drafter to generate denoising steps and an RL-based scheduler to dynamically adapt to time-varying task difficulty. The method achieves up to 4.17x faster inference with high accuracy, enabling real-time diffusion-based control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Introduction to Symbolic Regression in the Physical Sciences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [symbolic regression], [symbolic regression, automated equation discovery, effective theories, surrogate models, complexity control, feature selection]</li>
<li class=""><strong>authors:</strong> Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira, Gabriel Kronberger</li>
<li class=""><strong>institution:</strong> University of Oxford, University of Portsmouth, University of Applied Sciences Upper Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15920</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces symbolic regression (SR) as a method for discovering interpretable mathematical equations from data in the physical sciences. It reviews SR&#x27;s foundations, applications, methodological considerations, and challenges. The authors conclude that SR is a rapidly advancing and increasingly relevant tool for scientific discovery and empirical modeling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Surrogate Neural Architecture Codesign Package (SNAC-Pack)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural architecture search, FPGA deployment, hardware-aware optimization, surrogate modeling, multi-objective optimization]</li>
<li class=""><strong>authors:</strong> Jason Weitz, Dmitri Demler, Benjamin Hawks, Nhan Tran, Javier Duarte</li>
<li class=""><strong>institution:</strong> University of California San Diego, Fermi National Accelerator Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SNAC-Pack, a framework that automates neural network design for FPGA deployment by combining architecture search with a resource and latency estimator, avoiding time-intensive synthesis for each candidate. It demonstrates the framework on a physics classification task, achieving competitive accuracy and resource efficiency on an FPGA, showing the potential of hardware-aware NAS for constrained environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [SageAttention, Sparse-Linear Attention (SLA), rCM, W8A8 quantization, step distillation]</li>
<li class=""><strong>authors:</strong> Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shengshu Technology, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16093" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16093</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TurboDiffusion is a framework that accelerates video diffusion models by 100-200 times using attention acceleration (low-bit SageAttention and Sparse-Linear Attention), step distillation (rCM), and W8A8 quantization. Experiments on several models show it achieves this speedup on a single GPU while maintaining comparable video quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Science Consultant Agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Retrieval-Augmented Generation (RAG), fine-tuning, knowledge distillation, prompting, AutoML]</li>
<li class=""><strong>authors:</strong> Karthikeyan K, Philip Wu, Xin Tang, Alexandre Alves</li>
<li class=""><strong>institution:</strong> Duke University, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16171</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Science Consultant Agent, a web-based AI tool that uses structured questionnaires, literature-backed recommendations, and prototype generation to guide practitioners in selecting optimal AI modeling strategies. It aims to prevent resource misallocation by providing evidence-based guidance, moving beyond brute-force exploration or example-induced bias to accelerate development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pretrained Battery Transformer (PBT): A battery life prediction foundation model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [foundation model, transformer, mixture-of-expert layers, transfer learning, battery life prediction]</li>
<li class=""><strong>authors:</strong> Ruifeng Tan, Weixiang Hong, Jia Li, Jiaqiang Huang, Tong-Yi Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Shanghai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16334</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Pretrained Battery Transformer (PBT), a foundation model for battery life prediction that uses domain-knowledge-encoded mixture-of-expert layers. It is trained on diverse lithium-ion battery datasets and achieves state-of-the-art performance through transfer learning, establishing a pathway for universal battery lifetime prediction systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Multi-Fidelity Delayed Acceptance, Markov Chain Monte Carlo, Bayesian inverse problems, deep neural networks, multi-fidelity neural networks, hierarchical MCMC]</li>
<li class=""><strong>authors:</strong> Filippo Zacchei, Paolo Conti, Attilio Alberto Frangi, Andrea Manzoni</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, The Alan Turing Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16430</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Multi-Fidelity Delayed Acceptance MCMC scheme that uses deep neural networks to combine predictions from solvers of varying fidelity, avoiding expensive high-fidelity simulations during online inference. This method improves the accuracy of low-fidelity approximations, leading to better mixing and faster posterior sampling. The approach is validated on two benchmark inverse problems, demonstrating significant computational savings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual autoregressive models, stage-aware acceleration, semantic irrelevance, low-rank approximation, plug-and-play acceleration, next-scale prediction]</li>
<li class=""><strong>authors:</strong> Senmao Li, Kai Wang, Salman Khan, Fahad Shahbaz Khan, Jian Yang, Yaxing Wang</li>
<li class=""><strong>institution:</strong> Nankai University, City University of Hong Kong (Dongguan), Mohamed bin Zayed University of Artificial Intelligence, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16483</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StageVAR is a stage-aware acceleration framework for Visual Autoregressive (VAR) models that analyzes the generation process, identifying that early steps are critical for semantics and structure while later steps can be pruned or approximated. It introduces a plug-and-play strategy exploiting semantic irrelevance and low-rank properties in late-stage computations without extra training. The method achieves up to 3.4x speedup with minimal quality loss, outperforming existing baselines and demonstrating the effectiveness of stage-aware design for efficient image generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] XTC, A Research Platform for Optimizing AI Workload Operators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [scheduling language, code generation, performance evaluation, compiler, autotuning]</li>
<li class=""><strong>authors:</strong> Pompougnac Hugo, Guillon Christophe, Noiry Sylvain, Dutilleul Alban, Iooss Guillaume, Rastello Fabrice</li>
<li class=""><strong>institution:</strong> Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces XTC, a research platform that provides a unified scheduling language and API to decouple optimization strategies from compiler-specific code generation and measurement. This enables portable experimentation and fair performance comparison across different compiler frameworks. The main conclusion is that XTC accelerates research on AI operator optimization by providing a common, reproducible framework for scheduling and evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [display advertising], [self-supervised learning, sequential modeling, distributional pretraining, hybrid objective]</li>
<li class=""><strong>authors:</strong> Sullivan Castro, Artem Betlei, Thomas Di Martino, Nadir El Manouzi</li>
<li class=""><strong>institution:</strong> Criteo AI Lab, École Nationale des Ponts et Chaussées</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Abacus, a self-supervised pretraining method for sequential user modeling that predicts the empirical frequency distribution of user events to align with useful counting statistics. It combines this with a hybrid objective that unites distributional prediction with sequential learning. Experiments show that this approach accelerates downstream task convergence and improves AUC by up to 6.1% compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [dark channel prior, atmospheric scattering model, gamma-adaptive reconstruction, transmission approximation, fractional top-pixel averaging]</li>
<li class=""><strong>authors:</strong> Ayush Bhavsar</li>
<li class=""><strong>institution:</strong> National Institute of Technology, Raipur</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16609" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16609</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Hazedefy, a lightweight dehazing pipeline based on the Dark Channel Prior and atmospheric scattering model, optimized for real-time video on consumer hardware. It uses fast approximations like gamma-adaptive reconstruction and a stabilized atmospheric light estimator to improve speed and stability. The method achieves enhanced visibility and contrast without GPU acceleration, making it suitable for mobile and embedded applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [log-linear sparse attention, hierarchical top-k selection, hierarchical kv enrichment, gpu implementation]</li>
<li class=""><strong>authors:</strong> Yifan Zhou, Zeqi Xiao, Tianyi Wei, Shuai Yang, Xingang Pan</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16615</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Log-linear Sparse Attention (LLSA), a trainable hierarchical sparse attention mechanism that reduces both selection and attention costs from quadratic to log-linear complexity for Diffusion Transformers. It uses hierarchical Top-K selection and KV enrichment to preserve global context efficiently. LLSA accelerates DiT training and inference significantly while maintaining generation quality, offering a scalable solution for long-sequence visual generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion models, vision foundation models, semantic compressor, representation alignment, SiT backbone, global-local-latent joint modeling]</li>
<li class=""><strong>authors:</strong> Giorgos Petsangourakis, Christos Sgouropoulos, Bill Psomas, Theodoros Giannakopoulos, Giorgos Sfikas, Ioannis Kakogeorgiou</li>
<li class=""><strong>institution:</strong> IIT, National Centre for Scientific Research “Demokritos”, University of West Attica, Czech Technical University in Prague</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16636</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces REGLUE, a latent diffusion framework that jointly models VAE latents, compressed local patch semantics, and a global image token within a single SiT backbone to enhance semantic supervision. It uses a nonlinear semantic compressor to aggregate multi-layer VFM features and an external alignment loss for regularization. The method improves FID and accelerates convergence on ImageNet, demonstrating the importance of spatial semantics and nonlinear compression in diffusion models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [layer-wise caching, semantic similarity, fingerprinting, adaptive eviction, key-value cache, transformer acceleration]</li>
<li class=""><strong>authors:</strong> Harsh Vardhan Bansal</li>
<li class=""><strong>institution:</strong> Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16843" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16843</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LLMCache, a model-agnostic layer-wise caching framework that accelerates transformer inference by reusing intermediate activations for semantically similar inputs. It uses a lightweight fingerprinting mechanism for matching and adaptive strategies for cache management. Experiments show up to 3.1x inference speedup with minimal accuracy degradation, demonstrating its practicality for real-world deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [adaptive latent prediction, normalized facial expression block, dynamic sliding-window, higher-order latent derivatives, identity-agnostic facial expression features, weighted blending]</li>
<li class=""><strong>authors:</strong> Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu</li>
<li class=""><strong>institution:</strong> Fudan University, Microsoft Research Asia, Xi’an Jiaotong University, Tencent Inc., Tongyi Lab (Alibaba Group)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16900</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashPortrait introduces an end-to-end video diffusion transformer that uses adaptive latent prediction to skip denoising steps and a normalized facial expression block to align features, achieving up to 6x faster inference for infinite-length portrait animation while preserving identity consistency. Experiments demonstrate its effectiveness in generating smooth, ID-preserving long videos both qualitatively and quantitatively.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer encoder, self-supervised learning, foundation model, edge deployment, ultra-low-power microcontroller, gesture classification, kinematic regression]</li>
<li class=""><strong>authors:</strong> Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini</li>
<li class=""><strong>institution:</strong> ETH Zurich, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15729</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces TinyMyo, a lightweight Transformer-based foundation model pre-trained with self-supervised learning for EMG signal processing. It demonstrates strong generalization across multiple tasks like gesture classification and speech recognition while being deployable on an ultra-low-power microcontroller. The work provides an open-source, efficient model for edge-based EMG applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial risk management], [Bayesian modeling, LSTM, GARCH, Value-at-Risk, logistic regression, state-space model]</li>
<li class=""><strong>authors:</strong> Sharif Al Mamun, Rakib Hossain, Md. Jobayer Rahman, Malay Kumar Devnath, Farhana Afroz, Lisan Al Amin</li>
<li class=""><strong>institution:</strong> iLynx Inc., Cognitive Links, United International University, University of Maryland Baltimore County, Washington University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15739</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Bayesian analytics framework for financial risk management, integrating models like discount-factor DLM, Bayesian logistic regression, and hierarchical Beta state-space for uncertainty quantification in volatility forecasting, fraud detection, and compliance monitoring. It shows that the framework improves risk assessment accuracy and interpretability, with GPU acceleration offering significant speedups, though challenges like sparse data remain.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Riemannian Stochastic Interpolants for Amorphous Particle Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative modeling for physics], [Riemannian stochastic interpolants, equivariant flow matching, graph neural networks, periodic boundary conditions]</li>
<li class=""><strong>authors:</strong> Louis Grenioux, Leonardo Galliano, Ludovic Berthier, Giulio Biroli, Marylou Gabrié</li>
<li class=""><strong>institution:</strong> École normale supérieure (ENS), CNRS, Sorbonne Université, Université de Paris, École polytechnique, Institut Polytechnique de Paris, Università di Trieste, ESPCI Paris</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16607" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16607</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a generative framework combining Riemannian stochastic interpolants and equivariant flow matching to sample equilibrium configurations of amorphous particle systems. The method incorporates periodic boundary conditions and particle symmetries by adapting an equivariant graph neural network to operate on a torus. The authors demonstrate that enforcing these geometric and symmetry constraints significantly improves generative performance for model glass-forming materials.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-27T11:22:39.000Z" itemprop="dateModified">Dec 27, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251208-20251214"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251208-20251214</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251222-20251228"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251222-20251228</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-15" class="table-of-contents__link toc-highlight">2025-12-15</a></li><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li><li><a href="#2025-12-19" class="table-of-contents__link toc-highlight">2025-12-19</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251215/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>