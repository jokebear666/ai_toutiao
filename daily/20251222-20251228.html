<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 | AI头条"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/20251222-20251228"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"20251222-20251228","item":"https://jokebear666.github.io/ai_toutiao/daily/20251222-20251228"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.9ae66a68.css">
<script src="/ai_toutiao/assets/js/runtime~main.0e9c9a8b.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.b0d4a715.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<p><strong>cs.DC total: 4</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP2]</li>
<li class=""><strong>authors:</strong> Kwangjun Ahn, Noah Amsel, John Langford</li>
<li class=""><strong>institution:</strong> Microsoft Research, AI Frontiers, NYU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16928</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated learning, byzantine-robust aggregation, privacy-preserving, dimensionality reduction, secure multi-party computation, adaptive tuning]</li>
<li class=""><strong>authors:</strong> Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu</li>
<li class=""><strong>institution:</strong> Nankai University, University of Louisville, University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training, Federated Learning (FL), Gossip Learning]</li>
<li class=""><strong>authors:</strong> Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas</li>
<li class=""><strong>institution:</strong> University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17352</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, FlashCodec, UnifiedServe]</li>
<li class=""><strong>authors:</strong> Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng</li>
<li class=""><strong>institution:</strong> Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17574</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 25</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), turn-level MDP, advantage estimation, multi-turn RL]</li>
<li class=""><strong>authors:</strong> Junbo Li, Peng Zhou, Rui Meng, Meet P. Vadera, Lihong Li, Yang Li</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17008" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17008</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Turn-PPO, a reinforcement learning method that applies Proximal Policy Optimization at the turn level instead of the token level for training LLM agents in multi-turn tasks. It demonstrates that this approach is more robust and effective than the commonly used GRPO method, particularly for long-horizon reasoning scenarios, as validated on the WebShop and Sokoban datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific ai evaluation], [Practical Inquiry Model (PIM), SGI-Bench, Test-Time Reinforcement Learning (TTRL), retrieval-augmented novelty, agent-based evaluation]</li>
<li class=""><strong>authors:</strong> Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu</li>
<li class=""><strong>institution:</strong> Shanghai Artificial Intelligence Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16969</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient boosting, DQN, ensemble learning, Bellman residual, non-stationary environments]</li>
<li class=""><strong>authors:</strong> Chang-Hwan Lee, Chanseung Lee</li>
<li class=""><strong>institution:</strong> Florida Atlantic University, Morrow Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GB-DQN, a method that uses gradient boosting to create an ensemble of Q-networks, where each new network learns the residual error of the current ensemble to adapt to non-stationary environments. Experiments show that GB-DQN achieves faster recovery and greater robustness compared to standard DQN and other baselines in tasks with changing dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [knowledge graph question answering], [reinforcement learning, subgraph selection, graph pruning, llm fine-tuning, relation-centric reasoning]</li>
<li class=""><strong>authors:</strong> Yinxu Tang, Chengsong Huang, Jiaxin Huang, William Yeoh</li>
<li class=""><strong>institution:</strong> Washington University in St. Louis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17043" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17043</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/069fd74faaf7500b76c5bd6958a190a707d8ccbe86484c3026a357b38657a47a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/069fd74faaf7500b76c5bd6958a190a707d8ccbe86484c3026a357b38657a47a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UniRel-R1, a framework for relation-centric knowledge graph question answering that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The method is designed to identify compact and informative subgraph answers by rewarding specific relations and lower-degree entities. Experiments show it outperforms baselines in connectivity and reward and generalizes well to unseen entities and relations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [AIXI, Choquet integrals, imprecise probability theory, semimeasure loss, utility functions]</li>
<li class=""><strong>authors:</strong> Cole Wyeth, Marcus Hutter</li>
<li class=""><strong>institution:</strong> University of Waterloo, Google DeepMind, Australian National University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper generalizes the AIXI reinforcement learning agent to handle a wider class of utility functions, confronting the ambiguity of finite history predictions by interpreting belief distributions as imprecise probabilities. It explores computing expected utilities using Choquet integrals from imprecise probability theory and investigates their computability. The authors show that the standard recursive value function is a special case, but the most general utilities under a &quot;death interpretation&quot; cannot be characterized by these integrals.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]</li>
<li class=""><strong>authors:</strong> Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman</li>
<li class=""><strong>institution:</strong> Toyota Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17091</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, skill library, sequential rollout, skill-integrated reward, GRPO, self-improving agent]</li>
<li class=""><strong>authors:</strong> Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, AWS Agentic AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17102" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17102</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fdb48e8df3d2e43c1e8301082bd3478f7894eef19c7194ccd54fb1c77738ef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fdb48e8df3d2e43c1e8301082bd3478f7894eef19c7194ccd54fb1c77738ef_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SAGE, a reinforcement learning framework that enhances LLM-based agents by integrating a skill library through sequential rollouts and a skill-integrated reward. This approach enables agents to accumulate and reuse skills across tasks for continual self-improvement. Experiments show SAGE improves task completion accuracy while significantly reducing interaction steps and token usage compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [interactive reinforcement learning, multi-teacher learning, Q-learning, teacher selection, concept drift]</li>
<li class=""><strong>authors:</strong> Maher Mesto, Francisco Cruz</li>
<li class=""><strong>institution:</strong> University of New South Wales, Universidad Central de Chile</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, retrieval-augmented generation, multi-modal large language models, explainable AI]</li>
<li class=""><strong>authors:</strong> Shengwei Zhao, Jingwen Yao, Sitong Wei, Linhai Xu, Yuying Liu, Dong Zhang, Zhiqiang Tian, Shaoyi Du</li>
<li class=""><strong>institution:</strong> Xi’an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17194</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e101261754ebc1d899bc11f5f5d9245217cf53bcbfc614d62f737f8cbd530473_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e101261754ebc1d899bc11f5f5d9245217cf53bcbfc614d62f737f8cbd530473_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MMRAG-RFT, a two-stage reinforcement fine-tuning framework for explainable multi-modal retrieval-augmented generation. The method uses rule-based and reasoning-based reinforcement learning to filter documents and jointly optimize ranking and answer generation, achieving state-of-the-art results on benchmark datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [latent modulation, variational autoencoder, reinforcement learning, supervised fine-tuning, reasoning strategies, controllable exploration]</li>
<li class=""><strong>authors:</strong> Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng</li>
<li class=""><strong>institution:</strong> Alibaba Group, Shanghai Jiao Tong University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Reasoning Palette, a framework that uses a latent variable from a VAE to modulate a model&#x27;s reasoning trajectory via prepended token prefixes, enabling diverse strategic exploration. It shows that this approach improves exploration efficiency in RL training and leads to consistent performance gains over standard methods on reasoning benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, group relative policy optimization, knowledge graph consistency, entity-relation matching, process supervision, hard-example mining]</li>
<li class=""><strong>authors:</strong> Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang</li>
<li class=""><strong>institution:</strong> Xidian University, Brown University, Capital Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17213</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [curriculum learning, supervised fine-tuning, reinforcement learning, perception-grounded chain-of-thought, pivotal perception reward]</li>
<li class=""><strong>authors:</strong> Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma</li>
<li class=""><strong>institution:</strong> Meituan, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17227" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17227</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-stage curriculum framework to address visual forgetting in multimodal reasoning. It first builds an abstract reasoning backbone using text-only data and then uses reinforcement learning with a novel reward to teach the model a strategic policy for when to perceive visual information. This approach transforms the model into a more strategic and visually grounded reasoner.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bisimulation metric, generalized bisimulation metric, Markov decision process, policy transfer, state aggregation, sampling-based estimation]</li>
<li class=""><strong>authors:</strong> Zhenyu Tao, Wei Xu, Xiaohu You</li>
<li class=""><strong>institution:</strong> Southeast University, Purple Mountain Laboratories</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17265" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17265</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a Generalized Bisimulation Metric (GBSM) to measure state similarity between different Markov Decision Processes, establishing its fundamental mathematical properties. The authors leverage GBSM to theoretically analyze tasks like policy transfer and state aggregation, obtaining tighter performance bounds than previous methods. Numerical results validate the effectiveness of GBSM for multi-MDP scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [information theory, mutual information, reinforcement learning, distribution shift, role-playing models]</li>
<li class=""><strong>authors:</strong> Yongqi Li, Hao Lang, Fei Huang, Tieyun Qian, Yongbin Li</li>
<li class=""><strong>institution:</strong> Wuhan University, Tongyi Lab, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17270</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85ede876c98e7e8d1d360bf9eb2cb767cc2570e218ebc72489f68b5cec65ce55_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an information-theoretic metric called reasoning-based effective mutual information difference (R-EMID) to measure and analyze the generalization degradation of role-playing models under distribution shifts. It also proposes a co-evolving reinforcement learning framework to improve response probability estimation for calculating R-EMID. The main conclusion is that user shift poses the highest risk to model performance and reinforcement learning is the most effective approach for enhancing generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [large language models, turn-based battle system, strategic decision-making, content generation, procedural generation, adaptive difficulty]</li>
<li class=""><strong>authors:</strong> Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar</li>
<li class=""><strong>institution:</strong> Birla Institute of Technology and Science, Pilani</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17308" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17308</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a turn-based Pokémon battle system where LLMs act as agents, making tactical decisions based on a structured battle state without domain-specific training. The core method involves evaluating LLMs on strategic reasoning and their ability to generate novel game content. The main conclusion is that LLMs can function as dynamic game opponents and designers, offering a practical alternative to reinforcement learning for strategic games.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Xiaomi MiMo-VL-Miloco Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [supervised fine-tuning, reinforcement learning, Group Relative Policy Optimization, chain-of-thought supervision, token-budget-aware reasoning, quantization]</li>
<li class=""><strong>authors:</strong> Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu</li>
<li class=""><strong>institution:</strong> Xiaomi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17436</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MiMo-VL-Miloco-7B, a vision-language model specialized for smart-home understanding, built via a two-stage training pipeline combining supervised fine-tuning and reinforcement learning. The model achieves leading performance on home-scenario tasks like gesture recognition and also shows gains on general multimodal and language reasoning benchmarks. The authors conclude that targeted home-scenario training enhances activity understanding and can improve text-only reasoning with minimal trade-offs on other tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, independent proximal policy optimization, agent-based modeling, electricity markets, capacity markets, contracts for difference]</li>
<li class=""><strong>authors:</strong> Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni</li>
<li class=""><strong>institution:</strong> Politecnico di Milano, CMCC Foundation - Euro-Mediterranean Center on Climate Change, RFF-CMCC European Institute on Economics and the Environment, Bocconi University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17444</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework, using independent proximal policy optimization, to model investment decisions by generation companies in long-term electricity markets. The model is applied to a stylized Italian electricity system to test various market designs and policy scenarios. The results demonstrate that market design is critical for achieving decarbonization targets while mitigating price volatility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [safe reinforcement learning], [safe reinforcement learning, predictive safety representations, constrained markov decision processes, waymo open motion dataset, nuplan, srpl]</li>
<li class=""><strong>authors:</strong> Mahesh Keswani, Raunak Bhattacharyya</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Delhi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40dd4c52ca67a3f1ecc6ec7068de338a3616940aa4e51935c5ce5f30430ebbfe_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates the Safety Representations for Safer Policy Learning (SRPL) framework, which augments SafeRL agents with a predictive model of future constraint violations to improve the safety-performance trade-off in autonomous driving. Experiments on real-world datasets (Waymo Open Motion Dataset and NuPlan) show that SRPL can lead to statistically significant improvements in success rate and cost reduction, and enhances robustness to noise and generalization in cross-dataset evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [prescriptive process monitoring], [backward induction, causal learning, reinforcement learning, sequential decision-making]</li>
<li class=""><strong>authors:</strong> Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt</li>
<li class=""><strong>institution:</strong> KU Leuven, Technical University of Munich (TUM)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17629" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17629</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SCOPE, a prescriptive process monitoring approach that uses backward induction and causal learning to recommend aligned sequences of interventions for optimizing key performance indicators. It directly leverages observational data without needing process approximations for reinforcement learning. Experiments show SCOPE outperforms existing techniques in optimizing KPIs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Trust-Region Adaptive Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [post-training], [Trust-Region Adaptive Policy Optimization, Trust-Region SFT, forward KL divergence, reverse KL, adaptive prefix-selection, supervised fine-tuning, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Ant Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17636</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a948c761d13b2b79a39ce9d5e992677a2054a69ebce16f21dcf30264ab34a82f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TRAPO, a hybrid framework that interleaves supervised fine-tuning and reinforcement learning within each training instance to unify expert supervision and self-exploration. It stabilizes training with Trust-Region SFT and an adaptive prefix-selection mechanism. Experiments on mathematical reasoning benchmarks show TRAPO outperforms standard pipelines and state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [timed reward machines, tabular Q-learning, timed automata, counterfactual-imagining]</li>
<li class=""><strong>authors:</strong> Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska</li>
<li class=""><strong>institution:</strong> Tata Institute of Fundamental Research, Max Planck Institute for Software Systems, University of Oxford</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17637" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17637</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes timed reward machines (TRMs), an extension of reward machines that incorporates timing constraints into the reward specification for reinforcement learning. The authors develop model-free RL algorithms, specifically using tabular Q-learning integrated with abstractions of timed automata and counterfactual-imagining heuristics, to learn optimal policies. The experimental results show that their approach successfully learns policies that achieve high rewards while satisfying the specified timing constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [energy-based models, gradient-based refinement, hindsight goal relabeling, latent-space planning]</li>
<li class=""><strong>authors:</strong> Carlos Vélez García, Miguel Cazorla, Jorge Pomares</li>
<li class=""><strong>institution:</strong> INESCOP, University of Alicante</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17846" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17846</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Planning as Descent (PaD), a method for offline goal-conditioned reinforcement learning that learns an energy function over latent trajectories and performs planning via gradient-based refinement in this energy landscape. It achieves state-of-the-art 95% success on cube manipulation tasks, demonstrating that verification-driven trajectory synthesis outperforms direct policy learning, especially when trained on noisy data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [ViPR, ViPR-Eureka, ViPR-RL, behavior cloning, VLM-in-the-loop Parallel Refinement, LLM-guided contact sampling, sim-to-real transfer, GPU simulation]</li>
<li class=""><strong>authors:</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</li>
<li class=""><strong>institution:</strong> Robotics and AI Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17853</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3552d146c759bb8b81dd4441230819f44e04ae7530ed1ec49cc21133ed3f116_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AnyTask, an automated framework that uses massively parallel GPU simulation and foundation models to generate diverse robot manipulation tasks and expert demonstration data. It introduces three agents (ViPR, ViPR-Eureka, ViPR-RL) for synthesizing demonstrations, which are used to train behavior cloning policies. These policies achieve a 44% average success rate when deployed directly on real robot hardware for various manipulation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [imitation learning, distributionally robust control, layered control architecture, Taylor Series Imitation Learning (TaSIL), L1-Distributionally Robust Adaptive Control (L1-DRAC), certifiable autonomy]</li>
<li class=""><strong>authors:</strong> Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, University of Pennsylvania, California Institute of Technology, Massachusetts Institute of Technology, Lockheed Martin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17899</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Distributionally Robust Imitation Policy (DRIP) architecture, a layered control framework that integrates Taylor Series Imitation Learning (TaSIL) and L1-Distributionally Robust Adaptive Control (L1-DRAC) to address different sources of distribution shift. The main conclusion is that this integration enables the design of certifiable autonomy pipelines by guaranteeing performance certificates for the entire control system, combining learning-based components with model-based decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [reinforcement learning, flow control, differentiable solvers, transfer learning, benchmark platform]</li>
<li class=""><strong>authors:</strong> Christian Lagemann, Sajeda Mokbel, Miro Gondrum, Mario Rüttgers, Jared Callaham, Ludger Paehler, Samuel Ahnert, Nicholas Zolman, Kai Lagemann, Nikolaus Adams, Matthias Meinke, Wolfgang Schröder, Jean-Christophe Loiseau, Esther Lagemann, Steven L. Brunton</li>
<li class=""><strong>institution:</strong> University of Washington, RWTH Aachen University, Inha University, Technical University of Munich, German Center for Neurodegenerative Diseases, Arts et Métiers Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17534</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11a2356fa2a2cafe6887d85b978c67e18494f41d547e787460e381132d0f9508_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces HydroGym, a reinforcement learning platform designed for fluid dynamics control, featuring both non-differentiable and differentiable solvers to improve sample efficiency. The platform includes 42 validated environments and demonstrates that RL agents can discover robust control principles, achieving significant drag reduction and efficient adaptation to new conditions via transfer learning.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 10</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reinforcement learning, model predictive control, MPPI, hierarchical planning, adaptive sampling]</li>
<li class=""><strong>authors:</strong> Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman</li>
<li class=""><strong>institution:</strong> Toyota Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17091</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep unrolled model, Restormer, learned coil sensitivity map estimator, sampling-aware weighted data consistency, universal conditioning, progressive cascade expansion training]</li>
<li class=""><strong>authors:</strong> Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang</li>
<li class=""><strong>institution:</strong> Johns Hopkins University, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17137</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [evaluation framework], [LLM-as-a-Judge, regression, MetricBank, retrieval, human feedback correlation]</li>
<li class=""><strong>authors:</strong> Michael J. Ryan, Yanzhe Zhang, Amol Salunkhe, Yi Chu, Di Xu, Diyi Yang</li>
<li class=""><strong>institution:</strong> Stanford University, American Express</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17267" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17267</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74363ce6b272cc866e0e9407e36b6e57863b7642ae94357d478230d1f46735a0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AutoMetrics, a framework that synthesizes evaluation metrics by combining retrieved metrics from a curated bank with automatically generated LLM-as-a-Judge criteria, composed via regression to maximize correlation with human feedback. It demonstrates that AutoMetrics significantly improves correlation with human judgments over standard LLM-as-a-Judge approaches while requiring minimal human feedback data. The method can serve as an effective proxy reward for optimizing AI applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Kolmogorov-Arnold Networks (KANs), multilayer perceptron networks (MLPs), Physics-informed Neural Networks, nonlocal consistency loss, integro-differential equations (IDEs), fractional PDEs]</li>
<li class=""><strong>authors:</strong> Farinaz Mostajeran, Aruzhan Tleubek, Salah A Faroughi</li>
<li class=""><strong>institution:</strong> University of Utah</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17273" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17273</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MINPO, a unified neural framework that learns nonlocal operators and their inverses using KANs or MLPs to solve integro-differential equations. It enforces coherence between the learned operator and solution via a nonlocal consistency loss. The method is shown to be accurate and robust across diverse kernel types and dimensionalities, generalizing beyond problem-specific formulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [speech processing], [voice activity detection, vision transformer, MFCC, Gammatone filter bank cepstral coefficients, dataset augmentation, out-of-distribution evaluation]</li>
<li class=""><strong>authors:</strong> Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan</li>
<li class=""><strong>institution:</strong> Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17281" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17281</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [memristive architecture, minion recurrent unit, weighted-bit streaming, experience replay, mixed-signal accelerator, on-chip continual learning]</li>
<li class=""><strong>authors:</strong> Abdullah M. Zyarah, Dhireesha Kudithipudi</li>
<li class=""><strong>institution:</strong> University of Texas at San Antonio, University of Baghdad</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17299</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces M2RU, a mixed-signal hardware architecture that implements the Minion Recurrent Unit for efficient on-chip continual learning at the edge. It uses weighted-bit streaming and experience replay to enable energy-efficient temporal processing and stable adaptation. The results show significant energy efficiency improvements and a long operational lifetime, establishing M2RU as a scalable platform for edge-level temporal intelligence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [feature caching, selective computation, constraint-aware scheduling, temporal redundancy, activation schedule]</li>
<li class=""><strong>authors:</strong> Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen</li>
<li class=""><strong>institution:</strong> South China University of Technology, South China Agricultural University, Pazhou Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17298" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17298</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ProCache, a training-free framework to accelerate Diffusion Transformers (DiTs) by using dynamic feature caching. It introduces a constraint-aware caching pattern search to create non-uniform activation schedules and a selective computation module to mitigate error accumulation. Experiments show ProCache achieves significant speedups with minimal quality loss, outperforming prior caching methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [democratic systems], [fair voting methods, cumulative voting, equal shares, proportional representation, participatory budgeting, AI voting assistance]</li>
<li class=""><strong>authors:</strong> Evangelos Pournaras</li>
<li class=""><strong>institution:</strong> University of Leeds</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17461</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes that combining expressive ballot formats like cumulative voting with proportional aggregation methods like equal shares constitutes a &quot;fair voting method.&quot; It concludes that such methods enhance democratic legitimacy, accelerate impactful outcomes in areas like welfare and education, and serve as a safeguard against biases in emerging AI-assisted voting scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [computational behavioral measurement, video analysis, facial expression, head movement, body action, open-source toolkit, modularity, interpretability]</li>
<li class=""><strong>authors:</strong> Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc</li>
<li class=""><strong>institution:</strong> The Children&#x27;s Hospital of Philadelphia, University of Pennsylvania, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17655</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Bitbox, an open-source behavioral imaging toolbox that provides a standardized interface for extracting high-level behavioral measurements from video using multiple face, head, and body processors. It is designed to bridge the translational gap by making advanced computational analysis accessible to behavioral and clinical researchers without requiring engineering expertise. The authors conclude that Bitbox will accelerate the integration of computational behavioral measurement into behavioral, clinical, and mental health research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [molecular design], [Bayesian optimization, generative models, multi-objective optimization, acquisition function, qPMHI, Monte Carlo sampling]</li>
<li class=""><strong>authors:</strong> Madhav R. Muthyala, Farshud Sorourifar, Tianhong Tan, You Peng, Joel A. Paulson</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, The Ohio State University, The Dow Chemical Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17659" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17659</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a &quot;generate-then-optimize&quot; framework for de novo molecular design, which uses a generative model to create candidate molecules and a novel acquisition function called qPMHI to efficiently select batches for evaluation. The method demonstrates significant improvements over existing approaches in sample efficiency and performance, as shown in benchmarks and a case study on designing organic cathode materials for batteries.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<p><strong>cs.DC total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nihir Chadderwala</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17913</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17918</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17910</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17941</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Fast Online Digital Twinning on FPGA for Mission Critical Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17942" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17942</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fast Online Digital Twinning on FPGA for Mission Critical Applications</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yi Yang, Ziyu Lin, Liesheng Wei</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18127</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Constrained Cuts, Flows, and Lattice-Linearity</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Robert Streit, Vijay K. Garg</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18141" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18141</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Constrained Cuts, Flows, and Lattice-Linearity</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18194</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18318</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18334</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Grammateia Kotsialou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18444</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Snowveil: A Framework for Decentralised Preference Discovery</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18674</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Real-Time Digital Twin for Adaptive Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18894" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18894</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Real-Time Digital Twin for Adaptive Scheduling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ivan Čilić, Ivana Podnar Žarko, Pantelis Frangoudis, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18915</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19103</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Timely Parameter Updating in Over-the-Air Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19131</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19179</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Simulations between Strongly Sublinear MPC and Node-Capacitated Clique</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Philipp Schneider, Julian Werthmann</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19326</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Simulations between Strongly Sublinear MPC and Node-Capacitated Clique</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19342</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19606</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, João Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medić, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Benoît Commerçon, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18883" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18883</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 59</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lihui Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17912" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17912</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Karthik Prabhakar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17943</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SuperFlow: Training Flow Matching Models with RL on the Fly</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaijie Chen, Zhiyang Xu, Ying Shen, Zihao Lin, Yuguang Yao, Lifu Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17951" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17951</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperFlow: Training Flow Matching Models with RL on the Fly</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17979</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43cecfb66c204c7d6add4e7c40f9325f7867c22be2179de9305d482292a7a4dd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43cecfb66c204c7d6add4e7c40f9325f7867c22be2179de9305d482292a7a4dd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18014" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18014</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Autonomous Navigation in Endovascular Interventions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tudor Jianu, Anh Nguyen, Sebastiano Fichera, Pierre Berthet-Rayne</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18081" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18081</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f911d278f0ab6bda3ee8c65e700063ac44b97c8d600de90a9982d9c8c3b33f81_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Autonomous Navigation in Endovascular Interventions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Cristiano da Costa Cunha, Wei Liu, Tim French, Ajmal Mian</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18135</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afd713c843807b61ef24ae3fe42d41c20e6fbaeaff735c0c77378e6c26ab19a6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afd713c843807b61ef24ae3fe42d41c20e6fbaeaff735c0c77378e6c26ab19a6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On Swarm Leader Identification using Probing Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Stergios E. Bachoumas, Panagiotis Artemiadis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18146</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On Swarm Leader Identification using Probing Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zihao Deng, Yijia Li, Renrui Zhang, Peijun Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18189" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18189</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee57353b28c6f902f2d47ba6499f6f3db69fe4e0b025fa2fd4ed11eff3c1c003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee57353b28c6f902f2d47ba6499f6f3db69fe4e0b025fa2fd4ed11eff3c1c003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18215</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Stable and Efficient Single-Rollout RL for Multimodal Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Harsh Rathva, Ojas Srivastava, Pruthwik Mishra</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18309" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18309</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18317" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18317</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18333</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Youssef Mahran, Zeyad Gamal, Ayman El-Badawy</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18336" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18336</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Monitoring Monitorability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Melody Y. Guan, Miles Wang, Micah Carroll, Zehao Dou, Annie Y. Wei, Marcus Williams, Benjamin Arnav, Joost Huizinga, Ian Kivlichan, Mia Glaese, Jakub Pachocki, Bowen Baker</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18311</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b80143b15f0287eaa0d31decbf1a350d64c8110ec245d21e81c64ae73cd6febc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b80143b15f0287eaa0d31decbf1a350d64c8110ec245d21e81c64ae73cd6febc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Monitoring Monitorability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Amirreza Abbasi, Mohsen Hooshmand</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18445</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e732525ad7eeb9bd777c68adaa24b4e1209e8441252397e0784b8ce41a3a00d7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> On the Universality of Transformer Architectures; How Much Attention Is Enough?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] When Robots Say No: The Empathic Ethical Disobedience Benchmark</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dmytro Kuzmenko, Nadiya Shvai</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18474" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18474</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/083ac27bf9c1ba565d26a4c6417b20f994336e4ffb3cb410b3b93d3ef36fb6aa_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> When Robots Say No: The Empathic Ethical Disobedience Benchmark</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> John Cao, Luca Furieri</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18540" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18540</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43fe9d2dae8cd7dd8a3ae293cabf5bd434385dd8877da1630a601a47c5dc1a7f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18552" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18552</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Toward Training Superintelligent Software Agents through Self-Play SWE-RL</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Distributionally Robust Multi-Agent Reinforcement Learning for Intelligent Traffic Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shuwei Pei, Joran Borger, Arda Kosay, Muhammed O. Sayin, Saeed Ahmed</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18558" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18558</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c1324b0e30f974791cfbd8d505d68abcbf0e5d1e0c030b0e526accb5fbb97de_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c1324b0e30f974791cfbd8d505d68abcbf0e5d1e0c030b0e526accb5fbb97de_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Distributionally Robust Multi-Agent Reinforcement Learning for Intelligent Traffic Control</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> John Chen, Sihan Cheng, Can Gurkan, Ryan Lay, Moez Salahuddin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18564</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b0a311cb9d10337e5f3590761c621c192dd12a7c496b0cd7891fbccd0f8367_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b0a311cb9d10337e5f3590761c621c192dd12a7c496b0cd7891fbccd0f8367_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18604" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18604</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng, Kaitong Cai, Jian Wang, Keze Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18623</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18622" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18622</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Offline Reinforcement Learning for End-to-End Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chihiro Noguchi, Takaki Yamamoto</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18662" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18662</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Reinforcement Learning for End-to-End Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xue Yang, Michael Schukat, Junlin Lu, Patrick Mannion, Karl Mason, Enda Howley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18670</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhiquan Tan, Yinrong Hong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18730</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726f00d4aa0c339e8cdaf688c05c28499bf469ac19b055e49f249d532aa40b2_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Minh Vu, Konstantinos Slavakis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18763" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18763</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e19f67ca81a26c636db40a8c5fd0bedfcf549bc2e97dbcd90530ca1de4a7f861_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18745</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Guohui Zhang, Hu Yu, Xiaoxiao Ma, Yaning Pan, Hang Xu, Feng Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18766</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Word to World: Can Large Language Models be Implicit Text-based World Models?</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, Mengdi Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18832" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18832</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7359ba2b2edde0aa35db68272c398f7be194a19334cd0996c3e220c7df0b0c05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7359ba2b2edde0aa35db68272c398f7be194a19334cd0996c3e220c7df0b0c05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Word to World: Can Large Language Models be Implicit Text-based World Models?</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Feeza Khan Khanzada, Jaerock Kwon</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18850" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18850</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6819b32f0d91763f868ca298c9e844896fa38b755e6a9bf4eed0851e5ae3cbf1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18857" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18857</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Qiyu Wu, Toshiyuki Sekiya, Christof Monz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18906" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18906</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fe48c6335a2271e131697f68ce5e1bd4c38bb02d11e569ba97f897ef4100cd_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fe48c6335a2271e131697f68ce5e1bd4c38bb02d11e569ba97f897ef4100cd_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Framework for Deploying Learning-based Quadruped Loco-Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yadong Liu, Jianwei Liu, He Liang, Dimitrios Kanoulas</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18938</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed6b45b02a235e7607255d7b5204abe59d049aea04c8c4caa01b35f1f4f309d9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Framework for Deploying Learning-based Quadruped Loco-Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Debamita Ghosh, George K. Atia, Yue Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18957</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f848ae68e82b8a23f061ffdf3e4e75811fcab48a03fe5070cce95c8a38a027b7_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yizhi Wang, Linan Yue, Min-Ling Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18956" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18956</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19001" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19001</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peiqing Lu, Yuan Zhang, Haoyun Zhang, Jiasen Zheng, Kejian Tong, Wenjun Wu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19093" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19093</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ba3857cb85f2a61550d6204fcd94a616e3814c7b544954750bbe22dbc8e0777_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ba3857cb85f2a61550d6204fcd94a616e3814c7b544954750bbe22dbc8e0777_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19083</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92e42fabfcde19e98a2f6371e650103f1dd9895fac5630cbb65993eda35b116a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao, Guojun Yin, Wei Lin, Ran He</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19126</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea562f5265303d30f48412af8f0c2c84f8e98bc5e8f45118efe7f417df403e8d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea562f5265303d30f48412af8f0c2c84f8e98bc5e8f45118efe7f417df403e8d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19133</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jun Li, Zikun Chen, Haibo Chen, Shuo Chen, Jian Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19300" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19300</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xu Zhang, Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning-Assisted Multi-Operator Variable Neighborhood Search for Urban Cable Routing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wei Liu, Tao Zhang, Chenhui Lin, Kaiwen Li, Rui Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19321" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19321</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3d94e7990bf940cb4e587f12cfcbb0d8438eef591b7db89e5a77d7daec8f35d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3d94e7990bf940cb4e587f12cfcbb0d8438eef591b7db89e5a77d7daec8f35d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning-Assisted Multi-Operator Variable Neighborhood Search for Urban Cable Routing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Enhancing PLS of Indoor IRS-VLC Systems for Colluding and Non-Colluding Eavesdroppers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rashid Iqbal, Ahmed Zoha, Salama Ikki, Muhammad Ali Imran, Hanaa Abumarshoud</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19339" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19339</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/851f695ae948a59a83b4e4a7e1c623d8507df46c587ff957f421fe69eb4b5b05_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/851f695ae948a59a83b4e4a7e1c623d8507df46c587ff957f421fe69eb4b5b05_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Enhancing PLS of Indoor IRS-VLC Systems for Colluding and Non-Colluding Eavesdroppers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] First-Order Representation Languages for Goal-Conditioned RL</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Simon Ståhlberg, Hector Geffner</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19355" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19355</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a8381365dc741932409d148f8d829e1cb922492f327e8a311d02d4ed8ad6d56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a8381365dc741932409d148f8d829e1cb922492f327e8a311d02d4ed8ad6d56_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> First-Order Representation Languages for Goal-Conditioned RL</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning General Policies with Policy Gradient Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Simon Ståhlberg, Blai Bonet, Hector Geffner</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19366" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19366</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning General Policies with Policy Gradient Methods</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Isshaan Singh, Divyansh Chawla, Anshu Garg, Shivin Mangal, Pallavi Gupta, Khushi Agarwal, Nimrat Singh Khalsa, Nandan Patel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19361" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19361</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afec297ffe8c1a2ba4e79439bf065a0920537f7decb882056013d6e7740f8e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afec297ffe8c1a2ba4e79439bf065a0920537f7decb882056013d6e7740f8e9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CodeSimpleQA: Scaling Factuality in Code Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19424</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91e94588098b016a931e564501e220f391d8186f55b4c39b93da715a899afb11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91e94588098b016a931e564501e220f391d8186f55b4c39b93da715a899afb11_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CodeSimpleQA: Scaling Factuality in Code Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xueming Yan, Bo Yin, Yaochu Jin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19516" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19516</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yongxin Wang, Zhicheng Yang, Meng Cao, Mingfei Han, Haokun Lin, Yingying Zhu, Xiaojun Chang, Xiaodan Liang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19554" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19554</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19576</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19583" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19583</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50064ac2ceaf4f963ab285d471e35de23cbb546a9b072cfb14bd945c6439276e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19673</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19691</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3b7aa03e4ca854038444e0521aaa9d1a6b6c20e2054b4637dd76d61ecac2014_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3b7aa03e4ca854038444e0521aaa9d1a6b6c20e2054b4637dd76d61ecac2014_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sheryl Chen, Tony Wang, Kyle Feinstein</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17929</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yucheng Yang, Chiyuan Wang, Andreas Schaab, Benjamin Moll</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18892" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18892</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haishan Ye</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19104" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19104</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e1de9f54709e8dfd7512b0bf65bde1fbf69bbf9510338a82e721bb9ab43307b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e1de9f54709e8dfd7512b0bf65bde1fbf69bbf9510338a82e721bb9ab43307b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 34</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Farida Mohsen, Ali Safa</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17958</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Joshua Gibson, Kapil Dhakal</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18034</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee8143ffe65f15c2c3477c3c466f8d3d7e3d40519caf5667a1e168e357e8ce6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee8143ffe65f15c2c3477c3c466f8d3d7e3d40519caf5667a1e168e357e8ce6b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Changxu Duan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18122" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18122</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nuntipat Narkthong, Xiaolin Xu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18132" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18132</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5249dae70a6ea951da7229a4606840eb6080d2d0b091828439fd7dc722b3fe4e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5249dae70a6ea951da7229a4606840eb6080d2d0b091828439fd7dc722b3fe4e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tsung-Han Lu, Zheyu Li, Minxuan Zhou, Tajana Rosing</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18158</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c0e2dda9beb3cd09d48e66d5bd1e2e0d25dcf0ff68f729ae40eea17980532c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c0e2dda9beb3cd09d48e66d5bd1e2e0d25dcf0ff68f729ae40eea17980532c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18269</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards Efficient Agents: A Co-Design of Inference Architecture and System</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weizhe Lin, Hui-Ling Zhen, Shuai Yang, Xian Wang, Renxi Liu, Hanting Chen, Wangze Zhang, Chuansai Zhou, Yiming Li, Chen Chen, Xing Li, Zhiyuan Yang, Xiaosong Li, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan, Yunhe Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18337" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18337</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7ed4e66482149eeb38c88b95b26442424c5cb783935dd1d2ae998dcbe934a3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7ed4e66482149eeb38c88b95b26442424c5cb783935dd1d2ae998dcbe934a3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards Efficient Agents: A Co-Design of Inference Architecture and System</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wonseok Choi, Hyunah Yu, Jongmin Kim, Hyesung Ji, Jaiyoung Park, Jung Ho Ahn</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18345" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18345</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e4c078425f57860f1303fe0449ae500adae2d4acf07294f16f8ed22a154bad_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e4c078425f57860f1303fe0449ae500adae2d4acf07294f16f8ed22a154bad_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Akul Malhotra, Sumeet Kumar Gupta</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18459" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18459</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97e8d699ae3dc2331ca317ecfd53fe4db9ae0d861417dcb1143bdade9d3ecfa0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97e8d699ae3dc2331ca317ecfd53fe4db9ae0d861417dcb1143bdade9d3ecfa0_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SoK: Understanding (New) Security Issues Across AI4Code Use Cases</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Qilong Wu, Taoran Li, Tianyang Zhou, Varun Chandrasekaran</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18456</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f652ae22c8ea45edbe21093e80ef932b29b1703110171928654985064e108e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f652ae22c8ea45edbe21093e80ef932b29b1703110171928654985064e108e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SoK: Understanding (New) Security Issues Across AI4Code Use Cases</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Overcoming Spectral Bias via Cross-Attention</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiaodong Feng, Tao Tang, Xiaoliang Wan, Tao Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18586</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a27de306c0dce8a093c2cb1cd83c5b2a1515e2b795e63c3f3140fda5f27898_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a27de306c0dce8a093c2cb1cd83c5b2a1515e2b795e63c3f3140fda5f27898_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Overcoming Spectral Bias via Cross-Attention</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yifan Zhao, Xinglong Yu, Yi Sun, Honglin Kuang, Jun Han</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18589</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1feca70dc6f2a05e35a48f6366ccdcc1accb232952639d7388e6316ca2ff652_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1feca70dc6f2a05e35a48f6366ccdcc1accb232952639d7388e6316ca2ff652_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18604" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18604</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9df5c14f10e3a4031cb92513653314edaf2d17ccb1f345c4f9e818b04c5c9203_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jianglin Lu, Yuanwei Wu, Ziyi Zhao, Hongcheng Wang, Felix Jimenez, Abrar Majeedi, Yun Fu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18599</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pierre Colombo, Malik Boudiaf, Allyn Sweet, Michael Desa, Hongxi Wang, Kevin Candra, Syméon del Marmol</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18658" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18658</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99cb2c0d15c54813fefddf4670e9d0dc5b70cdc79ff9a13a7d5fb4f3a38f7143_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99cb2c0d15c54813fefddf4670e9d0dc5b70cdc79ff9a13a7d5fb4f3a38f7143_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xue Yang, Michael Schukat, Junlin Lu, Patrick Mannion, Karl Mason, Enda Howley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18670</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f001f825ba30968846f540ea46b7c77510d728bd232f6fe2aedc626451956364_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Thorsten Hellert, Nikolay Agladze, Alex Giovannone, Jan Jug, Frank Mayet, Mark Sherwin, Antonin Sulc, Chris Tennant</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18779</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4811b7a6317b8d33b2f92f5249b3e215e280fef25b4d76030d5495c78a7d02f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4811b7a6317b8d33b2f92f5249b3e215e280fef25b4d76030d5495c78a7d02f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuanchao Xu, Fengyi Li, Masahiro Fujisawa, Youssef Marzouk, Isao Ishikawa</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18837" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18837</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3838b816a163dddae4907465cfe37d64e1f909b2317422f86a8a2c6ad7e98898_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generative Modeling through Spectral Analysis of Koopman Operator</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Generalized Chebyshev acceleration on the unit disc</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Nurgül Gökgöz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18848" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18848</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f63d665dcf2ac69573f3e985999d052f51eac8e7aa11582ea4d1a0234129824_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f63d665dcf2ac69573f3e985999d052f51eac8e7aa11582ea4d1a0234129824_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generalized Chebyshev acceleration on the unit disc</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohamad Zamini, Diksha Shukla</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18910</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andrew Polar, Michael Poluektov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18921</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e53714df09977ebe2db3d05959d679946262bcbddc4ea0acb1d3d3511f211611_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Merging of Kolmogorov-Arnold networks trained on disjoint datasets</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19004" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19004</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Weiyi Lyu, Xinming Fang, Jun Wang, Jun Shi, Guixu Zhang, Juncheng Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19095" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19095</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Towards a collaborative digital platform for railway infrastructure projects</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pierre Jehel, Pierre-Étienne Gautier, Judicaël Dehotin, Flavien Viguier</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19169</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3874b7105990b7eea9564e4b1e9114f5f53c8e95ba6b7ddfc4422d6b9542ed5a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3874b7105990b7eea9564e4b1e9114f5f53c8e95ba6b7ddfc4422d6b9542ed5a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards a collaborative digital platform for railway infrastructure projects</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Phase-space entropy at acquisition reflects downstream learnability</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xiu-Cheng Wang, Jun-Jie Zhanga, Nan Cheng, Long-Gang Pang, Taijiao Du, Deyu Meng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19223" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19223</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6f7f5d98244f4027de001e9a63b027e02247bcd25714fdcf9483af34b3cbf0c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Phase-space entropy at acquisition reflects downstream learnability</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tarek Bouazza, Alessandro Melis, Soulaimane Berkane, Robert Mahony, Tarek Hamel</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19245" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19245</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d513f5a88f2a529f413e52c4a1f8a346c5bbe6ab056bc09090ccb37d3041a044_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Emir Devlet Ertörer, Cem Ünsalan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19304" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19304</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f352565f0530b4ee02ed15da3cf6f6c724c3dee282c3ce8dcec4f2bb7a8bed00_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f352565f0530b4ee02ed15da3cf6f6c724c3dee282c3ce8dcec4f2bb7a8bed00_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19390" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19390</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Evelyn Zhang, Fufu Yu, Aoqi Wu, Zichen Wen, Ke Yan, Shouhong Ding, Biqing Qi, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19443</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Gauss-Newton-Induced Structure-Exploiting Algorithm for Differentiable Optimal Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuankun Chen, Zifei Nie, Xun Gong, Yunfeng Hu, Hong Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19447" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19447</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74ef4be4d92e46d4eb48894c23b5b114f6542e056ba7735882b575a70ef70317_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74ef4be4d92e46d4eb48894c23b5b114f6542e056ba7735882b575a70ef70317_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> A Gauss-Newton-Induced Structure-Exploiting Algorithm for Differentiable Optimal Control</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] An Agentic Framework for Autonomous Materials Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zeyu Xia, Jinzhe Ma, Congjie Zheng, Shufei Zhang, Yuqiang Li, Hang Su, P. Hu, Changshui Zhang, Xingao Gong, Wanli Ouyang, Lei Bai, Dongzhan Zhou, Mao Su</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19458" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19458</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc8796e6d842dffe17cec24dc175dc3c7a315afb61353b40a3cd5603693d9a1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc8796e6d842dffe17cec24dc175dc3c7a315afb61353b40a3cd5603693d9a1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Agentic Framework for Autonomous Materials Computation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li Fei-fei, Ehsan Adeli</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19526" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19526</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc62413ed86e23bc11d3109dd3f6d9f5cdc6abfea9e0de269b698952b18c550_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc62413ed86e23bc11d3109dd3f6d9f5cdc6abfea9e0de269b698952b18c550_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Muhammad Osama Imran, Roshni Lulla, Rodney Sappington</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17989</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088c001d289b3633293d46545215e3c6c3c647164a557d5d4458682598dc319a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088c001d289b3633293d46545215e3c6c3c647164a557d5d4458682598dc319a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Evolutionary Cooperation with Game Transitions via Markov Decision Chain in Networked Population</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chaoyang Luo, Yuji Zhang, Minyu Feng, Attila Szolnoki</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18972" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18972</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af9b1c10d4d6654d591148d5bf2617a4e1b09646e5af8cfba3db0476ae3943b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af9b1c10d4d6654d591148d5bf2617a4e1b09646e5af8cfba3db0476ae3943b4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Evolutionary Cooperation with Game Transitions via Markov Decision Chain in Networked Population</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<p><strong>cs.DC total: 15</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251224] Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19777</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Holoscope: Open and Lightweight Distributed Telescope &amp; Honeypot Platform</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Andrea Sordello, Marco Mellia, Idilio Drago, Rodolfo Valentim, Francesco Musumeci, Massimo Tornatore, Federico Cerutti, Martino Trevisan, Alessio Botta, Willen Borges Coelho</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19842" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19842</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Holoscope: Open and Lightweight Distributed Telescope &amp; Honeypot Platform</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] An Adaptive Distributed Stencil Abstraction for GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aditya Bhosale, Laxmikant Kale</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19851</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Adaptive Distributed Stencil Abstraction for GPUs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19849</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UCCL-EP: Portable Expert-Parallel Communication</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Pengchao Han, Xi Huang, Yi Fang, Guojun Han</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19972" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19972</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Scaling Point-based Differentiable Rendering for Large-scale Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20017</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling Point-based Differentiable Rendering for Large-scale Reconstruction</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yaojian Chen, Si-Qiu Gong, Lin Gan, Yanfei Liu, An Yang, Yinuo Wang, Chao-yang Lu, Guangwen Yang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20064" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20064</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Population Protocols Revisited: Parity and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Leszek Gąsieniec, Tytus Grodzicki, Tomasz Jurdziński, Jakub Kowalski, Grzegorz Stachowiak</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20163" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20163</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Population Protocols Revisited: Parity and Beyond</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20178</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reaching Agreement Among Reasoning LLM Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chaoyi Ruan, Yiliang Wang, Ziji Shi, Jialin Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20184</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Reaching Agreement Among Reasoning LLM Agents</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yinan Ni, Xiao Yang, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20210</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20363</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mohammad Walid Charrwi, Zaid Hussain</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20394" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20394</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] WOC: Dual-Path Weighted Object Consensus Made Efficient</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tanisha Fonseca, Gengrui Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20485</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WOC: Dual-Path Weighted Object Consensus Made Efficient</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Rui Pan, Zhuofu Chen, Ravi Netravali</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20573</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 31</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251224] Holographic MIMO Empowered NOMA-ISAC for 6G: Rate-Splitting Enhanced Near-Field Modeling, Multi-Objective Optimization, and Statistical Performance Validation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sumita Majhi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19699</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2183a3ab117ce8aa35a27ef5ef1f90999864af516e6e9184883dd8c000154d9c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2183a3ab117ce8aa35a27ef5ef1f90999864af516e6e9184883dd8c000154d9c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Holographic MIMO Empowered NOMA-ISAC for 6G: Rate-Splitting Enhanced Near-Field Modeling, Multi-Objective Optimization, and Statistical Performance Validation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19696" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19696</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhan Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19717</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Tiny, On-Device Decision Makers with the MiniConv Library</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Carlos Purves</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19726</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tiny, On-Device Decision Makers with the MiniConv Library</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Hard Negative Sample-Augmented DPO Post-Training for Small Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Haocheng Lu, Minjun Zhu, Henry Yu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19728</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c371de45b1b11fd08dee5a184a8d1c0b7de0a0ff5ecd8c96c44e8fddf3eabedc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hard Negative Sample-Augmented DPO Post-Training for Small Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19738</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa995500ed323b0099b96191763cdf14300972d797c5bdf631faa22d483ef8d4_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learning to Design City-scale Transit Routes</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Bibek Poudel, Weizi Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19767</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48f5a0e4652c24c99f21521fcb359e848d25f4d63cc6c81bee61cbd2088a47c6_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Learning to Design City-scale Transit Routes</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19920</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] An Optimal Policy for Learning Controllable Dynamics by Exploration</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Peter N. Loxley</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20053</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> An Optimal Policy for Learning Controllable Dynamics by Exploration</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Scaling Reinforcement Learning for Content Moderation with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20061</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cc1d5bc88350e4d91579fed9a3b17abade80dc25754379e8e11ce92e39ca7d5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cc1d5bc88350e4d91579fed9a3b17abade80dc25754379e8e11ce92e39ca7d5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Scaling Reinforcement Learning for Content Moderation with Large Language Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Chaithra, Kamesh Kadimisetty, Biju R Mohan</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20082" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20082</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8f2d13846195cc68b294529fa5d22600c76c1ff5581ee402ddf30ac1c06da72_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8f2d13846195cc68b294529fa5d22600c76c1ff5581ee402ddf30ac1c06da72_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20092" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20092</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f21f9409d7ddcd534ada400fa2d7b093de0f8e8672067e512129d84ead74883_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f21f9409d7ddcd534ada400fa2d7b093de0f8e8672067e512129d84ead74883_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Information-directed sampling for bandits: a primer</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Annika Hirling, Giorgio Nicoletti, Antonio Celani</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20096</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bda193cb418441da95d0440f5f59e93b002d7bab9c57780520c427e9d3126c5_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Information-directed sampling for bandits: a primer</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20111</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuanhao Chen, Qi Liu, Pengbin Chen, Zhongjian Qiao, Yanjie Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20115</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5c5ab959b1aabba8373388341144fb9d59c759f4a45a536ba853663551ebb84_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhuo Yang, Yeyun chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20135</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/354a0c5aa0cbd47dcbbf13234c253d1300ed7b0266e8ead6da21580f2b96f942_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/354a0c5aa0cbd47dcbbf13234c253d1300ed7b0266e8ead6da21580f2b96f942_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Multi-hop Reasoning via Early Knowledge Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yuxin Wang, Shicheng Fang, Bo Wang, Qi Luo, Xuanjing Huang, Yining Zheng, Xipeng Qiu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20144" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20144</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d88b182b244d4a4ceb1d9822c08a4a4d748f40278f40b510b9dc474ab390f2c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d88b182b244d4a4ceb1d9822c08a4a4d748f40278f40b510b9dc474ab390f2c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Multi-hop Reasoning via Early Knowledge Alignment</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Dan Chen, Heye Huang, Tiantian Chen, Zheng Li, Yongji Li, Yuhui Xu, Sikai Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20179</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba879d80c9d58b4f8d7942ebda77c8c4b993bb9018f484a08adfe360eb40c02c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba879d80c9d58b4f8d7942ebda77c8c4b993bb9018f484a08adfe360eb40c02c_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FaithLens: Detecting and Explaining Faithfulness Hallucination</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20182</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FaithLens: Detecting and Explaining Faithfulness Hallucination</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Offline Safe Policy Optimization From Heterogeneous Feedback</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ze Gong, Pradeep Varakantham, Akshat Kumar</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20173</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9834f9cb644c8389de473430a70e825274ad26459f3ce94c2018ac8228df6f9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9834f9cb644c8389de473430a70e825274ad26459f3ce94c2018ac8228df6f9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Offline Safe Policy Optimization From Heterogeneous Feedback</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Edge-Served Congestion Control for Wireless Multipath Transmission with a Transformer Agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Liang Wang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20186</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16b93be39f0376f25deb8cc4fbada1a95fee8a407d89b5267be07145e8a1b9e3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16b93be39f0376f25deb8cc4fbada1a95fee8a407d89b5267be07145e8a1b9e3_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Edge-Served Congestion Control for Wireless Multipath Transmission with a Transformer Agent</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Heekang Song, Wan Choi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20201" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20201</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cef2c0107d65c30e332cf8fcbb67cfa02aecf72c8313c8aa9f6eed48502a4093_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cef2c0107d65c30e332cf8fcbb67cfa02aecf72c8313c8aa9f6eed48502a4093_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kausthubh Manda, Raghuram Bharadwaj Diddigi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20220</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96bb0ad288cee461b985922b13e1446ef6b03b8ee1b5f20f1e908e0e81174e2b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Divya Vijay, Vignesh Ethiraj</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20275</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da6037bde0b2a44f38e40927d7e2b880cc97a5f8fde73345c4fad4c78baf4836_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da6037bde0b2a44f38e40927d7e2b880cc97a5f8fde73345c4fad4c78baf4836_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20312" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20312</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Identifying Appropriately-Sized Services with Deep Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Syeda Tasnim Fabiha, Saad Shafiq, Wesley Klewerton Guez Assunção, Nenad Medvidović</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20381" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20381</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/295a39edb17ec8824ad34080df5c2a960bf42d578ae6e9a2db55f3775d90e225_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/295a39edb17ec8824ad34080df5c2a960bf42d578ae6e9a2db55f3775d90e225_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Identifying Appropriately-Sized Services with Deep Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Recurrent Off-Policy Deep Reinforcement Learning Doesn&#x27;t Have to be Slow</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tyler Clark, Christine Evers, Jonathon Hare</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20513" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20513</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfed8377a526fc8b1686b7063bcf7aa6afc36205aa596ab393544501923d4a4a_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Recurrent Off-Policy Deep Reinforcement Learning Doesn&#x27;t Have to be Slow</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Performative Policy Gradient: Optimality in Performative Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20576" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20576</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Performative Policy Gradient: Optimality in Performative Reinforcement Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20605</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20589</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20618</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongVideoAgent: Multi-Agent Reasoning with Long Videos</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 22</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251224] Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Vasileios C. Pezoulas, Nikolaos S. Tachos, Eleni Georga, Kostas Marias, Manolis Tsiknakis, Dimitrios I. Fotiadis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19718</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/690024688881d156d3131447c4b795c922984c2e3732a32a3b139b183a1be23e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] High-Performance Self-Supervised Learning by Joint Training of Flow Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kosuke Ukita, Tsuyoshi Okita</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19729" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19729</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> High-Performance Self-Supervised Learning by Joint Training of Flow Matching</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] EdgeFlex-Transformer: Transformer Inference for Edge Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Shoaib Mohammad, Guanqun Song, Ting Zhu</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19741" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19741</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a472c5a3779ea5b970e030fee19030719f5db1d085d239de2dc5df41dffd7439_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EdgeFlex-Transformer: Transformer Inference for Edge Devices</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Ilsun Chang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19750" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19750</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22261640471da63aebf047b2045859846dac3cc2b34448b820a7154bc31cfb4f_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22261640471da63aebf047b2045859846dac3cc2b34448b820a7154bc31cfb4f_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Tingjia Miao, Jiawen Dai, Jingkun Liu, Jinxin Tan, Muhua Zhang, Wenkai Jin, Yuwen Du, Tian Jin, Xianghe Pang, Zexi Liu, Tu Guo, Zhengliang Zhang, Yunjie Huang, Shuo Chen, Rui Ye, Yuzhi Zhang, Linfeng Zhang, Kun Chen, Wei Wang, Weinan E, Siheng Chen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19799" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19799</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1922ac933b302c99f4a3c639911ffa3980ae43238fad1b247af369017413128_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1922ac933b302c99f4a3c639911ffa3980ae43238fad1b247af369017413128_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kian Godhwani, David Benrimoh</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20022" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20022</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd553d2c8dfa3e24074d8dc0752a5348a2dc9e7e83526071de4a16c28bb018b6_w640_q70.png" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd553d2c8dfa3e24074d8dc0752a5348a2dc9e7e83526071de4a16c28bb018b6_w640_q70.png</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] VSA<!-- -->:Visual-Structural<!-- --> Alignment for UI-to-Code</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Xian Wu, Ming Zhang, Zhiyu Fang, Fei Li, Bin Wang, Yong Jiang, Hao Zhou</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20034</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96d552b0d871e15200119995d18b2c4223e07f8d868c7cc4caaa9ee14883b636_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96d552b0d871e15200119995d18b2c4223e07f8d868c7cc4caaa9ee14883b636_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VSA<!-- -->:Visual-Structural<!-- --> Alignment for UI-to-Code</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mingue Park, Jisung Hwang, Seungwoo Yoo, Kyeongmin Yeo, Minhyuk Sung</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20063</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1f331573ffd5fdda741a1d0056236e66fefa140bd0c2b8c4b173e3519544061_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Huizheng Wang, Taiquan Wei, Hongbin Wang, Zichuan Wang, Xinru Tang, Zhiheng Yue, Shaojun Wei, Yang Hu, Shouyi Yin</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20198" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20198</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e2bc2f765accf0fbb24e05e708bd3a2b62c961a86def9137c9976aa8b753f85_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e2bc2f765accf0fbb24e05e708bd3a2b62c961a86def9137c9976aa8b753f85_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] TongSIM: A General Platform for Simulating Intelligent Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20206</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca507a85ff857b30b18d4ea2014b82001e6b5d0adac56afe981969173bc45325_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca507a85ff857b30b18d4ea2014b82001e6b5d0adac56afe981969173bc45325_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TongSIM: A General Platform for Simulating Intelligent Machines</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] DeepONet-accelerated Bayesian inversion for moving boundary problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Marco A. Iglesias, Michael. E. Causon, Mikhail Y. Matveev, Andreas Endruweit, Michael .V. Tretyakov</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20268</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca57b0bdfa7635748d75eae61d7af10d528a81997788a87eb9ab2703231769b_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeepONet-accelerated Bayesian inversion for moving boundary problems</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Ferroelectric FET-based Logic-in-Memory Encoder for Hyperdimensional Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Arka Chakraborty, Franz Müller, Thomas Kämpfe, Shubham Sahay</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20302</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6feea0ba4ef2950fd67ff0b965cd0f25df3de85c5b2c14b3b86666f9afc997e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6feea0ba4ef2950fd67ff0b965cd0f25df3de85c5b2c14b3b86666f9afc997e_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Ferroelectric FET-based Logic-in-Memory Encoder for Hyperdimensional Computing</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mrinmay Sen, Subhrajit Nag</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20329" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20329</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f60b7e61ea92d36b52de81bc3df02c7af567b1cd9eb4626ad016c3ce36765e1_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Emilia Majerz, Witold Dzwinel, Jacek Kitowski</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20346" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20346</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d6aa7726e909753c196217ccc3bdf17d07a9339b0fc1d35361587823848df71_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linfei Li, Lin Zhang, Zhong Wang, Ying Shen</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20377" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20377</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Kanchon Gharami, Sanjiv Kumar Sarkar, Yongxin Liu, Shafika Showkat Moni</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20405</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594ed7491abb39e62fd7b3759310d809c622b7da530d48b7e61a79c247aa8e25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594ed7491abb39e62fd7b3759310d809c622b7da530d48b7e61a79c247aa8e25_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Linfeng Zhang, Siheng Chen, Yuzhu Cai, Jingyi Chai, Junhan Chang, Kun Chen, Zhi X. Chen, Zhaohan Ding, Yuwen Du, Yuanpeng Gao, Yuan Gao, Jing Gao, Zhifeng Gao, Qiangqiang Gu, Yanhui Hong, Yuan Huang, Xi Fang, Xiaohong Ji, Guolin Ke, Zixing Lei, Xinyu Li, Yongge Li, Ruoxue Liao, Hang Lin, Xiaolu Lin, Yuxiang Liu, Xinzijian Liu, Zexi Liu, Jintan Lu, Tingjia Miao, Haohui Que, Weijie Sun, Yanfeng Wang, Bingyang Wu, Tianju Xue, Rui Ye, Jinzhe Zeng, Duo Zhang, Jiahui Zhang, Linfeng Zhang, Tianhan Zhang, Wenchang Zhang, Yuzhi Zhang, Zezhong Zhang, Hang Zheng, Hui Zhou, Tong Zhu, Xinyu Zhu, Qingguo Zhou, Weinan E</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20469" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20469</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efbb4cf73bd5e97a426fe07fa2444d9fce83c1cc337fc7a02676141bf805fbd9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efbb4cf73bd5e97a426fe07fa2444d9fce83c1cc337fc7a02676141bf805fbd9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> He Zhu, Zheng Liu, Xingyang Li, Anbang Wu, Jieru Zhao, Fangxin Liu, Yiming Gan, Jingwen Leng, Yu Feng</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20495</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1ba8a7f58cc0b9807b31889823cb7497448ae885dece28021d7b9eda1828c58_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1ba8a7f58cc0b9807b31889823cb7497448ae885dece28021d7b9eda1828c58_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Alexandros Christoforos, Chadbourne Davis</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20604" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20604</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/740b7502b49c1387c998a9fd8b95bff7878c9c8ecb80d21efccf1c64db303459_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/740b7502b49c1387c998a9fd8b95bff7878c9c8ecb80d21efccf1c64db303459_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] QMBench: A Research Level Benchmark for Quantum Materials Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Yanzhen Wang, Yiyang Jiang, Diana Golovanova, Kamal Das, Hyeonhu Bae, Yufei Zhao, Huu-Thong Le, Abhinava Chatterjee, Yunzhe Liu, Chao-Xing Liu, Felipe H. da Jornada, Binghai Yan, Xiao-Liang Qi</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19753" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19753</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a5901ce24dd3558937f8fb69f70cd14d0da814e05be0cd205e3d5fb0d138661_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a5901ce24dd3558937f8fb69f70cd14d0da814e05be0cd205e3d5fb0d138661_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QMBench: A Research Level Benchmark for Quantum Materials Research</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Towards a point-to-point CV-QKD system: Implementation challenges and perspectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Davi Juvêncio Gomes de Sousa, Nelson Alves Ferreira Neto, Christiano M. S. Nascimento, Lucas Q. Galvão, Mauro Queiroz Nooblath Neto, Micael Andrade Dias, Cássio de Castro Silva, Braian Pinheiro da Silva, Alexandre B. Tacla, Valéria Loureiro da Silva</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19834</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa7fbf5acad2dd0ae547f58bc198ca67d3b36ccc31ad024285f0358a08cb6c9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfa7fbf5acad2dd0ae547f58bc198ca67d3b36ccc31ad024285f0358a08cb6c9_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Towards a point-to-point CV-QKD system: Implementation challenges and perspectives</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Mebin Jose, Jisha Francis, Sudheesh Kumar Kattumannil</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20305" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20305</a></li>
<li class=""><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f5766a6c4e9984e1a7c6234a1bf0776e7a9fad9ef648cc4ca17ad3ba035698_w640_q70.webp</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<p><strong>cs.DC total: 11</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [hierarchical autoregressive model, KV-cache optimization, memory-bound inference, multi-resolution context, throughput-quality trade-off]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai</p>
</li>
<li class="">
<p><strong>institution:</strong> Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20687</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer&#x27;s flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SoK: Speedy Secure Finality</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [blockchain consensus], [finality, consensus protocol, Ethereum, Gasper, reorg resilience]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yash Saraswat, Abhimanyu Nag</p>
</li>
<li class="">
<p><strong>institution:</strong> Indian Institute of Technology, Roorkee; University of Alberta</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20715</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides a systematic survey of the state-of-the-art in Speedy Secure Finality (SSF) protocols, tracing their evolution from foundational works like Goldfish to RLMD-GHOST. 2. Introduces and explains core theoretical primitives for understanding SSF, such as reorganization resilience and the generalized sleepy model. 3. Analyzes the practical trade-offs of Single Slot Finality and surveys the 3-Slot Finality (3SF) protocol as a pragmatic solution balancing fast finality with Ethereum&#x27;s engineering constraints.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper surveys research on Speedy Secure Finality (SSF) to reduce the long confirmation latency in Ethereum&#x27;s Gasper protocol. It reviews the evolution of fast finality protocols, analyzes their design trade-offs, and highlights the 3-Slot Finality protocol as a practical synthesis. The main conclusion is that 3SF offers a viable path to achieve faster, secure finality while addressing the network&#x27;s practical limitations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [cluster infrastructure], [multi-runtime middleware, hybrid AI-HPC workflows, uniform abstractions, Dragon, vLLM]</p>
</li>
<li class="">
<p><strong>authors:</strong> Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha</p>
</li>
<li class="">
<p><strong>institution:</strong> Rutgers University, Hewlett Packard Enterprise, Brookhaven National Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20795</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RHAPSODY, a multi-runtime middleware that composes and coordinates existing runtimes to support heterogeneous AI-HPC workloads within a single job allocation. 2. Introduces uniform abstractions for tasks, services, resources, and execution policies to manage conflicting requirements of simulations, AI services, and agentic workflows. 3. Demonstrates minimal runtime overhead, scalability for inference workloads, and efficient AI-HPC coupling in evaluations on leadership-class HPC platforms.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of executing hybrid AI-HPC workflows, which combine simulations, training, and inference with conflicting runtime requirements. It proposes RHAPSODY, a middleware that coordinates existing runtimes through uniform abstractions instead of replacing them. Evaluation shows RHAPSODY enables efficient, scalable execution of these heterogeneous workloads with minimal overhead.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Stochastic well-structured transition systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [distributed computing theory], [well-structured transition systems, population protocols, probabilistic scheduling, computational complexity, BPP]</p>
</li>
<li class="">
<p><strong>authors:</strong> James Aspnes</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20939</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Defines a new class of stochastic well-structured transition systems (SWSTSs) that unifies models like population protocols and chemical reaction networks under a probabilistic scheduling rule. 2. Proves fundamental limitations on phase clocks in SWSTSs, showing they either stop or tick too fast in expected polynomial time. 3. Provides an exact characterization of computational power, showing augmented SWSTSs compute exactly BPP languages, while unaugmented ones compute symmetric BPL languages.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper extends the theory of well-structured transition systems by incorporating probabilistic scheduling, creating a new class called stochastic well-structured transition systems (SWSTSs). It proves that any phase clock implementation in these systems has polynomial expected duration, and that terminating computations finish in expected polynomial time. These results lead to an exact characterization of computational power, showing that augmented SWSTSs compute exactly the languages in BPP.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [communication &amp; networking], [4D Gaussian Splatting, video streaming, integer linear programming, pruning, keyframe selection]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhe Wang, Jinghang Li, Yifei Zhu</p>
</li>
<li class="">
<p><strong>institution:</strong> Shanghai Jiao Tong University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20943</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by &gt;20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [3D parallelism, heterogeneous GPUs, spot instances, load balancing, fault-tolerance]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20953</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces AutoHet, a system that automatically identifies optimal parallelism plans for distributed training on heterogeneous GPUs, supporting asymmetric 3D parallelism. 2. Proposes a theoretical optimization model for device grouping and load balancing to minimize per-iteration training time across GPUs with diverse capabilities. 3. Presents an efficient recovery strategy for spot instance preemption that prioritizes retrieving training states locally to minimize checkpoint downloads from cloud storage.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of distributed training of large language models on heterogeneous GPU clusters, particularly with spot instances. It proposes AutoHet, a system that automatically optimizes 3D parallelism plans and load balancing for such environments and includes an efficient fault-tolerance mechanism. Evaluations show AutoHet achieves significant speedups in both training throughput and recovery speed compared to existing systems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [distributed attention, communication efficiency, Ring-Attention, communication-computation ratio, scalability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20968" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20968</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm training], [spot instance, online scheduling, deadline-aware, LoRA, integer programming]</p>
</li>
<li class="">
<p><strong>authors:</strong> Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu</p>
</li>
<li class="">
<p><strong>institution:</strong> Fudan University, University of Oregon, Inria</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20967</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm&#x27;s performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [parallel computing], [hypergraph, GPU, dynamic data structure, triad counting, parallel algorithm]</p>
</li>
<li class="">
<p><strong>authors:</strong> S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das</p>
</li>
<li class="">
<p><strong>institution:</strong> Missouri University of Science and Technology, University of North Texas</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21009</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed ESCHER, a novel GPU-centric parallel data structure for efficient representation and management of large-scale dynamic hypergraphs. 2. Designed a hypergraph triad-count update framework that minimizes redundant computation by leveraging ESCHER&#x27;s dynamic operation capabilities. 3. Demonstrated significant performance improvements, achieving speedups of up to 104.5x, 473.7x, and 112.5x for different triad counting types on real-world and synthetic datasets.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational challenge of analyzing large, dynamic hypergraphs, which lack efficient specialized data structures. It proposes ESCHER, a GPU-centric data structure for representing hypergraph evolution, and a corresponding triad-counting update framework. The method achieves substantial speedups over state-of-the-art approaches in counting various types of hypergraph triads.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [zero-knowledge proofs, trusted execution environments, blockchain, medical AI, verifiable aggregation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Savvy Sharma, George Petrovic, Sarthak Kaushik</p>
</li>
<li class="">
<p><strong>institution:</strong> George Brown Polytechnic</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21048</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator&#x27;s computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Declarative distributed broadcast using three-valued modal logic and semitopologies</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [distributed algorithms], [modal logic, declarative specification, semitopologies, three-valued logic, axiomatic theories]</p>
</li>
<li class="">
<p><strong>authors:</strong> Murdoch J. Gabbay</p>
</li>
<li class="">
<p><strong>institution:</strong> Heriot-Watt University (inferred from author&#x27;s affiliation)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21137</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel method to formally specify distributed algorithms as declarative axiomatic theories using modal logic, 2. Demonstrates the method&#x27;s application and scalability on concrete protocols (voting, broadcast, agreement), 3. Shows the method&#x27;s practical utility by finding errors in a proposed industrial protocol.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel declarative approach for specifying distributed algorithms using three-valued modal logic and semitopologies. It demonstrates the method on protocols like Bracha Broadcast, providing a compact, human-readable specification that abstracts away low-level implementation details. The approach enables precise reasoning about correctness and has been used to find errors in industrial protocols.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251225] BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [on-device ai], [1-bit quantization, Deep Q-Network (DQN), edge inference, multi-objective RL, model compression]</p>
</li>
<li class="">
<p><strong>authors:</strong> Ravi Gupta, Shabista Haider</p>
</li>
<li class="">
<p><strong>institution:</strong> AMD, Oracle</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20623" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20623</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel architecture integrating 1-bit quantized LLMs with DQN for multi-objective optimization of smart home lighting. 2. Voice command integration via Google Home and IFTTT webhooks for natural user interaction. 3. Comprehensive evaluation demonstrating the feasibility of intelligent adaptive control on sub-$50 hardware, with significant energy and latency improvements.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc8c0af6233346bd047dc958868370625b7e65b546832d11939a371662fc4980_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc8c0af6233346bd047dc958868370625b7e65b546832d11939a371662fc4980_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes BitRL-Light, a framework that combines a 1-bit quantized LLM with Deep Q-Network reinforcement learning for real-time smart home lighting control on edge devices. The system optimizes for energy efficiency and user comfort, achieving substantial energy savings and low latency on a Raspberry Pi. The work demonstrates that adaptive AI control is feasible on resource-constrained hardware without cloud dependency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Quantum-Inspired MARL, Variational Quantum Circuits (VQC), Centralized Training Decentralized Execution (CTDE), UAV-assisted 6G, Exploration-Exploitation Tradeoff]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mazyar Taghavi, Javad Vahidi</p>
</li>
<li class="">
<p><strong>institution:</strong> Iran University of Science and Technology, Intelligent Knowledge City</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20624</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel quantum-inspired framework integrating variational quantum circuits (VQCs) and QAOA with classical MARL to optimize the exploration-exploitation tradeoff. 2. Incorporates complementary probabilistic modeling (Bayesian inference, Gaussian processes) to capture latent environmental dynamics in a cooperative UAV scenario. 3. Demonstrates through experiments that the framework improves sample efficiency, convergence speed, and coverage performance compared to classical baselines like PPO and DDPG.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a quantum-inspired multi-agent reinforcement learning framework to optimize the exploration-exploitation balance for UAV-assisted 6G network deployment. The method integrates variational quantum circuits and probabilistic modeling within a centralized training, decentralized execution paradigm. The results show it achieves superior performance in coverage and convergence compared to classical MARL methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent systems], [Differentiable Price Mechanism, Dominant Strategy Incentive Compatibility, VCG-equivalent incentive, Dec-POMDPs, Bayesian Incentive Compatibility]</p>
</li>
<li class="">
<p><strong>authors:</strong> Stefano Grassi</p>
</li>
<li class="">
<p><strong>institution:</strong> None (No affiliation or email domain provided in the given content)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20688</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [tool-augmented agent, agentic reinforcement learning, supervised fine-tuning (SFT), request-level asynchronous rollout, prefix-aware load balancing]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Tsinghua University, Tencent Hunyuan</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20745</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networks], [Deep Reinforcement Learning (DRL), Reconfigurable Intelligent Surfaces (RIS), Energy Harvesting (EH)]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur</p>
</li>
<li class="">
<p><strong>institution:</strong> Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20739" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20739</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [RLVR, causal reasoning, generalization, supervised fine-tuning, large language models]</p>
</li>
<li class="">
<p><strong>authors:</strong> Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei</p>
</li>
<li class="">
<p><strong>institution:</strong> Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20760</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR&#x27;s benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model&#x27;s initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model&#x27;s initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Safety Alignment of LMs via Non-cooperative Games</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [non-cooperative game, adversarial training, preference-based reward, online reinforcement learning, safety alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Anselm Paulus, Ilia Kulikov, Brandon Amos, Rémi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta (FAIR), University of Tübingen</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20806" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20806</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/facebookresearch/advgame" target="_blank" rel="noopener noreferrer" class="">https://github.com/facebookresearch/advgame</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a new paradigm for safety alignment by framing it as a non-zero-sum game between an Attacker LM and a Defender LM. 2. Proposes joint training of the LMs via online reinforcement learning with a preference-based reward signal to reduce reward hacking. 3. Demonstrates that the method (AdvGame) produces a Defender LM with improved safety and utility and an Attacker LM that serves as a strong red-teaming agent.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99d494326c3e6d7e063baf364aff968ae0d21f53ab3f3e8b4214c548e5ac79b4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99d494326c3e6d7e063baf364aff968ae0d21f53ab3f3e8b4214c548e5ac79b4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of aligning language models for safety without sacrificing utility. It proposes AdvGame, a method that frames safety alignment as a non-cooperative game between an Attacker and a Defender LM, training them jointly with online RL using preference-based rewards. The results show the approach yields a more helpful and safe Defender and a powerful general-purpose Attacker for red-teaming.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [parameterized actions, state abstraction, action abstraction, TD(λ), sample efficiency]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava</p>
</li>
<li class="">
<p><strong>institution:</strong> Arizona State University, Brown University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20831" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20831</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/AAIR-lab/PEARL.git" target="_blank" rel="noopener noreferrer" class="">https://github.com/AAIR-lab/PEARL.git</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts, Mamba-Transformer, LatentMoE, NVFP4, multi-environment reinforcement learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia</p>
</li>
<li class="">
<p><strong>institution:</strong> NVIDIA</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20856" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20856</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [epistemic asymmetry, Beta-Bernoulli distribution, epistemic caching, forgetting factor, active learning]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng</p>
</li>
<li class="">
<p><strong>institution:</strong> Kwansei Gakuin University, Victoria University of Wellington</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20884</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A formal probabilistic framework using a Beta-Bernoulli model with a forgetting factor to quantify epistemic uncertainty and provide a non-altruistic motive for knowledge sharing among LLM agents. 2. The introduction of epistemic caching, a resource management mechanism that leverages the forgetting factor to dynamically prioritize the active head of non-stationary knowledge distributions for scalable deployment. 3. Demonstrating how accumulated belief states can serve as verifiable reward signals for RLHF and high-quality data filters for SFT, bridging inference-time interaction with long-term model alignment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f9f5d3701afff9c1243309bc058183ccd079615b358ea6fcd7f66333c45b2ae_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f9f5d3701afff9c1243309bc058183ccd079615b358ea6fcd7f66333c45b2ae_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper identifies epistemic asymmetry as a key limitation where LLM agents are unidirectional knowledge consumers. To address this, it proposes a probabilistic framework that models agent belief to create a self-interested motive for sharing knowledge, framed as optimal active learning. Simulations show this uncertainty-driven strategy outperforms random baselines in dynamic environments.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [edge computing], [UAV trajectory optimization, task offloading, mobility prediction, deep reinforcement learning, Transformer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Siqi Mu, Shuo Wen, Yang Lu, Ruihong Jiang, Bo Ai</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Sport University, Beijing Jiaotong University, Beijing University of Posts and Telecommunications</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20902</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Establishes an embodied AI-enhanced IoMT edge computing framework for dynamic UAV service provisioning. 2. Proposes a novel hierarchical multi-scale Transformer-based model for predicting WBAN user mobility from historical trajectory data. 3. Designs a prediction-enhanced deep reinforcement learning algorithm that integrates mobility forecasts to jointly optimize UAV flight trajectory and task offloading decisions.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75676682cad2a49439c91ac390cbbb997d6b492883c198c681369ae72675ee8a_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75676682cad2a49439c91ac390cbbb997d6b492883c198c681369ae72675ee8a_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of minimizing task completion time for WBAN users by optimizing UAV trajectory and task offloading under energy constraints. It proposes an embodied AI framework that uses a Transformer-based model to predict user mobility and a DRL algorithm to make intelligent optimization decisions. Simulation results show the proposed method outperforms existing benchmarks.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), ChemBERTa, ESM-2, reaction-template, de novo drug design]</p>
</li>
<li class="">
<p><strong>authors:</strong> R Yadunandan, Nimisha Ghosh</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Computer Science and Engineering, Shiv Nadar University Chennai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20958</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/YadunandanRaman/ReACT-Drug/" target="_blank" rel="noopener noreferrer" class="">https://github.com/YadunandanRaman/ReACT-Drug/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [se], [repository-level code understanding], [LLM agent, reinforcement learning, tool usage, code navigation, execution-aware]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu</p>
</li>
<li class="">
<p><strong>institution:</strong> Affiliation not explicitly stated in provided text. Email domains suggest potential institutions, but cannot be reliably inferred from given content.</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20957</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes RepoNavigator, an LLM agent that uses a single, execution-aware tool (&quot;jump to definition&quot;) for navigating code repositories, simplifying agent control and aligning with code execution logic. 2. Introduces an end-to-end Reinforcement Learning (RL) training method for the agent directly from a pretrained model, eliminating the need for closed-source model distillation. 3. Demonstrates state-of-the-art performance on repository-level issue localization, showing that smaller RL-trained models (e.g., 7B) can outperform larger baseline models (e.g., 14B, 32B) and even closed-source models like Claude-3.7.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6254efa02c0725684b783a26c76c1825bf8aaa25aee61fb7aaea40887f0efc46_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6254efa02c0725684b783a26c76c1825bf8aaa25aee61fb7aaea40887f0efc46_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the challenge of locating code to modify in large software repositories. It proposes RepoNavigator, an LLM agent trained with Reinforcement Learning to use a single &quot;jump to definition&quot; tool for navigation. Experiments show this approach achieves state-of-the-art performance, with smaller models outperforming larger baselines, proving the efficiency of a simple, execution-aware tool combined with RL training.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Bayesian Reinforcement Learning, Meta-Reinforcement Learning, Generalised Linear Models, Learnable Basis Functions, Variational Inference]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jingyang You, Hanna Kurniawati</p>
</li>
<li class="">
<p><strong>institution:</strong> Australian National University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20974</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [agent system], [agentic AI, network slicing, retrieval-augmented generation (RAG), deep reinforcement learning (DRL), incremental memory]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xudong Wang, Lei Feng, Ruichen Zhang, Fanqin Zhou, Hongyang Du, Wenjing Li, Dusit Niyato, Abbas Jamalipour, Ping Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing University of Posts and Telecommunications, Nanyang Technological University, University of Hong Kong, University of Sydney</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20997" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20997</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an LLM-empowered agentic AI framework for QoE-aware network slicing management in IIoT, integrating reasoning, planning, and adaptation. 2. Introduces a workflow combining a RAG module for semantic intent inference, a DRL-based orchestrator for configuration, and an incremental memory mechanism for continual learning. 3. Demonstrates through a case study that the approach significantly outperforms baselines in balancing latency, reliability, and cost, achieving up to 19% improvement in slice availability.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6d0ea3b1d71ed82cf27b71d380028320830b53455079f5e91ce58cec48576dc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6d0ea3b1d71ed82cf27b71d380028320830b53455079f5e91ce58cec48576dc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of dynamic and QoE-aware network slicing management in Industrial IoT, where traditional methods struggle. It proposes an LLM-empowered agentic AI approach that uses RAG for intent understanding, DRL for orchestration, and incremental memory for adaptation. The method is shown to outperform baselines, improving slice availability by up to 19%.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Policy-Conditioned Policies for Multi-Agent Task Solving</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Program Equilibrium, Programmatic Iterated Best Response (PIBR), policy-conditioning, large language models, textual gradients]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yue Lin, Shuhui Zhu, Wenhao Li, Ang Li, Dan Qiao, Pascal Poupart, Hongyuan Zha, Baoxiang Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen; University of Waterloo; Tongji University; Vector Institute</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21024" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21024</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a paradigm shift by representing agent policies as human-interpretable source code, bridging the gap between opaque neural policies and the need for strategy comprehension in multi-agent settings. 2. Introduces Programmatic Iterated Best Response (PIBR), a novel algorithm that uses LLMs as point-wise best-response operators to synthesize and refine policy code based on game utility and unit tests. 3. Operationalizes the game-theoretic concept of Program Equilibrium for modern learning, demonstrating its effectiveness on coordination games and a cooperative foraging environment.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8665c80d59a606e3c0796d224d372080c9552f5b48a9e7e797a73cad25b1b7e7_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8665c80d59a606e3c0796d224d372080c9552f5b48a9e7e797a73cad25b1b7e7_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of dynamic strategy adaptation in multi-agent tasks by representing policies as interpretable source code and using Large Language Models (LLMs) to optimize them. The core method, Programmatic Iterated Best Response (PIBR), leverages LLMs to iteratively refine an agent&#x27;s policy code in response to an opponent&#x27;s strategy using textual feedback. The approach successfully solves standard coordination games and a cooperative environment, demonstrating a practical bridge between theoretical Program Equilibrium and modern AI learning.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] LSTM-Based Modeling and Reinforcement Learning Control of a Magnetically Actuated Catheter</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning control], [LSTM, Actor-Critic, DQN, Magnetic Catheter, Hysteresis Modeling]</p>
</li>
<li class="">
<p><strong>authors:</strong> Arya Rashidinejad Meibodi, Mahbod Gholamali Sinaki, Khalil Alipour</p>
</li>
<li class="">
<p><strong>institution:</strong> University of Tehran, K. N. Toosi University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21063</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Developed a high-fidelity LSTM-based surrogate model to capture the nonlinear and hysteretic dynamics of a magnetically actuated catheter system. 2. Successfully trained and compared two Reinforcement Learning agents (DQN and actor-critic) for catheter tip control using the LSTM model, avoiding damage to the physical setup. 3. Demonstrated the superior performance of the actor-critic controller over DQN for both regulation and path-following tasks, highlighting its suitability for dynamic navigation in practical applications.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c443f82eab9e78f9d18be161319dc29d64928c34c2ce1370c7de7d7a0e3dae9e_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c443f82eab9e78f9d18be161319dc29d64928c34c2ce1370c7de7d7a0e3dae9e_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a novel approach for controlling a magnetically actuated catheter. It first models the system&#x27;s complex dynamics using an LSTM neural network and then uses this model to train and compare DQN and actor-critic reinforcement learning controllers. The results show that the actor-critic controller outperforms DQN, providing more accurate and smoother control for tasks like path following.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Dyna-Style RL, SINDy, TD3, Model-based RL, Bi-rotor Control]</p>
</li>
<li class="">
<p><strong>authors:</strong> Karim Abdelsalam, Zeyad Gamal, Ayman El-Badawy</p>
</li>
<li class="">
<p><strong>institution:</strong> German University in Cairo</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21081" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21081</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a Dyna-Style RL framework that integrates SINDy for data-driven dynamics modeling with TD3 for policy learning., 2. Introduces a method to periodically inject synthetic rollouts from the learned SINDy model into the RL replay buffer to improve sample efficiency., 3. Demonstrates the framework&#x27;s effectiveness on a bi-rotor system, showing superior accuracy and robustness in stabilization and trajectory tracking compared to direct model-free RL.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/508875b78acef042533866808253222b6799135bb65f28295e2b374106b10052_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a hybrid control framework combining Sparse Identification of Nonlinear Dynamics (SINDy) and Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning to efficiently control nonlinear systems. The SINDy model generates synthetic data to augment real-world training, improving sample efficiency. The method is validated on a bi-rotor system, showing better performance than direct model-free RL.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [aerial manipulation, underactuated system, proximal policy optimization (PPO), incremental nonlinear dynamic inversion (INDI), sim-to-real transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shlok Deshmukh, Javier Alonso-Mora, Sihao Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Delft University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21085" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21085</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a reinforcement learning-based whole-body control strategy for a lightweight, underactuated aerial manipulator (DSAM) to achieve full 6-DoF end-effector pose control. 2. Employs a hybrid control architecture where a PPO agent generates high-level feedforward commands, tracked by an INDI attitude controller and a PID joint controller for robustness. 3. Demonstrates robust real-world performance with centimeter/degree-level accuracy under external disturbances like heavy loads and pushing, bridging the sim-to-real gap.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the control challenge of a lightweight, underactuated aerial manipulator. It uses reinforcement learning (PPO) to train a controller in simulation, which generates commands for a hybrid low-level controller (INDI+PID). Flight experiments show the method achieves precise and robust end-effector pose control, enabling contact-rich tasks with a simple platform.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [latent solvability, mid-stage scientific training, chemical reasoning, rule-based rewards, symbolic competence]</p>
</li>
<li class="">
<p><strong>authors:</strong> Andres M Bran, Tong Xie, Shai Pranesh, Jeffrey Meng, Xuan Vu Nguyen, Jeremy Goumaz, David Ming Segura, Ruizhi Xu, Dongzhan Zhou, Wenjie Zhang, Bram Hoex, Philippe Schwaller</p>
</li>
<li class="">
<p><strong>institution:</strong> École Polytechnique Fédérale de Lausanne (EPFL), University of New South Wales (UNSW), Green Dynamics, Shanghai Artificial Intelligence Laboratory</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21231</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Identifies two necessary conditions for RL-based chemical reasoning: symbolic competence and latent chemical knowledge. 2. Proposes MiST (mid-stage scientific training), a set of techniques including data-mixing with SMILES/CIF-aware pre-processing and continued pre-training. 3. Demonstrates that MiST significantly improves latent solvability and enables RL to achieve large accuracy gains on challenging chemical tasks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734a7266913201c1c5abeaa4d54fa794bfab81e86b7cffa89c8c4dc72702a8fa_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the problem that reinforcement learning for chemical reasoning fails unless the base model already has some latent solvability. The authors propose MiST, a mid-stage training pipeline involving data pre-processing and continued pre-training, to build the necessary prerequisites. Their method substantially boosts model performance on tasks like organic reaction naming and inorganic material generation, establishing clear prerequisites for training chemical reasoning models.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Learning-Enabled Elastic Network Topology for Distributed ISAC Service Provisioning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sys], [wireless networking], [elastic network topology, integrated sensing and communication, multi-agent deep reinforcement learning, utility-to-signaling ratio, cell-free networks]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jie Chen, Xianbin Wang</p>
</li>
<li class="">
<p><strong>institution:</strong> Western University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20722</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes an elastic network topology (ENT) that dynamically aggregates localized cell-centric networks into federated cell-free networks to balance signaling overhead and resource utilization for distributed ISAC services. 2. Develops a two-phase operation protocol for autonomous service classification and resource partitioning, and a utility-to-signaling ratio (USR) metric to quantify the performance-overhead tradeoff. 3. Formulates a USR maximization problem and addresses it with a multi-agent deep reinforcement learning (MADRL) framework to handle the distributed optimization and incomplete channel state information.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4bda2cdb692e390582f5a46dcb2d70525e6975ebe9d56858ec390078ecf53e9_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4bda2cdb692e390582f5a46dcb2d70525e6975ebe9d56858ec390078ecf53e9_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an elastic network topology to flexibly support distributed integrated sensing and communication services by dynamically aggregating cell-centric networks into cell-free federations. A two-phase protocol and a utility-to-signaling ratio metric are introduced, and the resulting optimization problem is solved using a multi-agent deep reinforcement learning approach. Simulation results validate the effectiveness of the proposed design and algorithm.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 17</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251225] Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [federated learning], [Intent-Based Networking, Industrial Internet of Things, Asynchronous Federated Learning, Strategy Similarity, Multimodal Intent Alignment]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shaowen Qin, Jianfeng Zeng, Haodong Guo, Xiaohuan Li, Jiawen Kang, Qian Chen, Dusit Niyato</p>
</li>
<li class="">
<p><strong>institution:</strong> Guilin University of Electronic Technology, Guangdong University of Technology, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20627" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20627</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes FEIBN, a federated learning framework for distributed policy verification in IIoT, enhancing privacy by avoiding raw data exposure. 2. Introduces SSAFL, a strategy similarity-aware mechanism for efficient node selection and asynchronous model updates to reduce communication overhead. 3. Leverages LLMs to align multimodal user intents into structured strategy tuples for automated policy generation and verification.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17f53425c1430ea3ad8516d92a010c39c2936d478f64e21bad2b552f30214b23_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17f53425c1430ea3ad8516d92a010c39c2936d478f64e21bad2b552f30214b23_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes FEIBN, a federated evaluation framework for Intent-Based Networking in IIoT. It uses LLMs to translate intents and a strategy similarity-aware asynchronous federated learning mechanism (SSAFL) for efficient, private policy verification. Experiments show SSAFL improves accuracy, convergence speed, and reduces cost by 27.8% compared to a baseline.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [multi-agent reinforcement learning], [Quantum-Inspired MARL, Variational Quantum Circuits (VQC), Centralized Training Decentralized Execution (CTDE), UAV-assisted 6G, Exploration-Exploitation Tradeoff]</p>
</li>
<li class="">
<p><strong>authors:</strong> Mazyar Taghavi, Javad Vahidi</p>
</li>
<li class="">
<p><strong>institution:</strong> Iran University of Science and Technology, Intelligent Knowledge City</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20624</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a novel quantum-inspired framework integrating variational quantum circuits (VQCs) and QAOA with classical MARL to optimize the exploration-exploitation tradeoff. 2. Incorporates complementary probabilistic modeling (Bayesian inference, Gaussian processes) to capture latent environmental dynamics in a cooperative UAV scenario. 3. Demonstrates through experiments that the framework improves sample efficiency, convergence speed, and coverage performance compared to classical baselines like PPO and DDPG.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes a quantum-inspired multi-agent reinforcement learning framework to optimize the exploration-exploitation balance for UAV-assisted 6G network deployment. The method integrates variational quantum circuits and probabilistic modeling within a centralized training, decentralized execution paradigm. The results show it achieves superior performance in coverage and convergence compared to classical MARL methods.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [sparse attention, diffusion models, long-text generation, soft absorbing state, computational complexity]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alexandros Christoforos, Chadbourne Davis</p>
</li>
<li class="">
<p><strong>institution:</strong> Suffolk University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20724" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20724</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces SA-DiffuSeq, a diffusion framework that integrates sparse attention to improve scalability for long-document modeling. 2. Proposes a novel soft absorbing state tailored to sparse attention dynamics to stabilize diffusion trajectories and accelerate sequence reconstruction. 3. Demonstrates superior training efficiency and sampling speed compared to state-of-the-art diffusion baselines, especially on extended sequences.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the high computational cost of diffusion models for long-text generation by proposing SA-DiffuSeq, which integrates sparse attention and a novel soft absorbing state. This method reduces complexity while maintaining generation quality, making it suitable for applications like scientific writing and code generation. The results show that incorporating structured sparsity is a promising direction for efficient long-text generation.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [formal verification, neural network robustness, early exits, adversarial perturbations, off-the-shelf solvers]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz</p>
</li>
<li class="">
<p><strong>institution:</strong> The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20755</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [gpu kernels], [matrix exponential, scaling and squaring, Taylor series, generative AI, numerical stability]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jorge Sastre, Daniel Faronbi, José Miguel Alonso, Peter Traver, Javier Ibáñez, Nuria Lloret</p>
</li>
<li class="">
<p><strong>institution:</strong> Universitat Politècnica de València, New York University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20777</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. An optimized Taylor-based algorithm for matrix exponential designed for high-throughput generative AI flows. 2. A rigorous error analysis and a dynamic selection strategy for Taylor order and scaling factor to minimize computation under error tolerance. 3. Extensive numerical experiments demonstrating significant acceleration and high numerical stability compared to state-of-the-art methods.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf8f618b421ab20009271fba403be914454dbd115182614b0f5256ce16218271_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper proposes an optimized Taylor-based algorithm for computing the matrix exponential, a key operation in generative AI. The method improves upon classical techniques like Paterson-Stockmeyer and includes a dynamic strategy to balance accuracy and speed. Experiments show it offers significant acceleration while maintaining high numerical stability for large-scale generative modeling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] pokiSEC: A Multi-Architecture, Containerized Ephemeral Malware Detonation Sandbox</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [malware analysis], [Docker, QEMU, KVM, Universal Entrypoint, ephemeral container]</p>
</li>
<li class="">
<p><strong>authors:</strong> Alejandro Avina, Yashas Hariprasad, Naveen Kumar Chaudhary</p>
</li>
<li class="">
<p><strong>institution:</strong> California State University, East Bay, National Forensic Sciences University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20860" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20860</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/PanLuvme/pokiSEC" target="_blank" rel="noopener noreferrer" class="">https://github.com/PanLuvme/pokiSEC</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A lightweight, ephemeral malware detonation sandbox packaged entirely inside a Docker container, integrating QEMU with hardware acceleration and a browser-based workflow. 2. A Universal Entrypoint that performs runtime host-architecture detection and selects validated hypervisor configurations, enabling a single container to launch Windows guests on both ARM64 and x86_64 hosts. 3. Validation of the system on Apple Silicon (ARM64) and Ubuntu (AMD64), demonstrating interactive performance suitable for analyst workflows and consistent teardown via ephemeral container lifecycles.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58898439e3246ec551b562bfb2380c7db3d016ac4d87b780a7f10e905d0bdaef_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58898439e3246ec551b562bfb2380c7db3d016ac4d87b780a7f10e905d0bdaef_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper presents pokiSEC, a containerized sandbox for dynamic malware analysis that packages QEMU and a browser interface into a Docker container to improve portability and automation. Its key innovation is a Universal Entrypoint that detects the host architecture and configures the hypervisor accordingly, allowing the same container to run on both ARM64 and x86_64 systems. The system was validated on Apple Silicon and Ubuntu, showing it provides interactive performance and clean isolation through ephemeral containers.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [graph neural network, surrogate model, multi-fidelity dataset, scaling laws, aerodynamic field prediction]</p>
</li>
<li class="">
<p><strong>authors:</strong> Yiren Shen, Juan J. Alonso</p>
</li>
<li class="">
<p><strong>institution:</strong> Stanford University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20941</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [Proximal Policy Optimization (PPO), ChemBERTa, ESM-2, reaction-template, de novo drug design]</p>
</li>
<li class="">
<p><strong>authors:</strong> R Yadunandan, Nimisha Ghosh</p>
</li>
<li class="">
<p><strong>institution:</strong> Department of Computer Science and Engineering, Shiv Nadar University Chennai</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20958</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/YadunandanRaman/ReACT-Drug/" target="_blank" rel="noopener noreferrer" class="">https://github.com/YadunandanRaman/ReACT-Drug/</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [point cloud upsampling], [flow matching, two-stage strategy, adaptive time scheduler, on-manifold constraints, recurrent interface network]</p>
</li>
<li class="">
<p><strong>authors:</strong> Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp</p>
</li>
<li class="">
<p><strong>institution:</strong> Lappeenranta-Lahti University of Technology LUT, The Hong Kong Polytechnic University, Karlsruhe Institute of Technology (KIT), Saarland University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20988" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20988</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Holmes-Alan/Enhanced_PUFM" target="_blank" rel="noopener noreferrer" class="">https://github.com/Holmes-Alan/Enhanced_PUFM</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A two-stage flow-matching strategy for improved geometric fidelity and distribution approximation, 2. A data-driven adaptive time scheduler to accelerate and stabilize inference, 3. The incorporation of on-manifold constraints and a recurrent interface network (RIN) to enhance surface alignment and feature interaction.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper presents PUFM++, an enhanced flow-matching framework for generating dense and accurate point clouds from sparse, noisy inputs. It introduces a two-stage flow strategy, an adaptive time scheduler, on-manifold constraints, and a recurrent network to improve fidelity, robustness, and efficiency. Experiments show it achieves state-of-the-art performance in point cloud upsampling.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial defense], [talking face generation, 3D neural field, adversarial perturbation, video defense, spatial-frequency optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao</p>
</li>
<li class="">
<p><strong>institution:</strong> Beijing Institute of Technology, Alibaba International Digital Commerce</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21019" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21019</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/Richen7418/VDF" target="_blank" rel="noopener noreferrer" class="">https://github.com/Richen7418/VDF</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. A novel video defense framework that perturbs the 3D information acquisition process of talking face generation models to protect portrait videos. 2. A similarity-guided parameter sharing mechanism to achieve high computational efficiency (47x acceleration). 3. A multi-scale dual-domain attention module to jointly optimize perturbations in both spatial and frequency domains for robustness and high visual fidelity.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the privacy threat posed by 3D-field talking face generation models by proposing an efficient video defense framework. The method introduces a parameter-sharing mechanism for speed and a dual-domain attention module to perturb 3D information while preserving video quality. Experiments show it offers strong defense, is 47x faster than baselines, and is robust against common attacks and transformations.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [reinforcement learning], [aerial manipulation, underactuated system, proximal policy optimization (PPO), incremental nonlinear dynamic inversion (INDI), sim-to-real transfer]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shlok Deshmukh, Javier Alonso-Mora, Sihao Sun</p>
</li>
<li class="">
<p><strong>institution:</strong> Delft University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21085" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21085</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes a reinforcement learning-based whole-body control strategy for a lightweight, underactuated aerial manipulator (DSAM) to achieve full 6-DoF end-effector pose control. 2. Employs a hybrid control architecture where a PPO agent generates high-level feedforward commands, tracked by an INDI attitude controller and a PID joint controller for robustness. 3. Demonstrates robust real-world performance with centimeter/degree-level accuracy under external disturbances like heavy loads and pushing, bridging the sim-to-real gap.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efeec33c78ae2b9603f2658eb97fb9fdc975e6b0f6021fd3feb1cf7c81bc0796_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the control challenge of a lightweight, underactuated aerial manipulator. It uses reinforcement learning (PPO) to train a controller in simulation, which generates commands for a hybrid low-level controller (INDI+PID). Flight experiments show the method achieves precise and robust end-effector pose control, enabling contact-rich tasks with a simple platform.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [others], [Model Predictive Control, MuJoCo, Web of Affine Spaces, finite differencing, iLQG]</p>
</li>
<li class="">
<p><strong>authors:</strong> Chen Liang, Daniel Rakita</p>
</li>
<li class="">
<p><strong>institution:</strong> Yale University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21109" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21109</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/chen-dylan-liang/mujoco" target="_blank" rel="noopener noreferrer" class="">https://github.com/chen-dylan-liang/mujoco</a> wasp mpc</p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduced Web of Affine Spaces (WASP) derivatives as a drop-in replacement for finite differencing in the MuJoCo MPC (MJPC) library to compute derivatives more efficiently and robustly. 2. Demonstrated that WASP integrates seamlessly across diverse robotic tasks, achieves up to 2x speedup over finite differencing, and outperforms stochastic sampling-based planners in MJPC. 3. Released an open-source implementation of MJPC with WASP derivatives fully integrated to support adoption and future research.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ada8447964fa0e64d90a6328db6ae0e8cb32dd7609195104d67ce039de9ad6b_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ada8447964fa0e64d90a6328db6ae0e8cb32dd7609195104d67ce039de9ad6b_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the computational bottleneck of using finite differencing for derivative calculations in MuJoCo-based Model Predictive Control (MPC). The authors propose using Web of Affine Spaces (WASP) derivatives within the MJPC library, which reuses prior derivative information to accelerate and stabilize computations. The method is shown to be robust, integrate seamlessly, and achieve up to a 2x speedup compared to finite differencing while also outperforming sampling-based planners.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Mixed Precision General Alternating-Direction Implicit Method for Solving Large Sparse Linear Systems</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [hpc], [numerical linear algebra], [mixed precision, alternating-direction implicit method, rounding error analysis, Gaussian Process Regression, GPU computation]</p>
</li>
<li class="">
<p><strong>authors:</strong> Jifeng Ge, Bastien Vieublé, Juan Zhang</p>
</li>
<li class="">
<p><strong>institution:</strong> Xiangtan University, Academy of Mathematics and Systems Science CAS</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21164" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21164</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed a three-precision formulation of the General Alternating-Direction Implicit (GADI) method to accelerate solving large sparse linear systems by using low precision for subsystems and high precision for residuals and updates. 2. Developed a rigorous rounding error analysis for the mixed-precision GADI method, establishing convergence rates and conditions for guaranteed convergence. 3. Introduced a systematic strategy using Gaussian Process Regression (GPR) to select the critical regularization parameter α and validated the approach with performance analysis on an NVIDIA A100 GPU, demonstrating significant speedups.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20819fa4bde24c5e32fff1eb73da57db5e29b6596269848aeebf9e887917ff41_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20819fa4bde24c5e32fff1eb73da57db5e29b6596269848aeebf9e887917ff41_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper introduces a mixed-precision version of the General Alternating-Direction Implicit (GADI) method to solve large sparse linear systems faster. The method solves subsystems in low precision (like Bfloat16) for speed, while keeping residual and solution updates in high precision for accuracy. The authors provide an error analysis, a strategy for parameter selection, and demonstrate speedups of up to 3.1x on GPU for large-scale problems.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [sec], [adversarial attacks], [hard-label black-box attacks, query efficiency, ray search optimization, Nesterov&#x27;s Accelerated Gradient, momentum-based optimization]</p>
</li>
<li class="">
<p><strong>authors:</strong> Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma</p>
</li>
<li class="">
<p><strong>institution:</strong> Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21241</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="https://github.com/machanic/hard_label_attacks" target="_blank" rel="noopener noreferrer" class="">https://github.com/machanic/hard_label_attacks</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov&#x27;s Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [cv], [surgical scene segmentation], [spiking neural networks, video transformer, masked autoencoding, real-time inference, surgical video]</p>
</li>
<li class="">
<p><strong>authors:</strong> Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan</p>
</li>
<li class="">
<p><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; University of Alberta; Yale University; Southern University of Science and Technology; Nanfang Hospital Southern Medical University; Shenzhen University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21284" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21284</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proposes SpikeSurgSeg, the first spike-driven video Transformer framework for surgical scene segmentation with real-time potential on non-GPU platforms. 2. Introduces a surgical-scene masked autoencoding pretraining strategy for SNNs using layer-wise tube masking to learn robust spatiotemporal representations from limited labeled data. 3. Designs a lightweight spike-driven segmentation head that maintains temporal consistency and the low-latency characteristics of SNNs.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper addresses the challenge of real-time surgical scene segmentation in resource-constrained environments by proposing SpikeSurgSeg, a spike-driven video Transformer based on Spiking Neural Networks (SNNs). The method uses a masked autoencoding pretraining strategy and a lightweight segmentation head to achieve efficient inference. Experiments show it matches the accuracy of state-of-the-art ANN models while reducing latency by at least 8x and achieving over 20x acceleration compared to foundation models, demonstrating its potential for time-critical surgical applications.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [mlsys], [diffusion models], [video generation, inference acceleration, redundancy elimination, autoregressive framework, feature caching]</p>
</li>
<li class="">
<p><strong>authors:</strong> Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua</p>
</li>
<li class="">
<p><strong>institution:</strong> Meta AI, Nanyang Technological University</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21338" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21338</a></p>
</li>
<li class="">
<p><strong>code:</strong> <a href="http://haonanqiu.com/projects/HiStream.html" target="_blank" rel="noopener noreferrer" class="">http://haonanqiu.com/projects/HiStream.html</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Introduces a three-axis redundancy elimination framework (spatial, temporal, timestep) for efficient high-resolution video generation. 2. Proposes a dual-resolution caching mechanism for spatial compression and a chunk-by-chunk strategy with a fixed-size anchor cache for temporal compression. 3. Demonstrates state-of-the-art visual quality with up to 107.5x faster denoising compared to baselines, making 1080p video generation practical.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> The paper addresses the computational bottleneck of high-resolution video generation by proposing HiStream, an autoregressive framework that eliminates redundancy across spatial, temporal, and timestep dimensions. The method achieves significant speedups (up to 107.5x) with negligible quality loss, making high-fidelity 1080p video generation scalable and practical.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] The Dynamical Anatomy of Anderson Acceleration<!-- -->:From<!-- --> Adaptive Momentum to Variable-Mass ODEs</strong></p>
<ul>
<li class="">
<p><strong>tags:</strong> [ai], [optimization algorithms], [Anderson acceleration, high-resolution ODEs, variable effective mass, Lyapunov analysis, Energy-Guarded Anderson Acceleration]</p>
</li>
<li class="">
<p><strong>authors:</strong> Kewang Chen, Yongqiu Jiang, Kees Vuik</p>
</li>
<li class="">
<p><strong>institution:</strong> Nanjing University of Information Science and Technology, Delft University of Technology</p>
</li>
<li class="">
<p><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21269</a></p>
</li>
<li class="">
<p><strong>contributions:</strong> 1. Proved that Anderson Acceleration can be exactly rewritten as an adaptive momentum method and converges to a second-order ODE with Variable Effective Mass in the high-resolution limit. 2. Identified the instability mechanism of standard AA via Lyapunov energy analysis, showing unchecked effective mass growth acts as negative damping. 3. Proposed a novel, thermodynamically consistent algorithm called Energy-Guarded Anderson Acceleration (EG-AA) with proven stability and improved convergence rates.</p>
</li>
<li class="">
<p><strong>thumbnail:</strong> <a href="https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c986ecaf2f7e88b09c248980802200bca289a4a46ee7e71b2b06dfcab59b08_w640_q70.webp" target="_blank" rel="noopener noreferrer" class="">https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97c986ecaf2f7e88b09c248980802200bca289a4a46ee7e71b2b06dfcab59b08_w640_q70.webp</a></p>
</li>
<li class="">
<p><strong>Simple LLM Summary:</strong> This paper analyzes the dynamics of Anderson Acceleration (AA) through high-resolution ODEs, revealing its instability stems from variable effective mass acting as negative damping. Based on this insight, the authors propose Energy-Guarded Anderson Acceleration (EG-AA), an algorithm that enforces energy dissipation. Theoretical and numerical results show EG-AA provides improved convergence stability and rates compared to standard AA.</p>
</li>
<li class="">
<p><strong>Mindmap:</strong></p>
<!-- -->
</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T09:28:57.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/20251229-20260104"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251229-20260104</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/20251222/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/20251222/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>