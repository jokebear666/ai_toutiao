<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-18"><meta data-rh="true" property="og:description" content="2025-12-18"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/category/cscv"},{"@type":"ListItem","position":3,"name":"20251215-20251221 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.cc04cc53.css">
<script src="/ai_toutiao/assets/js/runtime~main.25ffb784.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.d71a22b2.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/category/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads"><div class="content-main"><div class="content-inner"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]</li>
<li class=""><strong>authors:</strong> Krunal Jesani, Dmitry Ignatov, Radu Timofte</li>
<li class=""><strong>institution:</strong> University of Würzburg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]</li>
<li class=""><strong>authors:</strong> Ryan Cartularo</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [planner-executor framework, large language models (LLMs), vision-language models (VLMs), segmentation models, image processing]</li>
<li class=""><strong>authors:</strong> Idan Tankel, Nir Mazor, Rafi Brada, Christina LeBedis, Guy ben-Yosef</li>
<li class=""><strong>institution:</strong> GE Healthcare Technology and Innovation Center, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework that uses a planner-executor approach, where an LLM planner generates Python scripts to automate the detection and reporting of incidental findings in abdominal CT scans, and an executor runs these scripts using VLMs and segmentation models. The method is fully automatic and end-to-end. The results show that this framework outperforms pure VLM-based approaches in accuracy and efficiency for managing incidental findings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [optical-SAR dataset, label transfer, foundation models, amplitude change detection, continued pretraining, preprocessing]</li>
<li class=""><strong>authors:</strong> Paul Weinmann, Ferdinand Schenck, Martin Šiklar</li>
<li class=""><strong>institution:</strong> LiveEO GmbH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14755</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkyCap, a dataset pairing optical and SAR imagery for change detection, and uses optical-to-SAR label transfer to train models without expert SAR annotations. It benchmarks foundation models on this dataset and finds that an optical model with specific preprocessing outperforms SAR-specific models, highlighting the sensitivity of results to preprocessing alignment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [mixture-of-experts, reinforcement fine-tuning, semantic similarity reward, vision language model, socially compliant navigation]</li>
<li class=""><strong>authors:</strong> Tomohito Kawabata, Xinyu Zhang, Ling Xiao</li>
<li class=""><strong>institution:</strong> Hokkaido University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14757</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant robot navigation, which is fine-tuned using reinforcement learning with a novel semantic similarity reward. The method balances navigation accuracy and computational efficiency by exploring different small language models and vision encoders. Experiments show it outperforms baseline rewards and achieves a good trade-off for real-time deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [expert system, computer vision, phrase correlation, skeleton analysis, unsupervised deep learning, image feature embeddings, optical music recognition]</li>
<li class=""><strong>authors:</strong> Fan Bu, Rongfeng Li, Zijin Li, Ya Li, Linfeng Fan, Pei Huang</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Central Conservatory of Music, Shanghai Normal University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14758" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14758</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a hybrid expert-system pipeline for optical recognition of printed Chinese Jianpu musical scores with lyrics, combining traditional computer-vision techniques with unsupervised deep-learning modules. The system successfully digitizes a large collection of folk songs into MusicXML and MIDI without requiring massive annotated data. It achieves high-precision recognition for both melody and aligned lyrics, demonstrating a balance between interpretability and accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, cross-attention, residual dense blocks, multi-resolution attention, cross-domain consistency loss]</li>
<li class=""><strong>authors:</strong> Afrah Shaahid, Muzammil Behzad</li>
<li class=""><strong>institution:</strong> King Fahd University of Petroleum and Minerals, SDAIA–KFUPM Joint Research Center for Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14760</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AquaDiff, a diffusion-based framework for underwater image enhancement that uses a chromatic prior-guided color compensation strategy and a conditional diffusion process with cross-attention. It achieves superior color correction and competitive overall image quality compared to state-of-the-art methods across diverse underwater conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [self-reflection, cross-model verification, dual-assessment, uncertainty estimation, selective prediction, hallucination mitigation]</li>
<li class=""><strong>authors:</strong> Xixian Wu, Yang Ou, Pengchao Tian, Zian Yang, Jielei Zhang, Peiyi Li, Longwen Gao</li>
<li class=""><strong>institution:</strong> Bilibili Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14770</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DAVR, a framework that enhances VQA reliability by combining a model&#x27;s self-assessment of its answer confidence with cross-verification using external models to reduce hallucinations. It achieved top results in a reliability challenge, demonstrating its effectiveness in improving the trustworthiness of vision-language model responses.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video question answering], [multi-evidence integration, Minimum Required Frame-Set (MRFS), compositional tasks, retrieval deficit, fusion deficit]</li>
<li class=""><strong>authors:</strong> Dan Ben-Ami, Gabriele Serussi, Kobi Cohen, Chaim Baskin</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14870" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14870</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces HERBench, a new VideoQA benchmark designed to require the integration of at least three distinct pieces of visual evidence across different video segments, using a metric called Minimum Required Frame-Set (MRFS) to quantify this demand. Evaluating 13 state-of-the-art Video-LLMs reveals their performance is only slightly above random chance, exposing two major bottlenecks: a retrieval deficit in selecting key frames and a fusion deficit in integrating the provided evidence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Isolated Sign Language Recognition with Segmentation and Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [pose estimation, segmentation, ResNet, Transformer, isolated sign language recognition]</li>
<li class=""><strong>authors:</strong> Daniel Perkins, Davis Hunter, Dhrumil Patel, Galen Flanagan</li>
<li class=""><strong>institution:</strong> University of Tennessee, Knoxville</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a model for Isolated Sign Language Recognition that integrates pose estimation to extract joint coordinates, a segmentation module to isolate relevant pixels, and a hybrid ResNet-Transformer backbone to model spatial and temporal dependencies. This approach aims to reduce computational requirements while maintaining robustness to signer variation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [dermatoglyphic textual descriptors, cross-modal retrieval, text-image co-synthesis, virtual individuals, minutiae matching]</li>
<li class=""><strong>authors:</strong> Wenshuo Li, Majid Mirmehdi, Tilo Burghardt</li>
<li class=""><strong>institution:</strong> University of Bristol</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14878</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a visual-textual method for animal re-identification that pairs images with dermatoglyphic textual descriptions of coat patterns. It uses a text-image co-synthesis pipeline to generate virtual individuals, significantly improving cross-modal retrieval accuracy. The approach overcomes limitations of vision-only systems by enabling human-verifiable, text-to-visual identity matching.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [model adaptation], [task matrix, linear transformation, finetuning, linear probe, embedding space]</li>
<li class=""><strong>authors:</strong> Darrin O&#x27; Brien, Dhikshith Gajulapalli, Eric Xia</li>
<li class=""><strong>institution:</strong> Algoverse AI Research, Brown University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the concept of a &quot;task matrix,&quot; a linear transformation that maps a base model&#x27;s embedding state to a finetuned model&#x27;s state. It demonstrates that applying this matrix to a base model can outperform linear probes and sometimes approach full finetuning performance across vision and text models on various datasets. The results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vibe blending, vibe space, hierarchical graph manifold, geodesics, clip, diffusion models]</li>
<li class=""><strong>authors:</strong> Huzheng Yang, Katherine Xu, Andrew Lu, Michael D. Grossberg, Yutong Bai, Jianbo Shi</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, CUNY, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP to enable smooth, semantically consistent transitions between visual concepts for a novel task called Vibe Blending. It concludes that this method produces blends rated as more creative and coherent than current approaches, as evaluated by a framework combining human judgments, LLM reasoning, and a geometric difficulty score.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [foundation models, Gleason grading, whole slide images, self-supervised learning, feature embeddings, slide-level encoding, cross-slide accuracy, benchmark dataset]</li>
<li class=""><strong>authors:</strong> Joshua L. Ebbert, Dennis Della Corte</li>
<li class=""><strong>institution:</strong> Brigham Young University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14922" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14922</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PANDA-PLUS-Bench, a curated benchmark dataset of prostate biopsy whole slide images designed to evaluate the robustness of AI foundation models in distinguishing between biological features and slide-specific artifacts for Gleason grading. It tests seven models and finds that tissue-specific training (e.g., HistoEncoder) improves cross-slide accuracy and slide-level encoding, but all models show a significant gap between within-slide and cross-slide performance, highlighting a critical robustness issue.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving Pre-trained Segmentation Models using Post-Processing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adaptive post-processing, glioma segmentation, brain MRI, BraTS challenge, resource-aware AI]</li>
<li class=""><strong>authors:</strong> Abhijeet Parida, Daniel Capellán-Martín, Zhifan Jiang, Nishad Kulkarni, Krithika Iyer, Austin Tapp, Syed Muhammad Anwar, María J. Ledesma-Carbayo, Marius George Linguraru</li>
<li class=""><strong>institution:</strong> Children&#x27;s National Hospital, Universidad Politécnica de Madrid, George Washington University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14937" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14937</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes adaptive post-processing techniques to refine the segmentation outputs of large-scale pre-trained models for brain tumor (glioma) segmentation in MRI, addressing systematic errors like false positives. The method was validated in BraTS 2025 challenges, showing significant metric improvements. The work advocates for a shift from complex model architectures to efficient, clinically aligned post-processing for more precise and sustainable medical image analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]</li>
<li class=""><strong>authors:</strong> Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Snap Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Puzzle Curriculum GRPO for Vision-Centric Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [PC-GRPO, RL with Verifiable Rewards, self-supervised puzzle environments, difficulty-aware curriculum, reasoning-answer consistency]</li>
<li class=""><strong>authors:</strong> Ahmadreza Jeddi, Hakki Can Karaimer, Hue Nguyen, Zhongling Wang, Ke Zhao, Javad Rajabi, Ran Zhang, Raghav Goyal, Babak Taati, Radek Grzeszczuk</li>
<li class=""><strong>institution:</strong> Samsung Electronics, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14944</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Puzzle Curriculum GRPO (PC-GRPO), a supervision-free reinforcement learning method that uses self-supervised puzzle environments and a difficulty-aware curriculum to improve visual reasoning in vision language models. It enhances reasoning quality, training stability, and downstream accuracy without relying on annotations or external verifiers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multi-task learning, cross-attention, gated fusion, confidence-weighted fusion, adaptive fusion]</li>
<li class=""><strong>authors:</strong> Aref Farhadipour, Teodora Vukovic, Volker Dellwo, Petr Motlicek, Srikanth Madikeri</li>
<li class=""><strong>institution:</strong> University of Zurich, Idiap Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14961" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14961</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a robust trimodal person recognition framework that integrates voice, face, and gesture data using multi-task learning, cross-attention, and a gated, confidence-weighted fusion strategy to handle missing or degraded modalities. It achieves high accuracy on the CANDOR and VoxCeleb1 datasets and maintains performance even when modalities are unavailable, demonstrating robustness for real-world applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal reasoning, chain-of-thought prompting, ablation studies, occlusion-based interpretability, benchmark evaluation, modality fusion]</li>
<li class=""><strong>authors:</strong> Yiming Cui, Xin Yao, Yuxuan Qin, Xin Li, Shijin Wang, Guoping Hu</li>
<li class=""><strong>institution:</strong> State Key Laboratory of Cognitive Intelligence, iFLYTEK AI Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14989</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates 40 multimodal large language models on a benchmark of chemistry Olympiad questions requiring visual and textual reasoning. The core method involves using chain-of-thought prompting and interpretability techniques like ablation and occlusion. The main conclusion is that current models struggle with modality fusion, but chain-of-thought improves both accuracy and visual grounding, revealing critical limitations in scientific reasoning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Where is the Watermark? Interpretable Watermark Detection at the Block Level</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [image watermarking], [discrete wavelet transform, block-wise embedding, detection maps, post-hoc watermarking]</li>
<li class=""><strong>authors:</strong> Maria Bulychev, Neil G. Marchant, Benjamin I. P. Rubinstein</li>
<li class=""><strong>institution:</strong> University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14994" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14994</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an interpretable, post-hoc image watermarking method that embeds signals in the discrete wavelet transform domain using a statistical block-wise strategy. It generates detection maps to show which specific regions of an image are watermarked or altered. The method maintains strong robustness against common image transformations and high imperceptibility while providing more interpretable detection than prior approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [YOLOv11, ByteTrack, ZebraPose, support vector machine, keypoint trajectories, pose estimation]</li>
<li class=""><strong>authors:</strong> Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan</li>
<li class=""><strong>institution:</strong> Dalhousie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a computer vision framework that uses pose estimation and keypoint trajectories to classify social interactions in dairy cattle, moving beyond simple proximity measures. The method integrates object detection, tracking, and a support vector machine to distinguish affiliative from agonistic behaviors with 77.51% accuracy. The results demonstrate improved behavioral discrimination and establish a proof-of-concept for automated, interaction-aware social network analysis in precision livestock farming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video question generation], [video question generation, question-to-answer retrieval, EgoExoAsk dataset, expert knowledge elicitation]</li>
<li class=""><strong>authors:</strong> Huaying Zhang, Atsushi Hashimoto, Tosho Hirasawa</li>
<li class=""><strong>institution:</strong> OMRON SINIC X Corp., Hokkaido University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15006" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15006</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a retrieval-based evaluation protocol for video question generation (VQG) models, focusing on their ability to elicit expert knowledge. The method uses a question-to-answer retriever trained on a novel dataset, EgoExoAsk, to simulate expert communication. The main conclusion is that this metric effectively aligns with VQG settings, as models with richer context are evaluated better, validating the proposed framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Model Agnostic Preference Optimization for Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [preference optimization, dropout, stochastic segmentation, model-agnostic, DPO, fine-tuning]</li>
<li class=""><strong>authors:</strong> Yunseong Nam, Jiwon Jang, Dongkyu Won, Sang Hyun Park, Soopil Kim</li>
<li class=""><strong>institution:</strong> DGIST (Daegu Gyeongbuk Institute of Science and Technology)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15009</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MAPO, a model-agnostic preference optimization framework for medical image segmentation that uses dropout to generate stochastic segmentation hypotheses for training, eliminating the need for direct ground-truth supervision. It is designed to be compatible with various architectures like 2D/3D CNNs and Transformers. The method is shown to improve boundary accuracy, reduce overfitting, and provide more stable training compared to standard supervised approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]</li>
<li class=""><strong>authors:</strong> Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15047</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D Gaussian Splatting, super-resolution, multi-view attention, epipolar guidance, novel view synthesis]</li>
<li class=""><strong>authors:</strong> Kaizhe Zhang, Shinan Chen, Qian Zhao, Weizhan Zhang, Caixia Yan, Yudeng Xin</li>
<li class=""><strong>institution:</strong> Xi’an Jiaotong University, University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15048</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MVGSR, a framework for 3D Gaussian Splatting super-resolution that enhances detail and consistency by integrating multi-view information. Its core innovations are an auxiliary view selection method for unstructured datasets and an epipolar-constrained multi-view attention mechanism. The method achieves state-of-the-art performance on object-centric and scene-level benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [event camera, LED markers, noise filtering, spatiotemporal correlation, monocular vision, high-frequency deformation measurement]</li>
<li class=""><strong>authors:</strong> Yifei Bian, Banglei Guan, Zibin Liu, Ang Su, Shiyao Zhu, Yang Shang, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology, Hunan Provincial Key Laboratory of Image Measurement and Vision Navigation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15055</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to measure high-frequency structural deformations using an event camera and blinking LED markers. The core technique involves filtering noise from the asynchronous event stream and differentiating between motion-induced and LED-blinking events to extract marker positions. Experimental results confirm the method&#x27;s accuracy for planar deformation measurement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [wavelet analysis, memory bank, encoder-decoder, cross-attention, high-frequency feature fusion]</li>
<li class=""><strong>authors:</strong> Chenxiao Zhang, Runshi Zhang, Junchen Wang</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text. Affiliation inference is not possible from the author names alone without email domains or explicit institutional mentions.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15066</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a network combining wavelet analysis and a memory bank to segment objects in long ultrasound videos. The method uses memory-based wavelet convolution and high-frequency-aware feature fusion to capture fine details and track objects over time. It demonstrates improved segmentation accuracy, particularly for small nodules, on several ultrasound datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PMMD: A pose-guided multi-view multi-modal diffusion for person generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [diffusion training], [diffusion framework, multimodal encoder, ResCVA module, cross modal fusion, pose-guided generation]</li>
<li class=""><strong>authors:</strong> Ziyu Shang, Haoran Liu, Rongchao Zhang, Zhiqian Wei, Tongtong Feng</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen, City University of Hong Kong, Peking University, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15069</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PMMD is a pose-guided multi-view multimodal diffusion framework that synthesizes photorealistic person images by jointly modeling visual views, pose features, and text prompts. It introduces a ResCVA module for local detail enhancement and a cross-modal fusion module to integrate image semantics with text. Experiments show PMMD outperforms baselines in consistency, detail preservation, and controllability on the DeepFashion MultiModal dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Uni-Parser Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multi-expert architecture, cross-modal alignment, adaptive GPU load balancing, distributed inference, dynamic module orchestration]</li>
<li class=""><strong>authors:</strong> Xi Fang, Haoyi Tao, Shuwen Yang, Suyang Zhong, Haocheng Lu, Han Lyu, Chaozheng Huang, Xinyu Li, Linfeng Zhang, Guolin Ke</li>
<li class=""><strong>institution:</strong> DP Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15098</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uni-Parser is an industrial-grade document parsing engine that uses a modular, loosely coupled multi-expert architecture to extract and align information from text, equations, tables, figures, and chemical structures in PDFs. It is optimized for large-scale cloud deployment, achieving high throughput and cost efficiency, enabling the processing of billions of scientific and patent pages for downstream AI applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [zero-shot evaluation, text-to-image generation, low-level vision tasks, subjective visual quality, reference-based metrics]</li>
<li class=""><strong>authors:</strong> Jialong Zuo, Haoyou Deng, Hanyu Zhou, Jiaxin Zhu, Yicheng Zhang, Yiwei Zhang, Yongxin Yan, Kaixing Huang, Weisen Chen, Yongtai Deng, Rui Jin, Nong Sang, Changxin Gao</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15110" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15110</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a comprehensive zero-shot evaluation of the Nano Banana Pro text-to-image model across 14 low-level vision tasks using simple textual prompts. It finds that while the model produces subjectively superior visual quality by hallucinating plausible details, it underperforms specialist models on traditional pixel-level quantitative metrics due to its inherent stochasticity. The study concludes that Nano Banana Pro is a capable zero-shot contender but faces challenges in matching the high fidelity of domain-specific models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [particle filter, bird&#x27;s-eye-view (BEV), feature matching, cross-view geo-localization, absolute trajectory error (ATE)]</li>
<li class=""><strong>authors:</strong> Dongmyeong Lee, Jesse Quattrociocchi, Christian Ellis, Rwik Rana, Amanda Adkins, Adam Uccello, Garrett Warnell, Joydeep Biswas</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, DEVCOM Army Research Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15111</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BEV-Patch-PF, a sequential geo-localization system that uses a particle filter to match learned bird&#x27;s-eye-view features from onboard RGB-D images with patches from aerial feature maps. The method significantly outperforms retrieval-based baselines in off-road environments, achieving much lower trajectory error on both seen and unseen routes while running in real-time.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] 3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [3D animation generation], [2D-3D aligned proxy representation, single-image 3D animation, coarse 3D estimate, image-space generative priors]</li>
<li class=""><strong>authors:</strong> Yupeng Zhu, Xiongzhen Zhang, Ye Chen, Bingbing Ni</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15126</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes 3DProxyImg, a lightweight framework for generating controllable 3D animations from a single image. It uses a coarse 3D proxy to handle geometric control while leveraging learned 2D generative priors for high-fidelity appearance, enabling efficient animation with precise 3D-aware motion control. The method outperforms video-based approaches in identity preservation, consistency, and interactive control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Borrowing from anything: A generalizable framework for reference-guided instance editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [Spatial Alignment Module (SAM), Adaptive Residual Scaling Module (ARSM), Progressive Attention Fusion (PAF), diffusion models, semantic disentanglement]</li>
<li class=""><strong>authors:</strong> Shengxiao Zhou, Chenghua Li, Jianhao Huang, Qinghao Hu, Yifan Zhang</li>
<li class=""><strong>institution:</strong> University of Chinese Academy of Sciences, Nanjing Forestry University, Institute of Automation, Chinese Academy of Sciences, Nanjing Artificial Intelligence Research of IA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15138</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes GENIE, a framework for reference-guided instance editing that addresses semantic entanglement through three modules: a Spatial Alignment Module (SAM) for correcting pose and scale, an Adaptive Residual Scaling Module (ARSM) for amplifying intrinsic appearance cues, and a Progressive Attention Fusion (PAF) mechanism for rendering the appearance onto a target. The method achieves state-of-the-art performance on the AnyInsertion dataset, demonstrating high fidelity and robustness in disentanglement-based editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Chain-of-Thought reasoning, dynamic gating mechanism, multimodal fusion, action quality assessment, video understanding]</li>
<li class=""><strong>authors:</strong> Mengshi Qi, Yeteng Wu, Xianlin Zhang, Huadong Ma</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15153</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an Explainable Fitness Assessor framework that uses multimodal Chain-of-Thought reasoning and a dynamic gating mechanism to fuse visual and semantic information for human action form assessment. It introduces a new dataset, CoT-AFA, with detailed explanations for action standardization. The method shows improvements in explanation generation, action classification, and quality assessment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [BEV-grounding, Chain-of-Thought, reinforcement learning, determinantal point process, keyframe selection, spatial reward, pose querying]</li>
<li class=""><strong>authors:</strong> Jiaxu Wan, Xu Wang, Mengwei Xie, Hang Zhang, Mu Xu, Yang Han, Hong Zhang, Ding Yuan, Yifan Yang</li>
<li class=""><strong>institution:</strong> BUAA (Beihang University)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15160</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EagleVision introduces a dual-stage framework for spatial reasoning, combining a macro perception stage for selecting keyframes and a micro verification stage that uses BEV-grounded pose querying and reinforcement learning. It achieves state-of-the-art performance on VSI-Bench, demonstrating strong and generalizable spatial understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [criticality metrics, relevance classification, object detection, safety evaluation, bidirectional criticality rating, multi-metric aggregation, DeepAccident dataset]</li>
<li class=""><strong>authors:</strong> Jörg Gamerdinger, Sven Teufel, Stephan Amann, Oliver Bringmann</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15181</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes criticality metrics for evaluating the safety of object detection systems in automated driving. It proposes two novel strategies, bidirectional criticality rating and multi-metric aggregation, to improve classification accuracy. The approach demonstrates up to a 100% improvement in criticality classification accuracy, advancing the safety evaluation of automated vehicle perception systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [cross-modal ultra-scale learning, sparse multi-instance learning, cross-modal scale attention module, multi-modal feature fusion]</li>
<li class=""><strong>authors:</strong> Kaixing Long, Danyi Weng, Yun Mi, Zhentai Zhang, Yanmeng Lu, Jian Geng, Zhitao Zhou, Liming Zhong, Qianjin Feng, Wei Yang, Lei Cao</li>
<li class=""><strong>institution:</strong> Southern Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15171</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CMUS-Net, a cross-modal ultra-scale learning network designed to fuse features from three types of renal biopsy images (TEM, OM, IM) that have vastly different scales (nanometer vs. micrometer). The method uses a sparse multi-instance learning module and a cross-modal scale attention module to bridge the scale gap and enhance pathological semantic information for classification. The model achieves high accuracy in diagnosing multiple glomerular diseases and demonstrates superior performance compared to other multi-modal or multi-scale methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robust and Calibrated Detection of Authentic Multimedia Content</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deepfake detection], [resynthesis framework, authenticity index, adversarial robustness, calibrated detection, inversion techniques]</li>
<li class=""><strong>authors:</strong> Sarim Hashmi, Abdelrahman Elsayed, Mohammed Talha Alam, Samuele Poppi, Nils Lukas</li>
<li class=""><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15182</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a calibrated resynthesis framework to verify authentic multimedia content by distinguishing between authentic and plausibly deniable samples, rather than binary real vs. fake detection. It demonstrates that this method maintains low false positive rates and achieves adversarial robustness against compute-restricted adversaries, outperforming prior detection approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-scale parallel feature extraction, green channel guidance, channel-aware residual dense block, spatial adaptive normalization]</li>
<li class=""><strong>authors:</strong> Jianan Wang, Yang Hong, Hesong Li, Tao Wang, Songrong Liu, Ying Fu</li>
<li class=""><strong>institution:</strong> Beijing Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15186</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ERIENet, an efficient network for enhancing low-light RAW images using a fully-parallel multi-scale architecture and a green channel guidance branch to leverage the superior information in the green channel. It achieves high-quality reconstruction with reduced computational costs, outperforming state-of-the-art methods in both performance and real-time processing speed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [environmental perception safety metric, object detection safety, lane detection safety, joint safety assessment, criticality classification, DeepAccident dataset]</li>
<li class=""><strong>authors:</strong> Jörg Gamerdinger, Sven Teufel, Stephan Amann, Lukas Marc Listl, Oliver Bringmann</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15195</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel Environmental Perception Safety Metric (EPSM) that jointly evaluates the safety of object and lane detection systems for autonomous driving, integrating a lightweight object safety metric and considering task interdependence. It demonstrates that this approach identifies safety-critical errors missed by conventional metrics like precision and recall. The findings emphasize the need for safety-centric evaluation methods in autonomous vehicle perception.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Camera to World: A Plug-and-Play Module for Human Mesh Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [human mesh reconstruction, camera rotation estimation, depth maps, plug-and-play module, world coordinate system]</li>
<li class=""><strong>authors:</strong> Changhai Ma, Ziyu Wu, Yunkang Zhang, Qijun Ying, Boyan Liu, Xiaohui Cai</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15212" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15212</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Mesh-Plug, a plug-and-play module that transforms human meshes from camera to world coordinates by estimating camera rotation from RGB images and rendered depth maps. It uses a human-centered approach to predict camera pitch and refines the mesh pose. Experiments show it outperforms state-of-the-art methods on benchmark datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, image fusion], [Target-Background Contrast (TBC), Weber&#x27;s Law, no-reference metric, infrared-visible fusion, DroneVehicle dataset]</li>
<li class=""><strong>authors:</strong> Yufeng Xie</li>
<li class=""><strong>institution:</strong> Not specified (Author: Yufeng Xie)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15211" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15211</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new no-reference evaluation metric called Target-Background Contrast (TBC) for low-altitude infrared and visible image fusion. Inspired by Weber&#x27;s Law, TBC focuses on the relative contrast between salient thermal targets and their local background to avoid the &quot;Noise Trap&quot; of traditional metrics. Experiments show that TBC better aligns with human perception and provides a more reliable standard for evaluating fusion results in complex low-light scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer, frequency domain analysis, ZernikeVAE, physics-based synthesis]</li>
<li class=""><strong>authors:</strong> Xiyu Zhu, Wei Wang, Xin Yuan, Xiao Wang</li>
<li class=""><strong>institution:</strong> Wuhan University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15221</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SLCFormer, a transformer-based framework for nighttime lens flare removal, which integrates a frequency domain module for global context and a spatial module for local enhancement, alongside a physics-grounded flare synthesis method. The approach demonstrates state-of-the-art performance on the Flare7K++ dataset and shows robust generalization to real nighttime scenes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Null-LoRA: Low-Rank Adaptation on Null Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [parameter-efficient fine-tuning], [LoRA, null space, low-rank adaptation, singular value decomposition, parameter efficiency]</li>
<li class=""><strong>authors:</strong> Yi Zhang, Yulei Kang, Haoxuan Chen, Jinxuan Li, ian-Fang Hu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15233</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Null-LoRA, a parameter-efficient fine-tuning method that constrains low-rank adaptation updates within the null space of pre-trained model weights. It freezes portions of low-rank matrices and uses SVD to project updates, reducing redundancy and improving parameter efficiency. Experiments show it outperforms state-of-the-art methods with fewer parameters on vision-language tasks like image-text retrieval and VQA.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [visual enumeration, vision-language models, zero-shot counting, intermediate representations, object counting]</li>
<li class=""><strong>authors:</strong> Kuinan Hou, Jing Mi, Marco Zorzi, Lamberto Ballan, Alberto Testolin</li>
<li class=""><strong>institution:</strong> University of Padova</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares specialized counting architectures and vision-language models (VLMs) for visual enumeration. It finds that VLMs can match or surpass specialized models, especially when prompted to generate intermediate object representations, but still struggle with complex scenes, indicating a need for further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Cross-Modal Alignment Consistency (CMAC-MMD), vision-language model (VLM), intersectional fairness, diagnostic certainty, True Positive Rate (TPR), Area Under the Curve (AUC)]</li>
<li class=""><strong>authors:</strong> Yupeng Zhang, Adam G. Dunn, Usman Naseem, Jinman Kim</li>
<li class=""><strong>institution:</strong> The University of Sydney, Macquarie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15249</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training framework called Cross-Modal Alignment Consistency (CMAC-MMD) to reduce intersectional bias in vision-language models for medical diagnosis by standardizing diagnostic certainty across patient subgroups. The method improves both fairness, by reducing the gap in missed diagnoses, and overall accuracy, as demonstrated on skin lesion and glaucoma screening tasks. It provides a scalable approach for equitable clinical AI without requiring sensitive demographic data during inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Mamba architecture, in-context conditioning, multimodal interleaved scanning, cross-modal fusion, pan-sharpening, zero-shot super-resolution]</li>
<li class=""><strong>authors:</strong> Yingying Wang, Xuanhua He, Chen Wu, Jialing Huang, Suiyun Zhang, Rui Liu, Xinghao Ding, Haoxuan Che</li>
<li class=""><strong>institution:</strong> Xiamen University, The Hong Kong University of Science and Technology, University of Science and Technology of China, Huawei Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15261" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15261</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MMMamba, a cross-modal fusion framework based on the Mamba architecture for pan-sharpening, which also supports zero-shot image super-resolution. It introduces a multimodal interleaved scanning mechanism for efficient information exchange between panchromatic and multispectral images. Experiments show the method outperforms state-of-the-art techniques while maintaining linear computational complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird&#x27;s-Eye-View Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [knowledge distillation, bird&#x27;s-eye-view segmentation, cross-modality, voxel-aligned view transformer, LiDAR image representation, 360-degree camera]</li>
<li class=""><strong>authors:</strong> Wenke E, Yixin Sun, Jiaxu Liu, Hubert P. H. Shum, Amir Atapour-Abarghouei, Toby P. Breckon</li>
<li class=""><strong>institution:</strong> Durham University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15311</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces KD360-VoxelBEV, a cross-modality knowledge distillation framework where a Teacher network fusing LiDAR and camera data trains a lightweight Student network that uses only a single 360-degree panoramic camera for Bird&#x27;s-Eye-View segmentation. The method achieves significant performance gains and fast inference, demonstrating a practical, cost-effective solution for autonomous driving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-agent framework, large language models (LLMs), vision-language models (VLMs), CLIP, ViT classifier, synthetic data generation, zero-shot learning]</li>
<li class=""><strong>authors:</strong> Wangyu Wu, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</li>
<li class=""><strong>institution:</strong> Xi’an Jiaotong-Liverpool University, University of Liverpool, Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15310</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SynthSeg-Agents, a multi-agent framework using LLMs to generate synthetic training data without real images for zero-shot weakly supervised semantic segmentation. It employs a prompt refinement agent and an image generation agent with VLMs, followed by CLIP-based filtering and classifier relabeling. The method achieves competitive performance on standard datasets, demonstrating the potential of LLM-driven agents for cost-efficient segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [supervised contrastive learning, affinity scores, interpretable grading]</li>
<li class=""><strong>authors:</strong> Antony Jerald, Dattesh Shanbhag, Sudhanya Chatterjee</li>
<li class=""><strong>institution:</strong> GE HealthCare</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15315</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AutoMAC-MRI, an interpretable framework that uses supervised contrastive learning to detect and grade motion artifacts in MRI images. It computes grade-specific affinity scores to quantify an image&#x27;s proximity to each motion severity level, making the grading process transparent. The method was validated on over 5,000 expert-annotated brain MRI slices and shows potential for reducing unnecessary rescans and improving workflow efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [anomaly detection], [prototypical learning, context-aware segmentation, few-shot learning, pixel-level disparity classification, pseudo anomalies]</li>
<li class=""><strong>authors:</strong> Yuxin Jiang, Yunkang Cao, Weiming Shen</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15319</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PCSNet, a network combining prototypical feature adaptation and context-aware segmentation to address the domain gap in few-shot anomaly detection. It improves feature compactness for normal data and separation from anomalies, achieving superior performance on benchmark datasets and real-world inspection tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MECAD: A multi-expert architecture for continual anomaly detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-expert architecture, continual learning, anomaly detection, coreset selection, replay buffer, incremental learning, MVTec AD dataset, AUROC]</li>
<li class=""><strong>authors:</strong> Malihe Dahmardeh, Francesco Setti</li>
<li class=""><strong>institution:</strong> University of Verona, Qualyco S.r.l.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15323</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MECAD, a multi-expert architecture for continual anomaly detection that dynamically assigns experts to object classes and uses memory management with coreset selection and a replay buffer for incremental learning. The method achieves an average AUROC of 0.8259 on the MVTec AD dataset, reducing knowledge degradation compared to single-expert approaches and balancing efficiency and adaptability for industrial use.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [masked reverse knowledge distillation, image-level masking, feature-level masking, image anomaly detection, knowledge distillation]</li>
<li class=""><strong>authors:</strong> Yuxin Jiang, Yunkang Can, Weiming Shen</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15326</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Masked Reverse Knowledge Distillation (MRKD) method for image anomaly detection, which uses image-level and feature-level masking to transform reconstruction into restoration, mitigating overgeneralization. Experiments on the MVTec dataset show the method achieves state-of-the-art performance in anomaly detection and localization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Vision-based module for accurately reading linear scales in a laboratory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vision-based models, object detection, image classification, instance segmentation, feature extraction, orientation correction]</li>
<li class=""><strong>authors:</strong> Parvesh Saini, Soumyadipta Maiti, Beena Rai</li>
<li class=""><strong>institution:</strong> TCS Research, Tata Consultancy Services Limited</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15327</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a vision-based module that mimics a human-inspired approach to read measurements from linear scales, such as on syringes and measuring cylinders, by correcting orientation, isolating the scale region, and extracting features like markers and digits. The system&#x27;s readings were compared against human-read values and showed accurate correspondence, demonstrating its potential for laboratory automation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Preprocessing Framework for Video Machine Vision under Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [video compression, neural preprocessor, differentiable virtual codec, rate-accuracy optimization]</li>
<li class=""><strong>authors:</strong> Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie</li>
<li class=""><strong>institution:</strong> Peking University, Bytedance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15331</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [turn-level causal attention, interleaved masked autoregression, lightweight diffusion head, multimodal fusion]</li>
<li class=""><strong>authors:</strong> Junjie Chen, Fei Wang, Zhihao Huang, Qing Zhou, Kun Li, Dan Guo, Linfeng Zhang, Xun Yang</li>
<li class=""><strong>institution:</strong> Hefei University of Technology, IAI Hefei Comprehensive National Science Center, USTC, SJTU, TeleAI China Telecom, Northwestern Polytechnical University, United Arab Emirates University, Anhui Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15340</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TIMAR, a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual turns, using intra-turn multimodal fusion and turn-level causal attention to accumulate history. It employs a lightweight diffusion head to predict continuous and coherent head dynamics. Experiments show TIMAR reduces Fréchet Distance and MSE by 15-30% compared to prior methods, demonstrating improved temporal coherence and contextual responsiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Group Relative Policy Optimization (GRPO), Optimal Variance Filtering (OVF), Pro-GRPO, Expand-and-Prune, reward clustering, latent feature-based pruning]</li>
<li class=""><strong>authors:</strong> Shiran Ge, Chenyi Huang, Yuang Ai, Qihang Fan, Huaibo Huang, Ran He</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15347</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Pro-GRPO, a dynamic framework that improves Group Relative Policy Optimization (GRPO) for generative model alignment by integrating an &quot;Expand-and-Prune&quot; strategy. It first expands the sampling group for diversity and then prunes reward-clustered trajectories early using latent features to reduce computational cost. Experiments on diffusion and flow models show that Pro-GRPO is more efficient and effective than standard GRPO.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]</li>
<li class=""><strong>authors:</strong> Mikel Williams-Lekuona, Georgina Cosma</li>
<li class=""><strong>institution:</strong> Loughborough University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15372</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Emotion Recognition in Signers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [cross-lingual transfer, temporal segment selection, hand motion features, textual emotion recognition, facial expression analysis]</li>
<li class=""><strong>authors:</strong> Kotaro Funakoshi, Yaoxiong Zhu</li>
<li class=""><strong>institution:</strong> Institute of Science Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15376</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new dataset for emotion recognition in Japanese Sign Language and addresses the challenges of overlapping grammatical/affective expressions and data scarcity using cross-lingual transfer from textual emotion recognition in spoken language. The authors demonstrate that selecting specific temporal segments and incorporating hand motion features significantly improves emotion recognition performance in signers, establishing a stronger baseline than spoken language LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D semantic segmentation, point cloud, deep learning, domain gap analysis, mIoU]</li>
<li class=""><strong>authors:</strong> Maximilian Kellner, Mariana Ferrandon Cervantes, Yuandong Pan, Ruodan Lu, Ioannis Brilakis, Alexander Reiterer</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute for Physical Measurement Techniques IPM, University of Freiburg, University of Cambridge, Sichuan Highway Planning, Survey, Design and Research Institute Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15369" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15369</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SemanticBridge, a novel dataset for 3D semantic segmentation of bridges, and evaluates three state-of-the-art deep learning architectures on it. The study also analyzes the domain gap caused by different sensors, finding that while models perform robustly, sensor variations can reduce performance by up to 11.4% mIoU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [action anticipation, deep learning, video understanding, rebound prediction, temporal modeling]</li>
<li class=""><strong>authors:</strong> Arnau Barrera Roy, Albert Clapés Sintes</li>
<li class=""><strong>institution:</strong> Universitat de Barcelona, Computer Vision Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15386</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a deep learning-based method for anticipating rebounds in basketball broadcast videos, using a new large-scale dataset. It applies state-of-the-art action anticipation techniques to predict which team will gain possession after a shot. The results demonstrate the feasibility and challenges of predictive modeling in dynamic sports scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, distribution alignment, semantic graph, partially view-aligned clustering]</li>
<li class=""><strong>authors:</strong> Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong</li>
<li class=""><strong>institution:</strong> Shantou University, Huaqiao University, Southwest University, South China University of Technology, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15396</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMART, a model for partially view-aligned clustering that uses view distribution alignment and semantic matching contrastive learning to handle misaligned multi-view data. It aligns cross-view covariance matrices to reduce distribution shifts and leverages a semantic graph to guide contrastive learning, improving clustering performance. Experiments on eight datasets show that SMART outperforms existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [biomedical imaging], [channel-independent architecture, contrastive pretraining, self-supervised learning, lightweight CNN, multiplex imaging]</li>
<li class=""><strong>authors:</strong> Simon Gutwein, Arthur Longuefosse, Jun Seita, Sabine Taschner-Mandl, Roxane Licandro</li>
<li class=""><strong>institution:</strong> St. Anna Children&#x27;s Cancer Research Institute, RIKEN Center for Integrative Medical Sciences, Medical University of Vienna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15410</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a shallow Channel-Independent Model (CIM-S) for self-supervised representation learning on multiplexed tissue imaging data, which processes each protein marker channel separately to preserve specificity. The study finds that this lightweight, channel-independent approach outperforms standard early-fusion CNNs in retaining marker information and discriminating rare cell types, demonstrating that compact architectures can surpass deeper models for this task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [human-robot mutual imitation, kinematic rules, left/right hand coordinate systems, vision-language-action model, behavioral priors, cross-embodiment generalization]</li>
<li class=""><strong>authors:</strong> Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, ingkuan Song, Lianli Gao, Heng Tao Shen</li>
<li class=""><strong>institution:</strong> Tongji University, University of Electronic Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15411</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MiVLA, a vision-language-action model pre-trained using human-robot mutual imitation, which aligns human and robot action spaces via kinematic rules and coordinate systems to integrate behavioral knowledge from human videos and simulated robot data. This approach enhances generalization across different camera views, appearances, and robot embodiments. Experiments on multiple robots show MiVLA outperforms state-of-the-art models in both simulation and real-world control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [CLIP, StyleGAN, face template inversion, cross-modal feature interaction, attribute conditioning]</li>
<li class=""><strong>authors:</strong> Longchen Dai, Zixuan Shen, Zhiheng Zhou, Peipeng Yu, Zhihua Xia</li>
<li class=""><strong>institution:</strong> Jinan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15433</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CLIP-FTI, a method for inverting face recognition templates into photorealistic images by using CLIP to extract semantic facial attribute embeddings and fusing them with the template via a cross-modal network to condition a pretrained StyleGAN. The approach achieves state-of-the-art results in identity accuracy, fine-grained attribute recovery, and cross-model attack transferability compared to prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [monocular depth estimation, 3D hallucination, Laplacian-based evaluation, grounded self-distillation, depth foundation models]</li>
<li class=""><strong>authors:</strong> Hoang Nguyen, Xiaohao Xu, Xiaonan Huang</li>
<li class=""><strong>institution:</strong> University of Michigan, Ann Arbor</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15423</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a framework to address the &quot;3D Mirage&quot; problem in monocular depth estimation, where models hallucinate 3D structures from flat but ambiguous inputs like street art. The method includes a new benchmark (3D-Mirage), a Laplacian-based evaluation with metrics (DCS, CCS), and a parameter-efficient correction technique called Grounded Self-Distillation. The work concludes that depth model evaluation must shift from pixel accuracy to structural robustness to mitigate this safety-critical vulnerability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Step-GUI Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Calibrated Step Reward System, self-evolving training pipeline, GUI-MCP, hierarchical architecture, trajectory-level calibration]</li>
<li class=""><strong>authors:</strong> Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, Shiliang Yang, Zhirui Wang, Brian Li, Kang An, Chenyang Li, Lei Lei, Mengmeng Duan, Danxun Liang, Guodong Liu, Hang Cheng, Hao Wu, Jie Dong, Junhao Huang, Mei Chen, Renjie Yu, Shunshan Li, Xu Zhou, Yiting Dai, Yineng Deng, Yingdan Liang, Zelin Chen, Wen Sun, Chengxu Yan, Chunqin Xu, Dong Li, Fengqiong Xiao, Guanghao Fan, Guopeng Li, Guozhen Peng, Hongbing Li, Hang Li, Hongming Chen, Jingjing Xie, Jianyong Li, Jingyang Zhang, Jiaju Ren, Jiayu Yuan, Jianpeng Yin, Kai Cao, Liang Zhao, Liguo Tan, Liying Shi, Mengqiang Ren, Min Xu, Manjiao Liu, Mao Luo, Mingxin Wan, Na Wang, Nan Wu, Ning Wang, Peiyao Ma, Qingzhou Zhang, Qiao Wang, Qinlin Zeng, Qiong Gao, Qiongyao Li, Shangwu Zhong, Shuli Gao, Shaofan Liu, Shisi Gao, Shuang Luo, Xingbin Liu, Xiaojia Liu, Xiaojie Hou, Xin Liu, Xuanti Feng, Xuedan Cai, Xuan Wen, Xianwei Zhu, Xin Liang, Xin Liu, Xin Zhou, Yingxiu Zhao, Yukang Shi, Yunfang Xu, Yuqing Zeng, Yixun Zhang, Zejia Weng, Zhonghao Yan, Zhiguo Huang, Zhuoyu Wang, Zheng Ge, Jing Li, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Daxin Jiang</li>
<li class=""><strong>institution:</strong> GELab-Team, StepFun</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15431</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a self-evolving training pipeline using a Calibrated Step Reward System to efficiently generate high-quality GUI automation data. It presents the Step-GUI model family and the GUI-MCP protocol for standardized, privacy-preserving execution. The work demonstrates state-of-the-art performance on GUI benchmarks, advancing practical deployment of multimodal agents for everyday digital interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, plant phenotyping], [spatiotemporal-fusion, dual-decoder network, adaptive gating, geometric priors, motion consistency, negative gravitropism]</li>
<li class=""><strong>authors:</strong> Yueqianji Chen, Kevin Williams, John H. Doonan, Paolo Remagnino, Jo Hepworth</li>
<li class=""><strong>institution:</strong> Durham University, Aberystwyth University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ST-DETrack, a deep learning network that uses a dual-decoder architecture with an adaptive gating mechanism to fuse spatial and temporal evidence for tracking plant branches in time-series imagery. It demonstrates robust performance by maintaining branch identity across growth stages, significantly outperforming baseline methods on a Brassica napus dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ResNet-101, Inception v3, PyTorch, classification accuracy, mean Average Precision (mAP), parallel convolutions, multi-scale feature extraction]</li>
<li class=""><strong>authors:</strong> Malach Obisa Amonga, Benard Osero, Edna Too</li>
<li class=""><strong>institution:</strong> Chuka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15480" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15480</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares the deep learning architectures ResNet-101 and Inception v3 for wildlife object detection, using a PyTorch-based training and evaluation pipeline on a wildlife image dataset. The study found that Inception v3 slightly outperformed ResNet-101, achieving 95% accuracy and a mAP of 0.92, with both models proving effective but facing challenges with visually similar species and poor environmental conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer, ray-based representation, multi-view pose lifting, view fusion transformer, 3D human pose estimation]</li>
<li class=""><strong>authors:</strong> Seyed Abolfazl Ghasemzadeh, Alexandre Alahi, Christophe De Vleeschouwer</li>
<li class=""><strong>institution:</strong> UCLouvain, EPFL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15488</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RUMPL, a transformer-based model that lifts 2D human poses to 3D using a novel 3D ray-based representation of keypoints, making it independent of camera calibration and the number of views. It achieves universal deployment across arbitrary multi-view setups without retraining, significantly outperforming triangulation and other baselines in accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers (ViT), SegFormer, attention mechanisms, anomaly scoring, hybrid framework]</li>
<li class=""><strong>authors:</strong> Opeyemi Bamigbade, Mark Scanlon, John Sheppard</li>
<li class=""><strong>institution:</strong> South East Technological University, University College Dublin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image analysis], [deformable image registration, deep learning, iterative optimization, zero-shot generalization, domain shift, neuroimaging]</li>
<li class=""><strong>authors:</strong> Rohit Jena, Pratik Chaudhari, James C. Gee</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15505</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper independently re-evaluates claims of zero-shot generalization for deep learning-based deformable image registration methods, using rigorous protocols to assess performance on unseen MRI contrasts and resolutions. It finds that while deep methods perform well on in-distribution data, their performance degrades significantly on out-of-distribution contrasts and they face scalability issues on high-resolution images, aligning with established domain shift literature. The authors conclude that claims of universal zero-shot superiority require careful scrutiny and advocate for evaluation protocols that reflect practical clinical workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, disentangled control, pose transformation, expression latent, classifier-free guidance]</li>
<li class=""><strong>authors:</strong> Yuxiang Shi, Zhe Li, Yanwen Wang, Hao Zhu, Xun Cao, Ligang Liu</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Huawei, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15524</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DeX-Portrait, a method for portrait animation that uses explicit pose transformations and latent expression codes to achieve disentangled control over head pose and facial expression. It employs a dual-branch conditioning mechanism for pose and cross-attention for expression, along with a progressive hybrid classifier-free guidance for identity consistency. The method outperforms state-of-the-art baselines in animation quality and disentangled controllability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [confidence verbalization, confidence calibration, three-stage training, multimodal large language models, visual emotion comprehension]</li>
<li class=""><strong>authors:</strong> Daiqing Wu, Dongbao Yang, Can Ma. Yu Zhou</li>
<li class=""><strong>institution:</strong> Chinese Academy of Sciences, Nankai University, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15528</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EmoCaliber, a confidence-aware Multimodal Large Language Model (MLLM) for Visual Emotion Comprehension (VEC). It introduces a three-stage training framework to teach the model structured reasoning, confidence verbalization, and confidence calibration. Evaluations show EmoCaliber outperforms existing methods in both emotion prediction and confidence estimation, advancing reliable VEC systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3D Gaussian Splatting, feed-forward model, keypoint detection, self-supervised learning, novel view synthesis, camera pose estimation]</li>
<li class=""><strong>authors:</strong> Arthur Moreau, Richard Shaw, Michal Nazarczuk, Jisu Shin, Thomas Tanay, Zhensong Zhang, Songcen Xu, Eduardo Pérez-Pellitero</li>
<li class=""><strong>institution:</strong> Huawei Noah&#x27;s Ark Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15508</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level using a multi-resolution decoder inspired by keypoint detection, replacing rigid pixel grids with an adaptive &quot;Off The Grid&quot; distribution. The method, trained end-to-end with self-supervised learning, achieves state-of-the-art novel view synthesis using far fewer primitives, leading to more efficient and photorealistic scene generation. It also improves camera pose estimation, suggesting potential for training foundational models without labels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [encoder-only architecture, multi-task learning, GeoMELT, vision and language models, remote sensing]</li>
<li class=""><strong>authors:</strong> João Daniel Silva, Joao Magalhaes, Devis Tuia, Bruno Martins</li>
<li class=""><strong>institution:</strong> INESC-ID, University of Lisbon, Universidade NOVA de Lisboa, EPFL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15531</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GeoMELT, an efficient encoder-only model for multi-task learning in remote sensing vision-language tasks, such as image captioning and cross-modal retrieval. It addresses the high computational cost of large vision-language models by using a compact architecture. The results demonstrate the model&#x27;s effectiveness and efficiency on established benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BLANKET: Anonymizing Faces in Infant Video Recordings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, inpainting, face swapping, landmark preservation, keypoint detection consistency]</li>
<li class=""><strong>authors:</strong> Ditmar Hadera, Jan Cech, Miroslav Purkrabek, Matej Hoffmann</li>
<li class=""><strong>institution:</strong> Czech Technical University in Prague</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15542</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BLANKET, a two-stage method for anonymizing infant faces in videos by first generating a new compatible face using a diffusion-based inpainting model and then performing temporally consistent face swapping. It outperforms DeepPrivacy2 by better preserving facial attributes and reducing artifacts while effectively de-identifying the subjects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [SAM3, textual prompting, geometric prompting, hybrid prompting, lightweight fine-tuning, remote sensing segmentation, vision-language models]</li>
<li class=""><strong>authors:</strong> Roni Blushtein-Livnon, Osher Rafaeli, David Ioffe, Amir Boger, Karen Sandberg Esquenazi, Tal Svoray</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15564</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the SAM3 foundation model for remote sensing image segmentation using textual, geometric, and hybrid prompting strategies with lightweight fine-tuning. The main conclusion is that combining semantic and geometric cues yields the best performance, while text-only prompting performs poorly, especially for irregular targets, though light fine-tuning offers a practical trade-off for regular objects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [TED-6K benchmark, two-stage training, layer-wise weighting, multimodal large language model, text encoder, diffusion models]</li>
<li class=""><strong>authors:</strong> Bozhou Li, Sihan Yang, Yushuo Guan, Ruichuan An, Xinlong Chen, Yang Shi, Pengfei Wan, Wentao Zhang, Yuanxing zhang</li>
<li class=""><strong>institution:</strong> Peking University, Kuaishou Technology, Xi’an Jiaotong University, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15560</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GRAN-TED, a method to improve text encoders for diffusion models by first proposing a text-only benchmark (TED-6K) for efficient evaluation and then developing a two-stage training paradigm involving fine-tuning on a multimodal LLM and layer-wise weighting. This approach produces a superior text encoder that achieves state-of-the-art performance on the benchmark and enhances text-to-image and text-to-video generation quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [knowledge distillation, camera-radar fusion, intensity-aware distillation, multi-level distillation, BEV representation]</li>
<li class=""><strong>authors:</strong> Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</li>
<li class=""><strong>institution:</strong> German Research Center for Artificial Intelligence (DFKI), RPTU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes IMKD, an intensity-aware multi-level knowledge distillation framework for camera-radar 3D object detection. It uses a three-stage distillation strategy from LiDAR to enhance radar and fused representations while preserving each sensor&#x27;s unique characteristics. Experiments on nuScenes show IMKD outperforms prior distillation-based fusion methods, achieving 67.0% NDS and 61.0% mAP.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [monocular 3D instance segmentation, zero-shot learning, query refinement, spatial-semantic distillation, 3D query index memory, state-distribution token, reconstructive foundation model]</li>
<li class=""><strong>authors:</strong> Zhipeng Du, Duolikun Danier, Jan Eric Lenssen, Hakan Bilen</li>
<li class=""><strong>institution:</strong> University of Edinburgh, Max Planck Institute for Informatics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15577</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MoonSeg3R, a method for online zero-shot monocular 3D instance segmentation. It leverages a Reconstructive Foundation Model (CUT3R) for geometric priors and introduces modules for refining 2D masks into 3D queries and ensuring temporal consistency. The approach is the first to enable online 3D segmentation from a single RGB stream and achieves performance competitive with RGB-D-based systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, 3D reconstruction], [transformer, bias sinks, monocular training, multi-view supervision, latent avatar space]</li>
<li class=""><strong>authors:</strong> Tobias Kirschstein, Simon Giebenhain, Matthias Nießner</li>
<li class=""><strong>institution:</strong> Technical University of Munich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15599</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexAvatar introduces a transformer-based 3D portrait animation model with learnable data source tokens (bias sinks) to unify training on monocular and multi-view datasets. This approach leverages monocular data for generalization and multi-view data for 3D completeness, creating complete and animatable 3D head avatars from a single image. The method demonstrates strong performance in single-view, few-shot, and monocular avatar creation tasks, outperforming existing methods in view extrapolation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [RGBA-VAE, VLD-MMDiT, Multi-stage Training, diffusion model, layer decomposition]</li>
<li class=""><strong>authors:</strong> Shengming Yin, Zekai Zhang, Zecheng Tang, Kaiyuan Gao, Xiao Xu, Kun Yan, Jiahao Li, Yilei Chen, Yuxiang Chen, Heung-Yeung Shum, Lionel M. Ni, Jingren Zhou, Junyang Lin, Chenfei Wu</li>
<li class=""><strong>institution:</strong> HKUST(GZ), Alibaba, HKUST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15603" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15603</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Qwen-Image-Layered, an end-to-end diffusion model that decomposes an RGB image into multiple RGBA layers for inherent editability. It introduces key components like an RGBA-VAE and a VLD-MMDiT architecture, trained using a multi-stage strategy on data extracted from PSD files. Experiments show the method surpasses existing approaches in decomposition quality, establishing a new paradigm for consistent image editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robust Multi-view Camera Calibration from Dense Matches</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, camera calibration], [structure-from-motion, dense matches, correspondence subsampling, incremental view addition, robust estimation, radial distortion]</li>
<li class=""><strong>authors:</strong> Johannes Hägerlind, Bao-Long Tran, Urs Waldmann, Per-Erik Forssén</li>
<li class=""><strong>institution:</strong> Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a robust method for multi-view camera calibration by analyzing and improving components of a structure-from-motion pipeline. The core contributions are a strategy for subsampling dense correspondences and criteria for incrementally adding views. The evaluation shows the method significantly improves accuracy, especially for cameras with strong radial distortion, making it a useful tool for applications like animal behavior studies and forensic analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [Hough transform, gradient-by-ratio edge detection, double-weighted Hough transform, affine transformations, feature tracking]</li>
<li class=""><strong>authors:</strong> Morgan Coe, Gruffudd Jones, Leah-Nani Alconcel, Marina Gashinova</li>
<li class=""><strong>institution:</strong> University of Birmingham</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15618</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for reconstructing persistent features of satellites in ISAR images by detecting and tracking linear features across image sequences. The core techniques include using a gradient-by-ratio method for edge detection and a double-weighted Hough transform for accurate feature detection. The authors conclude that tracking features across frames increases confidence in detection and classification, as demonstrated with the robust detection of shadowing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [4D occupancy, spatio-temporal persistence, reactive forecasting, proactive forecasting, world model, voxel-based scene state, linear-complexity attention, recurrent state-space module, ego-motion compensation]</li>
<li class=""><strong>authors:</strong> Yu Zheng, Jie Hu, Kailun Yang, Jiaming Zhang</li>
<li class=""><strong>institution:</strong> Hunan University, ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15621</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces OccSTeP, a benchmark for 4D occupancy spatio-temporal persistence in autonomous driving, and proposes OccSTeP-WM, a tokenizer-free world model that uses a voxel-based scene state and a recurrent module to fuse spatio-temporal context. The method demonstrates robust performance in challenging scenarios with missing or noisy data, achieving significant improvements in semantic and occupancy IoU metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Towards Physically-Based Sky-Modeling For Image Based Lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer graphics, image-based lighting], [sky-modeling, High Dynamic Range Imagery (HDRI), Image-Based Lighting (IBL), Full Dynamic Range (FDR), deep neural network (DNN), parametric sky-models, tonemapping]</li>
<li class=""><strong>authors:</strong> Ian J. Maquignaz</li>
<li class=""><strong>institution:</strong> Université Laval</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15632" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15632</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AllSky, a physically-based sky-model learned directly from captured HDR imagery to generate accurate environment maps for image-based lighting. It demonstrates that current DNN-based sky-models fail to match the photorealism and full dynamic range of physically captured imagery. The work concludes that AllSky provides intuitive user control and achieves state-of-the-art performance, highlighting the limitations of existing models for accurate scene relighting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion transformer (DiT), in-context learning, Effect-LoRA, spatiotemporal sparse tokenization, two-stage training, few-shot video editing]</li>
<li class=""><strong>authors:</strong> Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao</li>
<li class=""><strong>institution:</strong> Communication University of China, National University of Singapore, Baidu Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15635</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IC-Effect is an instruction-guided video VFX editing framework based on Diffusion Transformers (DiT) that uses the source video as a contextual condition and employs a two-stage training strategy with Effect-LoRA for precise effect injection and background preservation. It introduces spatiotemporal sparse tokenization for computational efficiency. The method demonstrates high-quality, temporally consistent visual effects editing from limited data, enabling new possibilities for video creation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [knowledge distillation], [HALD, dataset distillation, soft labels, hard labels, local semantic drift, SRe2L, RDED, LPLD, FKD]</li>
<li class=""><strong>authors:</strong> Jiacheng Cui, Bingkui Tong, Xinyue Bi, Xiaohan Zhao, Jiacheng Liu, Zhiqiang shen</li>
<li class=""><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15647" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15647</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new training paradigm called HALD, which hybridizes soft and hard labels to mitigate local semantic drift in dataset distillation. The method uses hard labels as corrective anchors to align visual content with semantic supervision when only a few image crops are available. The approach outperforms prior state-of-the-art methods on ImageNet-1K, demonstrating that hard labels remain a crucial complementary tool in soft-label-dominated training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Direct Preference Optimization (DPO), MaskDPO, Conditional Asymmetric Preference Optimization, Shared Commonality Preference Optimization, foreground-conditioned inpainting]</li>
<li class=""><strong>authors:</strong> Qirui Li, Yizhe Tang, Ran Yi, Guangben Lu, Fangyuan Zou, Peng Shu, Huan Yu, Jie Jiang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Tencent</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15644" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15644</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes InpaintDPO, a Direct Preference Optimization (DPO) framework designed to mitigate spatial relationship hallucinations in foreground-conditioned image inpainting. It introduces three novel techniques—MaskDPO, Conditional Asymmetric Preference Optimization, and Shared Commonality Preference Optimization—to optimize background spatial relationships while preserving foreground integrity and boundary coherence. The method demonstrates superior performance in generating images with plausible spatial relationships between foreground subjects and their backgrounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Flow Matching, Classifier-Free Guidance, solution consistency loss, velocity ODE, one-step generation, Diffusion Transformer]</li>
<li class=""><strong>authors:</strong> Tianze Luo, Haotian Yuan, Zhuang Liu</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15657</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SoFlow, a framework for one-step generative modeling that uses a Flow Matching loss and a solution consistency loss to train models without requiring Jacobian-vector product calculations. It improves training efficiency by enabling Classifier-Free Guidance and achieves better FID-50K scores than MeanFlow models on ImageNet 256x256 when using the same DiT architecture and training epochs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-text compression, VTCBench, DeepSeek-OCR, Glyph, VTC-Retrieval, VTC-Reasoning, VTC-Memory]</li>
<li class=""><strong>authors:</strong> Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, CAS; Tencent Hunyuan Team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15649" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15649</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces VTCBench, the first benchmark to evaluate Vision-Language Models&#x27; ability to understand long context using Vision-Text Compression (VTC), a technique that converts long text into dense 2D images for token efficiency. The study systematically tests models on retrieval, reasoning, and memory tasks with VTC-compressed inputs. The main conclusion is that most VLMs perform poorly on long-context understanding with VTC, despite good OCR decoding, failing to capture long-range associations in the compressed visual context.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stylized Synthetic Augmentation further improves Corruption Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [data augmentation, neural style transfer, synthetic data, corruption robustness, TrivialAugment]</li>
<li class=""><strong>authors:</strong> Georg Siedel, Rojan Regmi, Abhirami Anand, Weijia Shao, Silvia Vock, Andrey Morozov</li>
<li class=""><strong>institution:</strong> University of Stuttgart, Federal Institute for Occupational Safety and Health (BAuA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer to improve the corruption robustness of deep vision models. It finds that stylizing synthetic images, despite lowering their FID score, is beneficial for training, and that this method can be effectively combined with rule-based augmentations like TrivialAugment. The approach achieves state-of-the-art robust accuracy on several image classification benchmarks under common corruptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]</li>
<li class=""><strong>authors:</strong> Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</li>
<li class=""><strong>institution:</strong> mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15692</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal large language model (MLLM), supervised fine-tuning (SFT), chain-of-thought (CoT), artifact reasoning, two-stage training, ViF-CoT-4K dataset, ViF-Bench benchmark]</li>
<li class=""><strong>authors:</strong> Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15693</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Skyra, a specialized multimodal large language model (MLLM) that detects AI-generated videos by identifying and reasoning about human-perceivable visual artifacts as grounded evidence. The method involves constructing a large-scale annotated artifact dataset (ViF-CoT-4K) and employing a two-stage training strategy to enhance artifact perception and explanation. The results show that Skyra outperforms existing methods on multiple benchmarks, providing an explainable approach to AI-generated video detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [vision-language models, diffusion autoencoders, diffusion DPO, two-alternative forced choice, perceptual loss, image compression]</li>
<li class=""><strong>authors:</strong> Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski, Li Fei-Fei, Jiajun Wu, Jason Zhang</li>
<li class=""><strong>institution:</strong> Stanford University, Google Research, Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15701</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes VLIC, a method that uses vision-language models (VLMs) as zero-shot judges to provide binary preferences between image reconstructions, which are then used to post-train a diffusion-based image compression system via Diffusion DPO. The core finding is that VLMs can effectively replicate human perceptual judgments, and calibrating the compression model on these VLM preferences yields state-of-the-art performance in human-aligned image compression.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video generation], [autoregressive video diffusion, self-resampling, resampling forcing, sparse causal mask, history routing]</li>
<li class=""><strong>authors:</strong> Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, Dahua Lin</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15702</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Resampling Forcing, an end-to-end training framework for autoregressive video diffusion models that addresses exposure bias through a self-resampling scheme and a sparse causal mask. It also proposes a history routing mechanism for efficient long-horizon generation. The method achieves performance comparable to distillation-based baselines and shows superior temporal consistency on longer videos.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-modal learning], [hierarchical gated fusion, transformer, masked alignment loss, over-positive penalty, active speaker detection]</li>
<li class=""><strong>authors:</strong> Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall</li>
<li class=""><strong>institution:</strong> Indiana University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15707</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GateFusion, a novel architecture for active speaker detection that uses a Hierarchical Gated Fusion Decoder (HiGate) to progressively integrate audio and visual features at multiple Transformer layers. The method is enhanced by two auxiliary losses, Masked Alignment Loss and Over-Positive Penalty, to improve multimodal alignment and suppress false positives. The model achieves state-of-the-art results on several challenging benchmarks, demonstrating superior performance and generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-View Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-view consistency, 3D-aware attention, feature matching, DINO, SAM, CLIP]</li>
<li class=""><strong>authors:</strong> Leo Segre, Or Hirschorn, Shai Avidan</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15708" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15708</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to convert standard 2D foundation models into multi-view consistent models by augmenting them with intermediate 3D-aware attention layers. This approach improves feature matching across different views of the same 3D scene without needing to build a 3D feature model. Quantitative experiments show the method significantly improves feature consistency and localization accuracy compared to baseline foundation models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [3D Gaussian Splatting, triangle mesh, hybrid representation, differentiable rendering, volumetric rendering, neural networks]</li>
<li class=""><strong>authors:</strong> Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola, Giljoo Nam</li>
<li class=""><strong>institution:</strong> Meta Codec Avatars Lab, Meta Reality Labs, Stellon Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15711</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Gaussian Pixel Codec Avatars (GPiCA), a hybrid avatar representation that combines a textured triangle mesh and 3D Gaussians within a unified differentiable rendering pipeline. This approach uses neural networks to decode a facial expression code into the mesh, texture, and Gaussians, which are rendered together. The method achieves the realism of Gaussian-based avatars while maintaining the rendering efficiency of mesh-based avatars, enabling photorealistic performance on mobile devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion paradigm, autoregressive models, fine-tuning, block-decoding, KV cache reuse, vision language models]</li>
<li class=""><strong>authors:</strong> Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DiffusionVL is a method that translates any powerful autoregressive vision-language model into a diffusion-based model through simple fine-tuning. It introduces a block-decoding design for arbitrary-length generation and KV cache reuse, achieving significant inference speedup. The approach shows that converting AR models to diffusion is highly effective, yielding strong performance gains and faster inference with minimal training data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] In Pursuit of Pixel Supervision for Visual Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [masked autoencoder, self-supervised learning, pixel-space learning, autoencoder, MAE]</li>
<li class=""><strong>authors:</strong> Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu</li>
<li class=""><strong>institution:</strong> Meta, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces &quot;Pixio&quot;, an enhanced masked autoencoder (MAE) model for self-supervised visual pre-training using pixel-level supervision. It is trained on 2B web images and demonstrates competitive performance across various downstream tasks like depth estimation and segmentation, showing pixel-space learning remains a strong alternative to latent-space methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Spatia: Video Generation with Updatable Spatial Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [spatial memory, 3D scene point cloud, visual SLAM, dynamic-static disentanglement, camera control, 3D-aware editing]</li>
<li class=""><strong>authors:</strong> Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu</li>
<li class=""><strong>institution:</strong> The University of Sydney, Microsoft Research, HKUST, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15716</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Spatia, a video generation framework that uses an updatable 3D scene point cloud as persistent spatial memory to enhance long-term spatial consistency. It iteratively generates video clips conditioned on this memory and updates it via visual SLAM, enabling applications like explicit camera control and 3D-aware interactive editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [adversarial agents, Monte Carlo Tree Search (MCTS), question chains, pyramid-like reasoning, vision language models (VLMs), fine-tuning, synthetic dataset]</li>
<li class=""><strong>authors:</strong> Yuqun Zhang, Yuxuan Zhao, Sijia Chen</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Yantai Research Institute, Harbin Engineering University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14735</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PyFi, a framework that uses a multi-agent adversarial mechanism under Monte Carlo Tree Search to synthesize a large-scale, pyramid-structured financial image QA dataset without human annotation. Fine-tuning VLMs on this dataset enables them to decompose complex financial questions into simpler sub-questions, leading to significant accuracy improvements on the benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [self-supervised learning, vision transformer, cross-scale distillation, magnification-invariant representation, whole-slide image analysis]</li>
<li class=""><strong>authors:</strong> Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner</li>
<li class=""><strong>institution:</strong> University of Kentucky, University of Pittsburgh, Emory University, University of California Davis, University of Texas Health</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14796</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Magnification-Aware Distillation (MAD), a self-supervised framework that learns unified image representations by linking low-magnification context with spatially aligned high-magnification detail in whole-slide images. The resulting foundation model, MAD-NP, demonstrates strong resolution-invariant learning, as shown by a classifier trained on 10x embeddings maintaining 96.7% performance on unseen 40x tiles. The work concludes that this approach enables scalable, magnification-robust analysis using a unified embedding space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [deep learning, video analysis, segmentation, classification, Fagotti score, diagnostic laparoscopy, Dice score, F1-score, RMSE]</li>
<li class=""><strong>authors:</strong> Riccardo Oliva, Farahdiba Zarin, Alice Zampolini Faustini, Armine Vardazaryan, Andrea Rosati, Vinkle Srivastav, Nunzia Del Villano, Jacques Marescaux, Giovanni Scambia, Pietro Mascagni, Nicolas Padoy, Anna Fagotti</li>
<li class=""><strong>institution:</strong> Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Università Cattolica del Sacro Cuore, IRCAD, University of Strasbourg, IHU Strasbourg, Università degli studi di Modena</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14797" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14797</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a deep learning system to analyze diagnostic laparoscopy videos for advanced ovarian cancer, automating the assessment of peritoneal carcinosis and predicting the Fagotti score to guide surgical decisions. The AI model segments anatomical structures and tumor lesions, then classifies video-level features to estimate surgical feasibility. The results demonstrate reproducible performance, suggesting AI can standardize intraoperative tumor burden assessment and support clinical decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Gaussian Parameterization for Direct Atomic Structure Identification in Electron Tomography</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational imaging], [Gaussian parameterization, atomic electron tomography, inverse problem, atom tracing, transmission electron microscopy]</li>
<li class=""><strong>authors:</strong> Nalini M. Singh, Tiffany Chien, Arthur R.C. McCray, Colin Ophus, Laura Waller</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new method for atomic electron tomography that directly solves for atomic positions and properties by parameterizing the structure as a collection of learnable Gaussians, rather than first reconstructing a volumetric image. This approach incorporates a strong physical prior, which improves robustness to imaging artifacts like those from limited tilt angles. Simulated and experimental results demonstrate its potential for practical materials characterization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [meta-learning, few-shot learning, weakly-supervised segmentation, sparse labels, Omni meta-training, Efficient Omni ProtoSeg (EO-ProtoSeg)]</li>
<li class=""><strong>authors:</strong> Pandega Abyan Zumarsyah, Igi Ardiyanto, Hanung Adi Nugroho</li>
<li class=""><strong>institution:</strong> Universitas Gadjah Mada</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15061</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes improved meta-learners for few-shot, weakly-supervised segmentation of the optic disc and cup in fundus images. The core method introduces Omni meta-training for balanced data usage and efficient versions to reduce computational costs, along with sparsification techniques for generating scribbles. The best model, EO-ProtoSeg, achieves high segmentation accuracy using only one sparsely labeled image, outperforming methods that require more labels and being comparable to heavier unsupervised domain adaptation approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generative Preprocessing for Image Compression with Pre-trained Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [Consistent Score Identity Distillation (CiD), Rate-Perception (R-P) optimization, Stable Diffusion 2.1, parameter-efficient fine-tuning, differentiable codec surrogate]</li>
<li class=""><strong>authors:</strong> Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang</li>
<li class=""><strong>institution:</strong> Bytedance Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15270</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-18T07:21:37.000Z" itemprop="dateModified">Dec 18, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/category/cscv"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">cs.CV</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/cscy"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.CY</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li></ul></div></div></div></div></div><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div>
</body>
</html>