<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_CV/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 (cs.CV) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 (cs.CV) | AI头条"><meta data-rh="true" name="description" content="2025-12-18"><meta data-rh="true" property="og:description" content="2025-12-18"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.CV","item":"https://jokebear666.github.io/ai_toutiao/daily/cscv"},{"@type":"ListItem","position":3,"name":"20251215-20251221 (cs.CV)","item":"https://jokebear666.github.io/ai_toutiao/daily/cs_CV/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.647bd1ff.css">
<script src="/ai_toutiao/assets/js/runtime~main.500c5cf3.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.1e3609bc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div class="layout-wrapper hide-doc-sidebar"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/arxiv-daily">Arxiv每日论文</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/arxiv-daily"><span title="Arxiv每日论文" class="linkLabel_WmDU">Arxiv每日论文</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csar"><span title="cs.AR" class="categoryLinkLabel_W154">cs.AR</span></a><button aria-label="Expand sidebar category &#x27;cs.AR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscc"><span title="cs.CC" class="categoryLinkLabel_W154">cs.CC</span></a><button aria-label="Expand sidebar category &#x27;cs.CC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscg"><span title="cs.CG" class="categoryLinkLabel_W154">cs.CG</span></a><button aria-label="Expand sidebar category &#x27;cs.CG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/daily/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Collapse sidebar category &#x27;cs.CV&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cs_CV/20251215-20251221"><span title="20251215-20251221 (cs.CV)" class="linkLabel_WmDU">20251215-20251221 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251222-20251228"><span title="20251222-20251228 (cs.CV)" class="linkLabel_WmDU">20251222-20251228 (cs.CV)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/cscv/20251229-20260104"><span title="20251229-20260104 (cs.CV)" class="linkLabel_WmDU">20251229-20260104 (cs.CV)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdl"><span title="cs.DL" class="categoryLinkLabel_W154">cs.DL</span></a><button aria-label="Expand sidebar category &#x27;cs.DL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csdm"><span title="cs.DM" class="categoryLinkLabel_W154">cs.DM</span></a><button aria-label="Expand sidebar category &#x27;cs.DM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cset"><span title="cs.ET" class="categoryLinkLabel_W154">cs.ET</span></a><button aria-label="Expand sidebar category &#x27;cs.ET&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csfl"><span title="cs.FL" class="categoryLinkLabel_W154">cs.FL</span></a><button aria-label="Expand sidebar category &#x27;cs.FL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Expand sidebar category &#x27;cs.LG&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cslo"><span title="cs.LO" class="categoryLinkLabel_W154">cs.LO</span></a><button aria-label="Expand sidebar category &#x27;cs.LO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csms"><span title="cs.MS" class="categoryLinkLabel_W154">cs.MS</span></a><button aria-label="Expand sidebar category &#x27;cs.MS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csoh"><span title="cs.OH" class="categoryLinkLabel_W154">cs.OH</span></a><button aria-label="Expand sidebar category &#x27;cs.OH&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspf"><span title="cs.PF" class="categoryLinkLabel_W154">cs.PF</span></a><button aria-label="Expand sidebar category &#x27;cs.PF&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/daily/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads daily-layout"><div class="content-main"><div class="content-inner"><div class="daily-nav"><a class="daily-nav-item" href="/ai_toutiao/daily/csai">AI</a><a class="daily-nav-item" href="/ai_toutiao/daily/csce">CE</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscl">CL</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscv">CV</a><a class="daily-nav-item" href="/ai_toutiao/daily/cslg">LG</a><a class="daily-nav-item" href="/ai_toutiao/daily/csdc">DC</a><a class="daily-nav-item" href="/ai_toutiao/daily/csmm">MM</a><a class="daily-nav-item" href="/ai_toutiao/daily/csne">NE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csro">RO</a><a class="daily-nav-item" href="/ai_toutiao/daily/csse">SE</a><a class="daily-nav-item" href="/ai_toutiao/daily/csni">NI</a><a class="daily-nav-item" href="/ai_toutiao/daily/cscy">CY</a></div><div class="daily-grid"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/daily/cscv"><span>cs.CV</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221 (cs.CV)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221 (cs.CV)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]</li>
<li class=""><strong>authors:</strong> Krunal Jesani, Dmitry Ignatov, Radu Timofte</li>
<li class=""><strong>institution:</strong> University of Würzburg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]</li>
<li class=""><strong>authors:</strong> Ryan Cartularo</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [planner-executor framework, large language models (LLMs), vision-language models (VLMs), segmentation models, image processing]</li>
<li class=""><strong>authors:</strong> Idan Tankel, Nir Mazor, Rafi Brada, Christina LeBedis, Guy ben-Yosef</li>
<li class=""><strong>institution:</strong> GE Healthcare Technology and Innovation Center, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework that uses a planner-executor approach, where an LLM planner generates Python scripts to automate the detection and reporting of incidental findings in abdominal CT scans, and an executor runs these scripts using VLMs and segmentation models. The method is fully automatic and end-to-end. The results show that this framework outperforms pure VLM-based approaches in accuracy and efficiency for managing incidental findings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [optical-SAR dataset, label transfer, foundation models, amplitude change detection, continued pretraining, preprocessing]</li>
<li class=""><strong>authors:</strong> Paul Weinmann, Ferdinand Schenck, Martin Šiklar</li>
<li class=""><strong>institution:</strong> LiveEO GmbH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14755</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SkyCap, a dataset pairing optical and SAR imagery for change detection, and uses optical-to-SAR label transfer to train models without expert SAR annotations. It benchmarks foundation models on this dataset and finds that an optical model with specific preprocessing outperforms SAR-specific models, highlighting the sensitivity of results to preprocessing alignment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [mixture-of-experts, reinforcement fine-tuning, semantic similarity reward, vision language model, socially compliant navigation]</li>
<li class=""><strong>authors:</strong> Tomohito Kawabata, Xinyu Zhang, Ling Xiao</li>
<li class=""><strong>institution:</strong> Hokkaido University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14757" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14757</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant robot navigation, which is fine-tuned using reinforcement learning with a novel semantic similarity reward. The method balances navigation accuracy and computational efficiency by exploring different small language models and vision encoders. Experiments show it outperforms baseline rewards and achieves a good trade-off for real-time deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [expert system, computer vision, phrase correlation, skeleton analysis, unsupervised deep learning, image feature embeddings, optical music recognition]</li>
<li class=""><strong>authors:</strong> Fan Bu, Rongfeng Li, Zijin Li, Ya Li, Linfeng Fan, Pei Huang</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Central Conservatory of Music, Shanghai Normal University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14758" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14758</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a hybrid expert-system pipeline for optical recognition of printed Chinese Jianpu musical scores with lyrics, combining traditional computer-vision techniques with unsupervised deep-learning modules. The system successfully digitizes a large collection of folk songs into MusicXML and MIDI without requiring massive annotated data. It achieves high-precision recognition for both melody and aligned lyrics, demonstrating a balance between interpretability and accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, cross-attention, residual dense blocks, multi-resolution attention, cross-domain consistency loss]</li>
<li class=""><strong>authors:</strong> Afrah Shaahid, Muzammil Behzad</li>
<li class=""><strong>institution:</strong> King Fahd University of Petroleum and Minerals, SDAIA–KFUPM Joint Research Center for Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14760</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AquaDiff, a diffusion-based framework for underwater image enhancement that uses a chromatic prior-guided color compensation strategy and a conditional diffusion process with cross-attention. It achieves superior color correction and competitive overall image quality compared to state-of-the-art methods across diverse underwater conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [self-reflection, cross-model verification, dual-assessment, uncertainty estimation, selective prediction, hallucination mitigation]</li>
<li class=""><strong>authors:</strong> Xixian Wu, Yang Ou, Pengchao Tian, Zian Yang, Jielei Zhang, Peiyi Li, Longwen Gao</li>
<li class=""><strong>institution:</strong> Bilibili Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14770</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DAVR, a framework that enhances VQA reliability by combining a model&#x27;s self-assessment of its answer confidence with cross-verification using external models to reduce hallucinations. It achieved top results in a reliability challenge, demonstrating its effectiveness in improving the trustworthiness of vision-language model responses.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video question answering], [multi-evidence integration, Minimum Required Frame-Set (MRFS), compositional tasks, retrieval deficit, fusion deficit]</li>
<li class=""><strong>authors:</strong> Dan Ben-Ami, Gabriele Serussi, Kobi Cohen, Chaim Baskin</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14870" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14870</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces HERBench, a new VideoQA benchmark designed to require the integration of at least three distinct pieces of visual evidence across different video segments, using a metric called Minimum Required Frame-Set (MRFS) to quantify this demand. Evaluating 13 state-of-the-art Video-LLMs reveals their performance is only slightly above random chance, exposing two major bottlenecks: a retrieval deficit in selecting key frames and a fusion deficit in integrating the provided evidence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Isolated Sign Language Recognition with Segmentation and Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [pose estimation, segmentation, ResNet, Transformer, isolated sign language recognition]</li>
<li class=""><strong>authors:</strong> Daniel Perkins, Davis Hunter, Dhrumil Patel, Galen Flanagan</li>
<li class=""><strong>institution:</strong> University of Tennessee, Knoxville</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a model for Isolated Sign Language Recognition that integrates pose estimation to extract joint coordinates, a segmentation module to isolate relevant pixels, and a hybrid ResNet-Transformer backbone to model spatial and temporal dependencies. This approach aims to reduce computational requirements while maintaining robustness to signer variation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [dermatoglyphic textual descriptors, cross-modal retrieval, text-image co-synthesis, virtual individuals, minutiae matching]</li>
<li class=""><strong>authors:</strong> Wenshuo Li, Majid Mirmehdi, Tilo Burghardt</li>
<li class=""><strong>institution:</strong> University of Bristol</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14878" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14878</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a visual-textual method for animal re-identification that pairs images with dermatoglyphic textual descriptions of coat patterns. It uses a text-image co-synthesis pipeline to generate virtual individuals, significantly improving cross-modal retrieval accuracy. The approach overcomes limitations of vision-only systems by enabling human-verifiable, text-to-visual identity matching.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [model adaptation], [task matrix, linear transformation, finetuning, linear probe, embedding space]</li>
<li class=""><strong>authors:</strong> Darrin O&#x27; Brien, Dhikshith Gajulapalli, Eric Xia</li>
<li class=""><strong>institution:</strong> Algoverse AI Research, Brown University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the concept of a &quot;task matrix,&quot; a linear transformation that maps a base model&#x27;s embedding state to a finetuned model&#x27;s state. It demonstrates that applying this matrix to a base model can outperform linear probes and sometimes approach full finetuning performance across vision and text models on various datasets. The results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vibe blending, vibe space, hierarchical graph manifold, geodesics, clip, diffusion models]</li>
<li class=""><strong>authors:</strong> Huzheng Yang, Katherine Xu, Andrew Lu, Michael D. Grossberg, Yutong Bai, Jianbo Shi</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, CUNY, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP to enable smooth, semantically consistent transitions between visual concepts for a novel task called Vibe Blending. It concludes that this method produces blends rated as more creative and coherent than current approaches, as evaluated by a framework combining human judgments, LLM reasoning, and a geometric difficulty score.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [foundation models, Gleason grading, whole slide images, self-supervised learning, feature embeddings, slide-level encoding, cross-slide accuracy, benchmark dataset]</li>
<li class=""><strong>authors:</strong> Joshua L. Ebbert, Dennis Della Corte</li>
<li class=""><strong>institution:</strong> Brigham Young University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14922" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14922</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PANDA-PLUS-Bench, a curated benchmark dataset of prostate biopsy whole slide images designed to evaluate the robustness of AI foundation models in distinguishing between biological features and slide-specific artifacts for Gleason grading. It tests seven models and finds that tissue-specific training (e.g., HistoEncoder) improves cross-slide accuracy and slide-level encoding, but all models show a significant gap between within-slide and cross-slide performance, highlighting a critical robustness issue.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving Pre-trained Segmentation Models using Post-Processing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [adaptive post-processing, glioma segmentation, brain MRI, BraTS challenge, resource-aware AI]</li>
<li class=""><strong>authors:</strong> Abhijeet Parida, Daniel Capellán-Martín, Zhifan Jiang, Nishad Kulkarni, Krithika Iyer, Austin Tapp, Syed Muhammad Anwar, María J. Ledesma-Carbayo, Marius George Linguraru</li>
<li class=""><strong>institution:</strong> Children&#x27;s National Hospital, Universidad Politécnica de Madrid, George Washington University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14937" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14937</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes adaptive post-processing techniques to refine the segmentation outputs of large-scale pre-trained models for brain tumor (glioma) segmentation in MRI, addressing systematic errors like false positives. The method was validated in BraTS 2025 challenges, showing significant metric improvements. The work advocates for a shift from complex model architectures to efficient, clinically aligned post-processing for more precise and sustainable medical image analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [diffusion transformer, video VAE, sliding window mechanism, motion-frame context, latent noise injection, MLLM director]</li>
<li class=""><strong>authors:</strong> Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Snap Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Puzzle Curriculum GRPO for Vision-Centric Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [PC-GRPO, RL with Verifiable Rewards, self-supervised puzzle environments, difficulty-aware curriculum, reasoning-answer consistency]</li>
<li class=""><strong>authors:</strong> Ahmadreza Jeddi, Hakki Can Karaimer, Hue Nguyen, Zhongling Wang, Ke Zhao, Javad Rajabi, Ran Zhang, Raghav Goyal, Babak Taati, Radek Grzeszczuk</li>
<li class=""><strong>institution:</strong> Samsung Electronics, University of Toronto, Vector Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14944" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14944</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Puzzle Curriculum GRPO (PC-GRPO), a supervision-free reinforcement learning method that uses self-supervised puzzle environments and a difficulty-aware curriculum to improve visual reasoning in vision language models. It enhances reasoning quality, training stability, and downstream accuracy without relying on annotations or external verifiers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multi-task learning, cross-attention, gated fusion, confidence-weighted fusion, adaptive fusion]</li>
<li class=""><strong>authors:</strong> Aref Farhadipour, Teodora Vukovic, Volker Dellwo, Petr Motlicek, Srikanth Madikeri</li>
<li class=""><strong>institution:</strong> University of Zurich, Idiap Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14961" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14961</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a robust trimodal person recognition framework that integrates voice, face, and gesture data using multi-task learning, cross-attention, and a gated, confidence-weighted fusion strategy to handle missing or degraded modalities. It achieves high accuracy on the CANDOR and VoxCeleb1 datasets and maintains performance even when modalities are unavailable, demonstrating robustness for real-world applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal reasoning, chain-of-thought prompting, ablation studies, occlusion-based interpretability, benchmark evaluation, modality fusion]</li>
<li class=""><strong>authors:</strong> Yiming Cui, Xin Yao, Yuxuan Qin, Xin Li, Shijin Wang, Guoping Hu</li>
<li class=""><strong>institution:</strong> State Key Laboratory of Cognitive Intelligence, iFLYTEK AI Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14989</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates 40 multimodal large language models on a benchmark of chemistry Olympiad questions requiring visual and textual reasoning. The core method involves using chain-of-thought prompting and interpretability techniques like ablation and occlusion. The main conclusion is that current models struggle with modality fusion, but chain-of-thought improves both accuracy and visual grounding, revealing critical limitations in scientific reasoning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Where is the Watermark? Interpretable Watermark Detection at the Block Level</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [image watermarking], [discrete wavelet transform, block-wise embedding, detection maps, post-hoc watermarking]</li>
<li class=""><strong>authors:</strong> Maria Bulychev, Neil G. Marchant, Benjamin I. P. Rubinstein</li>
<li class=""><strong>institution:</strong> University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14994" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14994</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an interpretable, post-hoc image watermarking method that embeds signals in the discrete wavelet transform domain using a statistical block-wise strategy. It generates detection maps to show which specific regions of an image are watermarked or altered. The method maintains strong robustness against common image transformations and high imperceptibility while providing more interpretable detection than prior approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [YOLOv11, ByteTrack, ZebraPose, support vector machine, keypoint trajectories, pose estimation]</li>
<li class=""><strong>authors:</strong> Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan</li>
<li class=""><strong>institution:</strong> Dalhousie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a computer vision framework that uses pose estimation and keypoint trajectories to classify social interactions in dairy cattle, moving beyond simple proximity measures. The method integrates object detection, tracking, and a support vector machine to distinguish affiliative from agonistic behaviors with 77.51% accuracy. The results demonstrate improved behavioral discrimination and establish a proof-of-concept for automated, interaction-aware social network analysis in precision livestock farming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video question generation], [video question generation, question-to-answer retrieval, EgoExoAsk dataset, expert knowledge elicitation]</li>
<li class=""><strong>authors:</strong> Huaying Zhang, Atsushi Hashimoto, Tosho Hirasawa</li>
<li class=""><strong>institution:</strong> OMRON SINIC X Corp., Hokkaido University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15006" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15006</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a retrieval-based evaluation protocol for video question generation (VQG) models, focusing on their ability to elicit expert knowledge. The method uses a question-to-answer retriever trained on a novel dataset, EgoExoAsk, to simulate expert communication. The main conclusion is that this metric effectively aligns with VQG settings, as models with richer context are evaluated better, validating the proposed framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Model Agnostic Preference Optimization for Medical Image Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [preference optimization, dropout, stochastic segmentation, model-agnostic, DPO, fine-tuning]</li>
<li class=""><strong>authors:</strong> Yunseong Nam, Jiwon Jang, Dongkyu Won, Sang Hyun Park, Soopil Kim</li>
<li class=""><strong>institution:</strong> DGIST (Daegu Gyeongbuk Institute of Science and Technology)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15009</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MAPO, a model-agnostic preference optimization framework for medical image segmentation that uses dropout to generate stochastic segmentation hypotheses for training, eliminating the need for direct ground-truth supervision. It is designed to be compatible with various architectures like 2D/3D CNNs and Transformers. The method is shown to improve boundary accuracy, reduce overfitting, and provide more stable training compared to standard supervised approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [embodied navigation], [3D scene graphs, hierarchical traversable graphs, movable obstacles, path planning, scene understanding]</li>
<li class=""><strong>authors:</strong> Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15047</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HERO, a framework for building Hierarchical Traversable 3D Scene Graphs that model movable obstacles as pathways by capturing their interactivity and semantics. This redefinition of traversability allows for more efficient navigation planning in obstructed environments. The results show HERO significantly reduces path length in partially obstructed scenes and increases success rate in fully obstructed ones compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D Gaussian Splatting, super-resolution, multi-view attention, epipolar guidance, novel view synthesis]</li>
<li class=""><strong>authors:</strong> Kaizhe Zhang, Shinan Chen, Qian Zhao, Weizhan Zhang, Caixia Yan, Yudeng Xin</li>
<li class=""><strong>institution:</strong> Xi’an Jiaotong University, University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15048</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MVGSR, a framework for 3D Gaussian Splatting super-resolution that enhances detail and consistency by integrating multi-view information. Its core innovations are an auxiliary view selection method for unstructured datasets and an epipolar-constrained multi-view attention mechanism. The method achieves state-of-the-art performance on object-centric and scene-level benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [event camera, LED markers, noise filtering, spatiotemporal correlation, monocular vision, high-frequency deformation measurement]</li>
<li class=""><strong>authors:</strong> Yifei Bian, Banglei Guan, Zibin Liu, Ang Su, Shiyao Zhu, Yang Shang, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology, Hunan Provincial Key Laboratory of Image Measurement and Vision Navigation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15055</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to measure high-frequency structural deformations using an event camera and blinking LED markers. The core technique involves filtering noise from the asynchronous event stream and differentiating between motion-induced and LED-blinking events to extract marker positions. Experimental results confirm the method&#x27;s accuracy for planar deformation measurement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [wavelet analysis, memory bank, encoder-decoder, cross-attention, high-frequency feature fusion]</li>
<li class=""><strong>authors:</strong> Chenxiao Zhang, Runshi Zhang, Junchen Wang</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text. Affiliation inference is not possible from the author names alone without email domains or explicit institutional mentions.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15066</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a network combining wavelet analysis and a memory bank to segment objects in long ultrasound videos. The method uses memory-based wavelet convolution and high-frequency-aware feature fusion to capture fine details and track objects over time. It demonstrates improved segmentation accuracy, particularly for small nodules, on several ultrasound datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PMMD: A pose-guided multi-view multi-modal diffusion for person generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [diffusion training], [diffusion framework, multimodal encoder, ResCVA module, cross modal fusion, pose-guided generation]</li>
<li class=""><strong>authors:</strong> Ziyu Shang, Haoran Liu, Rongchao Zhang, Zhiqian Wei, Tongtong Feng</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen, City University of Hong Kong, Peking University, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15069</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PMMD is a pose-guided multi-view multimodal diffusion framework that synthesizes photorealistic person images by jointly modeling visual views, pose features, and text prompts. It introduces a ResCVA module for local detail enhancement and a cross-modal fusion module to integrate image semantics with text. Experiments show PMMD outperforms baselines in consistency, detail preservation, and controllability on the DeepFashion MultiModal dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Uni-Parser Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multi-expert architecture, cross-modal alignment, adaptive GPU load balancing, distributed inference, dynamic module orchestration]</li>
<li class=""><strong>authors:</strong> Xi Fang, Haoyi Tao, Shuwen Yang, Suyang Zhong, Haocheng Lu, Han Lyu, Chaozheng Huang, Xinyu Li, Linfeng Zhang, Guolin Ke</li>
<li class=""><strong>institution:</strong> DP Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15098</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Uni-Parser is an industrial-grade document parsing engine that uses a modular, loosely coupled multi-expert architecture to extract and align information from text, equations, tables, figures, and chemical structures in PDFs. It is optimized for large-scale cloud deployment, achieving high throughput and cost efficiency, enabling the processing of billions of scientific and patent pages for downstream AI applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [zero-shot evaluation, text-to-image generation, low-level vision tasks, subjective visual quality, reference-based metrics]</li>
<li class=""><strong>authors:</strong> Jialong Zuo, Haoyou Deng, Hanyu Zhou, Jiaxin Zhu, Yicheng Zhang, Yiwei Zhang, Yongxin Yan, Kaixing Huang, Weisen Chen, Yongtai Deng, Rui Jin, Nong Sang, Changxin Gao</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15110" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15110</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a comprehensive zero-shot evaluation of the Nano Banana Pro text-to-image model across 14 low-level vision tasks using simple textual prompts. It finds that while the model produces subjectively superior visual quality by hallucinating plausible details, it underperforms specialist models on traditional pixel-level quantitative metrics due to its inherent stochasticity. The study concludes that Nano Banana Pro is a capable zero-shot contender but faces challenges in matching the high fidelity of domain-specific models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [particle filter, bird&#x27;s-eye-view (BEV), feature matching, cross-view geo-localization, absolute trajectory error (ATE)]</li>
<li class=""><strong>authors:</strong> Dongmyeong Lee, Jesse Quattrociocchi, Christian Ellis, Rwik Rana, Amanda Adkins, Adam Uccello, Garrett Warnell, Joydeep Biswas</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, DEVCOM Army Research Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15111" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15111</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BEV-Patch-PF, a sequential geo-localization system that uses a particle filter to match learned bird&#x27;s-eye-view features from onboard RGB-D images with patches from aerial feature maps. The method significantly outperforms retrieval-based baselines in off-road environments, achieving much lower trajectory error on both seen and unseen routes while running in real-time.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] 3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [3D animation generation], [2D-3D aligned proxy representation, single-image 3D animation, coarse 3D estimate, image-space generative priors]</li>
<li class=""><strong>authors:</strong> Yupeng Zhu, Xiongzhen Zhang, Ye Chen, Bingbing Ni</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15126</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes 3DProxyImg, a lightweight framework for generating controllable 3D animations from a single image. It uses a coarse 3D proxy to handle geometric control while leveraging learned 2D generative priors for high-fidelity appearance, enabling efficient animation with precise 3D-aware motion control. The method outperforms video-based approaches in identity preservation, consistency, and interactive control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Borrowing from anything: A generalizable framework for reference-guided instance editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [Spatial Alignment Module (SAM), Adaptive Residual Scaling Module (ARSM), Progressive Attention Fusion (PAF), diffusion models, semantic disentanglement]</li>
<li class=""><strong>authors:</strong> Shengxiao Zhou, Chenghua Li, Jianhao Huang, Qinghao Hu, Yifan Zhang</li>
<li class=""><strong>institution:</strong> University of Chinese Academy of Sciences, Nanjing Forestry University, Institute of Automation, Chinese Academy of Sciences, Nanjing Artificial Intelligence Research of IA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15138" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15138</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes GENIE, a framework for reference-guided instance editing that addresses semantic entanglement through three modules: a Spatial Alignment Module (SAM) for correcting pose and scale, an Adaptive Residual Scaling Module (ARSM) for amplifying intrinsic appearance cues, and a Progressive Attention Fusion (PAF) mechanism for rendering the appearance onto a target. The method achieves state-of-the-art performance on the AnyInsertion dataset, demonstrating high fidelity and robustness in disentanglement-based editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Chain-of-Thought reasoning, dynamic gating mechanism, multimodal fusion, action quality assessment, video understanding]</li>
<li class=""><strong>authors:</strong> Mengshi Qi, Yeteng Wu, Xianlin Zhang, Huadong Ma</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15153" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15153</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an Explainable Fitness Assessor framework that uses multimodal Chain-of-Thought reasoning and a dynamic gating mechanism to fuse visual and semantic information for human action form assessment. It introduces a new dataset, CoT-AFA, with detailed explanations for action standardization. The method shows improvements in explanation generation, action classification, and quality assessment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [BEV-grounding, Chain-of-Thought, reinforcement learning, determinantal point process, keyframe selection, spatial reward, pose querying]</li>
<li class=""><strong>authors:</strong> Jiaxu Wan, Xu Wang, Mengwei Xie, Hang Zhang, Mu Xu, Yang Han, Hong Zhang, Ding Yuan, Yifan Yang</li>
<li class=""><strong>institution:</strong> BUAA (Beihang University)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15160" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15160</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EagleVision introduces a dual-stage framework for spatial reasoning, combining a macro perception stage for selecting keyframes and a micro verification stage that uses BEV-grounded pose querying and reinforcement learning. It achieves state-of-the-art performance on VSI-Bench, demonstrating strong and generalizable spatial understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [criticality metrics, relevance classification, object detection, safety evaluation, bidirectional criticality rating, multi-metric aggregation, DeepAccident dataset]</li>
<li class=""><strong>authors:</strong> Jörg Gamerdinger, Sven Teufel, Stephan Amann, Oliver Bringmann</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15181" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15181</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes criticality metrics for evaluating the safety of object detection systems in automated driving. It proposes two novel strategies, bidirectional criticality rating and multi-metric aggregation, to improve classification accuracy. The approach demonstrates up to a 100% improvement in criticality classification accuracy, advancing the safety evaluation of automated vehicle perception systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [cross-modal ultra-scale learning, sparse multi-instance learning, cross-modal scale attention module, multi-modal feature fusion]</li>
<li class=""><strong>authors:</strong> Kaixing Long, Danyi Weng, Yun Mi, Zhentai Zhang, Yanmeng Lu, Jian Geng, Zhitao Zhou, Liming Zhong, Qianjin Feng, Wei Yang, Lei Cao</li>
<li class=""><strong>institution:</strong> Southern Medical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15171" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15171</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CMUS-Net, a cross-modal ultra-scale learning network designed to fuse features from three types of renal biopsy images (TEM, OM, IM) that have vastly different scales (nanometer vs. micrometer). The method uses a sparse multi-instance learning module and a cross-modal scale attention module to bridge the scale gap and enhance pathological semantic information for classification. The model achieves high accuracy in diagnosing multiple glomerular diseases and demonstrates superior performance compared to other multi-modal or multi-scale methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robust and Calibrated Detection of Authentic Multimedia Content</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [deepfake detection], [resynthesis framework, authenticity index, adversarial robustness, calibrated detection, inversion techniques]</li>
<li class=""><strong>authors:</strong> Sarim Hashmi, Abdelrahman Elsayed, Mohammed Talha Alam, Samuele Poppi, Nils Lukas</li>
<li class=""><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15182" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15182</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a calibrated resynthesis framework to verify authentic multimedia content by distinguishing between authentic and plausibly deniable samples, rather than binary real vs. fake detection. It demonstrates that this method maintains low false positive rates and achieves adversarial robustness against compute-restricted adversaries, outperforming prior detection approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-scale parallel feature extraction, green channel guidance, channel-aware residual dense block, spatial adaptive normalization]</li>
<li class=""><strong>authors:</strong> Jianan Wang, Yang Hong, Hesong Li, Tao Wang, Songrong Liu, Ying Fu</li>
<li class=""><strong>institution:</strong> Beijing Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15186" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15186</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ERIENet, an efficient network for enhancing low-light RAW images using a fully-parallel multi-scale architecture and a green channel guidance branch to leverage the superior information in the green channel. It achieves high-quality reconstruction with reduced computational costs, outperforming state-of-the-art methods in both performance and real-time processing speed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [environmental perception safety metric, object detection safety, lane detection safety, joint safety assessment, criticality classification, DeepAccident dataset]</li>
<li class=""><strong>authors:</strong> Jörg Gamerdinger, Sven Teufel, Stephan Amann, Lukas Marc Listl, Oliver Bringmann</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15195</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel Environmental Perception Safety Metric (EPSM) that jointly evaluates the safety of object and lane detection systems for autonomous driving, integrating a lightweight object safety metric and considering task interdependence. It demonstrates that this approach identifies safety-critical errors missed by conventional metrics like precision and recall. The findings emphasize the need for safety-centric evaluation methods in autonomous vehicle perception.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Camera to World: A Plug-and-Play Module for Human Mesh Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [human mesh reconstruction, camera rotation estimation, depth maps, plug-and-play module, world coordinate system]</li>
<li class=""><strong>authors:</strong> Changhai Ma, Ziyu Wu, Yunkang Zhang, Qijun Ying, Boyan Liu, Xiaohui Cai</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15212" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15212</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Mesh-Plug, a plug-and-play module that transforms human meshes from camera to world coordinates by estimating camera rotation from RGB images and rendered depth maps. It uses a human-centered approach to predict camera pitch and refines the mesh pose. Experiments show it outperforms state-of-the-art methods on benchmark datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, image fusion], [Target-Background Contrast (TBC), Weber&#x27;s Law, no-reference metric, infrared-visible fusion, DroneVehicle dataset]</li>
<li class=""><strong>authors:</strong> Yufeng Xie</li>
<li class=""><strong>institution:</strong> Not specified (Author: Yufeng Xie)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15211" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15211</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new no-reference evaluation metric called Target-Background Contrast (TBC) for low-altitude infrared and visible image fusion. Inspired by Weber&#x27;s Law, TBC focuses on the relative contrast between salient thermal targets and their local background to avoid the &quot;Noise Trap&quot; of traditional metrics. Experiments show that TBC better aligns with human perception and provides a more reliable standard for evaluating fusion results in complex low-light scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer, frequency domain analysis, ZernikeVAE, physics-based synthesis]</li>
<li class=""><strong>authors:</strong> Xiyu Zhu, Wei Wang, Xin Yuan, Xiao Wang</li>
<li class=""><strong>institution:</strong> Wuhan University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15221" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15221</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SLCFormer, a transformer-based framework for nighttime lens flare removal, which integrates a frequency domain module for global context and a spatial module for local enhancement, alongside a physics-grounded flare synthesis method. The approach demonstrates state-of-the-art performance on the Flare7K++ dataset and shows robust generalization to real nighttime scenes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Null-LoRA: Low-Rank Adaptation on Null Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [parameter-efficient fine-tuning], [LoRA, null space, low-rank adaptation, singular value decomposition, parameter efficiency]</li>
<li class=""><strong>authors:</strong> Yi Zhang, Yulei Kang, Haoxuan Chen, Jinxuan Li, ian-Fang Hu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15233" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15233</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Null-LoRA, a parameter-efficient fine-tuning method that constrains low-rank adaptation updates within the null space of pre-trained model weights. It freezes portions of low-rank matrices and uses SVD to project updates, reducing redundancy and improving parameter efficiency. Experiments show it outperforms state-of-the-art methods with fewer parameters on vision-language tasks like image-text retrieval and VQA.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [visual enumeration, vision-language models, zero-shot counting, intermediate representations, object counting]</li>
<li class=""><strong>authors:</strong> Kuinan Hou, Jing Mi, Marco Zorzi, Lamberto Ballan, Alberto Testolin</li>
<li class=""><strong>institution:</strong> University of Padova</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares specialized counting architectures and vision-language models (VLMs) for visual enumeration. It finds that VLMs can match or surpass specialized models, especially when prompted to generate intermediate object representations, but still struggle with complex scenes, indicating a need for further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Cross-Modal Alignment Consistency (CMAC-MMD), vision-language model (VLM), intersectional fairness, diagnostic certainty, True Positive Rate (TPR), Area Under the Curve (AUC)]</li>
<li class=""><strong>authors:</strong> Yupeng Zhang, Adam G. Dunn, Usman Naseem, Jinman Kim</li>
<li class=""><strong>institution:</strong> The University of Sydney, Macquarie University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15249" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15249</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training framework called Cross-Modal Alignment Consistency (CMAC-MMD) to reduce intersectional bias in vision-language models for medical diagnosis by standardizing diagnostic certainty across patient subgroups. The method improves both fairness, by reducing the gap in missed diagnoses, and overall accuracy, as demonstrated on skin lesion and glaucoma screening tasks. It provides a scalable approach for equitable clinical AI without requiring sensitive demographic data during inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Mamba architecture, in-context conditioning, multimodal interleaved scanning, cross-modal fusion, pan-sharpening, zero-shot super-resolution]</li>
<li class=""><strong>authors:</strong> Yingying Wang, Xuanhua He, Chen Wu, Jialing Huang, Suiyun Zhang, Rui Liu, Xinghao Ding, Haoxuan Che</li>
<li class=""><strong>institution:</strong> Xiamen University, The Hong Kong University of Science and Technology, University of Science and Technology of China, Huawei Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15261" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15261</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MMMamba, a cross-modal fusion framework based on the Mamba architecture for pan-sharpening, which also supports zero-shot image super-resolution. It introduces a multimodal interleaved scanning mechanism for efficient information exchange between panchromatic and multispectral images. Experiments show the method outperforms state-of-the-art techniques while maintaining linear computational complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird&#x27;s-Eye-View Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [knowledge distillation, bird&#x27;s-eye-view segmentation, cross-modality, voxel-aligned view transformer, LiDAR image representation, 360-degree camera]</li>
<li class=""><strong>authors:</strong> Wenke E, Yixin Sun, Jiaxu Liu, Hubert P. H. Shum, Amir Atapour-Abarghouei, Toby P. Breckon</li>
<li class=""><strong>institution:</strong> Durham University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15311" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15311</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces KD360-VoxelBEV, a cross-modality knowledge distillation framework where a Teacher network fusing LiDAR and camera data trains a lightweight Student network that uses only a single 360-degree panoramic camera for Bird&#x27;s-Eye-View segmentation. The method achieves significant performance gains and fast inference, demonstrating a practical, cost-effective solution for autonomous driving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-agent framework, large language models (LLMs), vision-language models (VLMs), CLIP, ViT classifier, synthetic data generation, zero-shot learning]</li>
<li class=""><strong>authors:</strong> Wangyu Wu, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</li>
<li class=""><strong>institution:</strong> Xi’an Jiaotong-Liverpool University, University of Liverpool, Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15310</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SynthSeg-Agents, a multi-agent framework using LLMs to generate synthetic training data without real images for zero-shot weakly supervised semantic segmentation. It employs a prompt refinement agent and an image generation agent with VLMs, followed by CLIP-based filtering and classifier relabeling. The method achieves competitive performance on standard datasets, demonstrating the potential of LLM-driven agents for cost-efficient segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [supervised contrastive learning, affinity scores, interpretable grading]</li>
<li class=""><strong>authors:</strong> Antony Jerald, Dattesh Shanbhag, Sudhanya Chatterjee</li>
<li class=""><strong>institution:</strong> GE HealthCare</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15315" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15315</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AutoMAC-MRI, an interpretable framework that uses supervised contrastive learning to detect and grade motion artifacts in MRI images. It computes grade-specific affinity scores to quantify an image&#x27;s proximity to each motion severity level, making the grading process transparent. The method was validated on over 5,000 expert-annotated brain MRI slices and shows potential for reducing unnecessary rescans and improving workflow efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [anomaly detection], [prototypical learning, context-aware segmentation, few-shot learning, pixel-level disparity classification, pseudo anomalies]</li>
<li class=""><strong>authors:</strong> Yuxin Jiang, Yunkang Cao, Weiming Shen</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15319</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PCSNet, a network combining prototypical feature adaptation and context-aware segmentation to address the domain gap in few-shot anomaly detection. It improves feature compactness for normal data and separation from anomalies, achieving superior performance on benchmark datasets and real-world inspection tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MECAD: A multi-expert architecture for continual anomaly detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-expert architecture, continual learning, anomaly detection, coreset selection, replay buffer, incremental learning, MVTec AD dataset, AUROC]</li>
<li class=""><strong>authors:</strong> Malihe Dahmardeh, Francesco Setti</li>
<li class=""><strong>institution:</strong> University of Verona, Qualyco S.r.l.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15323" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15323</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MECAD, a multi-expert architecture for continual anomaly detection that dynamically assigns experts to object classes and uses memory management with coreset selection and a replay buffer for incremental learning. The method achieves an average AUROC of 0.8259 on the MVTec AD dataset, reducing knowledge degradation compared to single-expert approaches and balancing efficiency and adaptability for industrial use.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [masked reverse knowledge distillation, image-level masking, feature-level masking, image anomaly detection, knowledge distillation]</li>
<li class=""><strong>authors:</strong> Yuxin Jiang, Yunkang Can, Weiming Shen</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15326</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Masked Reverse Knowledge Distillation (MRKD) method for image anomaly detection, which uses image-level and feature-level masking to transform reconstruction into restoration, mitigating overgeneralization. Experiments on the MVTec dataset show the method achieves state-of-the-art performance in anomaly detection and localization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Vision-based module for accurately reading linear scales in a laboratory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vision-based models, object detection, image classification, instance segmentation, feature extraction, orientation correction]</li>
<li class=""><strong>authors:</strong> Parvesh Saini, Soumyadipta Maiti, Beena Rai</li>
<li class=""><strong>institution:</strong> TCS Research, Tata Consultancy Services Limited</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15327" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15327</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a vision-based module that mimics a human-inspired approach to read measurements from linear scales, such as on syringes and measuring cylinders, by correcting orientation, isolating the scale region, and extracting features like markers and digits. The system&#x27;s readings were compared against human-read values and showed accurate correspondence, demonstrating its potential for laboratory automation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Preprocessing Framework for Video Machine Vision under Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [video compression, neural preprocessor, differentiable virtual codec, rate-accuracy optimization]</li>
<li class=""><strong>authors:</strong> Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie</li>
<li class=""><strong>institution:</strong> Peking University, Bytedance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15331" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15331</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [turn-level causal attention, interleaved masked autoregression, lightweight diffusion head, multimodal fusion]</li>
<li class=""><strong>authors:</strong> Junjie Chen, Fei Wang, Zhihao Huang, Qing Zhou, Kun Li, Dan Guo, Linfeng Zhang, Xun Yang</li>
<li class=""><strong>institution:</strong> Hefei University of Technology, IAI Hefei Comprehensive National Science Center, USTC, SJTU, TeleAI China Telecom, Northwestern Polytechnical University, United Arab Emirates University, Anhui Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15340</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TIMAR, a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual turns, using intra-turn multimodal fusion and turn-level causal attention to accumulate history. It employs a lightweight diffusion head to predict continuous and coherent head dynamics. Experiments show TIMAR reduces Fréchet Distance and MSE by 15-30% compared to prior methods, demonstrating improved temporal coherence and contextual responsiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Group Relative Policy Optimization (GRPO), Optimal Variance Filtering (OVF), Pro-GRPO, Expand-and-Prune, reward clustering, latent feature-based pruning]</li>
<li class=""><strong>authors:</strong> Shiran Ge, Chenyi Huang, Yuang Ai, Qihang Fan, Huaibo Huang, Ran He</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15347</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Pro-GRPO, a dynamic framework that improves Group Relative Policy Optimization (GRPO) for generative model alignment by integrating an &quot;Expand-and-Prune&quot; strategy. It first expands the sampling group for diversity and then prunes reward-clustered trajectories early using latent features to reduce computational cost. Experiments on diffusion and flow models show that Pro-GRPO is more efficient and effective than standard GRPO.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]</li>
<li class=""><strong>authors:</strong> Mikel Williams-Lekuona, Georgina Cosma</li>
<li class=""><strong>institution:</strong> Loughborough University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15372</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Emotion Recognition in Signers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [cross-lingual transfer, temporal segment selection, hand motion features, textual emotion recognition, facial expression analysis]</li>
<li class=""><strong>authors:</strong> Kotaro Funakoshi, Yaoxiong Zhu</li>
<li class=""><strong>institution:</strong> Institute of Science Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15376" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15376</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new dataset for emotion recognition in Japanese Sign Language and addresses the challenges of overlapping grammatical/affective expressions and data scarcity using cross-lingual transfer from textual emotion recognition in spoken language. The authors demonstrate that selecting specific temporal segments and incorporating hand motion features significantly improves emotion recognition performance in signers, establishing a stronger baseline than spoken language LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [3D semantic segmentation, point cloud, deep learning, domain gap analysis, mIoU]</li>
<li class=""><strong>authors:</strong> Maximilian Kellner, Mariana Ferrandon Cervantes, Yuandong Pan, Ruodan Lu, Ioannis Brilakis, Alexander Reiterer</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute for Physical Measurement Techniques IPM, University of Freiburg, University of Cambridge, Sichuan Highway Planning, Survey, Design and Research Institute Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15369" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15369</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SemanticBridge, a novel dataset for 3D semantic segmentation of bridges, and evaluates three state-of-the-art deep learning architectures on it. The study also analyzes the domain gap caused by different sensors, finding that while models perform robustly, sensor variations can reduce performance by up to 11.4% mIoU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [action anticipation, deep learning, video understanding, rebound prediction, temporal modeling]</li>
<li class=""><strong>authors:</strong> Arnau Barrera Roy, Albert Clapés Sintes</li>
<li class=""><strong>institution:</strong> Universitat de Barcelona, Computer Vision Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15386</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a deep learning-based method for anticipating rebounds in basketball broadcast videos, using a new large-scale dataset. It applies state-of-the-art action anticipation techniques to predict which team will gain possession after a shot. The results demonstrate the feasibility and challenges of predictive modeling in dynamic sports scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, distribution alignment, semantic graph, partially view-aligned clustering]</li>
<li class=""><strong>authors:</strong> Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong</li>
<li class=""><strong>institution:</strong> Shantou University, Huaqiao University, Southwest University, South China University of Technology, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15396</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMART, a model for partially view-aligned clustering that uses view distribution alignment and semantic matching contrastive learning to handle misaligned multi-view data. It aligns cross-view covariance matrices to reduce distribution shifts and leverages a semantic graph to guide contrastive learning, improving clustering performance. Experiments on eight datasets show that SMART outperforms existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [biomedical imaging], [channel-independent architecture, contrastive pretraining, self-supervised learning, lightweight CNN, multiplex imaging]</li>
<li class=""><strong>authors:</strong> Simon Gutwein, Arthur Longuefosse, Jun Seita, Sabine Taschner-Mandl, Roxane Licandro</li>
<li class=""><strong>institution:</strong> St. Anna Children&#x27;s Cancer Research Institute, RIKEN Center for Integrative Medical Sciences, Medical University of Vienna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15410" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15410</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a shallow Channel-Independent Model (CIM-S) for self-supervised representation learning on multiplexed tissue imaging data, which processes each protein marker channel separately to preserve specificity. The study finds that this lightweight, channel-independent approach outperforms standard early-fusion CNNs in retaining marker information and discriminating rare cell types, demonstrating that compact architectures can surpass deeper models for this task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [human-robot mutual imitation, kinematic rules, left/right hand coordinate systems, vision-language-action model, behavioral priors, cross-embodiment generalization]</li>
<li class=""><strong>authors:</strong> Zhenhan Yin, Xuanhan Wang, Jiahao Jiang, Kaiyuan Deng, Pengqi Chen, Shuangle Li, Chong Liu, Xing Xu, ingkuan Song, Lianli Gao, Heng Tao Shen</li>
<li class=""><strong>institution:</strong> Tongji University, University of Electronic Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15411</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MiVLA, a vision-language-action model pre-trained using human-robot mutual imitation, which aligns human and robot action spaces via kinematic rules and coordinate systems to integrate behavioral knowledge from human videos and simulated robot data. This approach enhances generalization across different camera views, appearances, and robot embodiments. Experiments on multiple robots show MiVLA outperforms state-of-the-art models in both simulation and real-world control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [CLIP, StyleGAN, face template inversion, cross-modal feature interaction, attribute conditioning]</li>
<li class=""><strong>authors:</strong> Longchen Dai, Zixuan Shen, Zhiheng Zhou, Peipeng Yu, Zhihua Xia</li>
<li class=""><strong>institution:</strong> Jinan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15433</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CLIP-FTI, a method for inverting face recognition templates into photorealistic images by using CLIP to extract semantic facial attribute embeddings and fusing them with the template via a cross-modal network to condition a pretrained StyleGAN. The approach achieves state-of-the-art results in identity accuracy, fine-grained attribute recovery, and cross-model attack transferability compared to prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [monocular depth estimation, 3D hallucination, Laplacian-based evaluation, grounded self-distillation, depth foundation models]</li>
<li class=""><strong>authors:</strong> Hoang Nguyen, Xiaohao Xu, Xiaonan Huang</li>
<li class=""><strong>institution:</strong> University of Michigan, Ann Arbor</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15423</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a framework to address the &quot;3D Mirage&quot; problem in monocular depth estimation, where models hallucinate 3D structures from flat but ambiguous inputs like street art. The method includes a new benchmark (3D-Mirage), a Laplacian-based evaluation with metrics (DCS, CCS), and a parameter-efficient correction technique called Grounded Self-Distillation. The work concludes that depth model evaluation must shift from pixel accuracy to structural robustness to mitigate this safety-critical vulnerability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Step-GUI Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Calibrated Step Reward System, self-evolving training pipeline, GUI-MCP, hierarchical architecture, trajectory-level calibration]</li>
<li class=""><strong>authors:</strong> Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, Shiliang Yang, Zhirui Wang, Brian Li, Kang An, Chenyang Li, Lei Lei, Mengmeng Duan, Danxun Liang, Guodong Liu, Hang Cheng, Hao Wu, Jie Dong, Junhao Huang, Mei Chen, Renjie Yu, Shunshan Li, Xu Zhou, Yiting Dai, Yineng Deng, Yingdan Liang, Zelin Chen, Wen Sun, Chengxu Yan, Chunqin Xu, Dong Li, Fengqiong Xiao, Guanghao Fan, Guopeng Li, Guozhen Peng, Hongbing Li, Hang Li, Hongming Chen, Jingjing Xie, Jianyong Li, Jingyang Zhang, Jiaju Ren, Jiayu Yuan, Jianpeng Yin, Kai Cao, Liang Zhao, Liguo Tan, Liying Shi, Mengqiang Ren, Min Xu, Manjiao Liu, Mao Luo, Mingxin Wan, Na Wang, Nan Wu, Ning Wang, Peiyao Ma, Qingzhou Zhang, Qiao Wang, Qinlin Zeng, Qiong Gao, Qiongyao Li, Shangwu Zhong, Shuli Gao, Shaofan Liu, Shisi Gao, Shuang Luo, Xingbin Liu, Xiaojia Liu, Xiaojie Hou, Xin Liu, Xuanti Feng, Xuedan Cai, Xuan Wen, Xianwei Zhu, Xin Liang, Xin Liu, Xin Zhou, Yingxiu Zhao, Yukang Shi, Yunfang Xu, Yuqing Zeng, Yixun Zhang, Zejia Weng, Zhonghao Yan, Zhiguo Huang, Zhuoyu Wang, Zheng Ge, Jing Li, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Daxin Jiang</li>
<li class=""><strong>institution:</strong> GELab-Team, StepFun</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15431" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15431</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a self-evolving training pipeline using a Calibrated Step Reward System to efficiently generate high-quality GUI automation data. It presents the Step-GUI model family and the GUI-MCP protocol for standardized, privacy-preserving execution. The work demonstrates state-of-the-art performance on GUI benchmarks, advancing practical deployment of multimodal agents for everyday digital interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, plant phenotyping], [spatiotemporal-fusion, dual-decoder network, adaptive gating, geometric priors, motion consistency, negative gravitropism]</li>
<li class=""><strong>authors:</strong> Yueqianji Chen, Kevin Williams, John H. Doonan, Paolo Remagnino, Jo Hepworth</li>
<li class=""><strong>institution:</strong> Durham University, Aberystwyth University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ST-DETrack, a deep learning network that uses a dual-decoder architecture with an adaptive gating mechanism to fuse spatial and temporal evidence for tracking plant branches in time-series imagery. It demonstrates robust performance by maintaining branch identity across growth stages, significantly outperforming baseline methods on a Brassica napus dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [ResNet-101, Inception v3, PyTorch, classification accuracy, mean Average Precision (mAP), parallel convolutions, multi-scale feature extraction]</li>
<li class=""><strong>authors:</strong> Malach Obisa Amonga, Benard Osero, Edna Too</li>
<li class=""><strong>institution:</strong> Chuka University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15480" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15480</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares the deep learning architectures ResNet-101 and Inception v3 for wildlife object detection, using a PyTorch-based training and evaluation pipeline on a wildlife image dataset. The study found that Inception v3 slightly outperformed ResNet-101, achieving 95% accuracy and a mAP of 0.92, with both models proving effective but facing challenges with visually similar species and poor environmental conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer, ray-based representation, multi-view pose lifting, view fusion transformer, 3D human pose estimation]</li>
<li class=""><strong>authors:</strong> Seyed Abolfazl Ghasemzadeh, Alexandre Alahi, Christophe De Vleeschouwer</li>
<li class=""><strong>institution:</strong> UCLouvain, EPFL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15488</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RUMPL, a transformer-based model that lifts 2D human poses to 3D using a novel 3D ray-based representation of keypoints, making it independent of camera calibration and the number of views. It achieves universal deployment across arbitrary multi-view setups without retraining, significantly outperforming triangulation and other baselines in accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Vision Transformers (ViT), SegFormer, attention mechanisms, anomaly scoring, hybrid framework]</li>
<li class=""><strong>authors:</strong> Opeyemi Bamigbade, Mark Scanlon, John Sheppard</li>
<li class=""><strong>institution:</strong> South East Technological University, University College Dublin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image analysis], [deformable image registration, deep learning, iterative optimization, zero-shot generalization, domain shift, neuroimaging]</li>
<li class=""><strong>authors:</strong> Rohit Jena, Pratik Chaudhari, James C. Gee</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15505" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15505</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper independently re-evaluates claims of zero-shot generalization for deep learning-based deformable image registration methods, using rigorous protocols to assess performance on unseen MRI contrasts and resolutions. It finds that while deep methods perform well on in-distribution data, their performance degrades significantly on out-of-distribution contrasts and they face scalability issues on high-resolution images, aligning with established domain shift literature. The authors conclude that claims of universal zero-shot superiority require careful scrutiny and advocate for evaluation protocols that reflect practical clinical workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, disentangled control, pose transformation, expression latent, classifier-free guidance]</li>
<li class=""><strong>authors:</strong> Yuxiang Shi, Zhe Li, Yanwen Wang, Hao Zhu, Xun Cao, Ligang Liu</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Huawei, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15524</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DeX-Portrait, a method for portrait animation that uses explicit pose transformations and latent expression codes to achieve disentangled control over head pose and facial expression. It employs a dual-branch conditioning mechanism for pose and cross-attention for expression, along with a progressive hybrid classifier-free guidance for identity consistency. The method outperforms state-of-the-art baselines in animation quality and disentangled controllability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [confidence verbalization, confidence calibration, three-stage training, multimodal large language models, visual emotion comprehension]</li>
<li class=""><strong>authors:</strong> Daiqing Wu, Dongbao Yang, Can Ma. Yu Zhou</li>
<li class=""><strong>institution:</strong> Chinese Academy of Sciences, Nankai University, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15528</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EmoCaliber, a confidence-aware Multimodal Large Language Model (MLLM) for Visual Emotion Comprehension (VEC). It introduces a three-stage training framework to teach the model structured reasoning, confidence verbalization, and confidence calibration. Evaluations show EmoCaliber outperforms existing methods in both emotion prediction and confidence estimation, advancing reliable VEC systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [3D Gaussian Splatting, feed-forward model, keypoint detection, self-supervised learning, novel view synthesis, camera pose estimation]</li>
<li class=""><strong>authors:</strong> Arthur Moreau, Richard Shaw, Michal Nazarczuk, Jisu Shin, Thomas Tanay, Zhensong Zhang, Songcen Xu, Eduardo Pérez-Pellitero</li>
<li class=""><strong>institution:</strong> Huawei Noah&#x27;s Ark Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15508" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15508</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level using a multi-resolution decoder inspired by keypoint detection, replacing rigid pixel grids with an adaptive &quot;Off The Grid&quot; distribution. The method, trained end-to-end with self-supervised learning, achieves state-of-the-art novel view synthesis using far fewer primitives, leading to more efficient and photorealistic scene generation. It also improves camera pose estimation, suggesting potential for training foundational models without labels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [encoder-only architecture, multi-task learning, GeoMELT, vision and language models, remote sensing]</li>
<li class=""><strong>authors:</strong> João Daniel Silva, Joao Magalhaes, Devis Tuia, Bruno Martins</li>
<li class=""><strong>institution:</strong> INESC-ID, University of Lisbon, Universidade NOVA de Lisboa, EPFL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15531</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GeoMELT, an efficient encoder-only model for multi-task learning in remote sensing vision-language tasks, such as image captioning and cross-modal retrieval. It addresses the high computational cost of large vision-language models by using a compact architecture. The results demonstrate the model&#x27;s effectiveness and efficiency on established benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BLANKET: Anonymizing Faces in Infant Video Recordings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [diffusion model, inpainting, face swapping, landmark preservation, keypoint detection consistency]</li>
<li class=""><strong>authors:</strong> Ditmar Hadera, Jan Cech, Miroslav Purkrabek, Matej Hoffmann</li>
<li class=""><strong>institution:</strong> Czech Technical University in Prague</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15542" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15542</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BLANKET, a two-stage method for anonymizing infant faces in videos by first generating a new compatible face using a diffusion-based inpainting model and then performing temporally consistent face swapping. It outperforms DeepPrivacy2 by better preserving facial attributes and reducing artifacts while effectively de-identifying the subjects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [SAM3, textual prompting, geometric prompting, hybrid prompting, lightweight fine-tuning, remote sensing segmentation, vision-language models]</li>
<li class=""><strong>authors:</strong> Roni Blushtein-Livnon, Osher Rafaeli, David Ioffe, Amir Boger, Karen Sandberg Esquenazi, Tal Svoray</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15564</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the SAM3 foundation model for remote sensing image segmentation using textual, geometric, and hybrid prompting strategies with lightweight fine-tuning. The main conclusion is that combining semantic and geometric cues yields the best performance, while text-only prompting performs poorly, especially for irregular targets, though light fine-tuning offers a practical trade-off for regular objects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [TED-6K benchmark, two-stage training, layer-wise weighting, multimodal large language model, text encoder, diffusion models]</li>
<li class=""><strong>authors:</strong> Bozhou Li, Sihan Yang, Yushuo Guan, Ruichuan An, Xinlong Chen, Yang Shi, Pengfei Wan, Wentao Zhang, Yuanxing zhang</li>
<li class=""><strong>institution:</strong> Peking University, Kuaishou Technology, Xi’an Jiaotong University, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15560</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GRAN-TED, a method to improve text encoders for diffusion models by first proposing a text-only benchmark (TED-6K) for efficient evaluation and then developing a two-stage training paradigm involving fine-tuning on a multimodal LLM and layer-wise weighting. This approach produces a superior text encoder that achieves state-of-the-art performance on the benchmark and enhances text-to-image and text-to-video generation quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [knowledge distillation, camera-radar fusion, intensity-aware distillation, multi-level distillation, BEV representation]</li>
<li class=""><strong>authors:</strong> Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</li>
<li class=""><strong>institution:</strong> German Research Center for Artificial Intelligence (DFKI), RPTU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes IMKD, an intensity-aware multi-level knowledge distillation framework for camera-radar 3D object detection. It uses a three-stage distillation strategy from LiDAR to enhance radar and fused representations while preserving each sensor&#x27;s unique characteristics. Experiments on nuScenes show IMKD outperforms prior distillation-based fusion methods, achieving 67.0% NDS and 61.0% mAP.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [monocular 3D instance segmentation, zero-shot learning, query refinement, spatial-semantic distillation, 3D query index memory, state-distribution token, reconstructive foundation model]</li>
<li class=""><strong>authors:</strong> Zhipeng Du, Duolikun Danier, Jan Eric Lenssen, Hakan Bilen</li>
<li class=""><strong>institution:</strong> University of Edinburgh, Max Planck Institute for Informatics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15577</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes MoonSeg3R, a method for online zero-shot monocular 3D instance segmentation. It leverages a Reconstructive Foundation Model (CUT3R) for geometric priors and introduces modules for refining 2D masks into 3D queries and ensuring temporal consistency. The approach is the first to enable online 3D segmentation from a single RGB stream and achieves performance competitive with RGB-D-based systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, 3D reconstruction], [transformer, bias sinks, monocular training, multi-view supervision, latent avatar space]</li>
<li class=""><strong>authors:</strong> Tobias Kirschstein, Simon Giebenhain, Matthias Nießner</li>
<li class=""><strong>institution:</strong> Technical University of Munich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15599</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexAvatar introduces a transformer-based 3D portrait animation model with learnable data source tokens (bias sinks) to unify training on monocular and multi-view datasets. This approach leverages monocular data for generalization and multi-view data for 3D completeness, creating complete and animatable 3D head avatars from a single image. The method demonstrates strong performance in single-view, few-shot, and monocular avatar creation tasks, outperforming existing methods in view extrapolation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [RGBA-VAE, VLD-MMDiT, Multi-stage Training, diffusion model, layer decomposition]</li>
<li class=""><strong>authors:</strong> Shengming Yin, Zekai Zhang, Zecheng Tang, Kaiyuan Gao, Xiao Xu, Kun Yan, Jiahao Li, Yilei Chen, Yuxiang Chen, Heung-Yeung Shum, Lionel M. Ni, Jingren Zhou, Junyang Lin, Chenfei Wu</li>
<li class=""><strong>institution:</strong> HKUST(GZ), Alibaba, HKUST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15603" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15603</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Qwen-Image-Layered, an end-to-end diffusion model that decomposes an RGB image into multiple RGBA layers for inherent editability. It introduces key components like an RGBA-VAE and a VLD-MMDiT architecture, trained using a multi-stage strategy on data extracted from PSD files. Experiments show the method surpasses existing approaches in decomposition quality, establishing a new paradigm for consistent image editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robust Multi-view Camera Calibration from Dense Matches</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, camera calibration], [structure-from-motion, dense matches, correspondence subsampling, incremental view addition, robust estimation, radial distortion]</li>
<li class=""><strong>authors:</strong> Johannes Hägerlind, Bao-Long Tran, Urs Waldmann, Per-Erik Forssén</li>
<li class=""><strong>institution:</strong> Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a robust method for multi-view camera calibration by analyzing and improving components of a structure-from-motion pipeline. The core contributions are a strategy for subsampling dense correspondences and criteria for incrementally adding views. The evaluation shows the method significantly improves accuracy, especially for cameras with strong radial distortion, making it a useful tool for applications like animal behavior studies and forensic analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [Hough transform, gradient-by-ratio edge detection, double-weighted Hough transform, affine transformations, feature tracking]</li>
<li class=""><strong>authors:</strong> Morgan Coe, Gruffudd Jones, Leah-Nani Alconcel, Marina Gashinova</li>
<li class=""><strong>institution:</strong> University of Birmingham</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15618" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15618</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for reconstructing persistent features of satellites in ISAR images by detecting and tracking linear features across image sequences. The core techniques include using a gradient-by-ratio method for edge detection and a double-weighted Hough transform for accurate feature detection. The authors conclude that tracking features across frames increases confidence in detection and classification, as demonstrated with the robust detection of shadowing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [4D occupancy, spatio-temporal persistence, reactive forecasting, proactive forecasting, world model, voxel-based scene state, linear-complexity attention, recurrent state-space module, ego-motion compensation]</li>
<li class=""><strong>authors:</strong> Yu Zheng, Jie Hu, Kailun Yang, Jiaming Zhang</li>
<li class=""><strong>institution:</strong> Hunan University, ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15621" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15621</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces OccSTeP, a benchmark for 4D occupancy spatio-temporal persistence in autonomous driving, and proposes OccSTeP-WM, a tokenizer-free world model that uses a voxel-based scene state and a recurrent module to fuse spatio-temporal context. The method demonstrates robust performance in challenging scenarios with missing or noisy data, achieving significant improvements in semantic and occupancy IoU metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Towards Physically-Based Sky-Modeling For Image Based Lighting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer graphics, image-based lighting], [sky-modeling, High Dynamic Range Imagery (HDRI), Image-Based Lighting (IBL), Full Dynamic Range (FDR), deep neural network (DNN), parametric sky-models, tonemapping]</li>
<li class=""><strong>authors:</strong> Ian J. Maquignaz</li>
<li class=""><strong>institution:</strong> Université Laval</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15632" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15632</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AllSky, a physically-based sky-model learned directly from captured HDR imagery to generate accurate environment maps for image-based lighting. It demonstrates that current DNN-based sky-models fail to match the photorealism and full dynamic range of physically captured imagery. The work concludes that AllSky provides intuitive user control and achieves state-of-the-art performance, highlighting the limitations of existing models for accurate scene relighting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion transformer (DiT), in-context learning, Effect-LoRA, spatiotemporal sparse tokenization, two-stage training, few-shot video editing]</li>
<li class=""><strong>authors:</strong> Yuanhang Li, Yiren Song, Junzhe Bai, Xinran Liang, Hu Yang, Libiao Jin, Qi Mao</li>
<li class=""><strong>institution:</strong> Communication University of China, National University of Singapore, Baidu Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15635</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IC-Effect is an instruction-guided video VFX editing framework based on Diffusion Transformers (DiT) that uses the source video as a contextual condition and employs a two-stage training strategy with Effect-LoRA for precise effect injection and background preservation. It introduces spatiotemporal sparse tokenization for computational efficiency. The method demonstrates high-quality, temporally consistent visual effects editing from limited data, enabling new possibilities for video creation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [knowledge distillation], [HALD, dataset distillation, soft labels, hard labels, local semantic drift, SRe2L, RDED, LPLD, FKD]</li>
<li class=""><strong>authors:</strong> Jiacheng Cui, Bingkui Tong, Xinyue Bi, Xiaohan Zhao, Jiacheng Liu, Zhiqiang shen</li>
<li class=""><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15647" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15647</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new training paradigm called HALD, which hybridizes soft and hard labels to mitigate local semantic drift in dataset distillation. The method uses hard labels as corrective anchors to align visual content with semantic supervision when only a few image crops are available. The approach outperforms prior state-of-the-art methods on ImageNet-1K, demonstrating that hard labels remain a crucial complementary tool in soft-label-dominated training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Direct Preference Optimization (DPO), MaskDPO, Conditional Asymmetric Preference Optimization, Shared Commonality Preference Optimization, foreground-conditioned inpainting]</li>
<li class=""><strong>authors:</strong> Qirui Li, Yizhe Tang, Ran Yi, Guangben Lu, Fangyuan Zou, Peng Shu, Huan Yu, Jie Jiang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Tencent</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15644" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15644</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes InpaintDPO, a Direct Preference Optimization (DPO) framework designed to mitigate spatial relationship hallucinations in foreground-conditioned image inpainting. It introduces three novel techniques—MaskDPO, Conditional Asymmetric Preference Optimization, and Shared Commonality Preference Optimization—to optimize background spatial relationships while preserving foreground integrity and boundary coherence. The method demonstrates superior performance in generating images with plausible spatial relationships between foreground subjects and their backgrounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Flow Matching, Classifier-Free Guidance, solution consistency loss, velocity ODE, one-step generation, Diffusion Transformer]</li>
<li class=""><strong>authors:</strong> Tianze Luo, Haotian Yuan, Zhuang Liu</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15657</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SoFlow, a framework for one-step generative modeling that uses a Flow Matching loss and a solution consistency loss to train models without requiring Jacobian-vector product calculations. It improves training efficiency by enabling Classifier-Free Guidance and achieves better FID-50K scores than MeanFlow models on ImageNet 256x256 when using the same DiT architecture and training epochs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision-text compression, VTCBench, DeepSeek-OCR, Glyph, VTC-Retrieval, VTC-Reasoning, VTC-Memory]</li>
<li class=""><strong>authors:</strong> Hongbo Zhao, Meng Wang, Fei Zhu, Wenzhuo Liu, Bolin Ni, Fanhu Zeng, Gaofeng Meng, Zhaoxiang Zhang</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science &amp; Innovation, CAS; Tencent Hunyuan Team</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15649" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15649</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces VTCBench, the first benchmark to evaluate Vision-Language Models&#x27; ability to understand long context using Vision-Text Compression (VTC), a technique that converts long text into dense 2D images for token efficiency. The study systematically tests models on retrieval, reasoning, and memory tasks with VTC-compressed inputs. The main conclusion is that most VLMs perform poorly on long-context understanding with VTC, despite good OCR decoding, failing to capture long-range associations in the compressed visual context.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stylized Synthetic Augmentation further improves Corruption Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [data augmentation, neural style transfer, synthetic data, corruption robustness, TrivialAugment]</li>
<li class=""><strong>authors:</strong> Georg Siedel, Rojan Regmi, Abhirami Anand, Weijia Shao, Silvia Vock, Andrey Morozov</li>
<li class=""><strong>institution:</strong> University of Stuttgart, Federal Institute for Occupational Safety and Health (BAuA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer to improve the corruption robustness of deep vision models. It finds that stylizing synthetic images, despite lowering their FID score, is beneficial for training, and that this method can be effectively combined with rule-based augmentations like TrivialAugment. The approach achieves state-of-the-art robust accuracy on several image classification benchmarks under common corruptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]</li>
<li class=""><strong>authors:</strong> Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</li>
<li class=""><strong>institution:</strong> mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15692</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal large language model (MLLM), supervised fine-tuning (SFT), chain-of-thought (CoT), artifact reasoning, two-stage training, ViF-CoT-4K dataset, ViF-Bench benchmark]</li>
<li class=""><strong>authors:</strong> Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15693" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15693</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Skyra, a specialized multimodal large language model (MLLM) that detects AI-generated videos by identifying and reasoning about human-perceivable visual artifacts as grounded evidence. The method involves constructing a large-scale annotated artifact dataset (ViF-CoT-4K) and employing a two-stage training strategy to enhance artifact perception and explanation. The results show that Skyra outperforms existing methods on multiple benchmarks, providing an explainable approach to AI-generated video detection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [vision-language models, diffusion autoencoders, diffusion DPO, two-alternative forced choice, perceptual loss, image compression]</li>
<li class=""><strong>authors:</strong> Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski, Li Fei-Fei, Jiajun Wu, Jason Zhang</li>
<li class=""><strong>institution:</strong> Stanford University, Google Research, Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15701</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes VLIC, a method that uses vision-language models (VLMs) as zero-shot judges to provide binary preferences between image reconstructions, which are then used to post-train a diffusion-based image compression system via Diffusion DPO. The core finding is that VLMs can effectively replicate human perceptual judgments, and calibrating the compression model on these VLM preferences yields state-of-the-art performance in human-aligned image compression.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [video generation], [autoregressive video diffusion, self-resampling, resampling forcing, sparse causal mask, history routing]</li>
<li class=""><strong>authors:</strong> Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, Dahua Lin</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15702" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15702</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Resampling Forcing, an end-to-end training framework for autoregressive video diffusion models that addresses exposure bias through a self-resampling scheme and a sparse causal mask. It also proposes a history routing mechanism for efficient long-horizon generation. The method achieves performance comparable to distillation-based baselines and shows superior temporal consistency on longer videos.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-modal learning], [hierarchical gated fusion, transformer, masked alignment loss, over-positive penalty, active speaker detection]</li>
<li class=""><strong>authors:</strong> Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall</li>
<li class=""><strong>institution:</strong> Indiana University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15707" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15707</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GateFusion, a novel architecture for active speaker detection that uses a Hierarchical Gated Fusion Decoder (HiGate) to progressively integrate audio and visual features at multiple Transformer layers. The method is enhanced by two auxiliary losses, Masked Alignment Loss and Over-Positive Penalty, to improve multimodal alignment and suppress false positives. The model achieves state-of-the-art results on several challenging benchmarks, demonstrating superior performance and generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-View Foundation Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-view consistency, 3D-aware attention, feature matching, DINO, SAM, CLIP]</li>
<li class=""><strong>authors:</strong> Leo Segre, Or Hirschorn, Shai Avidan</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15708" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15708</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to convert standard 2D foundation models into multi-view consistent models by augmenting them with intermediate 3D-aware attention layers. This approach improves feature matching across different views of the same 3D scene without needing to build a 3D feature model. Quantitative experiments show the method significantly improves feature consistency and localization accuracy compared to baseline foundation models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [3D Gaussian Splatting, triangle mesh, hybrid representation, differentiable rendering, volumetric rendering, neural networks]</li>
<li class=""><strong>authors:</strong> Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola, Giljoo Nam</li>
<li class=""><strong>institution:</strong> Meta Codec Avatars Lab, Meta Reality Labs, Stellon Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15711" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15711</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Gaussian Pixel Codec Avatars (GPiCA), a hybrid avatar representation that combines a textured triangle mesh and 3D Gaussians within a unified differentiable rendering pipeline. This approach uses neural networks to decode a facial expression code into the mesh, texture, and Gaussians, which are rendered together. The method achieves the realism of Gaussian-based avatars while maintaining the rendering efficiency of mesh-based avatars, enabling photorealistic performance on mobile devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [diffusion paradigm, autoregressive models, fine-tuning, block-decoding, KV cache reuse, vision language models]</li>
<li class=""><strong>authors:</strong> Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DiffusionVL is a method that translates any powerful autoregressive vision-language model into a diffusion-based model through simple fine-tuning. It introduces a block-decoding design for arbitrary-length generation and KV cache reuse, achieving significant inference speedup. The approach shows that converting AR models to diffusion is highly effective, yielding strong performance gains and faster inference with minimal training data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] In Pursuit of Pixel Supervision for Visual Pre-training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [masked autoencoder, self-supervised learning, pixel-space learning, autoencoder, MAE]</li>
<li class=""><strong>authors:</strong> Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu</li>
<li class=""><strong>institution:</strong> Meta, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces &quot;Pixio&quot;, an enhanced masked autoencoder (MAE) model for self-supervised visual pre-training using pixel-level supervision. It is trained on 2B web images and demonstrates competitive performance across various downstream tasks like depth estimation and segmentation, showing pixel-space learning remains a strong alternative to latent-space methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Spatia: Video Generation with Updatable Spatial Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [spatial memory, 3D scene point cloud, visual SLAM, dynamic-static disentanglement, camera control, 3D-aware editing]</li>
<li class=""><strong>authors:</strong> Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu</li>
<li class=""><strong>institution:</strong> The University of Sydney, Microsoft Research, HKUST, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15716" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15716</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Spatia, a video generation framework that uses an updatable 3D scene point cloud as persistent spatial memory to enhance long-term spatial consistency. It iteratively generates video clips conditioned on this memory and updates it via visual SLAM, enabling applications like explicit camera control and 3D-aware interactive editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [adversarial agents, Monte Carlo Tree Search (MCTS), question chains, pyramid-like reasoning, vision language models (VLMs), fine-tuning, synthetic dataset]</li>
<li class=""><strong>authors:</strong> Yuqun Zhang, Yuxuan Zhao, Sijia Chen</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Yantai Research Institute, Harbin Engineering University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14735</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PyFi, a framework that uses a multi-agent adversarial mechanism under Monte Carlo Tree Search to synthesize a large-scale, pyramid-structured financial image QA dataset without human annotation. Fine-tuning VLMs on this dataset enables them to decompose complex financial questions into simpler sub-questions, leading to significant accuracy improvements on the benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [self-supervised learning, vision transformer, cross-scale distillation, magnification-invariant representation, whole-slide image analysis]</li>
<li class=""><strong>authors:</strong> Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner</li>
<li class=""><strong>institution:</strong> University of Kentucky, University of Pittsburgh, Emory University, University of California Davis, University of Texas Health</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14796</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Magnification-Aware Distillation (MAD), a self-supervised framework that learns unified image representations by linking low-magnification context with spatially aligned high-magnification detail in whole-slide images. The resulting foundation model, MAD-NP, demonstrates strong resolution-invariant learning, as shown by a classifier trained on 10x embeddings maintaining 96.7% performance on unseen 40x tiles. The work concludes that this approach enables scalable, magnification-robust analysis using a unified embedding space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [deep learning, video analysis, segmentation, classification, Fagotti score, diagnostic laparoscopy, Dice score, F1-score, RMSE]</li>
<li class=""><strong>authors:</strong> Riccardo Oliva, Farahdiba Zarin, Alice Zampolini Faustini, Armine Vardazaryan, Andrea Rosati, Vinkle Srivastav, Nunzia Del Villano, Jacques Marescaux, Giovanni Scambia, Pietro Mascagni, Nicolas Padoy, Anna Fagotti</li>
<li class=""><strong>institution:</strong> Fondazione Policlinico Universitario Agostino Gemelli IRCCS, Università Cattolica del Sacro Cuore, IRCAD, University of Strasbourg, IHU Strasbourg, Università degli studi di Modena</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14797" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14797</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a deep learning system to analyze diagnostic laparoscopy videos for advanced ovarian cancer, automating the assessment of peritoneal carcinosis and predicting the Fagotti score to guide surgical decisions. The AI model segments anatomical structures and tumor lesions, then classifies video-level features to estimate surgical feasibility. The results demonstrate reproducible performance, suggesting AI can standardize intraoperative tumor burden assessment and support clinical decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Gaussian Parameterization for Direct Atomic Structure Identification in Electron Tomography</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational imaging], [Gaussian parameterization, atomic electron tomography, inverse problem, atom tracing, transmission electron microscopy]</li>
<li class=""><strong>authors:</strong> Nalini M. Singh, Tiffany Chien, Arthur R.C. McCray, Colin Ophus, Laura Waller</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15034" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15034</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new method for atomic electron tomography that directly solves for atomic positions and properties by parameterizing the structure as a collection of learnable Gaussians, rather than first reconstructing a volumetric image. This approach incorporates a strong physical prior, which improves robustness to imaging artifacts like those from limited tilt angles. Simulated and experimental results demonstrate its potential for practical materials characterization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [meta-learning, few-shot learning, weakly-supervised segmentation, sparse labels, Omni meta-training, Efficient Omni ProtoSeg (EO-ProtoSeg)]</li>
<li class=""><strong>authors:</strong> Pandega Abyan Zumarsyah, Igi Ardiyanto, Hanung Adi Nugroho</li>
<li class=""><strong>institution:</strong> Universitas Gadjah Mada</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15061" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15061</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes improved meta-learners for few-shot, weakly-supervised segmentation of the optic disc and cup in fundus images. The core method introduces Omni meta-training for balanced data usage and efficient versions to reduce computational costs, along with sparsification techniques for generating scribbles. The best model, EO-ProtoSeg, achieves high segmentation accuracy using only one sparsely labeled image, outperforming methods that require more labels and being comparable to heavier unsupervised domain adaptation approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generative Preprocessing for Image Compression with Pre-trained Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [Consistent Score Identity Distillation (CiD), Rate-Perception (R-P) optimization, Stable Diffusion 2.1, parameter-efficient fine-tuning, differentiable codec surrogate]</li>
<li class=""><strong>authors:</strong> Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang</li>
<li class=""><strong>institution:</strong> Bytedance Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15270</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-19">2025-12-19<a href="#2025-12-19" class="hash-link" aria-label="Direct link to 2025-12-19" title="Direct link to 2025-12-19" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251219] D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [CLIP, Stable Diffusion XL, zero-shot classification, demographic bias mitigation, data generation]</li>
<li class=""><strong>authors:</strong> Javon Hickmon</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15747" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15747</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Surely Large Multimodal Models (Don&#x27;t) Excel in Visual Species Recognition?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [few-shot learning, post-hoc correction, large multimodal models, visual species recognition, prompting, re-ranking]</li>
<li class=""><strong>authors:</strong> Tian Liu, Anwesha Basu, James Caverlee, Shu Kong</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University, University of Macau, Institute of Collaborative Innovation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15748" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15748</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Post-hoc Correction (POC), a method that uses a Large Multimodal Model (LMM) to re-rank the top predictions from a few-shot learning expert model for Visual Species Recognition. It concludes that while LMMs alone underperform on this specialized task, they can effectively correct expert model errors, and POC significantly boosts few-shot learning accuracy without additional training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [generative adversarial networks, data augmentation, image-to-image translation, rule-based mask warping, non-mask preservation loss]</li>
<li class=""><strong>authors:</strong> Yan Yang, George Bebis, Mircea Nicolescu</li>
<li class=""><strong>institution:</strong> University of Nevada, Reno</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-step data augmentation method for masked face detection and recognition, combining rule-based mask warping with GAN-based unpaired image-to-image translation to generate realistic masked faces. It introduces a non-mask preservation loss and stochastic noise injection to improve training and diversity. The approach shows qualitative improvements over rule-based methods alone and complements other GAN-based generation techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neuromorphic computing, intrinsic plasticity, Magnetic Tunnel Junctions (MTJs), hardware-software co-design, near-sensor processing, working memory]</li>
<li class=""><strong>authors:</strong> Jingli Liu, Huannan Zheng, Bohao Zou, Kezhou Yang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15829" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15829</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces IPNet, a neuromorphic architecture that uses Magnetic Tunnel Junction (MTJ) neurons with intrinsic plasticity to physically emulate human-like working memory. The hardware-software co-designed system achieves high accuracy on dynamic vision tasks and significantly reduces energy consumption and footprint compared to traditional models like LSTMs. The work demonstrates that bio-inspired intrinsic plasticity can provide superior processing efficiency and performance for real-time applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Large Video Planner Enables Generalizable Robot Control</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [robotics, foundation models], [large-scale video pretraining, generative video planning, vision-language-action (VLA), zero-shot planning, internet-scale video dataset]</li>
<li class=""><strong>authors:</strong> Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du</li>
<li class=""><strong>institution:</strong> MIT, UC Berkeley, Harvard</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15840" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15840</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an alternative paradigm for building robot foundation models by using large-scale video pretraining, rather than extending multimodal LLMs, to create a generative video planner. The model produces zero-shot video plans from novel instructions, which are then post-processed into executable robot actions. The approach demonstrates robust generalization and real-world feasibility in robot control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [self-supervised learning, vision-language alignment, I-JEPA, JARVIS, masked predictive loss, frozen vision foundation models]</li>
<li class=""><strong>authors:</strong> Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara</li>
<li class=""><strong>institution:</strong> University of Modena and Reggio Emilia, AMD Silo AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [mechanistic interpretability], [sparse autoencoder, ℓ1-regularization, Grad-FAM, weight-space editing, critical suppression threshold]</li>
<li class=""><strong>authors:</strong> Vegard Flovik</li>
<li class=""><strong>institution:</strong> DNV</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15938" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15938</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SALVE, a framework that uses an ℓ1-regularized sparse autoencoder to discover and validate latent features in neural networks, then performs precise weight-space edits to control model behavior. It demonstrates consistent control across ResNet-18 and ViT models, providing a methodology to turn feature discovery into actionable model edits for more transparent AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Verbalization of Path (VoP), Sparsely Grounded Visual Navigation, CityNav benchmark, multimodal large language models (MLLMs), cognitive map]</li>
<li class=""><strong>authors:</strong> Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Texas A&amp;M University, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15933" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15933</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the CityNav benchmark to evaluate multimodal large language models (MLLMs) on the knowledge-intensive task of city navigation without maps or GPS. It proposes the Verbalization of Path (VoP) method, which extracts an explicit cognitive map of landmarks and directions from the MLLM to guide sequential decision-making. The authors conclude that current MLLMs and standard reasoning techniques underperform in this setting, but VoP significantly improves navigation success.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [retrieval-augmented generation, 4D spatio-temporal reasoning, structured memory, vision-language models, embodied AI]</li>
<li class=""><strong>authors:</strong> Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15940" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15940</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces R4, a training-free framework that equips vision-language models with a structured, lifelong memory by anchoring object-level semantic descriptions in a 4D spatio-temporal database. This enables retrieval-augmented reasoning over space and time for embodied tasks like question answering and navigation. Experiments show that R4 substantially improves reasoning over spatio-temporal information compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multimodal evaluation], [perceptual grounding, robustness evaluation, controlled perturbations, pixel-based augmentations, diffusion-based stylized illusions, face matching, text-in-vision comprehension, image matching, grid pointing game, attribute localization]</li>
<li class=""><strong>authors:</strong> Tejas Anvekar, Fenil Bardoliya, Pavan K. Turaga, Chitta Baral, Vivek Gupta</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15949</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces The Perceptual Observatory, a framework for evaluating Multimodal Large Language Models (MLLMs) by testing their perceptual grounding and robustness through controlled tasks and systematic image perturbations. It concludes that current MLLM evaluations overemphasize end-task accuracy and lack rigorous assessment of fundamental visual understanding, revealing that model progress may rely more on textual knowledge than genuine visual perception.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Vision Language Model, Scene Graph, Supervised Fine-Tuning, Direct Preference Optimization, synthetic data]</li>
<li class=""><strong>authors:</strong> Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello</li>
<li class=""><strong>institution:</strong> University of Stuttgart, Bosch Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15957" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15957</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces CAMP-VLM, a framework that uses a Vision Language Model enhanced with scene graphs and contextual features to predict multi-human behaviors from a third-person view. It is fine-tuned with synthetic data using SFT and DPO, and the results show it outperforms the best baseline by up to 66.9% in prediction accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [vision-language models, few-shot learning, multispectral object detection, Grounding DINO, YOLO-World, modality integration]</li>
<li class=""><strong>authors:</strong> Manuel Nkegoum, Minh-Tan Pham, Élisa Fromont, Bruno Avignon, Sébastien Lefèvre</li>
<li class=""><strong>institution:</strong> Univ Bretagne Sud, IRISA; Univ Rennes, IRISA; ATERMES; UiT The Arctic University of Norway</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15971" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15971</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper adapts vision-language models (VLMs) like Grounding DINO and YOLO-World for few-shot multispectral object detection by integrating text, visual (RGB), and thermal (IR) modalities. It demonstrates that these VLM-based detectors outperform specialized models in data-scarce settings and achieve competitive results in fully supervised scenarios on benchmarks like FLIR and M3FD. The findings show that semantic priors from large-scale VLMs effectively transfer to unseen spectral domains, enabling data-efficient multispectral perception.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Are vision-language models ready to zero-shot replace supervised classification models in agriculture?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, agriculture], [vision-language models, zero-shot classification, multiple-choice prompting, open-ended prompting, semantic judging, YOLO11, AgML]</li>
<li class=""><strong>authors:</strong> Earl Ranario, Mason J. Earles</li>
<li class=""><strong>institution:</strong> University of California, Davis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15977</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks various open-source and closed-source vision-language models (VLMs) in a zero-shot setting on 27 agricultural classification datasets from AgML. It finds that VLMs significantly underperform a supervised YOLO11 baseline, with performance varying greatly based on prompting strategy and evaluation methodology. The authors conclude that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems but could serve as assistive tools when combined with constrained interfaces and domain-aware evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [ResNet50, PlantNet300K, deep visual embeddings, feature-space analysis, biodiversity estimation, selective mowing algorithm]</li>
<li class=""><strong>authors:</strong> Lars Beckers, Arno Waes, Aaron Van Campenhout, Toon Goedemé</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15993" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15993</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a robotic mowing system that uses a ResNet50 model, pretrained on PlantNet300K, to generate deep visual embeddings of vegetation. It estimates biodiversity from the dispersion of these embeddings and uses this to selectively deactivate the mower blades to preserve diverse patches. The results show a strong correlation between the embedding-space diversity and expert assessments, validating the approach for enhancing garden biodiversity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [diffusion model, bridge attention, action refinement]</li>
<li class=""><strong>authors:</strong> Liudi Yang, Yang Bai, George Eskandar, Fengyi Shen, Mohammad Altillawi, Dong Chen, Ziyuan Liu, Abhinav Valada</li>
<li class=""><strong>institution:</strong> University of Freiburg, Ludwig Maximilian University of Munich, Munich Center for Machine Learning (MCML), Technical University of Munich, Huawei Heisenberg Research Center (Munich)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16023" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16023</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces CoVAR, a method for co-generating video and robotic action sequences from text instructions using a multi-modal diffusion framework. It extends a pretrained video diffusion model with a parallel action diffusion model and connects them via a Bridge Attention mechanism for cross-modal interaction, along with an action refinement module. The approach demonstrates superior performance in generating high-quality videos and accurate actions compared to existing baselines, offering a scalable way to leverage video data for robotic learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [flow matching, adversarial policy, closed-loop evaluation, end-to-end autonomous driving, real-world image generation]</li>
<li class=""><strong>authors:</strong> Jiaheng Geng, Jiatong Du, Xinyu Zhang, Ye Li, Panqu Wang, Yanjun Huang</li>
<li class=""><strong>institution:</strong> Tongji University, ZERON</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16055" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16055</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a closed-loop evaluation platform for end-to-end autonomous driving that uses a flow matching-based real-world image generator and an adversarial traffic policy to create safety-critical corner cases. The platform efficiently generates realistic driving scenes to test models like UniAD and VAD, demonstrating their performance degradation in adversarial scenarios. This method effectively identifies model weaknesses to improve the safety and robustness of autonomous driving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Auto-Vocabulary 3D Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [auto-vocabulary 3D object detection, vision-language models, image captioning, pseudo 3D box generation, feature-space semantics expansion, semantic score]</li>
<li class=""><strong>authors:</strong> Haomeng Zhang, Kuan-Chuan Peng, Suhas Lohit, Raymond A. Yeh</li>
<li class=""><strong>institution:</strong> Purdue University, Mitsubishi Electric Research Laboratories (MERL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16077" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16077</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Auto-Vocabulary 3D Object Detection (AV3DOD), a framework that autonomously generates class names for detected 3D objects without user input by leveraging 2D vision-language models for semantic candidate generation. It achieves state-of-the-art performance in both localization and semantic quality on standard datasets, outperforming prior methods like CoDA.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [3D multi-channel patch diffusion model, FOD-patch adapter, voxel-level conditional coordinating module, SH attention module, spherical harmonics]</li>
<li class=""><strong>authors:</strong> Hao Tang, Hanyu Liu, Alessandro Perelli, Xi Chen, Chao Li</li>
<li class=""><strong>institution:</strong> University of Dundee, University of Bath, University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16075" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16075</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes FOD-Diff, a 3D multi-channel patch diffusion model that predicts high angular resolution fiber orientation distributions (HAR-FOD) from low angular resolution inputs. The method incorporates a patch adapter using brain anatomy priors, a conditional coordinating module, and a spherical harmonics attention module to handle complex coefficient correlations. The experimental results demonstrate that this approach achieves state-of-the-art performance in HAR-FOD prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LAPX: Lightweight Hourglass Network with Global Context</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hourglass network, self-attention, lightweight attention, global context, human pose estimation, edge device]</li>
<li class=""><strong>authors:</strong> Haopeng Zhao, Marsha Mariya Kappan, Mahdi Bamdad, Francisco Cruz</li>
<li class=""><strong>institution:</strong> University of New South Wales, Universidad Central de Chile</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16089" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16089</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LAPX, a lightweight Hourglass network enhanced with self-attention to capture global context for efficient human pose estimation. It refines stage design and attention modules to balance accuracy and speed, achieving competitive results on MPII and COCO datasets with only 2.3M parameters. The model demonstrates real-time performance suitable for deployment on edge devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Collimator-assisted high-precision calibration method for event cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computer vision, sensor calibration], [collimator, sphere motion model, nonlinear optimization, flickering star-based patterns, geometric calibration]</li>
<li class=""><strong>authors:</strong> Zibin Liu, Shunkun Liang, Banglei Guan, Dongcai Tan, Yang Shang, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16092" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16092</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a high-precision calibration method for event cameras using a collimator with flickering star-based patterns. The method first linearly solves camera parameters via a sphere motion model and then refines them with nonlinear optimization. The authors demonstrate that this approach outperforms existing calibration methods in accuracy and reliability for long-range measurement scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [SageAttention, Sparse-Linear Attention (SLA), rCM, W8A8 quantization, step distillation]</li>
<li class=""><strong>authors:</strong> Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shengshu Technology, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16093" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16093</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TurboDiffusion is a framework that accelerates video diffusion models by 100-200 times using attention acceleration (low-bit SageAttention and Sparse-Linear Attention), step distillation (rCM), and W8A8 quantization. Experiments on several models show it achieves this speedup on a single GPU while maintaining comparable video quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A Tri-Dynamic Preprocessing Framework for UGC Video Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Tri-Dynamic Preprocessing, adaptive factor, adaptive quantization level, adaptive lambda tradeoff, video compression, UGC, deep preprocessing]</li>
<li class=""><strong>authors:</strong> Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie</li>
<li class=""><strong>institution:</strong> Peking University, Bytedance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16101" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16101</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Tri-Dynamic Preprocessing (TDP) framework for UGC video compression, which adaptively adjusts preprocessing intensity, quantization level, and the rate-distortion tradeoff. This method addresses the high variability of UGC videos, where traditional deep preprocessing fails. Experiments show the framework achieves exceptional performance on large-scale test sets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Flexible Camera Calibration using a Collimator System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [collimator system, angle invariance constraint, spherical motion model, single image calibration]</li>
<li class=""><strong>authors:</strong> Shunkun Liang, Banglei Guan, Zhenbao Yu, Dongcai Tan, Pengju Sun, Zibin Liu, Qifeng Yu, Yang Shang</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16113</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel camera calibration method using a custom collimator system, which imposes an angle invariance constraint to reduce the relative motion between the target and camera to a 3DOF pure rotation. It proposes linear and minimal solvers for calibration and a single-image algorithm that eliminates the need for camera motion. Experiments show the method is feasible and superior to baseline approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [autoencoder, denoising, adversarial attack, Perlin noise, YOLOv5, object detection]</li>
<li class=""><strong>authors:</strong> Min Geun Song, Gang Min Kim, Woonmin Kim, Yongsik Kim, Jeonghyun Sim, Sangbeom Park, Huy Kang Kim</li>
<li class=""><strong>institution:</strong> Korea University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16123" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16123</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an autoencoder-based denoising defense to mitigate adversarial attacks on object detection models. The method uses a single-layer convolutional autoencoder to remove Perlin noise perturbations from images before feeding them to a YOLOv5 detector. The results show that this approach provides a partial recovery of detection performance without requiring model retraining.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [contrastive learning, latent space, action recognition, interaction detection]</li>
<li class=""><strong>authors:</strong> Ren Nakagawa, Yang Yang, Risa Shinoda, Hiroaki Santo, Kenji Oyama, Fumio Okura, Takenao Ohkawa</li>
<li class=""><strong>institution:</strong> Kobe University, The University of Osaka</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16133" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16133</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CattleAct, a data-efficient method for detecting interactions between grazing cattle by decomposing them into individual actions and learning a joint action-interaction latent space using contrastive learning. It demonstrates accurate interaction detection on a commercial-scale pasture, outperforming baseline methods. The approach addresses the challenge of rare interaction events by leveraging a pre-trained action model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [privacy and security], [machine unlearning, dual-view inference attack, privacy knowledge gain, membership inference, black-box attack]</li>
<li class=""><strong>authors:</strong> Lulu Xue, Shengshan Hu, Linqiang Qian, Peijin Guo, Yechao Zhang, Minghui Li, Yanjun Zhang, Dayong Ye, Leo Yu Zhang</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Tsinghua University, Nanyang Technological University, University of Technology Sydney, City University of Macau, Griffith University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16126" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16126</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DVIA, a Dual-View Inference Attack, which extracts membership information on retained data by querying both the original and unlearned models. It demonstrates that this dual-view setting amplifies privacy leakage compared to querying a single model. The main conclusion is that machine unlearning introduces new vulnerabilities, increasing the privacy risk for data that was not removed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [dual-spectral CT, hybrid reconstruction, UNet++, residual dynamic convolution, oblique projection modification technique (OPMT), basis material decomposition]</li>
<li class=""><strong>authors:</strong> Ze Yuan, Wenbin Li, Shusen Zhao</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; Southern University of Science and Technology; Detection Institute for Advanced Technology Longhua-Shenzhen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16140" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16140</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a hybrid framework for dual-spectral CT reconstruction that first uses a fast iterative method (OPMT) for initial material decomposition and then refines the result with a novel deep neural network called ResDynUNet++. This network, based on UNet++ with residual dynamic convolution blocks, is designed to reduce artifacts and improve image quality. Experiments on synthetic and clinical datasets show the method&#x27;s effectiveness and superior performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [SAM, graph neural network, few-shot learning, 3D part segmentation, geometric feature propagation]</li>
<li class=""><strong>authors:</strong> Yueyang Hu, Haiyong Jiang, Haoxuan Song, Jun Xiao, Hao Pan</li>
<li class=""><strong>institution:</strong> University of Chinese Academy of Sciences, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16143</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SegGraph, a novel framework for few-shot 3D part segmentation that constructs a graph of 2D SAM segments to model their spatial relationships and propagate features via a graph neural network. It outperforms existing baselines by at least 6.9% mIoU on PartNet-E, showing strong performance on small parts and boundaries due to its improved geometric understanding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [unsupervised domain adaptation], [prompt tuning, vision-language models, adversarial training, class mapping mechanism, marginal distribution alignment, conditional distribution alignment]</li>
<li class=""><strong>authors:</strong> Chao Li, Dasha Hu, Chengyang Li, Yuming Jiang, Yuncheng Shen</li>
<li class=""><strong>institution:</strong> Sichuan University, Zhaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16164" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16164</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes C-DGPA, a class-centric dual-alignment method for generative prompt adaptation in unsupervised domain adaptation. It uses a dual-branch architecture to synergistically align both marginal and conditional distributions between domains via adversarial training and a class mapping mechanism. Experiments on standard benchmarks show it achieves state-of-the-art results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Towards Closing the Domain Gap with Event Cameras</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [event cameras, domain gap, end-to-end driving, neural networks, illumination invariance]</li>
<li class=""><strong>authors:</strong> M. Oltan Sevinc, Liao Wu, Francisco Cruz</li>
<li class=""><strong>institution:</strong> University of New South Wales, Universidad Central de Chile</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16178</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using event cameras instead of traditional frame-based cameras for end-to-end autonomous driving to address the domain gap caused by day-night lighting differences. The method involves training neural networks on event camera data and evaluating their performance across different lighting conditions. The main conclusion is that event cameras maintain more consistent performance across lighting domains, showing smaller performance degradation and superior baseline performance in cross-domain scenarios compared to grayscale cameras.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [synthetic data generation, 4D human motion, pose estimation, domain-specific dataset, avatar animation]</li>
<li class=""><strong>authors:</strong> Jerrin Bright, Zhibo Wang, Dmytro Klepachevskyi, Yuhao Chen, Sirisha Rambhatla, David Clausi, John Zelek</li>
<li class=""><strong>institution:</strong> University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16199" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16199</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Avatar4D, a pipeline for generating customizable synthetic 4D human motion datasets for domain-specific applications like sports. It demonstrates the pipeline&#x27;s effectiveness by creating the Syn2Sport dataset and shows that models trained on this synthetic data can perform well in supervised learning and zero-shot transfer to real-world sports pose estimation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, group-relative proximal optimization, visual alignment, radiology report generation, medical vision-language models]</li>
<li class=""><strong>authors:</strong> Sarosij Bose, Ravi K. Rajendran, Biplob Debnath, Konstantinos Karydis, Amit K. Roy-Chowdhury, Srimat Chakradhar</li>
<li class=""><strong>institution:</strong> NEC Laboratories America, University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16201" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16201</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VALOR, a reinforcement learning-based framework using Group-Relative Proximal Optimization to align medical vision-language models. It improves report generation by first enhancing clinical terminology and then aligning visual features with disease findings. The method achieves better factual accuracy and visual grounding compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Open Ad-hoc Categorization with Contextualized Feature Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [CLIP, learnable context tokens, visual clustering, image-text alignment, saliency maps]</li>
<li class=""><strong>authors:</strong> Zilin Wang, Sangwoo Mo, Stella X. Yu, Sima Behpour, Liu Ren</li>
<li class=""><strong>institution:</strong> University of Michigan, UC Berkeley, Bosch Center for AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16202" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16202</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes OAK, a model for open ad-hoc categorization that introduces learnable context tokens into a frozen CLIP model and optimizes it with both image-text alignment and visual clustering objectives. It achieves state-of-the-art accuracy and concept discovery on benchmark datasets, producing interpretable saliency maps that highlight context-relevant image regions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Enhanced 3D Shape Analysis via Information Geometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [information geometry, gaussian mixture model, modified symmetric kullback-leibler divergence, statistical manifold]</li>
<li class=""><strong>authors:</strong> Amit Vishwakarma, K.S. Subrahamanian Moosath</li>
<li class=""><strong>institution:</strong> Indian Institute of Space Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16213</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an information geometric framework for 3D shape analysis by representing point clouds as Gaussian Mixture Models on a statistical manifold and proposes a Modified Symmetric Kullback-Leibler divergence with guaranteed bounds. The method provides numerically stable comparisons and is shown to outperform traditional geometric distances and existing KL approximations in experiments on human pose and animal shape datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [diffusion models], [encoder-decoder network, discretized Euler inversion, novel view synthesis, initial noise optimization]</li>
<li class=""><strong>authors:</strong> Zhihao Zhang, Xuejun Yang, Weihua Liu, Mouquan Shen</li>
<li class=""><strong>institution:</strong> Nanjing Tech University, Yongjiang Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16219</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a learning framework using an encoder-decoder network to transform random Gaussian noise into high-quality initial noise for diffusion-based novel view synthesis. The method employs a discretized Euler inversion technique to construct training data by injecting image semantics into noise. Experiments show this approach can be plugged into existing models like SV3D and MV-Adapter to significantly improve synthesis performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Image Compression Using Singular Value Decomposition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [image compression], [Singular Value Decomposition, low-rank matrix approximation, Frobenius error, compression ratio]</li>
<li class=""><strong>authors:</strong> Justin Jiang</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16226" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16226</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates using Singular Value Decomposition (SVD) and low-rank approximations for image compression, evaluating performance with relative Frobenius error and compression ratio. The method is applied to grayscale and multichannel images, but results show it consistently underperforms compared to standard formats like JPEG, JPEG2000, and WEBP, especially at low error tolerances where compressed sizes can exceed originals.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [interpretable convolutional neural networks, clinical decision trees, explainable AI, deep learning-based image analysis]</li>
<li class=""><strong>authors:</strong> Satya Narayana Panda, Vaishnavi Kukkala, Spandana Iyer</li>
<li class=""><strong>institution:</strong> University of New Haven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16235" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16235</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops a multi-modal AI framework that integrates deep learning-based image analysis with structured clinical data, including family history, using interpretable CNNs and clinical decision trees. It concludes that incorporating family history data enhances diagnostic accuracy for hereditary skin conditions and that the framework shows potential for improved early detection and personalized care within clinical workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [autoregressive generation, meanflow, bootstrap contextual encoding (bsce), causal context encoder, mlp-based velocity predictor]</li>
<li class=""><strong>authors:</strong> Zichen Geng, Zeeshan Hayder, Wei Liu, Hesheng Wang, Ajmal Mian</li>
<li class=""><strong>institution:</strong> The University of Western Australia, Commonwealth Scientific and Industrial Research Organisation (CSIRO), Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16234" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16234</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ARMFlow, an autoregressive MeanFlow framework for online 3D human reaction generation. It uses a causal context encoder and a velocity predictor, and introduces Bootstrap Contextual Encoding (BSCE) to reduce error accumulation. The method achieves state-of-the-art performance for online generation, matching offline methods while enabling real-time, single-step inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-view crowd counting, semi-supervised learning, model ranking, multi-view fusion, uncertainty estimation]</li>
<li class=""><strong>authors:</strong> Qi Zhang, Yunfei Gong, Zhidan Xie, Zhizi Wang, Antoni B. Chan, Hui Huang</li>
<li class=""><strong>institution:</strong> Shenzhen University, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16243" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16243</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two semi-supervised frameworks for multi-view crowd counting that rank fusion models based on either their predictions or uncertainties to leverage limited labeled data. The methods enforce that predictions should not decrease with more camera views and that uncertainties should not increase with more views. Experiments show these ranking-based approaches outperform other semi-supervised counting methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [conditional generative adversarial network (cGAN), pixel super-resolution (PSR), fluorescence lifetime imaging microscopy (FLIM)]</li>
<li class=""><strong>authors:</strong> Paloma Casteleiro Costa, Parnian Ghapandar Kashani, Xuhui Liu, Alexander Chen, Ary Portes, Julien Bec, Laura Marcu, Aydogan Ozcan</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles; University of California, Davis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16266</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces FLIM_PSR_k, a deep learning framework using a conditional generative adversarial network (cGAN) to perform pixel super-resolution on fluorescence lifetime images. It reconstructs high-resolution images from low-resolution inputs, achieving up to a 5x super-resolution factor and enabling faster acquisition. The method improves image quality and spatial resolution, advancing FLIM for faster, higher-resolution clinical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [TextEditBench, reasoning-aware editing, semantic expectation, text rendering, visual generation, multimodal models, diffusion models, evaluation benchmark]</li>
<li class=""><strong>authors:</strong> Rui Gui, Yang Wan, Haochen Han, Dongxing Mao, Fangming Liu, Min Li, Alex Jinpeng Wang</li>
<li class=""><strong>institution:</strong> Central South University, Pengcheng Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16270" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16270</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces TextEditBench, a comprehensive benchmark for evaluating text editing in images, focusing on reasoning-intensive scenarios that require understanding of semantics, context, and physical plausibility. It proposes a novel evaluation dimension called Semantic Expectation (SE) to measure a model&#x27;s ability to maintain consistency during editing. The main conclusion is that current state-of-the-art editing models still struggle with context-dependent reasoning and layout-aware integration, highlighting a significant gap in text-guided image editing capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] GFLAN: Generative Functional Layouts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [convolutional architecture, graph neural network, transformer, floor plan generation, topological planning, geometric realization]</li>
<li class=""><strong>authors:</strong> Mohamed Abouagour, Eleftherios Garyfallidis</li>
<li class=""><strong>institution:</strong> Indiana University Bloomington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16275" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16275</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GFLAN, a two-stage generative framework for automated floor plan generation that first allocates room centroids via a convolutional model and then regresses room boundaries using a Transformer-augmented graph neural network. The method effectively factors layout synthesis into topological planning and geometric realization to better capture architectural reasoning and functional constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing image retrieval], [contrastive learning, multi-label learning, label-aware sampling, frequency-sensitive weighting, dynamic-temperature scaling]</li>
<li class=""><strong>authors:</strong> Amna Amir, Erchan Aptoula</li>
<li class=""><strong>institution:</strong> Sabanci University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16294" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16294</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Multi-Label Adaptive Contrastive Learning (MACL), a method that extends contrastive learning for multi-label remote sensing image retrieval by integrating label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling. The approach is designed to handle challenges like semantic overlap and imbalanced label distributions. Experiments on benchmark datasets show that MACL outperforms existing contrastive-loss baselines, effectively mitigating semantic imbalance and improving retrieval reliability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] PixelArena: A benchmark for Pixel-Precision Visual Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [semantic segmentation, pixel-precision visual intelligence, zero-shot generation, face parsing, CelebAMask-HQ, COCO]</li>
<li class=""><strong>authors:</strong> Feng Liang, Sizhe Cheng, Chenqi Yi</li>
<li class=""><strong>institution:</strong> Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16303" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16303</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PixelArena, a benchmark that uses semantic segmentation tasks to evaluate the fine-grained image generation capabilities of multi-modal large language models. It finds that Gemini 3 Pro Image exhibits emergent zero-shot ability to generate accurate semantic masks, demonstrating significant progress in pixel-precision visual intelligence and generalization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [selective propagation, degradation-agnostic features, lightweight network, video restoration, temporal modeling]</li>
<li class=""><strong>authors:</strong> Haiyu Zhao, Yiwen Shan, Yuanbiao Gou, Xi Peng</li>
<li class=""><strong>institution:</strong> Sichuan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16313</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes LaverNet, a lightweight video restoration network that uses a selective propagation mechanism to transmit only degradation-agnostic features across frames. The method achieves strong all-in-one restoration performance with only 362K parameters, demonstrating that compact models can match or surpass larger existing models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sensor fusion], [ridge estimation, least squares, multicollinearity, laser ranging, vision, UAV localization]</li>
<li class=""><strong>authors:</strong> Huayu Huang, Chen Chen, Banglei Guan, Ze Tan, Yang Shang, Zhang Li, Qifeng Yu</li>
<li class=""><strong>institution:</strong> National University of Defense Technology, Hunan Provincial Key Laboratory of Image Measurement and Vision Navigation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16314" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16314</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fusion localization method for UAVs that combines vision and laser ranging data using ridge estimation to address multicollinearity issues in least squares under limited observation conditions. The method improves localization accuracy and robustness compared to single-sensor approaches, particularly in challenging scenarios like long distances and small intersection angles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Aggregated Sensing Quality (ASQ), Mutually Assisted Belief-aware Vehicle Dispatching, quality-informed incentive mechanism, mobile crowdsensing, sensing coverage, sensing reliability]</li>
<li class=""><strong>authors:</strong> Nan Zhou, Zuxin Li, Fanhang Man, Xuecheng Chen, Susu Xu, Fan Dang, Chaopeng Hong, Yunhao Liu, Xiao-Ping Zhang, Xinlei Chen</li>
<li class=""><strong>institution:</strong> Tsinghua University, Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16325" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16325</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes QUIDS, a multi-agent dispatching system that uses a novel Aggregated Sensing Quality metric and a belief-aware dispatching algorithm to allocate incentives and optimize sensing coverage and reliability in vehicular crowdsensing. Evaluation with real-world data shows QUIDS significantly improves sensing quality and reduces reconstruction errors compared to baseline methods, enabling low-cost, high-quality urban monitoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Collaborative Edge-to-Server Inference for Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [collaborative inference, edge-to-server, region of interest, attention-aware cropping, min-entropy, selective retransmission]</li>
<li class=""><strong>authors:</strong> Soochang Song, Yongjune Kim</li>
<li class=""><strong>institution:</strong> Pohang University of Science and Technology (POSTECH)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16349" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16349</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-stage collaborative inference framework for vision-language models where a server first processes a low-resolution global image and, if the inference uncertainty (measured by min-entropy) is high, requests a high-resolution local image of a task-relevant region from the edge device. This selective retransmission mechanism reduces communication costs while maintaining inference accuracy by leveraging both global and local visual information. Experiments demonstrate significant communication savings across multiple VLM architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [latent diffusion models, gain map, one-step denoising, HDR reconstruction, regression priors]</li>
<li class=""><strong>authors:</strong> Tao Hu, Weiyu Zhou, Yanjie Tu, Peng Wu, Wei Dong, Qingsen Yan, Yanning Zhang</li>
<li class=""><strong>institution:</strong> Northwestern Polytechnical University, Xi’an University of Architecture and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16357" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16357</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GMODiff reformulates HDR reconstruction as a gain map estimation task using a one-step diffusion process initialized from a regression-based estimate, combining perceptual quality from diffusion models with content fidelity from regression. It achieves high-quality results while being 100× faster than previous LDM-based methods and reducing hallucinations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [Identity Matching Graph, Mask-Query Attention, identity-embedded guidance, multi-scale matching, pre-classified sampling]</li>
<li class=""><strong>authors:</strong> Haotian Ling, Zequn Chen, Qiuying Chen, Donglin Di, Yongjia Ma, Hao Li, Chen Wei, Zhulin Tao, Xun Yang</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Li Auto, Communication University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16360" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16360</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces EverybodyDance, a method for multi-character animation that uses a bipartite Identity Matching Graph and Mask-Query Attention to enforce correct identity correspondence between characters. It proposes several targeted strategies, including identity-embedded guidance, to optimize this correspondence during training. Experiments show it outperforms state-of-the-art baselines in both identity correspondence and visual fidelity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [factorized video generation, text-to-video diffusion, large language model, text-to-image model, visual anchoring, temporal synthesis, prompt rewriting]</li>
<li class=""><strong>authors:</strong> Mariam Hassan, Bastien Van Delft, Wuyang Li, Alexandre Alahi</li>
<li class=""><strong>institution:</strong> École Polytechnique Fédérale de Lausanne (EPFL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16371" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16371</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Factorized Video Generation (FVG), a pipeline that decomposes text-to-video generation into three stages: using an LLM to rewrite the prompt for the initial scene, a T2I model to create a compositionally correct anchor frame, and a finetuned video model to animate it. This decoupling improves scene construction and temporal consistency, setting a new state-of-the-art on benchmarks and enabling faster sampling with fewer steps.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Adaptive Frequency Domain Alignment Network for Medical image segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical image segmentation], [domain adaptation, frequency alignment, adversarial learning, spatial-frequency integration]</li>
<li class=""><strong>authors:</strong> Zhanwei Li, Liang Li, Jiawan Zhang</li>
<li class=""><strong>institution:</strong> Tianjin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16393" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16393</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Adaptive Frequency Domain Alignment Network (AFDAN), a novel domain adaptation framework that aligns features in the frequency domain to address data scarcity in medical image segmentation. It integrates adversarial domain learning, source-target frequency fusion, and spatial-frequency integration for robust cross-domain knowledge transfer. Experiments show AFDAN achieves state-of-the-art performance on vitiligo and retinal vessel segmentation benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, computer graphics], [Gaussian Splatting, neural radiance fields, triangulated surface, view-dependent neural texture, relightable Gaussian model, albedo texture, segmentation annotations]</li>
<li class=""><strong>authors:</strong> Haodi He, Jihun Yu, Ronald Fedkiw</li>
<li class=""><strong>institution:</strong> Epic Games, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16397" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16397</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for creating high-fidelity facial avatars by using Gaussian Splatting, constrained to an underlying triangulated surface, to reconstruct geometry and texture from a small set of uncalibrated images. The approach enables the generation of a standard mesh and a delit, high-resolution albedo texture for use in traditional graphics pipelines. The main conclusion is that this method facilitates scalable and democratized avatar creation, demonstrating utility in applications like text-driven asset generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CountZES: Counting via Zero-Shot Exemplar Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [zero-shot object counting, open-vocabulary detection, exemplar selection, density-guided, feature clustering, training-free]</li>
<li class=""><strong>authors:</strong> Muhammad Ibraheem Siddiqui, Muhammad Haris Khan</li>
<li class=""><strong>institution:</strong> Mohamed Bin Zayed University of Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16415" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16415</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CountZES, a training-free framework for zero-shot object counting that progressively selects diverse exemplars through three stages: detection-anchored, density-guided, and feature-consensus selection. This method refines open-vocabulary detections and uses self-supervised density and clustering to improve exemplar quality. Experiments show CountZES outperforms existing zero-shot counting methods and generalizes across natural, aerial, and medical domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] BrepLLM: Native Boundary Representation Understanding with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [adaptive UV sampling, hierarchical BrepEncoder, contrastive learning, Mixture-of-Query Experts (MQE), cross-modal alignment, multi-stage fine-tuning]</li>
<li class=""><strong>authors:</strong> Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang, Yilei Shi</li>
<li class=""><strong>institution:</strong> Northwestern Polytechnical University, Shanghai Artificial Intelligence Laboratory, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16413" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16413</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BrepLLM is a framework that enables Large Language Models to directly understand 3D Boundary Representation (Brep) data through a two-stage pipeline involving cross-modal alignment pre-training and multi-stage LLM fine-tuning. It uses a hierarchical encoder and a Mixture-of-Query Experts to process geometric and topological information. The method achieves state-of-the-art results on 3D object classification and captioning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [text-to-image generation], [geometric disentanglement, text embeddings, semantic leakage, subject consistency, training-free, diffusion models]</li>
<li class=""><strong>authors:</strong> Shangxun Li, Youngjung Uh</li>
<li class=""><strong>institution:</strong> Yonsei University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16443" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16443</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a training-free method that refines text embeddings from a geometric perspective to suppress unwanted semantics and prevent semantic leakage. This approach improves subject consistency and text alignment in text-to-image generation using a single prompt. Extensive experiments show it outperforms existing baselines like 1Prompt1Story.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [human motion generation], [diffusion-based motion generation, gaze priming, fine-tuning, reach success, prime success]</li>
<li class=""><strong>authors:</strong> Masashi Hatano, Saptarshi Sinha, Jacob Chalk, Wei-Hong Li, Hideo Saito, Dima Damen</li>
<li class=""><strong>institution:</strong> Keio University, University of Bristol</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16456</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a method for generating realistic human motion sequences that involve gaze priming and reaching for objects. It uses a text-conditioned diffusion model, fine-tuned on a curated dataset of gaze-primed motions, and evaluates success with novel metrics. The model achieves 60% prime success and 89% reach success on the HD-EPIC dataset when conditioned on the goal location.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [4D scene graph, spatio-temporal tokenized patch encoding, HDBSCAN clustering, SAM2 segmentation, SLAM]</li>
<li class=""><strong>authors:</strong> Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16461" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16461</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SNOW, a training-free framework that integrates Vision-Language Model semantics with 3D point cloud geometry and temporal consistency to build a unified 4D Scene Graph for open-world embodied reasoning. It achieves this through object-level proposals, Spatio-Temporal Tokenized Patch Encoding, and a SLAM backend for spatial grounding. Experiments show that SNOW sets new state-of-the-art performance, demonstrating the importance of structured 4D priors for autonomous robotics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [reinforcement learning, perception-reasoning cascade, self-consistent reasoning, ROUGE-1, human annotations]</li>
<li class=""><strong>authors:</strong> Yuan Li, Yahan Yu, Youyuan Lin, Yong-Hao Yang, Chenhui Chu, Shin&#x27;ya Nishida</li>
<li class=""><strong>institution:</strong> Kyoto University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16484" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16484</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to make blind image quality assessment (BIQA) models more human-like by using reinforcement learning guided by human annotations to align the model&#x27;s perception-reasoning process. The approach introduces a reward to encourage self-consistent reasoning from self-generated descriptions. The results show the model achieves competitive score prediction and significantly improves alignment with human reasoning chains, as measured by ROUGE-1.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [eye-behavior-aided multimodal emotion recognition, modality-adversarial feature decoupling, multitask Transformer, eye movement sequences, eye fixation maps]</li>
<li class=""><strong>authors:</strong> Kejun Liu, Yuanyuan Liu, Lin Wei, Chang Tang, Yibing Zhan, Zijing Chen, Zhe Chen</li>
<li class=""><strong>institution:</strong> China University of Geosciences (Wuhan), Huazhong University of Science and Technology, Wuhan University, La Trobe University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16485</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a multimodal emotion recognition dataset (EMER) incorporating eye behaviors and facial expressions, and proposes a Transformer-based model (EMERT) that uses modality-adversarial feature decoupling and multitask learning to bridge the gap between facial expression recognition and genuine emotion recognition. The results demonstrate that modeling eye behaviors significantly enhances emotion recognition performance, outperforming other state-of-the-art multimodal methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [visual autoregressive models, stage-aware acceleration, semantic irrelevance, low-rank approximation, plug-and-play acceleration, next-scale prediction]</li>
<li class=""><strong>authors:</strong> Senmao Li, Kai Wang, Salman Khan, Fahad Shahbaz Khan, Jian Yang, Yaxing Wang</li>
<li class=""><strong>institution:</strong> Nankai University, City University of Hong Kong (Dongguan), Mohamed bin Zayed University of Artificial Intelligence, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16483</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StageVAR is a stage-aware acceleration framework for Visual Autoregressive (VAR) models that analyzes the generation process, identifying that early steps are critical for semantics and structure while later steps can be pruned or approximated. It introduces a plug-and-play strategy exploiting semantic irrelevance and low-rank properties in late-stage computations without extra training. The method achieves up to 3.4x speedup with minimal quality loss, outperforming existing baselines and demonstrating the effectiveness of stage-aware design for efficient image generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [YOLO11-4K, multi-scale detection head, P2 layer, GhostConv, small object detection, 4K panoramic images]</li>
<li class=""><strong>authors:</strong> Huma Hafeez, Matthew Garratt, Jo Plested, Sankaran Iyer, Arcot Sowmya</li>
<li class=""><strong>institution:</strong> University of New South Wales</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16493</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces YOLO11-4K, an efficient object detection framework for 4K panoramic images, featuring a novel multi-scale detection head with a P2 layer for small objects and a GhostConv-based backbone to reduce complexity. It achieves a 75% latency reduction and higher accuracy compared to YOLO11, enabling robust real-time detection in high-resolution 360-degree environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [GUI grounding, hierarchical task taxonomy, multimodal large language models (MLLMs), cross-platform benchmark, data construction pipeline]</li>
<li class=""><strong>authors:</strong> Beitong Zhou, Zhexiao Huang, Yuan Guo, Zhangxuan Gu, Tianyu Xia, Zichen Luo, Fei Tang, Dehan Kong, Yanyi Shang, Suling Ou, Zhenlin Guo, Changhua Meng, Shuheng Shen</li>
<li class=""><strong>institution:</strong> AntGroup, iMean AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16501" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16501</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding across multiple platforms, featuring a hierarchical task taxonomy and a high-quality data construction pipeline. The experiments show that general-purpose multimodal models now match specialized GUI models on basic tasks, but advanced tasks still favor GUI-specialized models, highlighting the need for multi-tiered evaluation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [mixture-of-experts, monocular 3d human pose estimation, lifting-based methods, cross-expert knowledge aggregation, spatio-temporal contextual information]</li>
<li class=""><strong>authors:</strong> Mengyuan Liu, Jiajie Liu, Jinyan Zhang, Wenhao Li, Junsong Yuan</li>
<li class=""><strong>institution:</strong> Peking University, Nanyang Technological University, University at Buffalo, State University of New York</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16494" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16494</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PoseMoE, a Mixture-of-Experts network that disentangles the feature encoding for 2D pose and depth to improve monocular 3D human pose estimation. It introduces specialized expert modules and a cross-expert knowledge aggregation module to refine features and reduce the negative influence of uncertain depth. The method outperforms conventional lifting-based approaches on standard benchmarks like Human3.6M, MPI-INF-3DHP, and 3DPW.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [contrastive learning, self-supervised pretraining, snippet discrimination, multiscale feature fusion, U-shaped module]</li>
<li class=""><strong>authors:</strong> Qiushuo Cheng, Jingjing Liu, Catherine Morgan, Alan Whone, Majid Mirmehdi</li>
<li class=""><strong>institution:</strong> University of Bristol, North Bristol NHS Trust</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16504" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16504</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a self-supervised pretraining method for skeleton-based temporal action localization, using a snippet discrimination pretext task with contrastive learning and a U-shaped multiscale feature fusion module. The approach improves existing methods on the BABEL dataset and achieves state-of-the-art transfer learning performance on PKUMMD.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multi-scale attention, hierarchical residual encoding, spatial-and-channel attention, adaptive multi-scale feature fusion, Pix2PixHD, masked-MSE, VGG loss, edge loss, patch-LPIPS loss]</li>
<li class=""><strong>authors:</strong> Hossein Javidnia</li>
<li class=""><strong>institution:</strong> Trinity College Dublin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16511" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16511</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MAGINet, a multi-scale attention-guided network that predicts a light-normalized diffuse albedo map from a single RGB face image and then uses it to generate a full set of physically based rendering passes. The method achieves state-of-the-art performance for intrinsic decomposition, enabling high-quality face relighting and material editing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [test-time padding, adversarial detection, cosine similarity shift, trainable padding, similarity-aware ensemble, vision-language models, CLIP]</li>
<li class=""><strong>authors:</strong> Zhiwei Li, Yitian Pang, Weining Wang, Zhenan Sun, Qi Li</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16523" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16523</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Test-Time Padding (TTP), a lightweight defense framework for Vision-Language Models that detects adversarial inputs by analyzing the shift in feature embeddings after spatial padding and then adapts them using trainable padding and an ensemble strategy. The method consistently outperforms existing test-time defenses, significantly improving adversarial robustness without harming clean accuracy across various CLIP backbones and benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] 4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, 3D reconstruction], [4D reconstruction, rigid primitives, dense 2D correspondences, motion grouping, object permanence]</li>
<li class=""><strong>authors:</strong> Kirill Mazur, Marwan Taher, Andrew J. Davison</li>
<li class=""><strong>institution:</strong> Imperial College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16564" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16564</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for persistent 4D scene reconstruction from monocular video by decomposing the scene into rigid 3D primitives and jointly inferring their motion through optimization. The system maintains reconstructions of both visible and previously viewed parts of the scene, enabling complete replay. It outperforms existing methods on object scanning and multi-object datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [native 3D grounding, 3D bounding box prediction, spatial reasoning, depth estimation, chain-of-thought reasoning, data construction pipeline]</li>
<li class=""><strong>authors:</strong> Yuxin Wang, Lei Ke, Boqiang Zhang, Tianyuan Qu, Hanxun Yu, Zhenpeng Huang, Meng Yu, Dan Xu, Dong Yu</li>
<li class=""><strong>institution:</strong> HKUST, Tencent AI Lab, CUHK, ZJU, NJU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16561" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16561</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces N3D-VLM, a unified framework that integrates native 3D object perception for precise 3D grounding and explicit spatial reasoning in vision-language models. It uses a scalable data pipeline to lift 2D annotations into 3D and trains the model to localize objects and perform chain-of-thought reasoning in 3D space. The method achieves state-of-the-art performance on both 3D grounding and 3D spatial reasoning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [causal mechanisms, frequency domain analysis, discrete cosine transform, gaussian band-pass filter, domain generalization, semantic segmentation]</li>
<li class=""><strong>authors:</strong> Yin Zhang, Yongqiang Zhang, Yaoyue Zheng, Bogdan Raducanu, Dan Liu</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Universitat Autònoma de Barcelona, Inner Mongolia University, Xi’an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16567" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16567</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Causal-Tune, a fine-tuning strategy for Vision Foundation Models that uses Discrete Cosine Transform and a Gaussian band-pass filter to separate and suppress non-causal, artifact-related frequency components in features for improved domain generalization. The method demonstrates superior performance in cross-domain semantic segmentation, particularly under adverse weather conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [continuous time reconstruction, spatio-temporal velocity field, sequence-to-image forecasting, 3D voxel space]</li>
<li class=""><strong>authors:</strong> Nico Albert Disch, Saikat Roy, Constantin Ulrich, Yannick Kirchhoff, Maximilian Rokuss, Robin Peretzke, David Zimmerer, Klaus Maier-Hein</li>
<li class=""><strong>institution:</strong> German Cancer Research Center, University of Heidelberg, Heidelberg University Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16577</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CRONOS is a framework for forecasting 3D medical scan evolution over time by learning a spatio-temporal velocity field that transports past scans to a target volume at an arbitrary continuous timestamp. It outperforms other baselines on datasets including Cine-MRI, perfusion CT, and longitudinal MRI while remaining computationally competitive.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion model, Swin-transformer, text-conditioned generation, U-shaped architecture, adapted time step]</li>
<li class=""><strong>authors:</strong> Shaohua Wu, Tong Yu, Shenling Wang, Xudong Zhao</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16586" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16586</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Yuan-TecSwin, a text-to-image diffusion model that replaces CNN blocks with Swin-transformer blocks in its U-shaped architecture to better capture long-range semantic dependencies. It improves text-image alignment and uses an adapted time step strategy, achieving a state-of-the-art FID score of 1.37 on ImageNet, with generated images being highly photorealistic and difficult for humans to distinguish from real paintings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [Sketch-in-Latents, latent sketch tokens, visual-text imagination, unified reasoning, auto-regressive generation, latent visual semantics reconstruction]</li>
<li class=""><strong>authors:</strong> Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, Ruixuan Li</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Alibaba Cloud Computing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16584</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Sketch-in-Latents (SkiLa), a method that enables Multimodal Large Language Models to perform unified reasoning by generating both textual and continuous visual embeddings (latent sketch tokens) in an auto-regressive process. This approach allows dynamic alternation between textual thinking and visual sketching modes, enhancing visual imagination without predefined toolkits. Experiments show SkiLa achieves superior performance on vision-centric tasks and generalizes well across diverse multi-modal benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [dark channel prior, atmospheric scattering model, gamma-adaptive reconstruction, transmission approximation, fractional top-pixel averaging]</li>
<li class=""><strong>authors:</strong> Ayush Bhavsar</li>
<li class=""><strong>institution:</strong> National Institute of Technology, Raipur</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16609" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16609</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Hazedefy, a lightweight dehazing pipeline based on the Dark Channel Prior and atmospheric scattering model, optimized for real-time video on consumer hardware. It uses fast approximations like gamma-adaptive reconstruction and a stabilized atmospheric light estimator to improve speed and stability. The method achieves enhanced visibility and contrast without GPU acceleration, making it suitable for mobile and embedded applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Don&#x27;t Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [AI forensic agents, uncertainty-aware assessments, detector orchestration, multimedia forensics, authenticity verification]</li>
<li class=""><strong>authors:</strong> Giulia Boato, Andrea Montibeller, Edward Delp, Luisa Verdoliva, Daniele Miorandi</li>
<li class=""><strong>institution:</strong> Truebees, University of Trento, Purdue University, University of Naples Federico II</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16614</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework for AI forensic agents that autonomously orchestrate multiple forensic detectors to verify the authenticity of multimedia content. It argues that a holistic, uncertainty-calibrated approach is necessary to address the challenges posed by generative AI, moving beyond isolated, single-purpose detectors. The main conclusion is that such explainable, uncertainty-aware agents can improve the trustworthiness and interpretability of the forensic verification process.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [log-linear sparse attention, hierarchical top-k selection, hierarchical kv enrichment, gpu implementation]</li>
<li class=""><strong>authors:</strong> Yifan Zhou, Zeqi Xiao, Tianyi Wei, Shuai Yang, Xingang Pan</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16615</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Log-linear Sparse Attention (LLSA), a trainable hierarchical sparse attention mechanism that reduces both selection and attention costs from quadratic to log-linear complexity for Diffusion Transformers. It uses hierarchical Top-K selection and KV enrichment to preserve global context efficiently. LLSA accelerates DiT training and inference significantly while maintaining generation quality, offering a scalable solution for long-sequence visual generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DeContext as Defense: Safe Image Editing in Diffusion Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [attention-based perturbations, cross-attention pathways, early denoising steps, transformer blocks, DeContext]</li>
<li class=""><strong>authors:</strong> Linghui Shen, Mingyue Cui, Xingyi Yang</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16625" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16625</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes DeContext, a defense method that injects small, targeted perturbations into input images to weaken cross-attention pathways in diffusion transformer models, thereby blocking unauthorized in-context editing. The method is shown to effectively prevent identity-preserving malicious edits while preserving visual quality, demonstrating the robustness of attention-based perturbations as a defense.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [YOLOv11, Xception, data augmentation, Hotels-50K, TraffickCam]</li>
<li class=""><strong>authors:</strong> Kanwal Aftab, Graham Adams, Mark Scanlon</li>
<li class=""><strong>institution:</strong> University College Dublin, Case Western Reserve University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16620" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16620</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a three-stage deep learning pipeline for indoor multimedia geolocation by detecting and classifying electrical sockets in images to infer the country. The method uses YOLOv11 for detection and Xception for classification, achieving high accuracy on real-world datasets. It demonstrates a practical approach for digital forensic investigations, such as combating human trafficking, by leveraging standardized plug types as reliable indoor markers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SARMAE: Masked Autoencoder for SAR Representation Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [remote sensing], [masked autoencoder, self-supervised learning, speckle noise, representation learning, SAR-1M dataset, Speckle-Aware Representation Enhancement (SARE), Semantic Anchor Representation Constraint (SARC)]</li>
<li class=""><strong>authors:</strong> Danxu Liu, Di Wang, Hebaixu Wang, Haoyang Chen, Wentao Jiang, Yilin Cheng, Haonan Guo, Wei Cui, Jing Zhang</li>
<li class=""><strong>institution:</strong> Beijing Institute of Technology, Wuhan University, Fudan University, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16635" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16635</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SARMAE, a noise-aware masked autoencoder for self-supervised representation learning of Synthetic Aperture Radar (SAR) imagery. It introduces a large-scale dataset (SAR-1M) and two key components: SARE for speckle noise injection and SARC for semantic alignment using optical image priors. The method achieves state-of-the-art performance on various SAR image interpretation tasks like classification, detection, and segmentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [latent diffusion models, vision foundation models, semantic compressor, representation alignment, SiT backbone, global-local-latent joint modeling]</li>
<li class=""><strong>authors:</strong> Giorgos Petsangourakis, Christos Sgouropoulos, Bill Psomas, Theodoros Giannakopoulos, Giorgos Sfikas, Ioannis Kakogeorgiou</li>
<li class=""><strong>institution:</strong> IIT, National Centre for Scientific Research “Demokritos”, University of West Attica, Czech Technical University in Prague</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16636" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16636</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces REGLUE, a latent diffusion framework that jointly models VAE latents, compressed local patch semantics, and a global image token within a single SiT backbone to enhance semantic supervision. It uses a nonlinear semantic compressor to aggregate multi-layer VFM features and an external alignment loss for regularization. The method improves FID and accelerates convergence on ImageNet, demonstrating the importance of spatial semantics and nonlinear compression in diffusion models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [autoregressive neural rendering, ControlNet, ControlLoRA, G-buffer conditioning, temporal consistency, environment-specific training]</li>
<li class=""><strong>authors:</strong> Ole Beisswenger, Jan-Niklas Dihlmann, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16670" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16670</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FrameDiffuser is an autoregressive diffusion framework that generates photorealistic and temporally consistent video frames for interactive applications by conditioning on incoming G-buffer data and its own previous output. It uses a dual-conditioning architecture with ControlNet for structural guidance and ControlLoRA for temporal coherence. The paper concludes that environment-specific training of this model yields superior visual quality and consistency compared to generalized approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical imaging], [subject fingerprinting, triplet margin loss, ResNet-50, few-shot learning, latent space embedding]</li>
<li class=""><strong>authors:</strong> Gonçalo Gaspar Alves, Shekoufeh Gorgi Zadeh, Andreas Husch, Ben Bausch</li>
<li class=""><strong>institution:</strong> University of Luxembourg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16685</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a subject fingerprinting method for re-identifying individuals across medical datasets to prevent data leakage. Using a ResNet-50 model trained with triplet margin loss to map images to a latent space, the approach achieves high few-shot re-identification accuracy on both 3D MRI and 2D X-ray datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [deepfake detection, image inpainting, synthetic image detectors, generative AI, cybersecurity, localized manipulations]</li>
<li class=""><strong>authors:</strong> Serafino Pandolfini, Lorenzo Pellegrini, Matteo Ferrara, Davide Maltoni</li>
<li class=""><strong>institution:</strong> University of Bologna</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16688" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16688</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates state-of-the-art synthetic image detectors, originally trained to identify fully AI-generated images, on their ability to detect localized image manipulations like inpainting. The study uses multiple datasets with diverse generators and inpainting techniques. The main conclusion is that these detectors show partial transferability and can reliably detect medium-to-large area manipulations or regeneration-style inpainting, often outperforming existing ad hoc detection methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [landmark annotation, domain adaptation, multi-centre generalisation, benchmark dataset, deep learning, fetal biometry, ultrasound]</li>
<li class=""><strong>authors:</strong> Chiara Di Vece, Zhehua Mao, Netanell Avisdris, Brian Dromey, Raffaele Napolitano, Dafna Ben Bashat, Francisco Vasconcelos, Danail Stoyanov, Leo Joskowicz, Sophia Bano</li>
<li class=""><strong>institution:</strong> University College London, Tel Aviv Sourasky Medical Center, The Hebrew University of Jerusalem, UCLH NHS Foundation Trust, Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16710" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16710</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a multi-centre, multi-device benchmark dataset of fetal ultrasound images with expert anatomical landmark annotations to develop AI-assisted fetal growth assessment methods. Using an automatic biometry model, it demonstrates that training and evaluation confined to a single centre overestimates performance compared to multi-centre testing, highlighting the importance of domain adaptation for reliable generalization across clinical sites.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SDFoam: Signed-Distance Foam for explicit surface reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [signed distance field, Voronoi diagram, ray tracing, Eikonal regularization, neural radiance fields, 3D Gaussian Splatting, mesh reconstruction]</li>
<li class=""><strong>authors:</strong> Antonella Rech, Nicola Conci, Nicola Garau</li>
<li class=""><strong>institution:</strong> University of Trento, CNIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SDFoam, a hybrid method that jointly learns an implicit Signed Distance Field (SDF) and an explicit Voronoi Diagram to improve explicit surface reconstruction. It optimizes the scene via ray tracing with Eikonal regularization, aligning Voronoi cell faces with the SDF&#x27;s zero level set. This approach achieves more accurate mesh reconstruction with comparable rendering speed and visual quality to prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [virtual task-adaptive view, depth-aware module, dynamic coarse-to-fine procedure, 3D point cloud, foundation models]</li>
<li class=""><strong>authors:</strong> Yixiang Chen, Yan Huang, Keji He, Peiyan Li, Liang Wang</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; FiveAges; Shandong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16724" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16724</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VERM, a method that uses foundation models to generate a virtual, task-adaptive camera view from a 3D point cloud to reduce visual redundancy in robotic manipulation. This approach, combined with a depth-aware module and a coarse-to-fine procedure, improves efficiency and accuracy. Experiments show it outperforms previous methods while significantly speeding up both training and inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [transformer, memory banks, self-supervised learning, multi-view hand pose estimation, online gesture recognition]</li>
<li class=""><strong>authors:</strong> Haochen Chang, Pengfei Ren, Buyuan Zhang, Da Li, Tianhao Han, Haoyang Zhang, Liang Xie, Hongbo Chen, Erwei Yin</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, Beijing University of Posts and Telecommunications, Shanghai Jiao Tong University, Nankai University, Academy of Military Sciences, Tianjin Artificial Intelligence Innovation Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces OMG-Bench, a large-scale benchmark for skeleton-based online micro hand gesture recognition, and proposes HMATr, a hierarchical memory-augmented transformer framework that unifies gesture detection and classification. HMATr leverages memory banks to preserve historical context and uses learnable queries to encode gesture positions, outperforming state-of-the-art methods by 7.6% in detection rate.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Multimodal Diffusion Transformer (MM-DiT), unified triple attention, control-rectify flow matching (CRFM), task-oriented data synthesis, controllable generation]</li>
<li class=""><strong>authors:</strong> Yunkai Yang, Yudong Zhang, Kunquan Zhang, Jinxiao Zhang, Xinying Chen, Haohuan Fu, Runmin Dong</li>
<li class=""><strong>institution:</strong> Sun Yat-Sen University, Tsinghua University, Beijing Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16740" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16740</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a task-oriented data synthesis framework (TODSynth) for remote sensing semantic segmentation, featuring a Multimodal Diffusion Transformer with unified triple attention and a control-rectify flow matching sampling strategy. The method enhances synthetic data quality by jointly attending to text, image, and mask inputs and dynamically adjusting sampling based on semantic loss. Experiments show it outperforms existing controllable generation methods, producing more stable and task-relevant synthetic data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] TreeNet: A Light Weight Model for Low Bitrate Image Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [binary tree-structured encoder-decoder, attentional feature fusion, low bitrate compression, rate-distortion optimization, quantization, entropy coding]</li>
<li class=""><strong>authors:</strong> Mahadev Prasad Panda, Purnachandra Rao Makkena, Srivatsa Prativadibhayankaram, Siegfried Fößel, André Kaup</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute for Integrated Circuits IIS, Friedrich-Alexander-Universität Erlangen-Nürnberg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16743</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes TreeNet, a lightweight image compression model using a binary tree-structured encoder-decoder with attentional feature fusion to reduce complexity. It achieves a 4.83% BD-rate improvement over JPEG AI at low bitrates while reducing model complexity by 87.82%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FlowDet: Unifying Object Detection and Generative Transport Flows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [object detection], [conditional flow matching, generative transport, object detection, diffusiondet, flowdet]</li>
<li class=""><strong>authors:</strong> Enis Baty, C. P. Bridges, Simon Hadfield</li>
<li class=""><strong>institution:</strong> University of Surrey</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16771" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16771</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlowDet introduces a novel object detection method using Conditional Flow Matching to generalize generative transport for bounding box prediction, outperforming diffusion-based approaches. It learns straighter transport paths, enabling faster performance scaling with fewer inference steps and achieving gains over DiffusionDet on COCO and LVIS datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer graphics, 3D animation], [latent-space transformation, feed-forward framework, latent posing transformer, dense pose representation, adaptive completion module]</li>
<li class=""><strong>authors:</strong> Zhiyang Guo, Ori Zhang, Jax Xiang, Alan Zhao, Wengang Zhou, Houqiang Li</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16767</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Make-It-Poseable, a feed-forward framework that poses 3D humanoid characters by transforming their latent representations based on skeletal motion, rather than deforming mesh vertices. It uses a latent posing transformer and dense pose representation to achieve high-fidelity animation and handle topological changes. The method demonstrates superior posing quality and extends to 3D editing applications like part replacement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Kling-Omni Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [unified multimodal representation, end-to-end framework, large-scale pre-training, in-context generation, reasoning-based editing, multimodal instruction following]</li>
<li class=""><strong>authors:</strong> Kling Team, Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, Xiao Hu, Xiaohua Hu, Boyuan Jiang, Fangyuan Kong, Hang Li, Jie Li, Qingyu Li, Shen Li, Xiaohan Li, Yan Li, Jiajun Liang, Borui Liao, Yiqiao Liao, Weihong Lin, Quande Liu, Xiaokun Liu, Yilun Liu, Yuliang Liu, Shun Lu, Hangyu Mao, Yunyao Mao, Haodong Ouyang, Wenyu Qin, Wanqi Shi, Xiaoyu Shi, Lianghao Su, Haozhi Sun, Peiqin Sun, Pengfei Wan, Chao Wang, Chenyu Wang, Meng Wang, Qiulin Wang, Runqi Wang, Xintao Wang, Xuebo Wang, Zekun Wang, Min Wei, Tiancheng Wen, Guohao Wu, Xiaoshi Wu, Zhenhua Wu, Da Xie, Yingtong Xiong, Yulong Xu, Sile Yang, Zikang Yang, Weicai Ye, Ziyang Yuan, Shenglong Zhang, Shuaiyu Zhang, Yuanxing Zhang, Yufan Zhang, Wenzheng Zhao, Ruiliang Zhou, Yan Zhou, Guosheng Zhu, Yongjie Zhu</li>
<li class=""><strong>institution:</strong> Kuaishou Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16776" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16776</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kling-Omni is a generalist generative framework that synthesizes high-fidelity videos from multimodal inputs like text, images, and video contexts using an end-to-end, unified architecture. It demonstrates strong capabilities in in-context generation and reasoning-based editing, representing a significant step toward multimodal world simulators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [motion tracking], [state space model, kinematics-guided bidirectional scanning, mixed spatiotemporal representation learning, geometric angular velocity loss]</li>
<li class=""><strong>authors:</strong> Shuting Zhao, Zeyu Xiao, Xinrong Chen</li>
<li class=""><strong>institution:</strong> Fudan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16791" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16791</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes KineST, a kinematics-guided state space model for full-body motion tracking from sparse AR/VR signals. Its core innovations include a kinematics-guided bidirectional scanning strategy and mixed spatiotemporal representation learning to balance accuracy and smoothness. Experiments show the method achieves superior accuracy and temporal consistency within a lightweight framework.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] R3ST: A Synthetic 3D Dataset With Realistic Trajectories</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [synthetic dataset, trajectory forecasting, computer vision, 3D environment, real-world trajectories, SinD dataset]</li>
<li class=""><strong>authors:</strong> Simone Teglia, Claudia Melis Tonti, Francesco Pro, Leonardo Russo, Andrea Alfarano, Leonardo Pentassuglia, Irene Amerini</li>
<li class=""><strong>institution:</strong> Sapienza University of Rome, INSAIT (Sofia University), EURECOM</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16784</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces R3ST, a synthetic 3D dataset that integrates real-world vehicle trajectories from the SinD dataset into a generated synthetic environment. This approach aims to combine the precise annotation benefits of synthetic data with the realism of authentic human-driven motion. The main conclusion is that R3ST bridges the gap between synthetic data and realistic trajectories, advancing research in trajectory forecasting for road vehicles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [predictive kinematics, 3D Gaussian geometry, trajectory-level prediction, depth-based rendering, continuous-action policy]</li>
<li class=""><strong>authors:</strong> Jingjing Qian, Boyao Han, Chen Shi, Lei Xiao, Long Yang, Shaoshuai Shi, Li Jiang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen, Hunan University, LiAuto Inc., Voyager Research, Didi Chuxing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16811" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16811</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GeoPredict, a geometry-aware Vision-Language-Action framework that enhances robotic manipulation by incorporating predictive kinematic priors (for 3D keypoint trajectories) and predictive 3D Gaussian geometry modules. These modules provide training-time supervision via depth-based rendering, while inference remains lightweight. Experiments show GeoPredict outperforms existing VLA baselines in tasks requiring precise 3D reasoning and spatial awareness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DenseBEV: Transforming BEV Grid Cells into 3D Objects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [BEV, transformer, 3D object detection, multi-camera, anchor generation, Non-Maximum Suppression, temporal modeling]</li>
<li class=""><strong>authors:</strong> Marius Dähling, Sebastian Krebs, J. Marius Zöllner</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Mercedes-Benz AG, TU Delft, Research Center for Information Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16818" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16818</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DenseBEV, a method for multi-camera 3D object detection that uses BEV feature grid cells directly as object anchors instead of random queries, combined with a BEV-based Non-Maximum Suppression for efficient training. It shows significant performance improvements on nuScenes and Waymo datasets, particularly for small objects, achieving state-of-the-art results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Next-Generation License Plate Detection and Recognition System using YOLOv8</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [YOLOv8, license plate detection, character recognition, object detection, custom character sequencing, edge devices]</li>
<li class=""><strong>authors:</strong> Arslan Amin, Rafia Mumtaz, Muhammad Jawad Bashir, Syed Mohammad Hassan Zaidi</li>
<li class=""><strong>institution:</strong> National University of Sciences and Technology (NUST), Ghulam Ishaq Khan Institute of Engineering Sciences and Technology (GIKI)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16826" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16826</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a license plate detection and recognition system using YOLOv8 variants, with YOLOv8 Nano for plate detection and YOLOv8 Small for character recognition, along with a custom method for sequencing characters. The optimized pipeline achieves high precision and mAP50 scores, demonstrating computational efficiency suitable for deployment on edge devices in Intelligent Transportation Systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Radiology Report Generation with Layer-Wise Anatomical Attention</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [DINOv3, GPT-2, layer-wise anatomical attention, hierarchical Gaussian smoothing, segmentation masks, vision transformer]</li>
<li class=""><strong>authors:</strong> Emmanuel D. Muñiz-De-León, Jorge A. Rosales-de-Golferichs, Ana S. Muñoz-Rodríguez, Alejandro I. Trejo-Castro, Eduardo de Avila-Armenta, Antonio Martínez-Torteya</li>
<li class=""><strong>institution:</strong> Universidad de Monterrey, Tecnológico de Monterrey</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16841" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16841</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a compact image-to-text model for generating chest X-ray reports from a single frontal image. It uses a frozen DINOv3 Vision Transformer encoder and a GPT-2 decoder enhanced with a novel layer-wise anatomical attention mechanism that integrates lung and heart segmentation masks. The model demonstrates that this decoder-level anatomical guidance significantly improves clinical accuracy and structural coherence in generated reports, outperforming prior methods on standard metrics while being more resource-efficient.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [tactile sensing, egocentric video, cross-modal alignment, grasp understanding, retrieval benchmarks, classification benchmarks]</li>
<li class=""><strong>authors:</strong> Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang</li>
<li class=""><strong>institution:</strong> MIT, Duke University, Brown University, University of Washington, Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16842" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16842</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces OpenTouch, a multimodal dataset combining synchronized in-the-wild egocentric video, full-hand tactile signals, and hand-pose data. It uses this dataset to create benchmarks for retrieval and classification, demonstrating that tactile information is a powerful cue for understanding grasps and improving cross-modal alignment. The main conclusion is that touch data significantly grounds perception and action, advancing research in multimodal egocentric perception and contact-rich robotics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [text-to-image evaluation], [benchmark drift, GenEval, GenEval 2, Soft-TIFA, VQAScore, model-based evaluation, visual primitives, compositionality]</li>
<li class=""><strong>authors:</strong> Amita Kamath, Kai-Wei Chang, Ranjay Krishna, Luke Zettlemoyer, Yushi Hu, Marjan Ghazvininejad</li>
<li class=""><strong>institution:</strong> FAIR at Meta, University of Washington, University of California, Los Angeles, Allen Institute for AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16853</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper identifies benchmark drift as a major issue in automated text-to-image evaluation, showing that the popular GenEval benchmark has become misaligned with human judgment. To address this, the authors introduce a new benchmark, GenEval 2, with more challenging prompts and a new evaluation method called Soft-TIFA, which aggregates judgments on visual primitives and shows better alignment with human scores. The work concludes that continual audits and updates are essential for maintaining valid automated evaluation benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [RePlan, plan-then-execute, vision-language planner, diffusion editor, attention-region injection, GRPO-based reinforcement learning, IV-Edit benchmark]</li>
<li class=""><strong>authors:</strong> Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia</li>
<li class=""><strong>institution:</strong> Tencent AI Lab, CUHK, HKUST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16864" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16864</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RePlan, a framework for complex instruction-based image editing that uses a vision-language planner to decompose instructions and ground them to specific regions, followed by a diffusion-based editor to apply the changes. The method employs a training-free attention-region injection mechanism for precise multi-region edits and uses GRPO-based RL to improve planning. It demonstrates superior performance on complex instruction-visual tasks compared to existing baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Pixel Seal: Adversarial-only training for invisible image and video watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [adversarial-only training, three-stage training schedule, JND-based attenuation, temporal watermark pooling]</li>
<li class=""><strong>authors:</strong> Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko</li>
<li class=""><strong>institution:</strong> Meta FAIR</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16874" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16874</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Pixel Seal introduces an adversarial-only training paradigm for invisible watermarking, eliminating unreliable perceptual losses and using a three-stage schedule to decouple robustness and imperceptibility. It addresses high-resolution scaling with JND-based attenuation and adapts to video via temporal pooling. The method achieves state-of-the-art robustness and imperceptibility for image and video watermarking.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, immunofluorescence microscopy, collagen VI-related dystrophies, rare disease diagnosis, decentralized training]</li>
<li class=""><strong>authors:</strong> Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [video object segmentation, memory filtering, occlusion-aware memory, piecewise interpolation, feature-based re-identification, temporal voting]</li>
<li class=""><strong>authors:</strong> Valay Bundele, Mehran Hosseinzadeh, Hendrik P.A. Lensch</li>
<li class=""><strong>institution:</strong> University of Tuebingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ReMeDI-SAM3, a training-free method that enhances SAM3 for surgical instrument segmentation by introducing relevance-aware memory filtering, a piecewise interpolation scheme, and a feature-based re-identification module to handle occlusions. It achieves significant performance improvements over the baseline on surgical datasets, outperforming prior training-based approaches in a zero-shot setting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Sceniris: A Fast Procedural Scene Generation Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [procedural scene generation, batch sampling, GPU acceleration, collision checking, robot reachability, cuRobo]</li>
<li class=""><strong>authors:</strong> Jinghuan Shang, Harsh Patel, Ran Gong, Karl Schmeckpeper</li>
<li class=""><strong>institution:</strong> Robotics and AI Institute, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16896" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16896</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sceniris is a fast procedural scene generation framework that uses batch sampling and GPU-accelerated collision checking via cuRobo to create large-scale, collision-free 3D scenes. It achieves at least 234x speed-up over prior methods and includes optional robot reachability checks for manipulation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] M-PhyGs: Multi-Material Object Dynamics from Video</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision, physics-based modeling], [Multi-material Physical Gaussians (M-PhyGs), cascaded 3D and 2D losses, temporal mini-batching, continuum mechanical parameters, material segmentation]</li>
<li class=""><strong>authors:</strong> Norika Wada, Kohei Yamashita, Ryo Kawahara, Ko Nishino</li>
<li class=""><strong>institution:</strong> Graduate School of Informatics, Kyoto University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces M-PhyGs, a method that estimates the physical material composition and parameters (like Young&#x27;s modulus) of complex multi-material objects, such as flowers, from short videos. It jointly segments materials and recovers mechanical parameters using cascaded losses and temporal mini-batching. Experiments on the new Phlowers dataset show the method is accurate and effective for predicting object dynamics from visual data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cross-layer knowledge fusion MoE, VLLM, world-knowledge representation, token extraction, layer-wise fusion]</li>
<li class=""><strong>authors:</strong> Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu</li>
<li class=""><strong>institution:</strong> Northeastern University, LinkedIn, University of Wisconsin–Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16891" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16891</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [3D-aware expression distillation, Gaussian splatting, feedforward encoder, local fusion strategy, neural radiance fields]</li>
<li class=""><strong>authors:</strong> Kaiwen Jiang, Xueting Li, Seonwook Park, Ravi Ramamoorthi, Shalini De Mello, Koki Nagano</li>
<li class=""><strong>institution:</strong> University of California, San Diego, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16893" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16893</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method that distills knowledge from a 2D video diffusion model into a feedforward encoder to instantly create a 3D-consistent and expressive animatable head avatar from a single image. It uses a decoupled animation representation and an efficient local fusion strategy to achieve high expressivity without relying on pre-defined parametric models. The approach achieves real-time animation speeds (over 107 FPS) while maintaining high-quality expression transfer, surpassing methods that trade speed for quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reward models, multimodal benchmark, preference pairs, LLM-as-a-judge, Best-of-N sampling, ensemble filtering]</li>
<li class=""><strong>authors:</strong> Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad</li>
<li class=""><strong>institution:</strong> Meta (FAIR at Meta Superintelligence Labs)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16899" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16899</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Multimodal RewardBench 2 (MMRB2), a comprehensive benchmark for evaluating reward models on multimodal tasks involving interleaved text and image sequences. It finds that current models like Gemini 3 Pro achieve 75-80% accuracy, which is still below human performance (&gt;90%), and that benchmark performance strongly correlates with downstream task success.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [adaptive latent prediction, normalized facial expression block, dynamic sliding-window, higher-order latent derivatives, identity-agnostic facial expression features, weighted blending]</li>
<li class=""><strong>authors:</strong> Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu</li>
<li class=""><strong>institution:</strong> Fudan University, Microsoft Research Asia, Xi’an Jiaotong University, Tencent Inc., Tongyi Lab (Alibaba Group)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16900" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16900</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashPortrait introduces an end-to-end video diffusion transformer that uses adaptive latent prediction to skip denoising steps and a normalized facial expression block to align features, achieving up to 6x faster inference for infinite-length portrait animation while preserving identity consistency. Experiments demonstrate its effectiveness in generating smooth, ID-preserving long videos both qualitatively and quantitatively.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [meta-gradient, data selection, data rating, data pruning, Shift-Gsampling, multi-granularity perception]</li>
<li class=""><strong>authors:</strong> Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, South China University of Technology, Kuaishou Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16905" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16905</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Alchemist, a meta-gradient-based framework for selecting high-quality subsets from large-scale text-image datasets to improve training efficiency for Text-to-Image models. It uses a lightweight rater model to score samples based on gradient influence and a pruning strategy to select informative data. Experiments show that training on a 50% subset selected by Alchemist can outperform training on the full dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [VLM-guided encoding, diffusion transformer, Group Relative Policy Optimization (GRPO), reward optimization, synthetic data generation]</li>
<li class=""><strong>authors:</strong> Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma</li>
<li class=""><strong>institution:</strong> Brown University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16906" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16906</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes VIVA, a framework for instruction-based video editing that uses a VLM to encode instructions and visual context for a diffusion transformer, followed by a reward optimization stage (Edit-GRPO) to improve editing quality. It concludes that VIVA achieves superior instruction following and generalization compared to state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reasoning-to-motion framework, trajectory-token interface, stage-aware trajectory prediction, 6DoF trajectory generation, vision-language reasoning, progressive training]</li>
<li class=""><strong>authors:</strong> Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu</li>
<li class=""><strong>institution:</strong> Meta, University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16907" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16907</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the EgoMAN dataset and model, a reasoning-to-motion framework that links vision-language reasoning with motion generation via a trajectory-token interface to predict 3D hand trajectories. It demonstrates that progressive training aligning reasoning with motion dynamics yields accurate, stage-aware trajectories with generalization across real-world scenes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SFTok: Bridging the Performance Gap in Discrete Tokenizers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [self-forcing guided visual reconstruction, debias-and-fitting training strategy, discrete tokenizer, multi-step iterative mechanism, autoregressive paradigm]</li>
<li class=""><strong>authors:</strong> Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16910</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SFTok, a discrete image tokenizer that uses a multi-step iterative mechanism with self-forcing guided visual reconstruction and a debias-and-fitting training strategy to improve reconstruction quality. This approach resolves training-inference inconsistency and achieves state-of-the-art image reconstruction performance at high compression rates, bridging the performance gap with continuous tokenizers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SceneDiff: A Benchmark and Method for Multiview Object Change Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [multiview object change detection, 3D alignment, segmentation, image encoding, training-free approach]</li>
<li class=""><strong>authors:</strong> Yuqun Wu, Chih-hao Lin, Henry Che, Aditi Tiwari, Chuhang Zou, Shenlong Wang, Derek Hoiem</li>
<li class=""><strong>institution:</strong> University of Illinois at Urbana-Champaign, Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16908</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SceneDiff, a training-free method for multiview object change detection that aligns captures in 3D, extracts object regions, and compares spatial and semantic features to identify added, removed, or moved objects. It also presents the SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations. Experiments show that the method significantly outperforms existing approaches, achieving large relative improvements in average precision.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [scene graph, vision-language model, reinforcement learning, task planning, graph-then-plan, state-aware representation]</li>
<li class=""><strong>authors:</strong> Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, University of Maryland, College Park, University of Toronto</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16909" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16909</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces MomaGraph, a unified scene graph representation for embodied agents that integrates spatial, functional, and part-level information, and develops a 7B vision-language model (MomaGraph-R1) trained with reinforcement learning to predict task-oriented graphs for planning. The model, evaluated on a new dataset and benchmark, achieves state-of-the-art performance among open-source models for task planning and generalizes to real-robot experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [panoramic depth estimation, DINOv3-Large, pseudo-label curation, range mask head, sharpness-centric optimization, geometry-centric optimization]</li>
<li class=""><strong>authors:</strong> Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi</li>
<li class=""><strong>institution:</strong> Insta360 Research, University of California, San Diego, Wuhan University, University of California, Merced</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16913</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a foundation model for panoramic metric depth estimation, which uses a data-in-the-loop paradigm involving a large-scale dataset and a three-stage pseudo-label curation pipeline. The model employs DINOv3-Large as a backbone and introduces specialized components like a range mask head and geometry-centric optimization. Experiments show the model achieves robust, metrically consistent depth predictions with strong zero-shot generalization across diverse real-world scenes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [stereo conversion, depth-warp-inpaint, generative priors, cycle consistency loss, feed-forward model]</li>
<li class=""><strong>authors:</strong> Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, Ying-Cong Chen</li>
<li class=""><strong>institution:</strong> HKUST(GZ), HKUST, Kling Team (Kuaishou Technology), CUHK</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16915</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes StereoPilot, an efficient feed-forward model for monocular-to-stereo video conversion that directly synthesizes target views without explicit depth maps or iterative diffusion sampling. It introduces a unified dataset (UniStereo) and a learnable domain switcher to handle different stereo formats. Experiments show StereoPilot outperforms state-of-the-art methods in visual fidelity and computational efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] DVGT: Driving Visual Geometry Transformer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving, 3D reconstruction], [transformer, DINO backbone, cross-view spatial attention, cross-frame temporal attention, multi-view geometry, point map]</li>
<li class=""><strong>authors:</strong> Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Xiaomi EV, University of Macau, Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16919" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16919</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DVGT, a transformer-based model that reconstructs a global 3D point map from unposed multi-view image sequences using alternating attention mechanisms. It eliminates reliance on precise camera parameters and external sensor alignment, achieving superior performance across diverse driving scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AdaTooler-V: Adaptive Tool-Use for Images and Videos</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [AT-GRPO, reinforcement learning, adaptive tool-use, multimodal chain-of-thought, vision tools]</li>
<li class=""><strong>authors:</strong> Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue</li>
<li class=""><strong>institution:</strong> MMLab (CUHK), THU, SJTU, DB Group (CUHK), UCF, Sangfor, JMU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16918</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AdaTooler-V, a multimodal large language model that uses adaptive tool-use to decide when to invoke vision tools, reducing unnecessary inference overhead. It employs a reinforcement learning algorithm called AT-GRPO, which adjusts rewards based on a Tool Benefit Score to encourage tool-use only when beneficial. Experiments show AdaTooler-V outperforms existing methods, achieving 89.8% accuracy on the V* benchmark, surpassing models like GPT-4o and Gemini 1.5 Pro.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] EasyV2V: A High-quality Instruction-based Video Editing Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [instruction-based video editing, LoRA fine-tuning, sequence concatenation, spatiotemporal mask control, reference image conditioning, data curation]</li>
<li class=""><strong>authors:</strong> Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei</li>
<li class=""><strong>institution:</strong> KAUST, Snap Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16920" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16920</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EasyV2V is a framework for instruction-based video editing that simplifies architecture by using pretrained text-to-video models with light LoRA fine-tuning and sequence concatenation for conditioning. It unifies control through a single mask mechanism and supports flexible inputs like text, masks, and reference images. The method achieves state-of-the-art results by effectively curating diverse video data and leveraging existing model capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [reinforcement learning, fine-tuning, model auditing, capability gap discovery, counterfactual generation]</li>
<li class=""><strong>authors:</strong> Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</li>
<li class=""><strong>institution:</strong> Google, Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16921</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AuditDM, a framework that uses reinforcement learning to fine-tune a multimodal LLM as an &quot;auditor&quot; to generate challenging questions and counterfactual images that expose model weaknesses. This automated auditing process discovers interpretable failure cases, which are then used as annotation-free data for rectification. The method successfully identifies over 20 failure types and improves model performance, enabling a smaller 3B model to surpass a larger 28B model, suggesting targeted auditing is an effective path for model improvement as data scaling yields diminishing returns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Next-Embedding Prediction Makes Strong Vision Learners</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [self-supervised learning], [next-embedding predictive autoregression, causal masking, stop gradient, transformer, generative pretraining]</li>
<li class=""><strong>authors:</strong> Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu</li>
<li class=""><strong>institution:</strong> University of Michigan, New York University, Princeton University, University of Virginia</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16922" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16922</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Next-Embedding Predictive Autoregression (NEPA), a self-supervised method where a Transformer model is trained to predict future patch embeddings from past ones using causal masking. It demonstrates that this simple generative objective, without complex auxiliary losses, yields strong visual learners. The method achieves competitive image classification and segmentation results, offering a scalable alternative to traditional visual representation learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Generative Refocusing: Flexible Defocus Control from a Single Image</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [DeblurNet, BokehNet, semi-supervised training, EXIF metadata, generative refocusing]</li>
<li class=""><strong>authors:</strong> Chun-Wei Tuan Mu, Jia-Bin Huang, Yu-Lun Liu</li>
<li class=""><strong>institution:</strong> National Yang Ming Chiao Tung University, University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16923" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16923</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Generative Refocusing, a two-step method using DeblurNet to recover an all-in-focus image and BokehNet to synthesize controllable bokeh effects. Its key innovation is a semi-supervised training approach that combines synthetic paired data with unpaired real bokeh images and EXIF metadata. The method achieves state-of-the-art performance in defocus deblurring and bokeh synthesis, enabling flexible post-capture focus and aperture control from a single image.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [trajectory encoding, reference images, text prompts, controllable video generation, world events]</li>
<li class=""><strong>authors:</strong> Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen</li>
<li class=""><strong>institution:</strong> HKUST, Ant Group, ZJU, NEU, CUHK, NTU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16924" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16924</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents WorldCanvas, a multimodal framework for generating controllable world events by combining text prompts, user-defined trajectories, and reference images. This approach enables coherent video generation with features like multi-agent interactions and object entry/exit. It advances world models from passive predictors to interactive, user-shaped simulators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Foundation Models in Biomedical Imaging: Turning Hype into Reality</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [biomedical imaging], [foundation models, clinical reasoning, causal inference, multimodal data, trustworthiness, safety, bias]</li>
<li class=""><strong>authors:</strong> Amgad Muneer, Kai Zhang, Ibraheem Hamdi, Rizwan Qureshi, Muhammad Waqas, Shereen Fouad, Hazrat Ali, Syed Muhammad Anwar, Jia Wu</li>
<li class=""><strong>institution:</strong> The University of Texas MD Anderson Cancer Center, UTHealth Houston, Massachusetts Institute of Technology, Aston University, University of Stirling, George Washington University, Children&#x27;s National Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15808</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper critically analyzes the state of foundation models in biomedical imaging, examining their capabilities in clinical reasoning, spatial understanding, and multimodal integration. It concludes that while these models are powerful assistive tools, the field must move beyond scale to develop hybrid, causally aware, and verifiably safe systems that augment human expertise, as autonomous AI-doctors remain a distant vision.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] BioimageAIpub: a toolbox for AI-ready bioimaging data publishing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [data conversion, metadata wrangling, workflow automation, HuggingFace integration]</li>
<li class=""><strong>authors:</strong> Stefan Dvoretskii, Anwai Archit, Constantin Pape, Josh Moore, Marco Nolden</li>
<li class=""><strong>institution:</strong> DKFZ Division of Medical Computing, Georg-August-University Göttingen, German BioImaging, NFDI4BIOIMAGE, Open Microscopy Environment, Helmholtz Metadata Collaboration Hub Health, Heidelberg University Hospital</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15820" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15820</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces BioimageAIpub, a workflow designed to automate and streamline the conversion of bioimaging data and metadata into AI-ready formats. This tool facilitates seamless data upload to the HuggingFace platform, reducing the time researchers spend on data wrangling. The main conclusion is that this workflow addresses a key bottleneck in bioimage analysis by making datasets more accessible and directly usable for developing powerful AI tools.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MCR-VQGAN: A Scalable and Cost-Effective Tau PET Synthesis Approach for Alzheimer&#x27;s Disease Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [VQGAN, multi-scale convolutions, ResNet blocks, CBAM, GAN, image synthesis, CNN classifier]</li>
<li class=""><strong>authors:</strong> Jin Young Kim, Jeremy Hudson, Jeongchul Kim, Qing Lyu, Christopher T. Whitlow</li>
<li class=""><strong>institution:</strong> Wake Forest University School of Medicine, Yale School of Medicine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15947" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15947</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MCR-VQGAN, a model that synthesizes tau PET images from T1-weighted MRI scans by enhancing VQGAN with multi-scale convolutions, ResNet blocks, and attention modules. It demonstrates superior synthesis performance and shows that a CNN classifier achieves comparable accuracy on real and synthetic images, indicating the method&#x27;s clinical utility as a scalable surrogate for tau PET imaging.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] In search of truth: Evaluating concordance of AI-based anatomy segmentation models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [segmentation harmonization, 3D Slicer, OHIF Viewer, concordance evaluation, standard representation]</li>
<li class=""><strong>authors:</strong> Lena Giebeler, Deepa Krishnaswamy, David Clunie, Jakob Wasserthal, Lalith Kumar Shiyam Sundar, Andres Diaz-Pinto, Klaus H. Maier-Hein, Murong Xu, Bjoern Menze, Steve Pieper, Ron Kikinis, Andrey Fedorov</li>
<li class=""><strong>institution:</strong> Brigham and Women’s Hospital, RWTH Aachen University, University Hospital of Basel, Ludwig Maximilian University Munich, German Cancer Research Center (DKFZ), University of Zurich, NVIDIA, Isomics Inc, PixelMed Publishing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15921" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15921</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a practical framework for evaluating AI-based anatomy segmentation models when ground truth annotations are unavailable, by harmonizing segmentations into a standard representation and using tools like 3D Slicer and OHIF Viewer for comparison. The method was applied to six open-source models on CT scans, showing excellent agreement for some structures but inconsistencies for others like vertebrae and ribs. This approach aids in model selection by enabling automated inspection and detection of problematic results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [machine learning, graph analysis, multimodal X-ray imaging, microstructure-property relationships, particulate composites, solid-state batteries]</li>
<li class=""><strong>authors:</strong> Zebin Li, Shimao Deng, Yijin Liu, Jia-Mian Hu</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison, University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16085" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16085</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a machine learning framework that automatically converts multimodal X-ray images of particulate composites into topology-aware graphs to analyze microstructure. Applying this to solid-state battery cathodes reveals the importance of triple phase junctions and concurrent ion/electron conduction channels for electrochemical activity. The work establishes graph-based representation as a key method for linking imaging data to functional understanding in composite materials.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-30T11:07:23.000Z" itemprop="dateModified">Dec 30, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/daily/cscv"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">cs.CV</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/daily/cscv/20251222-20251228"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251222-20251228 (cs.CV)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li><li><a href="#2025-12-19" class="table-of-contents__link toc-highlight">2025-12-19</a></li></ul></div></div></div></div></div></div><aside class="content-right-sidebar"><div class="daily-time-sidebar"><div class="daily-time-title">选择时间</div><div class="daily-calendar"><div class="calendar-header"><button class="calendar-nav-btn">‹</button><div class="calendar-title">2025-12</div><button class="calendar-nav-btn">›</button></div><div class="calendar-weekdays"><div>一</div><div>二</div><div>三</div><div>四</div><div>五</div><div>六</div><div>日</div></div><div class="calendar-grid"><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-01">1</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-02">2</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-03">3</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-04">4</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-05">5</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-06">6</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251201-20251207#2025-12-07">7</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-08">8</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-09">9</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-10">10</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-11">11</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-12">12</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-13">13</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251208-20251214#2025-12-14">14</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-15">15</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-16">16</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-17">17</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-18">18</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-19">19</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-20">20</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251215-20251221#2025-12-21">21</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-22">22</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-23">23</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-24">24</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-25">25</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-26">26</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-27">27</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251222-20251228#2025-12-28">28</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251229-20260104#2025-12-29">29</a><a class="calendar-cell calendar-day is-today" href="/ai_toutiao/daily/cs_CV/20251229-20260104#2025-12-30">30</a><a class="calendar-cell calendar-day" href="/ai_toutiao/daily/cs_CV/20251229-20260104#2025-12-31">31</a></div></div></div></aside><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div></div>
</body>
</html>