<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/cs_LG/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 (cs.LG) | AI头条</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://jokebear666.github.io/ai_toutiao/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 (cs.LG) | AI头条"><meta data-rh="true" name="description" content="2025-12-18"><meta data-rh="true" property="og:description" content="2025-12-18"><link data-rh="true" rel="icon" href="/ai_toutiao/img/favicon_b.ico"><link data-rh="true" rel="canonical" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://jokebear666.github.io/ai_toutiao/category/daily"},{"@type":"ListItem","position":2,"name":"cs.LG","item":"https://jokebear666.github.io/ai_toutiao/category/cslg"},{"@type":"ListItem","position":3,"name":"20251215-20251221 (cs.LG)","item":"https://jokebear666.github.io/ai_toutiao/daily/cs_LG/20251215-20251221"}]}</script><link rel="stylesheet" href="/ai_toutiao/assets/css/styles.cc04cc53.css">
<script src="/ai_toutiao/assets/js/runtime~main.25ffb784.js" defer="defer"></script>
<script src="/ai_toutiao/assets/js/main.d71a22b2.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/ai_toutiao/img/favicon_b.ico"><link rel="preload" as="image" href="/ai_toutiao/img/ads_2.png"><link rel="preload" as="image" href="/ai_toutiao/img/ads_3.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai_toutiao/"><div class="navbar__logo"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai_toutiao/img/favicon_b.ico" alt="AI头条" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AI头条</b></a><a class="navbar__item navbar__link" href="/ai_toutiao/category/daily">Daily</a><a class="navbar__item navbar__link" href="/ai_toutiao/category/paper">Paper</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/ai_toutiao/sponsor/advertise">赞助商</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai_toutiao/intro"><span title="首页" class="linkLabel_WmDU">首页</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai_toutiao/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csai"><span title="cs.AI" class="categoryLinkLabel_W154">cs.AI</span></a><button aria-label="Expand sidebar category &#x27;cs.AI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csce"><span title="cs.CE" class="categoryLinkLabel_W154">cs.CE</span></a><button aria-label="Expand sidebar category &#x27;cs.CE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscl"><span title="cs.CL" class="categoryLinkLabel_W154">cs.CL</span></a><button aria-label="Expand sidebar category &#x27;cs.CL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscr"><span title="cs.CR" class="categoryLinkLabel_W154">cs.CR</span></a><button aria-label="Expand sidebar category &#x27;cs.CR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscv"><span title="cs.CV" class="categoryLinkLabel_W154">cs.CV</span></a><button aria-label="Expand sidebar category &#x27;cs.CV&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cscy"><span title="cs.CY" class="categoryLinkLabel_W154">cs.CY</span></a><button aria-label="Expand sidebar category &#x27;cs.CY&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csdb"><span title="cs.DB" class="categoryLinkLabel_W154">cs.DB</span></a><button aria-label="Expand sidebar category &#x27;cs.DB&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csdc"><span title="cs.DC" class="categoryLinkLabel_W154">cs.DC</span></a><button aria-label="Expand sidebar category &#x27;cs.DC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csds"><span title="cs.DS" class="categoryLinkLabel_W154">cs.DS</span></a><button aria-label="Expand sidebar category &#x27;cs.DS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgr"><span title="cs.GR" class="categoryLinkLabel_W154">cs.GR</span></a><button aria-label="Expand sidebar category &#x27;cs.GR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csgt"><span title="cs.GT" class="categoryLinkLabel_W154">cs.GT</span></a><button aria-label="Expand sidebar category &#x27;cs.GT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cshc"><span title="cs.HC" class="categoryLinkLabel_W154">cs.HC</span></a><button aria-label="Expand sidebar category &#x27;cs.HC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csir"><span title="cs.IR" class="categoryLinkLabel_W154">cs.IR</span></a><button aria-label="Expand sidebar category &#x27;cs.IR&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csit"><span title="cs.IT" class="categoryLinkLabel_W154">cs.IT</span></a><button aria-label="Expand sidebar category &#x27;cs.IT&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai_toutiao/category/cslg"><span title="cs.LG" class="categoryLinkLabel_W154">cs.LG</span></a><button aria-label="Collapse sidebar category &#x27;cs.LG&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai_toutiao/daily/cs_LG/20251215-20251221"><span title="20251215-20251221 (cs.LG)" class="linkLabel_WmDU">20251215-20251221 (cs.LG)</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csma"><span title="cs.MA" class="categoryLinkLabel_W154">cs.MA</span></a><button aria-label="Expand sidebar category &#x27;cs.MA&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csmm"><span title="cs.MM" class="categoryLinkLabel_W154">cs.MM</span></a><button aria-label="Expand sidebar category &#x27;cs.MM&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csne"><span title="cs.NE" class="categoryLinkLabel_W154">cs.NE</span></a><button aria-label="Expand sidebar category &#x27;cs.NE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csni"><span title="cs.NI" class="categoryLinkLabel_W154">cs.NI</span></a><button aria-label="Expand sidebar category &#x27;cs.NI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csos"><span title="cs.OS" class="categoryLinkLabel_W154">cs.OS</span></a><button aria-label="Expand sidebar category &#x27;cs.OS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cspl"><span title="cs.PL" class="categoryLinkLabel_W154">cs.PL</span></a><button aria-label="Expand sidebar category &#x27;cs.PL&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csro"><span title="cs.RO" class="categoryLinkLabel_W154">cs.RO</span></a><button aria-label="Expand sidebar category &#x27;cs.RO&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssc"><span title="cs.SC" class="categoryLinkLabel_W154">cs.SC</span></a><button aria-label="Expand sidebar category &#x27;cs.SC&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssd"><span title="cs.SD" class="categoryLinkLabel_W154">cs.SD</span></a><button aria-label="Expand sidebar category &#x27;cs.SD&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/csse"><span title="cs.SE" class="categoryLinkLabel_W154">cs.SE</span></a><button aria-label="Expand sidebar category &#x27;cs.SE&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai_toutiao/category/cssi"><span title="cs.SI" class="categoryLinkLabel_W154">cs.SI</span></a><button aria-label="Expand sidebar category &#x27;cs.SI&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai_toutiao/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai_toutiao/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai_toutiao/sponsor/advertise"><span title="赞助商" class="categoryLinkLabel_W154">赞助商</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="content-with-ads"><div class="content-main"><div class="content-inner"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai_toutiao/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai_toutiao/category/cslg"><span>cs.LG</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221 (cs.LG)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221 (cs.LG)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv251218] Autonomous Source Knowledge Selection in Multi-Domain Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [domain adaptation], [multi-domain adaptation, density-driven selection, pseudo-label enhancement, feature alignment, self-supervision]</li>
<li class=""><strong>authors:</strong> Keqiuyin Li, Jie Lu, Hua Zuo, Guangquan Zhang</li>
<li class=""><strong>institution:</strong> University of Technology Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14710" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14710</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AutoS, a method for unsupervised multi-domain adaptation that autonomously selects relevant source samples and models via a density-driven strategy and uses a pre-trained multimodal model to enhance pseudo-labels for self-supervision. Experiments show the method effectively improves transfer performance by focusing on the most transferable knowledge from multiple source domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Latent Class Reinforcement Learning (LCRL), Variational Bayes, Bayesian estimation, discrete choice models]</li>
<li class=""><strong>authors:</strong> Georges Sfeir, Stephane Hess, Thomas O. Hancock, Filipe Rodrigues, Jamal Amani Rad, Michiel Bliemer, Matthew Beck, Fayyaz Khan</li>
<li class=""><strong>institution:</strong> University of Leeds, Technical University of Denmark, University of Sydney, Al Yamamah University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Latent Class Reinforcement Learning (LCRL) model, estimated using Variational Bayes, to capture how travelers adapt their preferences through experience. The application to a driving simulator dataset reveals three distinct behavioral classes, showing heterogeneity in how individuals balance exploration and exploitation in their travel decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SGEMAS: A Self-Growing Ephemeral Multi-Agent System for Unsupervised Online Anomaly Detection via Entropic Homeostasis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent system, structural plasticity, variational free energy, metabolic lagrangian, stochastic thermodynamics, unsupervised anomaly detection]</li>
<li class=""><strong>authors:</strong> Mustapha Hamdi</li>
<li class=""><strong>institution:</strong> InnoDeep</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14708" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14708</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SGEMAS, a bio-inspired multi-agent system that uses agent birth/death and a variational free energy objective to achieve energy-efficient, unsupervised anomaly detection in physiological signals. It validates the approach on the MIT-BIH Arrhythmia Database, showing that this physics-based, energy-constrained model can detect anomalies in a zero-shot setting, outperforming a standard autoencoder baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Graph-Embedded Hazard Model (GEHM): Stochastic Network Survival Dynamics on Economic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [network economics], [graph p-Laplacian, accretive operator theory, nonlinear semigroup methods, stochastic analysis, PDE-SDE systems]</li>
<li class=""><strong>authors:</strong> Diego Vallarino</li>
<li class=""><strong>institution:</strong> Inter-American Development Bank (IDB)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Graph-Embedded Hazard Model (GEHM), a nonlinear PDE-SDE framework that couples graph-based p-Laplacian diffusion with stochastic drift to model survival dynamics on economic networks. The main conclusion is that hub dominance in networks like Barabási–Albert magnifies nonlinear gradients, compresses stability margins, and leads to heavy-tailed survival distributions with occasional explosive behavior.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [learnable Gabor filters, ResNeXt, squeeze-and-excitation attention, spectrograms, deep learning]</li>
<li class=""><strong>authors:</strong> Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut</li>
<li class=""><strong>institution:</strong> Flinders University, PrioriAnalytica</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14714" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14714</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [neural architecture search, large language models, image captioning, prompt engineering, CNN encoder, LSTM, GRU, Transformer, BLEU-4]</li>
<li class=""><strong>authors:</strong> Krunal Jesani, Dmitry Ignatov, Radu Timofte</li>
<li class=""><strong>institution:</strong> University of Würzburg</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents NN-Caption, a pipeline that uses a large language model (LLM) to automatically generate runnable image-captioning model architectures by composing CNN encoders and sequence decoders under a strict API. The method successfully produced dozens of models, with over half training successfully, demonstrating the promise of LLM-guided neural architecture search while highlighting challenges like code hallucinations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural-symbolic AI], [Vector Symbolic Architecture, self-attention, binding, unbinding, residual streams, hyperdimensional computing, chain-of-thought]</li>
<li class=""><strong>authors:</strong> Sahil Rajesh Dhayalkar</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14709" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14709</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper interprets transformer self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA), where attention performs soft binding/unbinding of roles and fillers. It uses this perspective to explain the models&#x27; reasoning capabilities and brittleness, and proposes VSA-inspired architectural modifications to improve logical reliability. The core conclusion is that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and robust reasoning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [Mixture-of-Experts (MoE), Hierarchical Gated Attention Network, CatBoost meta-learner, multimodal fusion, deep fusion, expert stacking, Quad-Modal Ensemble]</li>
<li class=""><strong>authors:</strong> Ryan Cartularo</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [benchmark evaluation, token efficiency score, model efficiency, performance metrics, GPT-OSS]</li>
<li class=""><strong>authors:</strong> Ziqian Bi, Danyang Zhang, Junhao Song, Chiung-Yi Tseng</li>
<li class=""><strong>institution:</strong> Purdue University, Imperial College London, Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14717" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14717</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks large language models, specifically the GPT-OSS family, across ten financial NLP tasks. It finds that the smaller GPT-OSS-20B model achieves comparable accuracy to its larger counterpart while demonstrating superior computational efficiency, challenging the assumption that model scale directly correlates with performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bit-level fault injection, gradient-based sensitivity estimation, differentiable fault analysis, semantic steering, SBERT embeddings, perplexity scoring]</li>
<li class=""><strong>authors:</strong> Zafaryab Haider, Md Hafizur Rahman, Shane Moeykens, Vijay Devabhaktuni, Prabuddha Chakraborty</li>
<li class=""><strong>institution:</strong> University of Maine, Illinois State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces BLADE, a differentiable fault analysis framework that uses gradient-based sensitivity estimation to identify and flip specific bits in a quantized vision-language model&#x27;s weights, steering its generated captions to change semantic meaning while preserving grammatical fluency. It concludes that semantic drift from low-level bit flips is predictable and controllable, revealing vulnerabilities in generative AI systems and opening pathways for robustness testing and adversarial defense.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical data synthesis], [Synthea, rule-based generation, statistical rules, glioblastoma, privacy-preserving, synthetic patient data]</li>
<li class=""><strong>authors:</strong> Arno Appenzeller, Nick Terzer, André Hohmeyer, Jan-Philipp Redlich, Sabine Luttmann, Friedrich Feuerhake, Nadine S. Schaadt, Timm Intemann, Sarah Teuber-Hanselmann, Stefan Nikolin, Joachim Weis, Klaus Kraywinkel, Pascal Birnstill</li>
<li class=""><strong>institution:</strong> Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14721" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14721</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces an automated method to generate Synthea rules for creating synthetic patient data from real-world tabular statistics, using glioblastoma as an example. The synthetic data reproduced known disease courses and largely retained the statistical properties of the original dataset. This demonstrates the potential of synthetic data for privacy-preserving medical research, though with noted limitations for clinical interpretation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Hybrid Attribution Priors for Explainable and Robust Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable ai], [class-aware attribution prior, explanation-guided learning, attribution priors, small language models, model interpretability]</li>
<li class=""><strong>authors:</strong> Zhuoran Zhang, Feng Zhang, Shangyuan Li, Yang Shi, Yuanxing Zhang, Wei Chen, Tengjiao Wang, Kam-Fai Wong</li>
<li class=""><strong>institution:</strong> Peking University, Kling Team, CUHK</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14719" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14719</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Class-Aware Attribution Prior (CAP) framework and its hybrid variant, CAP Hybrid, to generate more discriminative attribution priors for training small language models. By aligning a model&#x27;s self-attribution with these enriched priors, the method encourages the learning of diverse, decision-relevant features. Experiments show the approach consistently enhances both model interpretability and robustness across various data settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series forecasting], [spectral entropy, dependency evaluator, signed graph constructor, context spatial extractor, channel independence, channel dependence]</li>
<li class=""><strong>authors:</strong> Feng Xiong, Zongxia Xie, Yanru Sun, Haoyu Wang, Jianhong Lin</li>
<li class=""><strong>institution:</strong> Tianjin University, Fudan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14718</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SEED, a framework for multivariate time series forecasting that uses spectral entropy to evaluate and model spatial-temporal dependencies. It introduces components like a Dependency Evaluator and a Signed Graph Constructor to adaptively balance modeling strategies and preserve negative correlations. Experiments on 12 datasets show SEED achieves state-of-the-art performance, demonstrating its effectiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [graph neural network, diffusion model, generative modeling, unstructured meshes, urban flow simulation, score-based diffusion]</li>
<li class=""><strong>authors:</strong> Francisco Giral, Álvaro Manzano, Ignacio Gómez, Petros Koumoutsakos, Soledad Le Clainche</li>
<li class=""><strong>institution:</strong> Universidad Politécnica de Madrid, Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14725" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14725</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a generative diffusion framework that combines a hierarchical graph neural network with score-based diffusion modeling to synthesize steady-state urban wind fields from geometry data on unstructured meshes. The model generalizes to unseen geometries, accurately capturing key flow structures like wakes and recirculation zones, and provides uncertainty-aware predictions. This work represents a step towards foundation models for the built environment, enabling rapid evaluation of urban design decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical attention transformers, curriculum learning, groebner bases, polynomial systems, computational cost analysis]</li>
<li class=""><strong>authors:</strong> Mohamed Malhou, Ludovic Perret, Kristin Lauter</li>
<li class=""><strong>institution:</strong> Meta Superintelligence Labs (FAIR), Sorbonne Université, CNRS, LIP6, EPITA, EPITA Research Lab (LRE)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14722</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HATSolver, a method that improves upon prior transformer-based approaches by using Hierarchical Attention Transformers (HATs) to compute Gröbner bases for solving systems of multivariate polynomial equations. The HAT architecture incorporates a tree-structured inductive bias to model hierarchical data relationships, achieving significant computational savings over flat attention models. Combined with curriculum learning, the method can solve much larger problem instances than previous work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [Quantum Decision Transformer, Quantum-Inspired Attention, Quantum Feedforward Networks, entanglement, interference, offline reinforcement learning, Decision Transformer]</li>
<li class=""><strong>authors:</strong> Abraham Itzhak Weinberg</li>
<li class=""><strong>institution:</strong> AI-WEINBERG, AI Experts</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Quantum Decision Transformer (QDT), a novel architecture that integrates quantum-inspired attention with entanglement and quantum feedforward networks with interference to improve offline reinforcement learning. It demonstrates a dramatic performance improvement over standard Decision Transformers and shows that the synergy between its quantum-inspired components is crucial for its success.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [medical machine learning], [conformal prediction, uncertainty quantification, finite sample theory, calibration]</li>
<li class=""><strong>authors:</strong> Klaus-Rudolf Kladny, Bernhard Schölkopf, Lisa Koch, Christian F. Baumgartner, Michael Muehlebach</li>
<li class=""><strong>institution:</strong> Max Planck Institute for Intelligent Systems, University of Bern, University of Tübingen, University of Lucerne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper critically examines conformal prediction, a method for providing statistically guaranteed uncertainty estimates from machine learning models using a calibration dataset. It argues that while the theoretical guarantees hold for any calibration set size, the practical utility of these guarantees is highly dependent on having a sufficiently large calibration sample. This critique is particularly relevant for medical applications where data is often scarce, and the authors support their argument with an empirical demonstration on a medical image classification task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [KLEM, KL divergence, EM algorithm, Automatic Fare Collection, Automatic Vehicle Location, data-driven parameter estimation]</li>
<li class=""><strong>authors:</strong> Jie He, Yong Qin, Jianyuan Guo, Xuan Sun, Xuanchuan Zheng</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University, China Railway Signaling and Communication Research and Design Institute, Beijing Urban Construction Design &amp; Development Group Co.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14728" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14728</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fully data-driven method for inferring individual travel trajectories in urban rail transit systems by combining AFC and AVL data, using a novel parameter estimation method (KLEM) based on KL divergence and the EM algorithm. The approach achieves over 90% accuracy in trajectory inference during peak hours, validated with real-world data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [inference time feature injection, intra-day personalization, model-agnostic, batch-trained models, real-time recommendation]</li>
<li class=""><strong>authors:</strong> Qiang Chen, Venkatesh Ganapati Hegde, Hongfei Li</li>
<li class=""><strong>institution:</strong> Tubi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14734</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a lightweight, model-agnostic method that injects a user&#x27;s recent watch history into a batch-trained recommender system at inference time, overriding stale features without model retraining. The approach achieved a statistically significant 0.47% increase in user engagement metrics, demonstrating that intra-day personalization can meaningfully improve recommendations for long-form video streaming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [A/B testing, cost-aware optimization, scroll-depth analysis, unbiased data collection, dedicated exploration container]</li>
<li class=""><strong>authors:</strong> Qiang Chen, Venkatesh Ganapati Hegde</li>
<li class=""><strong>institution:</strong> Tubi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14733" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14733</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a method for safe content exploration in recommender systems by strategically placing a &quot;Something Completely Different&quot; row of randomized content only in low-engagement, high-reach scroll-depth regions of the UI. This approach preserves key business metrics while collecting unbiased interaction data. The collected data, when used for downstream candidate generation, significantly improves user engagement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [planner-executor framework, large language models (LLMs), vision-language models (VLMs), segmentation models, image processing]</li>
<li class=""><strong>authors:</strong> Idan Tankel, Nir Mazor, Rafi Brada, Christina LeBedis, Guy ben-Yosef</li>
<li class=""><strong>institution:</strong> GE Healthcare Technology and Innovation Center, Boston Medical Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework that uses a planner-executor approach, where an LLM planner generates Python scripts to automate the detection and reporting of incidental findings in abdominal CT scans, and an executor runs these scripts using VLMs and segmentation models. The method is fully automatic and end-to-end. The results show that this framework outperforms pure VLM-based approaches in accuracy and efficiency for managing incidental findings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Semantic Geometry for policy-constrained interpretation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [semantic interpretation], [geometric framework, spherical convex regions, constrained optimization, policy constraints, information theory, Bayesian inference, sheaf-theoretic semantics]</li>
<li class=""><strong>authors:</strong> Nikit Phadke</li>
<li class=""><strong>institution:</strong> Independent researcher (based on gmail address)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14731" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14731</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a geometric framework for semantic interpretation where meaning is represented as directions on a unit sphere and policy constraints are applied as explicit priors. This approach separates evidence processing from policy rules, enabling provable prevention of hallucinated commitments. Empirical validation on financial data shows zero hallucinated approvals, demonstrating the method&#x27;s effectiveness in high-stakes, policy-constrained domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] NoveltyRank: Estimating Conceptual Novelty of AI Papers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [natural language processing], [binary classification, pairwise comparison, semantic similarity, SPECTER2, fine-tuning, Qwen3-4B-Instruct-2507, SciBERT]</li>
<li class=""><strong>authors:</strong> Zhengxu Yan, Han Li, Yuming Feng</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14738" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14738</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes NoveltyRank, a model that estimates the conceptual novelty of AI papers using title, abstract, and semantic similarity to prior literature, evaluated through binary classification and pairwise comparison tasks. It fine-tunes Qwen3-4B-Instruct-2507 and SciBERT, benchmarking against GPT-5.1, and finds that task formulation and modeling choices significantly impact performance, with the implementation made publicly available.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [quantum machine learning, hybrid quantum computing, amplitude encoding, entanglement encoding, hierarchical threat detection, anomaly detection, intrusion confirmation, multiattack classification, deep neural networks, ensemble classifiers]</li>
<li class=""><strong>authors:</strong> Tan Le, Van Le, Sachin Shetty</li>
<li class=""><strong>institution:</strong> Hampton University, Virginia Polytechnic Institute and State University, Old Dominion University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hierarchical cybersecurity framework for Open Radio Access Networks (O-RAN) that integrates hybrid quantum computing and machine learning. The method uses quantum-inspired feature encodings (amplitude- and entanglement-based) with deep and ensemble classifiers for anomaly detection, intrusion confirmation, and multiattack classification. The framework demonstrates near-perfect accuracy, high recall, and strong interpretability, indicating readiness for scalable deployment in O-RAN environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Compute the edge p-Laplacian centrality for air traffic network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph theory, network analysis], [p-Laplacian centrality, line graph, un-normalized graph Laplacian, node centrality, edge centrality]</li>
<li class=""><strong>authors:</strong> Loc Hoang Tran, Bao Nguyen Tran, Luong Anh Tuan Nguyen</li>
<li class=""><strong>institution:</strong> Vietnam Aviation Academy</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method to compute edge p-Laplacian centrality for an air traffic network by first converting the network into a line graph and then computing the node p-Laplacian centrality on that line graph. The method is based on a novel un-normalized graph p-Laplacian operator. The experimental results show that this ranking method can be successfully implemented.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] CAPE: Capability Achievement via Policy Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [capability engineering, policy execution, specification language, verification, DPO, contextual objectivity, verification-fidelity scaling]</li>
<li class=""><strong>authors:</strong> David Ball</li>
<li class=""><strong>institution:</strong> Superficial Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14761" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14761</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces CAPE, a protocol for Capability Engineering that implements a Specify-&gt;Verify-&gt;Correct-&gt;Train loop to convert requirements into executable specifications and train models to satisfy them by default. It demonstrates that CAPE reduces policy violation rates by 81% compared to DPO and significantly lowers costs and development timelines by using reusable specifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Guided Discrete Diffusion for Constraint Satisfaction Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [generative models], [discrete diffusion, unsupervised learning, constraint satisfaction, sudoku]</li>
<li class=""><strong>authors:</strong> Justin Jung</li>
<li class=""><strong>institution:</strong> unknown</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14765" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14765</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an unsupervised discrete diffusion model to learn the distribution of Sudoku puzzles, aiming to capture their structural patterns. The method avoids the need for supervised datasets and directly handles the discrete nature of the constraint satisfaction problem. The authors demonstrate its capability to solve Sudoku puzzles without supervision.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [knowledge graph reasoning], [GR-Agent, adaptive graph reasoning, incomplete knowledge graphs, agent environment interaction, reasoning paths]</li>
<li class=""><strong>authors:</strong> Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Hongkuan Zhou, Jiaoyan Chen, Steffen Staab, Yuan He, Evgeny Kharlamov</li>
<li class=""><strong>institution:</strong> University of Oslo, Bosch Center for AI, University of Stuttgart, University of Oxford, Amazon, The University of Manchester, University of Southampton</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GR-Agent, an adaptive graph reasoning agent that formalizes knowledge graph question answering as agent-environment interaction to handle incomplete knowledge graphs by using graph reasoning tools and memory of evidence. It demonstrates that existing methods degrade under incompleteness, while GR-Agent outperforms non-training baselines and matches training-based methods in both complete and incomplete settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Weather Forecasts from a Decision Maker&#x27;s Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [weather forecasting], [decision calibration, probabilistic forecasting, ensemble forecasts, machine learning weather prediction, numerical weather prediction]</li>
<li class=""><strong>authors:</strong> Kornelius Raeth, Nicole Ludwig</li>
<li class=""><strong>institution:</strong> University of Tübingen, Tübingen AI Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a framework called decision calibration to evaluate weather forecasts based on their ability to improve decision-making, rather than just statistical accuracy. It compares Machine Learning and classical numerical weather prediction models on various decision tasks. The main conclusion is that forecast-level performance does not reliably predict decision-level performance, and the optimal model can change depending on the specific decision task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Incentives or Ontology? A Structural Rebuttal to OpenAI&#x27;s Hallucination Thesis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [large language models], [transformer architecture, structural hallucination, Licensing Oracle, hybrid systems, statistical ontology]</li>
<li class=""><strong>authors:</strong> Richard Ackermann, Simeon Emanuilov</li>
<li class=""><strong>institution:</strong> RA Software, Sofia University “St. Kliment Ohridski”</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14801</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper argues that hallucination in LLMs is an architectural inevitability of transformers, which model token co-occurrence rather than the world, and demonstrates through a Licensing Oracle that external truth-validation modules are required for reliable abstention. It concludes that hallucination is a structural property, not a correctable incentive problem, necessitating hybrid systems to separate linguistic fluency from epistemic responsibility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Unreliable Uncertainty Estimates with Monte Carlo Dropout</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [uncertainty estimation], [Monte Carlo dropout, Bayesian inference, Gaussian processes, Bayesian neural networks, epistemic uncertainty, aleatoric uncertainty]</li>
<li class=""><strong>authors:</strong> Aslak Djupskås, Alexander Johannes Stasik, Signe Riemer-Sørensen</li>
<li class=""><strong>institution:</strong> Norwegian University of Life Sciences, SINTEF AS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14851</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper empirically evaluates Monte Carlo Dropout (MCD) as a method for uncertainty estimation in deep learning. It finds that MCD&#x27;s uncertainty estimates are unreliable, particularly in extrapolation and interpolation regions, compared to traditional Bayesian approaches like Gaussian Processes and Bayesian Neural Networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [end-to-end spoken dialogue systems, audio language models, multi-turn evaluation, speech-to-speech, audio-native benchmark, inference memory, instruction retention, self coherence, voice editing]</li>
<li class=""><strong>authors:</strong> Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda Uçar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He</li>
<li class=""><strong>institution:</strong> Scale AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14865" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14865</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new &quot;Voice Editing&quot; axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network activation functions], [Fourier Analysis Network, Dual-Activation Layer, sine activation, gradient analysis, dying-ReLU problem]</li>
<li class=""><strong>authors:</strong> Sam Jeong, Hae Yong Kim</li>
<li class=""><strong>institution:</strong> University of São Paulo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14873" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14873</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the mechanism of the Fourier Analysis Network (FAN) and finds its benefit stems from the sine function&#x27;s non-zero derivative near zero, which mitigates vanishing gradients and the dying-ReLU problem. Based on this insight, the authors propose a new Dual-Activation Layer (DAL) that accelerates convergence and achieves equal or higher accuracy than conventional activations in evaluated tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [machine learning theory], [Bregman projection, entropy reservoir, information geometry, model collapse, self-referential learning]</li>
<li class=""><strong>authors:</strong> Jingwei Chen</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14879" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14879</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Entropy-Reservoir Bregman Projection (ERBP) framework, which models self-referential learning as a stochastic Bregman projection sequence in distribution space. It shows that injecting an entropy reservoir stabilizes the dynamics and prevents model collapse, unifying various empirical fixes into a single quantitative design rule.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [model adaptation], [task matrix, linear transformation, finetuning, linear probe, embedding space]</li>
<li class=""><strong>authors:</strong> Darrin O&#x27; Brien, Dhikshith Gajulapalli, Eric Xia</li>
<li class=""><strong>institution:</strong> Algoverse AI Research, Brown University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the concept of a &quot;task matrix,&quot; a linear transformation that maps a base model&#x27;s embedding state to a finetuned model&#x27;s state. It demonstrates that applying this matrix to a base model can outperform linear probes and sometimes approach full finetuning performance across vision and text models on various datasets. The results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [online learning], [weighted average, exponential weighted moving average, online linear regression, pseudo-inverse, coefficient of determination]</li>
<li class=""><strong>authors:</strong> Mohammad Abu-Shaira, Alejandro Rodriguez, Greg Speegle, Victor Sheng, Ishfaq Ahmad</li>
<li class=""><strong>institution:</strong> Baylor University, Texas Tech University, University of Texas at Arlington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14892" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14892</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces OLR-WA, a novel multivariate online linear regression model based on a weighted average approach. It demonstrates rapid convergence and performance comparable to batch regression, while uniquely handling both temporal drift and confidence-based data scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph learning], [graph neural networks, community detection, multilayer perceptrons, heterophilic graphs, homophilic graphs, node classification]</li>
<li class=""><strong>authors:</strong> Turja Kundu, Sanjukta Bhowmick</li>
<li class=""><strong>institution:</strong> University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14908" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14908</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ATLAS introduces a graph learning method that extracts multi-resolution community information and concatenates it with node features, then applies MLPs for classification, avoiding iterative aggregation. This approach improves accuracy on both homophilic and heterophilic graphs while enhancing scalability compared to traditional GNNs. The results show significant performance gains, particularly on heterophilic graphs, and offer a path toward explainable graph learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [imitation learning], [on-policy expert corrections, DAgger, covariate shift, rejection sampling, supervised fine-tuning, multi-turn agents]</li>
<li class=""><strong>authors:</strong> Niklas Lauffer, Xiang Deng, Srivatsa Kundurthy, Brad Kenstler, Jeff Da</li>
<li class=""><strong>institution:</strong> UC Berkeley, Cornell University, Scale AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14895" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14895</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel data generation method called on-policy expert corrections (OECs) to address covariate shift in imitation learning for multi-turn language model agents. The method generates partially on-policy data by starting rollouts with a student model and switching to an expert model mid-trajectory. Experiments on software engineering tasks show OECs yield a 13-14% improvement over traditional imitation learning, demonstrating the need to combine expert demonstrations with on-policy data for effective agent training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [LoRA, dataset translation, visual question answering, instruction tuning, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> George-Andrei Dima, Dumitru-Clementin Cercel</li>
<li class=""><strong>institution:</strong> National University of Science and Technology POLITEHNICA Bucharest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14926</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a parameter-efficient method for adapting vision-language models to Romanian by translating the Flickr30k dataset and generating QA pairs, then fine-tuning models like LLaMA, LLaVA, and Qwen2 using LoRA. The results show significant improvements in Romanian visual QA and image captioning, with the Qwen2-VL-RoVQA model achieving the best performance and reduced grammatical errors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [signal processing], [low-rank MMSE filter, Kronecker product representation, regularization parameter selection]</li>
<li class=""><strong>authors:</strong> Daniel Gomes de Pinho Zanco, Leszek Szczecinski, Jacob Benesty, Eduardo Vinicius Kuhn</li>
<li class=""><strong>institution:</strong> INRS–Institut National de la Recherche Scientifique, Federal University of Technology - Paraná</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14932" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14932</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an efficient method to determine the regularization parameter for low-rank Minimum Mean Squared Error (MMSE) filters using a Kronecker-product representation. It demonstrates that this parameter is intrinsically linked to rank selection and is critical for low-rank system performance. Simulation results validate the method, showing it outperforms commonly used approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [AI-Augmented Security Operations Center (AISOC), malware detection, log-anomaly detection, score fusion, Elasticsearch, Kibana, Metasploit, EC2]</li>
<li class=""><strong>authors:</strong> Nnamdi Philip Okonkwo, Lubna Luxmi Dhirani</li>
<li class=""><strong>institution:</strong> University of Limerick</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14935" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14935</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper implements an AI-Augmented Security Operations Center (AISOC) on AWS that fuses outputs from a malware detector and a log-anomaly detector to triage threats. The method uses calibrated score fusion to classify activity as NORMAL, SUSPICIOUS, or HIGH_CONFIDENCE_ATTACK. It concludes that this simple, fused approach can effectively enhance cloud SOC capabilities in cost-sensitive environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Boundary condition enforcement with PINNs: a comparative study and verification on 3D geometries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [physics-informed neural networks (PINNs), boundary condition enforcement, Dirichlet, Neumann, Robin, strong form PDE, finite element method]</li>
<li class=""><strong>authors:</strong> Conor Rowan, Kai Hampleman, Kurt Maute, Alireza Doostan</li>
<li class=""><strong>institution:</strong> University of Colorado Boulder</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14941</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares techniques for enforcing boundary conditions in Physics-Informed Neural Networks (PINNs) on complex 3D geometries and proposes a general solution framework. It verifies the methodology on 3D linear and nonlinear test problems, aiming to establish PINNs as a competitive numerical method against traditional approaches like the finite element method.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [knowledge distillation, byte-pair encoding, cross-tokenizer likelihood scoring, vocabulary misalignment, next-token probability]</li>
<li class=""><strong>authors:</strong> Buu Phan, Ashish Khisti, Karen Ullrich</li>
<li class=""><strong>institution:</strong> University of Toronto, Meta AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14954" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14954</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for cross-tokenizer likelihood scoring to enable knowledge distillation between language models with different vocabularies. It leverages the recursive structure of Byte-Pair Encoding to compute exact or approximate next-token probabilities. The approach reduces memory footprint and improves model performance on tasks like mathematical reasoning compared to baseline distillation methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV-cache management, lossy compression, adaptive eviction, utility function, multi-tier storage]</li>
<li class=""><strong>authors:</strong> Shaoting Feng, Yuhan Liu, Hanchen Li, Xiaokun Chen, Samuel Shen, Kuntai Du, Zhuohan Gu, Rui Zhang, Yuyang Huang, Yihua Cheng, Jiayi Yao, Qizheng Zhang, Ganesh Ananthanarayanan, Junchen Jiang</li>
<li class=""><strong>institution:</strong> University of Chicago, UC Berkeley, Tensormesh, Inc., MIT, UC Santa Cruz, Stanford, Microsoft</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14946" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14946</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EVICPRESS is a KV-cache management system that jointly optimizes lossy compression and adaptive eviction across multiple storage tiers using a unified utility function. It improves LLM inference efficiency by maximizing fast-tier cache hit rates while preserving generation quality through context-aware compression. Evaluations show it achieves up to 2.19x faster time-to-first-token at equivalent quality compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Intrusion Detection in Internet of Vehicles Using Machine Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [intrusion detection system, machine learning, multi-class classification, CAN bus, CICIoV2024 dataset]</li>
<li class=""><strong>authors:</strong> Hop Le, Izzat Alsmadi</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University-San Antonio</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14958</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a machine learning-based intrusion detection system to classify malicious CAN bus traffic in the Internet of Vehicles. It uses the CICIoV2024 benchmark dataset to analyze attack patterns like DoS and spoofing. The initial findings confirm a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Softly Constrained Denoisers for Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [diffusion models, denoiser, soft inductive bias, constraint compliance, guidance, misspecification]</li>
<li class=""><strong>authors:</strong> Victor M. Yeom Song, Severi Rissanen, Arno Solin, Samuel Kaski, Mingfei Sun</li>
<li class=""><strong>institution:</strong> Aalto University, University of Manchester, ELLIS Institute Finland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes softly constrained denoisers for diffusion models, integrating a guidance-inspired adjustment directly into the denoiser to bias samples towards satisfying given constraints. The method improves constraint compliance compared to standard denoisers while remaining flexible enough to deviate from misspecified constraints to better match observed data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prompt Repetition Improves Non-Reasoning LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prompt repetition, causal language model, attention mechanism, non-reasoning tasks]</li>
<li class=""><strong>authors:</strong> Yaniv Leviathan, Matan Kalman, Yossi Matias</li>
<li class=""><strong>institution:</strong> Google Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14982" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14982</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a simple method of repeating the input prompt to improve the performance of LLMs on non-reasoning tasks. This technique allows all prompt tokens to attend to each other within the causal attention mechanism, addressing order sensitivity. The authors demonstrate that this method boosts accuracy for models like Gemini, GPT, Claude, and Deepseek without increasing output length or latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic differential equations], [deep learning, elicitability, Picard iterations, recurrent neural network, feedforward network, McKean-Vlasov FBSDEs]</li>
<li class=""><strong>authors:</strong> Felipe J. P. Antunes, Yuri F. Saporito, Sebastian Jaimungal</li>
<li class=""><strong>institution:</strong> Getulio Vargas Foundation, University of Toronto, University of Oxford</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a new numerical method for solving McKean-Vlasov forward-backward stochastic differential equations with common noise by combining deep learning and elicitability to create a path-wise loss function. The approach uses neural networks to approximate the backward process and conditional expectations, avoiding costly nested Monte Carlo simulations. It is validated on models with known solutions and applied to complex economic models, demonstrating its accuracy and flexibility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [adaptive partitioning, model-based algorithm, diffusion processes, regret bounds, zooming dimension]</li>
<li class=""><strong>authors:</strong> Hanqing Jin, Renyuan Xu, Yanzhao Yang</li>
<li class=""><strong>institution:</strong> University of Oxford, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14991" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14991</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a model-based reinforcement learning algorithm that adaptively partitions the state-action space for controlled diffusion processes with unbounded continuous states. It refines partitions based on estimation bias versus statistical confidence, achieving regret bounds that extend to unbounded domains. The approach is validated through numerical experiments, including high-dimensional portfolio selection.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bug reproduction, LLM, iterative generate-validate-refine, agentic AI]</li>
<li class=""><strong>authors:</strong> Mehil B Shah, Mohammad Masudur Rahman, Foutse Khomh</li>
<li class=""><strong>institution:</strong> Dalhousie University, Polytechnique Montreal</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents RepGen, an automated approach that uses an LLM-based intelligent agent to reproduce deep learning bugs by constructing a learning-enhanced context and employing an iterative generate-validate-refine mechanism. It achieves an 80.19% reproduction rate on real-world bugs, significantly outperforming the state-of-the-art, and a developer study confirms it improves success rates and reduces time and cognitive load for bug reproduction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [Process Reward Model (PRM), Chain-of-Function, meta-learning, label correction, bi-level optimization, test-time scaling, LiveCodeBench]</li>
<li class=""><strong>authors:</strong> Ruiyi Zhang, Peijia Qin, Qi Cao, Pengtao Xie</li>
<li class=""><strong>institution:</strong> University of California, San Diego</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15000" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15000</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DreamPRM-Code, a process reward model for coding that treats functions as reasoning steps using a Chain-of-Function strategy and employs a meta-learning-based label correction mechanism to refine noisy intermediate training labels. It achieves state-of-the-art performance on LiveCodeBench, surpassing OpenAI o4-mini.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [masked language model, fine-tuning, semantic surrogates, deep neural network, BERT]</li>
<li class=""><strong>authors:</strong> Sogol Masoumzadeh, Yufei Li, Shane McIntosh, Dániel Varró, Lili Wei</li>
<li class=""><strong>institution:</strong> McGill University, University of Waterloo, Linköping University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15003" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15003</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SEBERTIS, a framework that fine-tunes bidirectional transformer models (like BERT) as Masked Language Models using semantically equivalent vocabulary (Semantic Surrogates) to create classifiers for security-related issue reports. This method reduces reliance on lexical shortcuts, enabling better detection of complex issues. The resulting classifier significantly outperforms existing ML and LLM baselines in precision, recall, and F1-score, demonstrating high effectiveness for real-time issue triage.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial time series analysis], [deterministic segmentation, event correlation, monotonic price runs, OHLCV data, explainable AI, structural decomposition]</li>
<li class=""><strong>authors:</strong> Sandeep Neela</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15008" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15008</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Stock Pattern Assistant (SPA), a deterministic framework that extracts structural monotonic price runs from daily stock data and correlates them with public events using a symmetric window to generate factual explanations. It demonstrates the method on several equities, showing it provides transparent, reproducible historical analysis without forecasting. The main conclusion is that SPA offers an interpretable tool for decomposing price structure, complementing analyst workflows and explainable-AI pipelines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Epistemic diversity across language models mitigates knowledge collapse</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [model collapse, epistemic diversity, AI ecosystem, self-training, distributed training]</li>
<li class=""><strong>authors:</strong> Damian Hodel, Jevin D. West</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15011" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15011</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper investigates whether diversity across language models (an &quot;AI ecosystem&quot;) can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Spectral Representation-based Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [spectral representations, spectral decomposition, transition operator, partially observable MDPs, model-free, model-based]</li>
<li class=""><strong>authors:</strong> Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai</li>
<li class=""><strong>institution:</strong> Georgia Tech, Harvard University, Google DeepMind, University of Alberta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces spectral representations, derived from the spectral decomposition of the transition operator, as a framework for reinforcement learning to address issues like theoretical ambiguity and optimization instability. It shows how to construct these representations for different system structures and extends the approach to partially observable environments. The proposed algorithms achieve performance comparable to or better than state-of-the-art methods on over 20 challenging control tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Meta-Prompting Protocol, Adversarial Trinity, DSPy, TextGrad, textual gradients, semantic computation graph]</li>
<li class=""><strong>authors:</strong> Fanzhe Fu</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15053" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15053</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Meta-Prompting Protocol, a framework that formalizes LLM orchestration as a programmable system using an adversarial topology (Generator, Auditor, Optimizer) to treat prompts as differentiable variables. It leverages textual critiques as gradients within a semantic computation graph to mitigate hallucination and improve reliability. The authors demonstrate its theoretical viability with tools like DSPy and TextGrad, proposing a foundation for deterministic &quot;Observable Software Engineering&quot; for probabilistic models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [conditional diffusion model, probabilistic forecasting, residual U-Net, cross-attention, uncertainty quantification, imputation-based sampling]</li>
<li class=""><strong>authors:</strong> Zijiang Yan, Yixiang Huang, Jianhua Pei, Hina Tabassum, Luca Chiaraviglio</li>
<li class=""><strong>institution:</strong> York University, Huazhong University of Science and Technology, University of Rome Tor Vergata</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15067</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces EMFusion, a conditional diffusion-based probabilistic forecasting framework that uses a residual U-Net with cross-attention to integrate contextual factors for frequency-selective EMF level prediction. It treats forecasting as an inpainting task to handle irregular data and provides explicit uncertainty estimates. The results show that EMFusion significantly outperforms baseline models in key forecasting metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [retrieval-augmented generation], [conformal prediction, semantic similarity, natural language inference, hallucination detection, text embeddings]</li>
<li class=""><strong>authors:</strong> Debu Sinha</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15068</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies conformal prediction to provide statistical guarantees for hallucination detection in RAG systems, rigorously evaluating embedding-based methods. It finds that while these methods work on synthetic data, they fail on real benchmarks due to the &quot;semantic illusion,&quot; where plausible hallucinations remain semantically similar to source documents. The study concludes that embedding-based detection is insufficient for production, as reasoning-based methods like GPT-4 as a judge perform significantly better.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-label classification], [feature engineering, large language models, code generation, label co-occurrence matrices, Pearson correlation, feedback mechanism]</li>
<li class=""><strong>authors:</strong> Wanfu Gao, Zebin He, Jun Gao</li>
<li class=""><strong>institution:</strong> Jilin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15082" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15082</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FEAML, an automated feature engineering method for multi-label classification that uses LLMs to generate code for creating new features based on metadata and label dependencies. It employs a feedback loop where feature effectiveness is evaluated using model accuracy and redundancy is checked with Pearson correlation to iteratively improve the LLM&#x27;s code generation. Empirical results show that FEAML outperforms other feature engineering methods on various multi-label datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Neural Modular Physics for Elastic Simulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [neural modular physics, modular neural simulator, elastic simulation, physical interpretability, intermediate physical quantities, graph neural networks, physics-informed neural networks]</li>
<li class=""><strong>authors:</strong> Yifei Li, Haixu Wu, Zeyi Xu, Tuur Stuyck, Wojciech Matusik</li>
<li class=""><strong>institution:</strong> MIT CSAIL, Shanghai University, Meta Reality Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15083" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15083</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Neural Modular Physics (NMP), a method that decomposes elastic dynamics into physically meaningful neural modules connected by intermediate physical quantities, combining neural network approximation with traditional simulator reliability. It demonstrates superior generalization, long-horizon stability, and better preservation of physical properties compared to monolithic neural simulators.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [stochastic modeling], [path signatures, multi-head self-attention, convolutional neural network, fractional Brownian motion, parameter estimation, Hurst parameter]</li>
<li class=""><strong>authors:</strong> Xianglin Wu, Chiheb Ben Hammouda, Cornelis W. Oosterlee</li>
<li class=""><strong>institution:</strong> Southwestern University of Finance and Economics, Utrecht University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15088</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SigMA, a neural architecture that combines path signatures with multi-head self-attention for parameter estimation in fractional Brownian motion-driven stochastic differential equations. The method outperforms baseline models like CNN, LSTM, and vanilla Transformer in accuracy and robustness on synthetic and real-world datasets. The results demonstrate that integrating signature transforms with attention provides an effective framework for inference in systems with rough temporal structure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PIP<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Net: Physics-informed Partition Penalty Deep Operator Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific machine learning], [operator learning, DeepONet, partition-of-unity, physics-informed neural networks, regularization, Burgers equation, Allen-Cahn equation]</li>
<li class=""><strong>authors:</strong> Hongjin Mi, Huiqiang Lun, Changhong Mou, Yeyu Zhang</li>
<li class=""><strong>institution:</strong> Shanghai University of Finance and Economics, York University, Utah State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15086" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15086</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes PIP² Net, a physics-informed operator learning network that introduces a simplified partition penalty based on partition-of-unity methods to regularize and stabilize trunk-network features in DeepONet. The method is evaluated on nonlinear PDEs like Burgers and Allen-Cahn equations, where it consistently outperforms baseline models in prediction accuracy and robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph representation learning], [graph convolution, unsupervised learning, node embeddings, homophily, non-homophilic graphs, intra-class similarity, inter-class separability]</li>
<li class=""><strong>authors:</strong> Sunwoo Kim, Soo Yong Lee, Kyungho Kim, Hyunjin Hwang, Jaemin Yoo, Kijung Shin</li>
<li class=""><strong>institution:</strong> KAIST (Korea Advanced Institute of Science and Technology)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15112" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15112</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FUEL, an unsupervised node representation learning method that adaptively adjusts the degree of graph convolution usage to enhance intra-class similarity and inter-class separability in the embedding space, using node feature clusters as proxy classes. It demonstrates state-of-the-art performance across diverse homophily levels in graphs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [sequence modeling], [attention, state space models, unified framework, interaction rank gap, head-count theorem, gradient highway]</li>
<li class=""><strong>authors:</strong> Ali Ghodsi</li>
<li class=""><strong>institution:</strong> University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15115" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15115</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified theoretical framework that connects attention mechanisms and state space models through an input-dependent interaction operator. It proves that representing a linear SSM requires a number of attention heads equal to the subspace dimension of its lag operators, and reveals a trade-off between the algebraic expressivity of the model and its ability to propagate gradients over long sequences.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series imputation], [diffusion model, Fourier transform, attention mechanism, gated convolution, frequency-domain modeling]</li>
<li class=""><strong>authors:</strong> Runze Li, Hanchen Wang, Wenjie Zhang, Binghao Li, Yu Zhang, Xuemin Lin, Ying Zhang</li>
<li class=""><strong>institution:</strong> University of New South Wales, University of Technology Sydney, Shanghai Jiao Tong University, Zhejiang Gongshang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15116" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15116</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FADTI is a diffusion-based framework for multivariate time series imputation that integrates a learnable Fourier Bias Projection module with self-attention and gated convolution to inject frequency-domain inductive bias. It outperforms state-of-the-art methods across multiple benchmarks, especially under high missing rates, by adaptively encoding both stationary and non-stationary patterns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Automatic Reward Shaping from Multi-Objective Human Heuristics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [reward shaping, multi-objective optimization, bi-level optimization, stochastic exploration]</li>
<li class=""><strong>authors:</strong> Yuqing Xie, Jiayu Chen, Wenhao Tang, Ya Zhang, Chao Yu, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15120" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15120</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MORSE, a framework that automatically combines multiple human-designed heuristic rewards into a unified reward function using bi-level optimization with stochastic exploration. It effectively balances conflicting objectives in robotic tasks, achieving performance comparable to manually tuned rewards in MuJoCo and Isaac Sim environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, adversarial training, dataset distillation, model trajectories, privacy-preserving]</li>
<li class=""><strong>authors:</strong> Mukur Gupta, Niharika Gupta, Saifur Rahman, Shantanu Pal, Chandan Karmakar</li>
<li class=""><strong>institution:</strong> Columbia University, Vellore Institute of Technology, Deakin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15123" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15123</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces TrajSyn, a framework that synthesizes a proxy dataset from the trajectories of client model updates in Federated Learning to enable server-side adversarial training without accessing raw client data. It demonstrates that this method improves adversarial robustness on image classification benchmarks without imposing extra computational burden on client devices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [agricultural machine learning], [XGBoost, Random Forest, LSTM, TCN, SHAP, temporal validation, generalization]</li>
<li class=""><strong>authors:</strong> Roland Baatz</li>
<li class=""><strong>institution:</strong> Leibniz Centre for Agricultural Landscape Research (ZALF)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15140" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15140</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study compares ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN) for predicting crop yield and anomalies in Germany, using SHAP for interpretability. It finds that models perform well on conventional test sets but generalize poorly to unseen temporal data, and that feature importance explanations can appear credible even when generalization fails, highlighting the need for validation-aware interpretation in agricultural ML.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [interpretability], [sparse autoencoders, sparse probes, concept disentanglement, steering experiments, multi-concept evaluation]</li>
<li class=""><strong>authors:</strong> Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, Patrik Reizinger</li>
<li class=""><strong>institution:</strong> Boston University, Harvard University, Mila – Quebec AI Institute, Goodfire, University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15134</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-concept evaluation framework to test whether interpretability methods like sparse autoencoders and sparse probes recover disentangled and independently manipulable concept representations. It finds that features often correspond to single concepts, but concepts are distributed across many features, and steering one feature typically affects multiple concepts, indicating a lack of true independence. The results highlight that correlational metrics are insufficient for proving disentanglement and underscore the need for compositional evaluations in interpretability research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] An Efficient Gradient-Based Inference Attack for Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [membership inference attack, federated learning, gradient-based methods, shadow training, attribute inference]</li>
<li class=""><strong>authors:</strong> Pablo Montaña-Fernández, Ines Ortega-Fernandez</li>
<li class=""><strong>institution:</strong> Gradiant</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15143" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15143</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a new gradient-based inference attack for federated learning that analyzes the temporal evolution of last-layer gradients across multiple training rounds. The attack uses a shadow model technique to learn gradient patterns and can be extended to attribute inference. The findings show that multi-round federated learning increases vulnerability to such attacks, with aggregators posing a greater threat than data owners.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Understanding NTK Variance in Implicit Neural Representations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural representations], [Neural Tangent Kernel, positional encoding, spherical normalization, Hadamard modulation, spectral bias]</li>
<li class=""><strong>authors:</strong> Chengguang Ou, Yixin Zhuang</li>
<li class=""><strong>institution:</strong> Fuzhou University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15169</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes how architectural choices in Implicit Neural Representations (INRs), such as positional encoding and normalization, affect the conditioning of the Neural Tangent Kernel (NTK) to mitigate spectral bias. It shows these components reduce NTK eigenvalue variance, leading to improved convergence and better high-frequency detail recovery. Experiments confirm faster convergence and enhanced reconstruction quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] BEAT2AASIST model with layer fusion for ESDD 2026 Challenge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [audio deepfake detection], [BEATs, AASIST, transformer layer fusion, vocoder-based data augmentation, dual-branch architecture]</li>
<li class=""><strong>authors:</strong> Sanghyeok Chung, Eujin Kim, Donggun Kim, Gaeun Heo, Jeongbin You, Nahyun Lee, Sunmook Choi, Soyul Han, Seungsang Oh, Il-Youp Kwak</li>
<li class=""><strong>institution:</strong> Korea University, Chung-Ang University, Cornell University, Hannam University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BEAT2AASIST, a model for environmental sound deepfake detection that extends BEATs-AASIST by splitting features into dual AASIST branches and incorporating transformer layer fusion strategies. It also uses vocoder-based data augmentation for robustness. The approach demonstrates competitive performance on the ESDD 2026 Challenge test sets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] DEER: Draft with Diffusion, Verify with Autoregressive Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, diffusion large language model, parallel decoding, draft-verify scheme, two-stage training, single-step decoding]</li>
<li class=""><strong>authors:</strong> Zicong Cheng, Guo-Wei Yang, Jia Li, Zhijie Deng, Meng-Hao Guo, Shi-Min Hu</li>
<li class=""><strong>institution:</strong> Tsinghua University, Proxseer Inc, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15176" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15176</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DEER, a speculative decoding framework that uses a diffusion large language model (dLLM) as a parallel drafter to generate candidate tokens, which are then verified by a target autoregressive model. The method overcomes the sequential bottleneck and trust collapse of traditional AR drafters through a two-stage training pipeline and single-step decoding. Experiments show DEER achieves significantly longer draft acceptance and higher speedups (e.g., 5.54x) compared to prior methods like EAGLE-3.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [cross-modal reconstruction, context-aware gating, context-caching, domain generalization, data-free customization]</li>
<li class=""><strong>authors:</strong> Liyu Zhang, Yejia Liu, Kwun Ho Liu, Runxi Huang, Xiaomin Ouyang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15206" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15206</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Chorus, a method that learns context representations via unsupervised cross-modal reconstruction and uses a lightweight gated head to dynamically integrate sensor and context information for model adaptation without target data. It demonstrates improved performance in unseen IoT contexts and maintains low latency on edge devices through a context-caching mechanism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Label-consistent clustering for evolving data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [clustering algorithms], [k-center, approximation algorithms, label-consistent clustering]</li>
<li class=""><strong>authors:</strong> Ameet Gadekar, Aristides Gionis, Thibault Marette</li>
<li class=""><strong>institution:</strong> CISPA Helmholtz Center for Information Security, KTH Royal Institute of Technology, Digital Futures</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15210</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the label-consistent k-center problem, which aims to update an existing clustering solution with new data while limiting changes to a given budget. It introduces two constant-factor approximation algorithms for this problem. The experimental evaluation on real-world datasets demonstrates the effectiveness of the proposed methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational catalysis], [deep generative model, periodic Brownian-bridge, equivariant graph neural network, DFT relaxation, adsorption energy, outlier detection]</li>
<li class=""><strong>authors:</strong> Songze Huo, Xiao-Ming Cao</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15228" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15228</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DBCata, a deep generative model that combines a periodic Brownian-bridge framework with an equivariant graph neural network to directly generate equilibrium adsorption structures from unrelaxed ones, bypassing explicit energy or force calculations. It achieves high-fidelity geometry generation and improves DFT accuracy, enabling accelerated high-throughput screening for catalysts like those in the oxygen reduction reaction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [EEND-EDA, RNN-based stitching, centroid refinement decoder, permutation-invariant training (PIT), online speaker diarization]</li>
<li class=""><strong>authors:</strong> Elio Gruttadauria, Mathieu Fontaine, Jonathan Le Roux, Slim Essid</li>
<li class=""><strong>institution:</strong> Télécom Paris, Institut polytechnique de Paris, Mitsubishi Electric Research Laboratories (MERL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15229" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15229</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces O-EENC-SD, an efficient online end-to-end neural clustering system for speaker diarization. It is based on EEND-EDA and features a novel RNN-based stitching mechanism and a centroid refinement decoder for online prediction. The system is shown to be competitive with state-of-the-art methods on the CallHome dataset while offering a better trade-off between diarization error rate and computational complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [visual enumeration, vision-language models, zero-shot counting, intermediate representations, object counting]</li>
<li class=""><strong>authors:</strong> Kuinan Hou, Jing Mi, Marco Zorzi, Lamberto Ballan, Alberto Testolin</li>
<li class=""><strong>institution:</strong> University of Padova</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically compares specialized counting architectures and vision-language models (VLMs) for visual enumeration. It finds that VLMs can match or surpass specialized models, especially when prompted to generate intermediate object representations, but still struggle with complex scenes, indicating a need for further research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [self-supervised pretraining, dual-masking strategy, embedding concatenation, foundational models, CBraMod encoder]</li>
<li class=""><strong>authors:</strong> Youssef Ghallab, Omar Iraqy, Mohamed Kandil, Mohamed Ashraf, Saadeldine Eletter, Morougue Ghazal, Ayman Khalafallah, Nagwa El-Makky</li>
<li class=""><strong>institution:</strong> Alexandria University, Mohamed bin Zayed University of Artificial Intelligence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15250" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15250</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for multi-modal physiological signal analysis by adapting a self-supervised foundational model (CBraMod) for ECG and EEG, using a dual-masking strategy for ECG, and fusing the modalities via simple embedding concatenation. The approach achieves near state-of-the-art performance in emotion recognition, demonstrating that well-designed foundational encoders with straightforward fusion can effectively leverage limited multi-modal data. The results highlight the potential of foundation-model approaches for scalable and label-efficient solutions in healthcare and affective computing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [continual learning], [selective subnetwork distillation, sparse distributed memory, top-k activation, knowledge distillation, catastrophic forgetting]</li>
<li class=""><strong>authors:</strong> Huiyan Xue, Xuming Ran, Yaxin Li, Qi Xu, Enhui Li, Yi Xu, Qiang Zhang</li>
<li class=""><strong>institution:</strong> Dalian University of Technology, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15267" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15267</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Selective Subnetwork Distillation (SSD), a continual learning method that uses distillation to transfer knowledge between task-specific sparse subnetworks in a Sparse Distributed Memory MLP, without needing replay or task labels. The method improves accuracy and retention by enabling structural realignment and cross-task knowledge reuse while preserving sparsity. Experiments on benchmark datasets show SSD offers a structurally grounded solution to mitigate forgetting in sparse continual learning systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Model inference for ranking from pairwise comparisons</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [ranking algorithms], [Bayesian inference, pairwise comparisons, latent strength, model inference, Chebyshev approximation, neural networks]</li>
<li class=""><strong>authors:</strong> Daniel Sánchez Catalina, George T. Cantwell</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15269" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15269</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an efficient Bayesian algorithm to jointly infer both the latent strengths of objects and the unknown function mapping strengths to win probabilities from pairwise comparison data. The method uses Chebyshev-based and neural network approaches to model this function without prior assumptions. Experimental results show robustness across model specifications and improved predictive accuracy, such as overcoming bookmaker profit margins in tennis data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Topological Metric for Unsupervised Embedding Quality Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [representation learning evaluation], [persistent homology, unsupervised metric, topological data analysis, embedding quality]</li>
<li class=""><strong>authors:</strong> Aleksei Shestov, Anton Klenitskiy, Daria Denisova, Amurkhan Dzagkoev, Daniil Petrovich, Andrey Savchenko, Maksim Makarenko</li>
<li class=""><strong>institution:</strong> Sber AI Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15285" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15285</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes &quot;Persistence&quot;, a fully unsupervised metric for evaluating embedding quality based on persistent homology from topological data analysis. It captures the multi-scale geometric and topological structure of embedding spaces. Empirical results show it achieves strong correlation with downstream task performance, outperforming existing unsupervised metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), Quantum Generative Adversarial Networks (QGANs)]</li>
<li class=""><strong>authors:</strong> Siva Sai, Ishika Goyal, Shubham Sharma, Sri Harshita Manuri, Vinay Chamola, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne, Birla Institute of Technology and Science, Pilani, APPCAIR</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15286</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper explores Quantum Machine Learning (QML) techniques, including QNNs, QSVMs, VQCs, and QGANs, for cybersecurity applications like intrusion detection and malware classification. It concludes that QML offers potential advantages for processing high-dimensional data and enhancing security in areas like cloud computing, but also discusses current limitations and future research directions needed to address them.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [8-bit training, activation checkpointing, offloading, copy-engine collectives, dynamic tensor-level scaling, ZeRO-1]</li>
<li class=""><strong>authors:</strong> Erik Schultheis, Dan Alistarh</li>
<li class=""><strong>institution:</strong> IST Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Time-Varying Audio Effect Modeling by End-to-End Adversarial Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Generative Adversarial Network (GAN), convolutional-recurrent architecture, adversarial training, State Prediction Network (SPN), chirp-train signals]</li>
<li class=""><strong>authors:</strong> Yann Bourdin, Pierrick Legrand, Fanny Roche</li>
<li class=""><strong>institution:</strong> Arturia, Inria, IMS, University of Bordeaux, Bordeaux INP</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15313" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15313</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-stage GAN framework for black-box modeling of time-varying audio effects, using only input-output recordings without needing control signals. The method involves an adversarial training phase followed by supervised fine-tuning with a State Prediction Network for synchronization. Experiments on a vintage phaser demonstrate the approach&#x27;s effectiveness in capturing time-varying dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [post-training quantization, membership inference, AdaRound, BRECQ, OBC, quantization, privacy-utility trade-off]</li>
<li class=""><strong>authors:</strong> Chenxiang Zhang, Tongxi Qu, Zhong Li, Tian Zhang, Jun Pang, Sjouke Mauw</li>
<li class=""><strong>institution:</strong> University of Luxembourg, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15335" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15335</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates how post-training quantization (PTQ) affects privacy by using membership inference attacks on models quantized with algorithms like AdaRound, BRECQ, and OBC. The study finds that lower-precision PTQ (e.g., 4-bit, 2-bit, 1.58-bit) significantly reduces privacy leakage, offering up to an order of magnitude less vulnerability compared to full-precision models, though at the cost of decreased utility. The results provide practical insights for balancing efficiency, accuracy, and privacy in model deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [phase adjustment, three-axis independent alignment, single-axis reference alignment, deep learning, vibration signals, predictive maintenance]</li>
<li class=""><strong>authors:</strong> Hiroyoshi Nagahama, Katsufumi Inoue, Masayoshi Todorokihara, Michifumi Yoshioka</li>
<li class=""><strong>institution:</strong> Osaka Metropolitan University, Seiko Epson Corp.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15344" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15344</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two phase-aware preprocessing strategies—three-axis independent and single-axis reference phase adjustment—to handle random phase variations in multi-axis vibration data for fault diagnosis. The methods are evaluated using a new rotor dataset and six deep learning models, showing consistent performance improvements, with the single-axis reference approach achieving up to 96.2% accuracy by preserving spatial phase relationships. The findings demonstrate that these phase alignment strategies are practical and scalable enhancements for predictive maintenance systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Group Relative Policy Optimization (GRPO), Optimal Variance Filtering (OVF), Pro-GRPO, Expand-and-Prune, reward clustering, latent feature-based pruning]</li>
<li class=""><strong>authors:</strong> Shiran Ge, Chenyi Huang, Yuang Ai, Qihang Fan, Huaibo Huang, Ran He</li>
<li class=""><strong>institution:</strong> Institute of Automation, Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15347</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Pro-GRPO, a dynamic framework that improves Group Relative Policy Optimization (GRPO) for generative model alignment by integrating an &quot;Expand-and-Prune&quot; strategy. It first expands the sampling group for diversity and then prunes reward-clustered trajectories early using latent features to reduce computational cost. Experiments on diffusion and flow models show that Pro-GRPO is more efficient and effective than standard GRPO.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [adaptive computation, early exit, dual-path training, image complexity classification, ConvNeXt-IC]</li>
<li class=""><strong>authors:</strong> Mikel Williams-Lekuona, Georgina Cosma</li>
<li class=""><strong>institution:</strong> Loughborough University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15372" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15372</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Regime-Aware Fusion Framework for Time Series Classification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [time series classification], [ROCKET, SAX, SFA, SHAP, representation fusion, meta-features]</li>
<li class=""><strong>authors:</strong> Honey Singh Chauhan, Zahraa S. Abdallah</li>
<li class=""><strong>institution:</strong> University of Bristol</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15378" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15378</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Fusion-3 (F3), a lightweight framework that adaptively fuses ROCKET, SAX, and SFA representations for time series classification. It shows that this selective fusion yields consistent improvements over ROCKET on specific types of datasets, which are identified by clustering datasets into interpretable regimes based on meta-features.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Remotely Detectable Robot Policy Watermarking</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [policy watermarking, colored noise coherency, glimpse sequence, remote detection, spectral signal, stochasticity]</li>
<li class=""><strong>authors:</strong> Michael Amir, Manon Flageat, Amanda Prorok</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15379" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15379</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Colored Noise Coherency (CoNoCo), a watermarking method that embeds a spectral signal into a robot&#x27;s motions using the policy&#x27;s inherent stochasticity for remote detection. It is designed to be detectable from noisy external observations like video footage without degrading policy performance. The work demonstrates robust detection across various remote modalities, providing a non-invasive way to verify the provenance of physical robot policies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [fault classification, fault localization, robustness evaluation, EMT simulation, sensor degradation]</li>
<li class=""><strong>authors:</strong> Julian Oelhaf, Mehran Pashaei, Georg Kordowich, Christian Bergler, Andreas Maier, Johann Jäger, Siming Bayer</li>
<li class=""><strong>institution:</strong> Friedrich-Alexander-Universität Erlangen-Nürnberg, Ostbayerische Technische Hochschule Amberg-Weiden</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15385" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15385</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework using high-fidelity EMT simulations to systematically evaluate the robustness of machine learning models for fault classification and localization in power systems under degraded sensor data conditions. The main conclusion is that fault classification is generally stable but fault localization is highly sensitive, with voltage data loss increasing error by over 150%, providing guidance for designing resilient ML-assisted protection systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-view clustering], [contrastive learning, distribution alignment, semantic graph, partially view-aligned clustering]</li>
<li class=""><strong>authors:</strong> Liang Peng, Yixuan Ye, Cheng Liu, Hangjun Che, Fei Wang, Zhiwen Yu, Si Wu, Hau-San Wong</li>
<li class=""><strong>institution:</strong> Shantou University, Huaqiao University, Southwest University, South China University of Technology, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15396" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15396</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SMART, a model for partially view-aligned clustering that uses view distribution alignment and semantic matching contrastive learning to handle misaligned multi-view data. It aligns cross-view covariance matrices to reduce distribution shifts and leverages a semantic graph to guide contrastive learning, improving clustering performance. Experiments on eight datasets show that SMART outperforms existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [bayesian reinforcement learning, epistemic uncertainty, minimax-optimal regret, sample complexity, infinite-horizon MDPs]</li>
<li class=""><strong>authors:</strong> Jianfei Ma, Wee Sun Lee</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15405" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15405</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EUBRL, a Bayesian reinforcement learning algorithm that uses epistemic uncertainty to guide exploration and reduce per-step regret. It provides theoretical guarantees for regret and sample complexity and demonstrates superior sample efficiency and scalability in sparse-reward, long-horizon tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [flow-based generative models, flow-matching, invertible flows, shared latent space, any-to-any generation]</li>
<li class=""><strong>authors:</strong> Yeonwoo Cha, Semin Kim, Jinhyeon Kwon, Seunghoon Hong</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15420</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlowBind is an efficient framework for any-to-any cross-modal generation that learns a shared latent space and uses modality-specific invertible flows, all optimized jointly with a single flow-matching objective. It reduces data requirements and computational cost by factorizing interactions through the latent space. Experiments show it achieves comparable quality to prior methods while using up to 6x fewer parameters and training 10x faster.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Statistics of Min-max Normalized Eigenvalues in Random Matrices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [random matrix theory], [min-max normalization, eigenvalue distribution, matrix factorization, cumulative distribution, scaling law]</li>
<li class=""><strong>authors:</strong> Hyakka Nakada, Shu Tanaka</li>
<li class=""><strong>institution:</strong> Keio University, Waseda University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15427" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15427</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies the statistical properties of min-max normalized eigenvalues in random matrices. It applies a proposed effective distribution to evaluate a scaling law for the cumulative distribution and derives the residual error from matrix factorization. Numerical experiments are conducted to verify these theoretical predictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [feature model-based reinforcement learning, actor-critic, Dyna-Q, model-based RL, model-free RL, multi-task control]</li>
<li class=""><strong>authors:</strong> Quanxi Zhou, Wencan Mao, Manabu Tsukada, John C.S. Lui, Yusheng Ji</li>
<li class=""><strong>institution:</strong> The University of Tokyo, National Institute of Informatics, The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15430" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15430</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FM-EAC, a feature model-based enhanced actor-critic algorithm that integrates planning, acting, and learning for multi-task control. It combines model-based and model-free reinforcement learning to improve generalizability across tasks. Simulations show it outperforms state-of-the-art methods in urban and agricultural applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Double Horizon Model-Based Policy Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [model-based reinforcement learning, policy optimization, distribution rollout, training rollout, double horizon]</li>
<li class=""><strong>authors:</strong> Akihiro Kubo, Paavo Parmas, Shin Ishii</li>
<li class=""><strong>institution:</strong> Advanced Telecommunications Research Institute, Kyoto University, The University of Tokyo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15439</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Double Horizon Model-Based Policy Optimization (DHMBPO), a method that separates the model rollout process into a long &quot;distribution rollout&quot; to mitigate distribution shift and a short &quot;training rollout&quot; for stable gradient estimation. This approach balances model bias and gradient variance. The method demonstrates superior sample efficiency and runtime compared to existing MBRL methods on continuous-control benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion inference], [chain-of-thought prompting, task instruction prompting, negative prompting, prompt re-writing, copyright mitigation]</li>
<li class=""><strong>authors:</strong> Neeraj Sarna, Yuanyuan Li, Michael von Gablenz</li>
<li class=""><strong>institution:</strong> Munich RE</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15442" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15442</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes combining chain-of-thought and task instruction prompting with negative prompting and prompt re-writing to reduce the generation of copyrighted content by text-to-image models. It evaluates the generated images based on their similarity to copyrighted material and relevance to the user input. The experiments across various models provide insights into the effectiveness of these techniques for different model complexities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Invertibility Loss, Jacobian matrix, spectral properties, adaptive noise perturbation, data reconstruction attacks, federated learning]</li>
<li class=""><strong>authors:</strong> Xiangrui Xu, Zhize Li, Yufei Han, Bin Wang, Jiqiang Liu, Wei Wang</li>
<li class=""><strong>institution:</strong> Beijing Jiaotong University, Singapore Management University, INRIA Rennes-Bretagne-Atlantique, Zhejiang Key Laboratory of AIoT Network and Data Security, Xi&#x27;an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15460" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15460</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a theoretical framework to assess and mitigate data reconstruction attacks in federated learning by proposing Invertibility Loss (InvLoss) to quantify attack risk and deriving a computable upper bound. The authors develop an attack-agnostic risk estimator and two adaptive noise perturbation defenses based on this framework. Experiments show the framework effectively evaluates and reduces privacy risks without harming model accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [metanetworks, graph neural networks, model editing, bias mitigation, weight pruning, data minimisation]</li>
<li class=""><strong>authors:</strong> Ioannis Kalogeropoulos, Giorgos Bouritsas, Yannis Panagakis</li>
<li class=""><strong>institution:</strong> National and Kapodistrian University of Athens, Archimedes/Athena RC, Visible Machines AI Research &amp; Social Awareness Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15469" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15469</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework using graph metanetworks to edit neural networks in a single inference step, aiming to enforce requirements like fairness or efficiency while preserving model utility. It demonstrates improved trade-offs in performance, compliance, and time efficiency compared to traditional post-processing or retraining methods across tasks such as bias mitigation and pruning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimisation, multi-stage workflows, proxy measurements, self-driving labs, autonomous experimentation]</li>
<li class=""><strong>authors:</strong> Luca Torresi, Pascal Friederich</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15483" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15483</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a multi-stage Bayesian optimisation method that uses intermediate proxy measurements to make dynamic decisions in self-driving labs. The method allows for flexible sampling of complex experimental workflows. The authors conclude that using proxy measurements significantly improves both the speed and optimality of finding solutions compared to conventional Bayesian optimisation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [machine learning reliability], [robustness quantification, uncertainty quantification, classifier reliability, epistemic uncertainty]</li>
<li class=""><strong>authors:</strong> Adrián Detavernier, Jasper De Bock</li>
<li class=""><strong>institution:</strong> Ghent University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15492</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares two methods for assessing the reliability of individual classifier predictions: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). It finds that neither method is universally superior; instead, they are complementary. A hybrid approach combining RQ and UQ is shown to outperform each method individually.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [transformer, multi-head self-attention, positional encoding, precision-focused loss, edge deployment, TensorFlow Lite, ONNX, TensorRT]</li>
<li class=""><strong>authors:</strong> Konstantinos Kalogiannis, Ahmed Mohamed Hussain, Hexu Li, Panos Papadimitratos</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Lenovo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15503" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15503</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons. It uses multi-head self-attention to capture spatiotemporal dependencies and a precision-focused loss to minimize false positives. The method demonstrates high performance and achieves sub-millisecond inference latency, making it suitable for deployment on resource-constrained edge platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Soft Geometric Inductive Bias for Object Centric Dynamics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [geometric deep learning], [geometric algebra neural networks, soft geometric inductive bias, equivariance, object-centric world models, autoregressive training, long-horizon rollouts]</li>
<li class=""><strong>authors:</strong> Hampus Linander, Conor Heins, Alexander Tschantz, Marco Perin, Christopher Buckley</li>
<li class=""><strong>institution:</strong> VERSES AI, University of Sussex</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15493" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15493</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using object-centric world models built with geometric algebra neural networks to provide a soft geometric inductive bias for learning physical dynamics. The method is evaluated on 2D rigid body simulations and shows that this soft bias leads to better long-horizon prediction fidelity compared to non-equivariant baselines, effectively balancing between strict symmetry constraints and unstructured learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Conditioned UNet for Music Source Separation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [conditioned UNet, music source separation, QSCNet, Sparse Compressed Network, Bandsplit RNN, Banquet, MoisesDb]</li>
<li class=""><strong>authors:</strong> Ken O&#x27;Hanlon, Basil Woods, Lin Wang, Mark Sandler</li>
<li class=""><strong>institution:</strong> Queen Mary University of London, AudioStrip Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Tracking Temporal Dynamics of Vector Sets with Gaussian Process</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [temporal analysis], [Gaussian Process, Random Fourier Features, vector sets, temporal dynamics, low-dimensional visualization]</li>
<li class=""><strong>authors:</strong> Taichi Aida, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi</li>
<li class=""><strong>institution:</strong> Tokyo Metropolitan University, Hitotsubashi University, National Institute for Japanese Language and Linguistics, National Institute of Advanced Industrial Science and Technology, The Institute of Statistical Mathematics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15538" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15538</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method to model time-varying vector sets using infinite-dimensional Gaussian Processes, approximating the latent function with Random Fourier Features to obtain compact, comparable representations over time. It demonstrates effectiveness in capturing temporal dynamics in crime distributions and word embeddings, providing interpretable, low-dimensional visualizations of structural changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [unsupervised learning], [multi-view learning, feature selection, instance selection, data imputation, cross-view neighborhood, data reconstruction]</li>
<li class=""><strong>authors:</strong> Yuxin Cai, Yanyong Huang, Jinyuan Chang, Dongjie Wang, Tianrui Li, Xiaoyi Jiang</li>
<li class=""><strong>institution:</strong> Southwestern University of Finance and Economics, University of Kansas, Southwest Jiaotong University, University of Münster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15574</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes JUICE, a method that jointly performs unsupervised feature and instance co-selection and cross-view data imputation for incomplete multi-view data in a unified framework. It uses cross-view neighborhood information to refine missing data imputation during reconstruction, which improves the selection of representative features and instances. Experiments show that JUICE outperforms existing state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [autonomous driving perception], [knowledge distillation, camera-radar fusion, intensity-aware distillation, multi-level distillation, BEV representation]</li>
<li class=""><strong>authors:</strong> Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</li>
<li class=""><strong>institution:</strong> German Research Center for Artificial Intelligence (DFKI), RPTU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15581" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15581</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes IMKD, an intensity-aware multi-level knowledge distillation framework for camera-radar 3D object detection. It uses a three-stage distillation strategy from LiDAR to enhance radar and fused representations while preserving each sensor&#x27;s unique characteristics. Experiments on nuScenes show IMKD outperforms prior distillation-based fusion methods, achieving 67.0% NDS and 61.0% mAP.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Corrective Diffusion Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [masked diffusion language model, corrective behavior, code revision benchmark, confidence-guided refinement, correction-oriented post-training]</li>
<li class=""><strong>authors:</strong> Shuibai Zhang, Fred Zhangzhi Peng, Yiheng Zhang, Jin Pan, Grigorios G. Chrysos</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, Duke University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15596" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15596</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a correction-oriented post-training method for diffusion language models to enable error-aware confidence and targeted refinement, which standard masked diffusion training fails to achieve. The method explicitly supervises visible incorrect tokens to improve corrective behavior. Experiments show the approach substantially outperforms standard models in correction tasks while also enhancing completion performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Evaluating Large Language Models in Scientific Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [scientific discovery evaluation], [scenario-grounded benchmark, two-phase evaluation, hypothesis generation, experiment design, result interpretation]</li>
<li class=""><strong>authors:</strong> Zhangde Song, Jieyu Lu, Yuanqi Du, Botao Yu, Thomas M. Pruyn, Yue Huang, Kehan Guo, Xiuzhe Luo, Yuanhao Qu, Yi Qu, Yinkai Wang, Haorui Wang, Jeff Guo, Jingru Gan, Parshin Shojaee, Di Luo, Andres M Bran, Gen Li, Qiyuan Zhao, Shao-Xiong Lennon Luo, Yuxuan Zhang, Xiang Zou, Wanru Zhao, Yifan F. Zhang, Wucheng Zhang, Shunan Zheng, Saiyang Zhang, Sartaaj Takrim Khan, Mahyar Rajabi-Kochi, Samantha Paradi-Maropakis, Tony Baltoiu, Fengyu Xie, Tianyang Chen, Kexin Huang, Weiliang Luo, Meijing Fang, Xin Yang, Lixue Cheng, Jiajun He, Soha Hassoun, Xiangliang Zhang, Wei Wang, Chandan K. Reddy, Chao Zhang, Zhiling Zheng, Mengdi Wang, Le Cong, Carla P. Gomes, Chang-Yu Hsieh, Aditya Nandy, Philippe Schwaller, Heather J. Kulik, Haojun Jia, Huan Sun, Seyed Mohamad Moosavi, Chenru Duan</li>
<li class=""><strong>institution:</strong> Deep Principle, Cornell University, The Ohio State University, University of Toronto, University of Notre Dame, QuEra Computing Inc., Stanford University, Harvard Law School, Tufts University, Georgia Institute of Technology, Ecole Polytechnique Federale de Lausanne, University of California, Los Angeles, Virginia Tech, Tsinghua University, Princeton University, Harvard University, University of Cambridge, The University of Texas at Austin, McGill University, University of Science and Technology of China, Massachusetts Institute of Technology, Zhejiang University, The Hong Kong University of Science and Technology, Washington University in St. Louis</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15567" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15567</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a two-phase Scientific Discovery Evaluation (SDE) framework, which uses scenario-grounded benchmarks across multiple scientific domains to assess LLMs on question-level accuracy and project-level tasks like hypothesis generation and experiment design. It concludes that current LLMs show a significant performance gap in scientific discovery compared to general benchmarks, exhibit diminishing returns from scaling, and are far from being general scientific &quot;superintelligences,&quot; though they still demonstrate promise in various discovery projects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Smoothing is N-simplicial Attention?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [attention mechanisms], [N-simplicial attention, higher-order interactions, Rotary Position Embeddings (RoPE), simplex selection, over-smoothing, Lipschitz bound]</li>
<li class=""><strong>authors:</strong> Alexandre Dussolle, Pietro Liò</li>
<li class=""><strong>institution:</strong> University of Cambridge, École des Ponts, IP Paris</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15600" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15600</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces N-simplicial attention, a method that generalizes standard attention to higher-order token interactions and adapts it for Rotary Position Embeddings (RoPE). It also proposes a cost-effective simplex selection mechanism to manage computational complexity. The authors demonstrate that, despite enabling higher-order interactions, N-simplicial attention itself suffers from over-smoothing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [language models], [autoregressive models, energy-based models, soft Bellman equation, maximum entropy reinforcement learning, distillation]</li>
<li class=""><strong>authors:</strong> Mathieu Blondel, Michael E. Sander, Germain Vivier-Ardisson, Tianlin Liu, Vincent Roulet</li>
<li class=""><strong>institution:</strong> Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15605" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15605</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes a formal bijection between autoregressive language models (ARMs) and energy-based models (EBMs) in function space, showing this equivalence corresponds to a special case of the soft Bellman equation in maximum entropy RL. It demonstrates the equivalence of their supervised learning and provides theoretical error bounds for distilling an EBM into an ARM. The results explain how ARMs, despite being trained on next-token prediction, can implicitly perform lookahead planning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [explainable recommendation], [behavior tokens, vector-quantized autoencoding, semantic alignment regularization, zero-shot recommendation]</li>
<li class=""><strong>authors:</strong> Xinshun Feng, Mingzhe Liu, Yi Qiao, Tongyu Zhu, Leilei Sun, Shuai Wang</li>
<li class=""><strong>institution:</strong> Beihang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15614" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15614</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BEAT, a framework that tokenizes user and item behaviors into discrete, interpretable sequences using a behavior vocabulary constructed via vector-quantized autoencoding. It introduces multi-level semantic supervision and semantic alignment regularization to bridge behavioral signals with language models. Experiments show BEAT improves zero-shot recommendation performance and generates coherent explanations, demonstrating that behavior tokens effectively capture fine-grained semantics for integration with large language models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Low-Rank Adaptation (LoRA), supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), rank sweep, representational drift, attention patterns]</li>
<li class=""><strong>authors:</strong> Darshita Rathore, Vineet Kumar, Chetna Bansal, Anindya Moitra</li>
<li class=""><strong>institution:</strong> PayPal</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15634</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper comprehensively evaluates the trade-offs between full supervised fine-tuning (SFT) and Low-Rank Adaptation (LoRA) for fine-tuning large language models. It finds that LoRA, especially at specific rank values, can achieve competitive or even superior performance to SFT on reasoning tasks, while also analyzing the structural changes in model representations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [continual learning], [energy-based model, progressive parameter selection, pseudo-sample generation, catastrophic forgetting mitigation]</li>
<li class=""><strong>authors:</strong> Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Feng Mi, Latifur Khan</li>
<li class=""><strong>institution:</strong> Mayo Clinic, Google, Texas A&amp;M University, The University of Texas at Dallas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15658" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15658</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PPSEBM, a framework combining an Energy-Based Model with Progressive Parameter Selection to address catastrophic forgetting in continual learning for NLP tasks. It uses task-specific parameters and generates pseudo-samples from prior tasks to retain past knowledge. Experimental results show PPSEBM outperforms state-of-the-art methods in mitigating forgetting.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] SoFlow: Solution Flow Models for One-Step Generative Modeling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [diffusion training], [Flow Matching, Classifier-Free Guidance, solution consistency loss, velocity ODE, one-step generation, Diffusion Transformer]</li>
<li class=""><strong>authors:</strong> Tianze Luo, Haotian Yuan, Zhuang Liu</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15657" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15657</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SoFlow, a framework for one-step generative modeling that uses a Flow Matching loss and a solution consistency loss to train models without requiring Jacobian-vector product calculations. It improves training efficiency by enabling Classifier-Free Guidance and achieves better FID-50K scores than MeanFlow models on ImageNet 256x256 when using the same DiT architecture and training epochs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [llm interpretability], [LatentQA, Activation Oracles, activation analysis, fine-tuning detection, natural language queries]</li>
<li class=""><strong>authors:</strong> Adam Karvonen, James Chua, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks</li>
<li class=""><strong>institution:</strong> MATS, Truthful AI, EPFL, ENS Paris-Saclay, Northeastern University, Anthropic</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15674</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Activation Oracles, models trained using the LatentQA approach to answer natural language questions about the internal activations of other LLMs. The core finding is that these oracles, especially when trained on diverse datasets, can generalize to out-of-distribution tasks and effectively verbalize hidden information, such as knowledge from fine-tuning, often matching or exceeding prior white-box interpretability methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Stylized Synthetic Augmentation further improves Corruption Robustness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computer vision], [data augmentation, neural style transfer, synthetic data, corruption robustness, TrivialAugment]</li>
<li class=""><strong>authors:</strong> Georg Siedel, Rojan Regmi, Abhirami Anand, Weijia Shao, Silvia Vock, Andrey Morozov</li>
<li class=""><strong>institution:</strong> University of Stuttgart, Federal Institute for Occupational Safety and Health (BAuA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer to improve the corruption robustness of deep vision models. It finds that stylizing synthetic images, despite lowering their FID score, is beneficial for training, and that this method can be effectively combined with rule-based augmentations like TrivialAugment. The approach achieves state-of-the-art robust accuracy on several image classification benchmarks under common corruptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [reinforcement learning], [gradient-guided reinforcement learning, G2RL, PPO, KL control, final-layer sensitivity]</li>
<li class=""><strong>authors:</strong> Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu</li>
<li class=""><strong>institution:</strong> Tencent AI Lab, University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces G2RL, a gradient-guided reinforcement learning framework that uses the model&#x27;s own gradient directions to guide exploration, rather than external heuristics like entropy bonuses. It shows that G2RL improves reasoning performance across multiple benchmarks by encouraging diverse and orthogonal update directions. The results indicate that a policy&#x27;s internal update geometry provides a more effective basis for exploration in LLM reinforcement learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [anomaly detection], [multivariate statistical analysis, Hotelling&#x27;s T^2 statistic, whitening transformation, Mahalanobis space, Laplacian interpolation, hypothesis testing]</li>
<li class=""><strong>authors:</strong> Oleg Melnikov, Yurii Dorofieiev, Yurii Shakhnovskiy, Huy Truong, Victoria Degeler</li>
<li class=""><strong>institution:</strong> Not explicitly provided; inferred from author names and context (likely academic/research institution in Ukraine and international collaboration, but cannot be confirmed without explicit affiliations)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15685" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15685</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SICAMS, a multivariate statistical framework that uses a whitening transformation and Hotelling&#x27;s T^2 statistic to detect, classify, and pre-localize anomalies like leaks in water distribution networks. The method demonstrates high sensitivity and reliability on benchmark data, enabling leak detection and size estimation without requiring a calibrated hydraulic model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Multi-Modal Semantic Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [semantic communication, cross-modal attention, transformer, adaptive resolution encoding, bandwidth-constrained transmission]</li>
<li class=""><strong>authors:</strong> Matin Mortaheb, Erciyes Karakaya, Sennur Ulukus</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15691" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15691</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-modal semantic communication framework that uses text-based user queries and a cross-modal attention mechanism to guide the extraction and adaptive-resolution transmission of task-relevant image patches. This approach overcomes limitations of self-attention-only methods in complex scenes and enables flexible, efficient communication under bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [video-action model, flow matching, inverse dynamics model, imitation learning, vision-language-action model]</li>
<li class=""><strong>authors:</strong> Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</li>
<li class=""><strong>institution:</strong> mimic robotics, Microsoft Zurich, ETH Zurich, ETH AI Center, UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15692" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15692</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces mimic-video, a Video-Action Model that pairs a pretrained internet-scale video model with a flow matching-based action decoder to generate robot actions from video latent representations. This approach leverages video pretraining to capture both semantics and visual dynamics, isolating the control problem and achieving state-of-the-art performance with 10x greater sample efficiency compared to traditional Vision-Language-Action models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] FrontierCS: Evolving Challenges for Evolving Intelligence</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [benchmarking], [benchmark, open-ended problems, competitive programming, NP-hard, automatic evaluation, expert reference solution]</li>
<li class=""><strong>authors:</strong> Qiuyang Mang, Wenhao Chai, Zhifei Li, Huanzhi Mao, Shang Zhou, Alexander Du, Hanchen Li, Shu Liu, Edwin Chen, Yichuan Wang, Xieting Chu, Zerui Cheng, Yuan Xu, Tian Xia, Zirui Wang, Tianneng Shi, Jianzhu Yao, Yilong Zhao, Qizheng Zhang, Charlie Ruan, Zeyu Shen, Kaiyuan Liu, Runyuan He, Dong Xing, Zerui Li, Zirong Zeng, Yige Jiang, Lufeng Cheng, Ziyi Zhao, Youran Sun, Wesley Zheng, Meiyuwang Zhang, Ruyi Ji, Xuechang Tu, Zihan Zheng, Zexing Chen, Kangyang Zhou, Zhaozi Wang, Jingbang Chen, Aleksandra Korolova, Peter Henderson, Pramod Viswanath, Vijay Ganesh, Saining Xie, Zhuang Liu, Dawn Song, Sewon Min, Ion Stoica, Joseph E. Gonzalez, Jingbo Shang, Alvin Cheung</li>
<li class=""><strong>institution:</strong> UC Berkeley, Princeton University, UCSD, X-camp Academy, Georgia Tech, Stanford University, University of Washington, Nanyang Technological University, University of Toronto, UIUC, University of Michigan, New York University, MIT</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15699" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15699</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces FrontierCS, a benchmark of 156 open-ended computer science problems where the optimal solution is unknown but can be objectively evaluated, requiring models to generate executable programs. It finds that current frontier reasoning models significantly lag behind human experts, and that merely increasing reasoning budgets or generating workable code does not close this performance gap.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Dynamic Rebatching for Efficient Early-Exit Inference with DREX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [early-exit, dynamic rebatching, copy-free buffer, SLA-aware scheduler, KV cache, state-copying]</li>
<li class=""><strong>authors:</strong> Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational biology/oncology], [physics-informed neural network (PINN), time-varying parameters, sparse data, combination therapy, mathematical modeling]</li>
<li class=""><strong>authors:</strong> Kayode Olumoyin, Lamees El Naqa, Katarzyna Rejniak</li>
<li class=""><strong>institution:</strong> H. Lee Moffitt Cancer Center and Research Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15706" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15706</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using a physics-informed neural network (PINN) to learn time-varying interactions between cell populations, such as bladder cancer tumors and immune cells, from sparse biological data. The method predicts subpopulation trajectories in response to combination anticancer therapies where direct observations are unavailable. The authors demonstrate that their approach yields results consistent with biological explanations, providing a framework for modeling evolving dynamics under external interventions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [predictive concept decoder, communication bottleneck, sparse concept list, encoder-decoder, auto-interp score, fine-tuning]</li>
<li class=""><strong>authors:</strong> Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt</li>
<li class=""><strong>institution:</strong> Transluce</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15712" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15712</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Predictive Concept Decoders (PCDs), an end-to-end trained architecture where an encoder compresses a model&#x27;s internal activations into a sparse list of concepts, and a decoder uses this list to answer questions about the model&#x27;s behavior. The method is pretrained on large datasets and then finetuned, showing that the interpretability and downstream performance of the bottleneck concepts improve with more data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [computational pathology], [self-supervised learning, vision transformer, cross-scale distillation, magnification-invariant representation, whole-slide image analysis]</li>
<li class=""><strong>authors:</strong> Mahmut S. Gokmen, Mitchell A. Klusty, Peter T. Nelson, Allison M. Neltner, Sen-Ching Samson Cheung, Thomas M. Pearce, David A Gutman, Brittany N. Dugger, Devavrat S. Bisht, Margaret E. Flanagan, V. K. Cody Bumgardner</li>
<li class=""><strong>institution:</strong> University of Kentucky, University of Pittsburgh, Emory University, University of California Davis, University of Texas Health</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14796" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14796</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Magnification-Aware Distillation (MAD), a self-supervised framework that learns unified image representations by linking low-magnification context with spatially aligned high-magnification detail in whole-slide images. The resulting foundation model, MAD-NP, demonstrates strong resolution-invariant learning, as shown by a classifier trained on 10x embeddings maintaining 96.7% performance on unseen 40x tiles. The work concludes that this approach enables scalable, magnification-robust analysis using a unified embedding space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Deep learning water-unsuppressed MRSI at ultra-high field for simultaneous quantitative metabolic, susceptibility and myelin water imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep learning, water-unsuppressed MRSI, WALINET+, ECCENTRIC sampling, quantitative susceptibility mapping, myelin water fraction imaging]</li>
<li class=""><strong>authors:</strong> Paul J. Weiser, Jiye Kim, Jongho Lee, Amirmohammad Shamaei, Gulnur Ungan, Malte Hoffmann, Antoine Klauser, Berkin Bilgic, Ovidiu C. Andronesi</li>
<li class=""><strong>institution:</strong> Massachusetts General Hospital, Seoul National University, University of Calgary, Medical University of Vienna, Siemens Healthineers International AG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14929" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14929</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a deep learning pipeline for water-unsuppressed MRSI at 7T, using a network called WALINET+ to remove water and lipid signals and sidebands, enabling simultaneous reconstruction of metabolic, susceptibility, and myelin water maps. The method achieves high-resolution quantitative imaging in a short scan time and shows good agreement with conventional techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Efficient Nudged Elastic Band Method using Neural Network Bayesian Algorithm Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Neural Network Bayesian Algorithm Execution (NN-BAX), Nudged Elastic Band (NEB), active learning, foundation models, Bayesian Experimental Design, minimum energy pathway (MEP)]</li>
<li class=""><strong>authors:</strong> Pranav Kakhandiki, Sathya Chitturi, Daniel Ratner, Sean Gasiorowski</li>
<li class=""><strong>institution:</strong> Stanford University, SLAC National Accelerator Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14993" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14993</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Neural Network Bayesian Algorithm Execution (NN-BAX), a framework that actively learns both the energy landscape and the minimum energy pathway (MEP) by fine-tuning a foundation model with targeted sampling. It achieves a one to two order of magnitude reduction in computationally expensive energy and force evaluations compared to the standard NEB method, with minimal accuracy loss, enabling faster MEP discovery for complex systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Adaptive Weighted Genetic Algorithm-Optimized SVR for Robust Long-Term Forecasting of Global Stock Indices for investment decisions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [financial forecasting], [genetic algorithm, support vector regression, long short-term memory, mean absolute percentage error, hyperparameter optimization]</li>
<li class=""><strong>authors:</strong> Mohit Beniwal</li>
<li class=""><strong>institution:</strong> Delhi Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15113</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an improved genetic algorithm-optimized support vector regression (IGA-SVR) model for long-term forecasting of global stock indices. It demonstrates superior accuracy and computational efficiency compared to LSTM and another GA-optimized SVR baseline, achieving significant reductions in MAPE.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Online Partitioned Local Depth for semi-supervised applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [online algorithms], [partitioned local depth, cohesion network, semi-supervised learning, anomaly detection, online algorithm]</li>
<li class=""><strong>authors:</strong> John D. Foley, Justin T. Lee</li>
<li class=""><strong>institution:</strong> Metron, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15436" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15436</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces online PaLD, an extension of the partitioned local depth algorithm designed for online and semi-supervised applications. It allows for pre-computing a cohesion network from a reference dataset with an O(n^3) upfront cost, enabling new data points to be integrated in O(n^2) time. The method is demonstrated for online anomaly detection and semi-supervised classification in healthcare datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, deep learning surrogate model, digital twin, autonomous control, pressure control]</li>
<li class=""><strong>authors:</strong> Guillermo Rodriguez-Llorente, Galo Gallardo, Rodrigo Morant Navascués, Nikita Khvatkin Petrovsky, Anderson Sabogal, Roberto Gómez-Espinosa Martín</li>
<li class=""><strong>institution:</strong> HI Iberia, Universidad Carlos III de Madrid, IFMIF-DONES Spain</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15521</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a fully data-driven approach for autonomous pressure control in a particle accelerator prototype. It uses a deep learning surrogate model as a digital twin to emulate system dynamics and trains a deep reinforcement learning agent within this simulation. The agent successfully learns a control policy to maintain pressure within strict operational limits, advancing intelligent autonomous control for next-generation accelerator facilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] ColliderML: The First Release of an OpenDataDetector High-Luminosity Physics Benchmark Dataset</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [OpenDataDetector, next-to-leading order matrix element, full simulation, digitisation, detector-level data, proton-proton collisions, High-Luminosity LHC, Hugging Face]</li>
<li class=""><strong>authors:</strong> Doğa Elitez, Paul Gessinger, Daniel Murnane, Marcus Selchou Raaholt, Andreas Salzburger, Stine Kofoed Skov, Andreas Stefl, Anna Zaborowska</li>
<li class=""><strong>institution:</strong> CERN, Johannes Gutenberg-Universität Mainz, Niels Bohr Institute, Lawrence Berkeley National Laboratory, University of Copenhagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15230" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15230</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ColliderML, a large-scale dataset of fully simulated and digitized proton-proton collision events for the High-Luminosity LHC. It is generated using modern next-to-leading order matrix element calculations, realistic pile-up overlay, and a validated detector geometry. The dataset aims to fill a gap for machine learning research on detector-level data, providing a benchmark for collider physics applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Photonics-Enhanced Graph Convolutional Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [photonic positional embeddings, graph convolutional networks, synthetic frequency lattices, intensity correlation matrices, hybrid photonic-electronic workflow]</li>
<li class=""><strong>authors:</strong> Yuan Wang, Oleksandr Kyriienko</li>
<li class=""><strong>institution:</strong> University of Sheffield</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15549" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15549</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid workflow that enhances Graph Convolutional Networks (GCNs) by using photonic positional embeddings derived from simulating light propagation on synthetic frequency lattices that match the input graph structure. The method generates internode intensity correlation matrices to provide global structural information to the GCN. The results show that these photonic embeddings outperform baseline Laplacian-based embeddings on molecular graph benchmarks, improving regression and classification performance, and support the potential for optical acceleration in graph machine learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] A Teacher-Student Perspective on the Dynamics of Learning Near the Optimal Point</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [neural network theory], [teacher-student model, gradient descent, Hessian eigenspectrum, Marchenko-Pastur distribution, polynomial activation]</li>
<li class=""><strong>authors:</strong> Carlos Couto, José Mourão, Mário A. T. Figueiredo, Pedro Ribeiro</li>
<li class=""><strong>institution:</strong> Instituto Superior Técnico, University of Lisbon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15606</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the learning dynamics of neural networks near an optimal point by characterizing the Hessian matrix&#x27;s eigenspectrum in teacher-student problems. It shows that for linear networks, the spectrum follows a convolution of scaled chi-square and Marchenko-Pastur distributions, and that smaller eigenvalues dictate long-term performance. Furthermore, it demonstrates that the Hessian rank acts as an effective parameter count for polynomial networks, while it remains full rank for networks with generic nonlinear activations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Learning continuous SOC-dependent thermal decomposition kinetics for Li-ion cathodes using KA-CRNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN), physics-encoded neural network, differential scanning calorimetry (DSC), kinetic parameter inference, thermal runaway prediction]</li>
<li class=""><strong>authors:</strong> Benjamin C. Koenig, Sili Deng</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15628</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies a physics-encoded Kolmogorov-Arnold Chemical Reaction Neural Network (KA-CRNN) to learn continuous state-of-charge (SOC)-dependent kinetic parameters for lithium-ion cathode decomposition directly from differential scanning calorimetry data. The method embeds a reaction pathway into the network, allowing parameters like activation energies to be represented as interpretable functions of SOC. The framework successfully models heat-release for several cathode materials, providing interpretable insights into decomposition mechanisms and establishing a foundation for more accurate thermal runaway prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Prospects for quantum advantage in machine learning from the representability of functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum machine learning], [parametrized quantum circuits, classical simulation, dequantization, function representability, tensor networks, stabilizer methods]</li>
<li class=""><strong>authors:</strong> Sergi Masot-Llima, Elies Gil-Fuster, Carlos Bravo-Prieto, Jens Eisert, and Tommaso Guaita</li>
<li class=""><strong>institution:</strong> Universitat de Barcelona, Barcelona Supercomputing Center, Freie Universität Berlin, Fraunhofer Heinrich Hertz Institute, Helmholtz-Zentrum Berlin für Materialien und Energie</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15661" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15661</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a framework connecting the structure of parametrized quantum circuits to the mathematical functions they can learn, analyzing how properties like circuit depth and non-Clifford gate count determine classical simulability. It concludes that this analysis reveals pathways to dequantization and distinguishes between fully simulatable, classically tractable, and robustly quantum models, providing a map to identify where genuine quantum advantage in machine learning may exist.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [high-dimensional statistics], [Partial Least Squares, Singular Value Decomposition, random matrix theory, spiked random matrix models, low-rank approximations]</li>
<li class=""><strong>authors:</strong> Victor Léger, Florent Chatelain</li>
<li class=""><strong>institution:</strong> Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15684" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15684</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a theoretical analysis of the Partial Least Squares method based on Singular Value Decomposition (PLS-SVD) for high-dimensional data integration. Using tools from random matrix theory, it characterizes the alignment between estimated and true latent directions, explaining the method&#x27;s performance and limitations. The analysis shows that PLS-SVD is asymptotically superior to separate principal component analysis for detecting a common latent subspace.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-18T07:21:37.000Z" itemprop="dateModified">Dec 18, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai_toutiao/category/cslg"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">cs.LG</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai_toutiao/category/csma"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cs.MA</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li></ul></div></div></div></div></div><aside class="content-right-ads"><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_2.png" alt="草莓师姐"></a><a class="content-ad" href="https://xhslink.com/m/2lTbaZQ1RbP" target="_blank" rel="noopener noreferrer"><img src="/ai_toutiao/img/ads_3.png" alt="草莓师姐"></a></aside></div></div></main></div></div></div></div>
</body>
</html>